{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md', './data_base/knowledge_db/prompt_engineering/9. 总结 Summary.md', './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md', './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md', './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md', './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md', './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md', './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md', './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md', './data_base/knowledge_db/easy_rl/强化学习入门指南.json', './data_base/knowledge_db/easy_rl/强化学习入门指南.mp4', './data_base/knowledge_db/easy_rl/强化学习入门指南.srt', './data_base/knowledge_db/easy_rl/强化学习入门指南.tsv', './data_base/knowledge_db/easy_rl/强化学习入门指南.txt', './data_base/knowledge_db/easy_rl/强化学习入门指南.vtt', './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "file_paths = []\n",
    "folder_path = \"./data_base/knowledge_db\"\n",
    "for root,dirs,files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root,file))\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_type = file_path.split(\".\")[-1]\n",
    "    if file_type == \"pdf\":\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    elif file_type == \"md\":\n",
    "        loaders.append(UnstructuredMarkdownLoader(file_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='第五章 推断\\n\\n在这一章中，我们将通过一个故事，引领你了解如何从产品评价和新闻文章中推导出情感和主题。\\n\\n让我们先想象一下，你是一名初创公司的数据分析师，你的任务是从各种产品评论和新闻文章中提取出关键的情感和主题。这些任务包括了标签提取、实体提取、以及理解文本的情感等等。在传统的机器学习流程中，你需要收集标签化的数据集、训练模型、确定如何在云端部署模型并进行推断。尽管这种方式可能会产生不错的效果，但完成这一全流程需要耗费大量的时间和精力。而且，每一个任务，比如情感分析、实体提取等等，都需要训练和部署单独的模型。\\n\\n然而，就在你准备投入繁重工作的时候，你发现了大型语言模型（LLM）。LLM 的一个明显优点是，对于许多这样的任务，你只需要编写一个 Prompt，就可以开始生成结果，大大减轻了你的工作负担。这个发现像是找到了一把神奇的钥匙，让应用程序开发的速度加快了许多。最令你兴奋的是，你可以仅仅使用一个模型和一个 API 来执行许多不同的任务，无需再纠结如何训练和部署许多不同的模型。\\n\\n让我们开始这一章的学习，一起探索如何利用 LLM 加快我们的工作进程，提高我们的工作效率。\\n\\n一、情感推断\\n\\n1.1 情感倾向分析\\n\\n让我们以一则电商平台上的台灯评论为例，通过此例，我们将学习如何对评论进行情感二分类（正面/负面）。\\n\\npython lamp_review = \"\"\" 我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\\\\ 我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\\\\ 几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\\\\ 在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！ \"\"\"\\n\\n接下来，我们将尝试编写一个 Prompt ，用以分类这条商品评论的情感。如果我们想让系统解析这条评论的情感倾向，只需编写“以下商品评论的情感倾向是什么？”这样的 Prompt ，再加上一些标准的分隔符和评论文本等。\\n\\n然后，我们将这个程序运行一遍。结果表明，这条商品评论的情感倾向是正面的，这似乎非常准确。尽管这款台灯并非完美无缺，但是这位顾客对它似乎相当满意。这个公司看起来非常重视客户体验和产品质量，因此，认定评论的情感倾向为正面似乎是正确的判断。\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 以下用三个反引号分隔的产品评论的情感是什么？\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n情感是积极的。\\n\\n如果你想要给出更简洁的答案，以便更容易进行后期处理，可以在上述 Prompt 基础上添加另一个指令：用一个单词回答：「正面」或「负面」。这样就只会打印出 “正面” 这个单词，这使得输出更加统一，方便后续处理。\\n\\n```python prompt = f\"\"\" 以下用三个反引号分隔的产品评论的情感是什么？\\n\\n用一个单词回答：「正面」或「负面」。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n正面\\n\\n1.2 识别情感类型\\n\\n接下来，我们将继续使用之前的台灯评论，但这次我们会试用一个新的 Prompt 。我们希望模型能够识别出评论作者所表达的情感，并且将这些情感整理为一个不超过五项的列表。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 识别以下评论的作者表达的情感。包含不超过五个项目。将答案格式化为以逗号分隔的单词列表。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n满意,感激,赞赏,信任,满足\\n\\n大型语言模型非常擅长从一段文本中提取特定的东西。在上面的例子中，评论所表达的情感有助于了解客户如何看待特定的产品。\\n\\n1.3 识别愤怒\\n\\n对于许多企业来说，洞察到顾客的愤怒情绪是至关重要的。这就引出了一个分类问题：下述的评论作者是否流露出了愤怒？因为如果有人真的情绪激动，那可能就意味着需要给予额外的关注，因为每一个愤怒的顾客都是一个改进服务的机会，也是一个提升公司口碑的机会。这时，客户支持或者客服团队就应该介入，与客户接触，了解具体情况，然后解决他们的问题。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 以下评论的作者是否表达了愤怒？评论用三个反引号分隔。给出是或否的答案。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n上面这个例子中，客户并没有生气。注意，如果使用常规的监督学习，如果想要建立所有这些分类器，不可能在几分钟内就做到这一点。我们鼓励大家尝试更改一些这样的 Prompt ，也许询问客户是否表达了喜悦，或者询问是否有任何遗漏的部分，并看看是否可以让 Prompt 对这个灯具评论做出不同的推论。\\n\\n二、信息提取\\n\\n2.1 商品信息提取\\n\\n信息提取是自然语言处理（NLP）的重要组成部分，它帮助我们从文本中抽取特定的、我们关心的信息。我们将深入挖掘客户评论中的丰富信息。在接下来的示例中，我们将要求模型识别两个关键元素：购买的商品和商品的制造商。\\n\\n想象一下，如果你正在尝试分析一个在线电商网站上的众多评论，了解评论中提到的商品是什么、由谁制造，以及相关的积极或消极情绪，将极大地帮助你追踪特定商品或制造商在用户心中的情感趋势。\\n\\n在接下来的示例中，我们会要求模型将回应以一个 JSON 对象的形式呈现，其中的 key 就是商品和品牌。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 从评论文本中识别以下项目： - 评论者购买的物品 - 制造该物品的公司\\n\\n评论文本用三个反引号分隔。将你的响应格式化为以 “物品” 和 “品牌” 为键的 JSON 对象。 如果信息不存在，请使用 “未知” 作为值。 让你的回应尽可能简短。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"物品\": \"卧室灯\",\\n  \"品牌\": \"Lumina\"\\n}\\n\\n如上所示，它会说这个物品是一个卧室灯，品牌是 Luminar，你可以轻松地将其加载到 Python 字典中，然后对此输出进行其他处理。\\n\\n2.2 综合情感推断和信息提取\\n\\n在上面小节中，我们采用了三至四个 Prompt 来提取评论中的“情绪倾向”、“是否生气”、“物品类型”和“品牌”等信息。然而，事实上，我们可以设计一个单一的 Prompt ，来同时提取所有这些信息。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 从评论文本中识别以下项目： - 情绪（正面或负面） - 审稿人是否表达了愤怒？（是或否） - 评论者购买的物品 - 制造该物品的公司\\n\\n评论用三个反引号分隔。将你的响应格式化为 JSON 对象，以 “情感倾向”、“是否生气”、“物品类型” 和 “品牌” 作为键。 如果信息不存在，请使用 “未知” 作为值。 让你的回应尽可能简短。 将 “是否生气” 值格式化为布尔值。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"情感倾向\": \"正面\",\\n  \"是否生气\": false,\\n  \"物品类型\": \"卧室灯\",\\n  \"品牌\": \"Lumina\"\\n}\\n\\n这个例子中，我们指导 LLM 将“是否生气”的情况格式化为布尔值，并输出 JSON 格式。你可以尝试对格式化模式进行各种变化，或者使用完全不同的评论来试验，看看 LLM 是否仍然可以准确地提取这些内容。\\n\\n三、主题推断\\n\\n大型语言模型的另一个很酷的应用是推断主题。假设我们有一段长文本，我们如何判断这段文本的主旨是什么？它涉及了哪些主题？让我们通过以下一段虚构的报纸报道来具体了解一下。\\n\\n```python\\n\\n中文\\n\\nstory = \"\"\" 在政府最近进行的一项调查中，要求公共部门的员工对他们所在部门的满意度进行评分。 调查结果显示，NASA 是最受欢迎的部门，满意度为 95％。\\n\\n一位 NASA 员工 John Smith 对这一发现发表了评论，他表示： “我对 NASA 排名第一并不感到惊讶。这是一个与了不起的人们和令人难以置信的机会共事的好地方。我为成为这样一个创新组织的一员感到自豪。”\\n\\nNASA 的管理团队也对这一结果表示欢迎，主管 Tom Johnson 表示： “我们很高兴听到我们的员工对 NASA 的工作感到满意。 我们拥有一支才华横溢、忠诚敬业的团队，他们为实现我们的目标不懈努力，看到他们的辛勤工作得到回报是太棒了。”\\n\\n调查还显示，社会保障管理局的满意度最低，只有 45％的员工表示他们对工作满意。 政府承诺解决调查中员工提出的问题，并努力提高所有部门的工作满意度。 \"\"\" ```\\n\\n3.1 推断讨论主题\\n\\n以上是一篇关于政府员工对其工作单位感受的虚构报纸文章。我们可以要求大语言模型确定其中讨论的五个主题，并用一两个词语概括每个主题。输出结果将会以逗号分隔的Python列表形式呈现。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 确定以下给定文本中讨论的五个主题。\\n\\n每个主题用1-2个词概括。\\n\\n请输出一个可解析的Python列表，每个元素是一个字符串，展示了一个主题。\\n\\n给定文本: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n[\\'NASA\\', \\'满意度\\', \\'评论\\', \\'管理团队\\', \\'社会保障管理局\\']\\n\\n3.2 为特定主题制作新闻提醒\\n\\n假设我们有一个新闻网站或类似的平台，这是我们感兴趣的主题：美国航空航天局、当地政府、工程、员工满意度、联邦政府等。我们想要分析一篇新闻文章，理解其包含了哪些主题。可以使用这样的 Prompt：确定以下主题列表中的每个项目是否是以下文本中的主题。以 0 或 1 的形式给出答案列表。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 判断主题列表中的每一项是否是给定文本中的一个话题，\\n\\n以列表的形式给出答案，每个元素是一个Json对象，键为对应主题，值为对应的 0 或 1。\\n\\n主题列表：美国航空航天局、当地政府、工程、员工满意度、联邦政府\\n\\n给定文本: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n[\\n  {\"美国航空航天局\": 1},\\n  {\"当地政府\": 1},\\n  {\"工程\": 0},\\n  {\"员工满意度\": 1},\\n  {\"联邦政府\": 1}\\n]\\n\\n从输出结果来看，这个 story 与关于“美国航空航天局”、“员工满意度”、“联邦政府”、“当地政府”有关，而与“工程”无关。这种能力在机器学习领域被称为零样本（Zero-Shot）学习。这是因为我们并没有提供任何带标签的训练数据，仅凭 Prompt ，它便能判定哪些主题在新闻文章中被包含。\\n\\n如果我们希望制定一个新闻提醒，我们同样可以运用这种处理新闻的流程。假设我对“美国航空航天局”的工作深感兴趣，那么你就可以构建一个如此的系统：每当出现与\\'美国宇航局\\'相关的新闻，系统就会输出提醒。\\n\\npython result_lst = eval(response) topic_dict = {list(i.keys())[0] : list(i.values())[0] for i in result_lst} print(topic_dict) if topic_dict[\\'美国航空航天局\\'] == 1: print(\"提醒: 关于美国航空航天局的新消息\")\\n\\n{\\'美国航空航天局\\': 1, \\'当地政府\\': 1, \\'工程\\': 0, \\'员工满意度\\': 1, \\'联邦政府\\': 1}\\n提醒: 关于美国航空航天局的新消息\\n\\n这就是我们关于推断的全面介绍。在短短几分钟内，我们已经能够建立多个用于文本推理的系统，这是以前需要机器学习专家数天甚至数周时间才能完成的任务。这一变化无疑是令人兴奋的，因为无论你是经验丰富的机器学习开发者，还是刚入门的新手，都能利用输入 Prompt 快速开始复杂的自然语言处理任务。\\n\\n英文版\\n\\n1.1 情感倾向分析\\n\\npython lamp_review = \"\"\" Needed a nice lamp for my bedroom, and this one had \\\\ additional storage and not too high of a price point. \\\\ Got it fast. The string to our lamp broke during the \\\\ transit and the company happily sent over a new one. \\\\ Came within a few days as well. It was easy to put \\\\ together. I had a missing part, so I contacted their \\\\ support and they very quickly got me the missing piece! \\\\ Lumina seems to me to be a great company that cares \\\\ about their customers and products!! \"\"\"\\n\\n```python prompt = f\"\"\" What is the sentiment of the following product review, which is delimited with triple backticks?\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nThe sentiment of the product review is positive.\\n\\n```python prompt = f\"\"\" What is the sentiment of the following product review, which is delimited with triple backticks?\\n\\nGive your answer as a single word, either \"positive\" \\\\ or \"negative\".\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\npositive\\n\\n1.2识别情感类型\\n\\n```python prompt = f\"\"\" Identify a list of emotions that the writer of the \\\\ following review is expressing. Include no more than \\\\ five items in the list. Format your answer as a list of \\\\ lower-case words separated by commas.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nsatisfied, pleased, grateful, impressed, happy\\n\\n1.3 识别愤怒\\n\\n```python prompt = f\"\"\" Is the writer of the following review expressing anger?\\\\ The review is delimited with triple backticks. \\\\ Give your answer as either yes or no.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nNo\\n\\n2.1 商品信息提取\\n\\n```python prompt = f\"\"\" Identify the following items from the review text: - Item purchased by reviewer - Company that made the item\\n\\nThe review is delimited with triple backticks. \\\\ Format your response as a JSON object with \\\\ \"Item\" and \"Brand\" as the keys. If the information isn\\'t present, use \"unknown\" \\\\ as the value. Make your response as short as possible.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"Item\": \"lamp\",\\n  \"Brand\": \"Lumina\"\\n}\\n\\n2.2 综合情感推断和信息提取\\n\\n```python prompt = f\"\"\" Identify the following items from the review text: - Sentiment (positive or negative) - Is the reviewer expressing anger? (true or false) - Item purchased by reviewer - Company that made the item\\n\\nThe review is delimited with triple backticks. \\\\ Format your response as a JSON object with \\\\ \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys. If the information isn\\'t present, use \"unknown\" \\\\ as the value. Make your response as short as possible. Format the Anger value as a boolean.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"Sentiment\": \"positive\",\\n  \"Anger\": false,\\n  \"Item\": \"lamp\",\\n  \"Brand\": \"Lumina\"\\n}\\n\\n3.1 推断讨论主题\\n\\n```python story = \"\"\" In a recent survey conducted by the government, public sector employees were asked to rate their level of satisfaction with the department they work at. The results revealed that NASA was the most popular department with a satisfaction rating of 95%.\\n\\nOne NASA employee, John Smith, commented on the findings, stating, \"I\\'m not surprised that NASA came out on top. It\\'s a great place to work with amazing people and incredible opportunities. I\\'m proud to be a part of such an innovative organization.\"\\n\\nThe results were also welcomed by NASA\\'s management team, with Director Tom Johnson stating, \"We are thrilled to hear that our employees are satisfied with their work at NASA. We have a talented and dedicated team who work tirelessly to achieve our goals, and it\\'s fantastic to see that their hard work is paying off.\"\\n\\nThe survey also revealed that the Social Security Administration had the lowest satisfaction rating, with only 45% of employees indicating they were satisfied with their job. The government has pledged to address the concerns raised by employees in the survey and work towards improving job satisfaction across all departments. \"\"\" ```\\n\\n```python prompt = f\"\"\" Determine five topics that are being discussed in the \\\\ following text, which is delimited by triple backticks.\\n\\nMake each item one or two words long.\\n\\nFormat your response as a list of items separated by commas. Give me a list which can be read in Python.\\n\\nText sample: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nsurvey, satisfaction rating, NASA, Social Security Administration, job satisfaction\\n\\npython response.split(sep=\\',\\')\\n\\n[\\'survey\\',\\n \\' satisfaction rating\\',\\n \\' NASA\\',\\n \\' Social Security Administration\\',\\n \\' job satisfaction\\']\\n\\n3.2 为特定主题制作新闻提醒\\n\\npython topic_list = [ \"nasa\", \"local government\", \"engineering\", \"employee satisfaction\", \"federal government\" ]\\n\\n```python prompt = f\"\"\" Determine whether each item in the following list of \\\\ topics is a topic in the text below, which is delimited with triple backticks.\\n\\nGive your answer as list with 0 or 1 for each topic.\\\\\\n\\nList of topics: {\", \".join(topic_list)}\\n\\nText sample: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n[1, 0, 0, 1, 1]\\n\\npython topic_dict = {topic_list[i] : eval(response)[i] for i in range(len(eval(response)))} print(topic_dict) if topic_dict[\\'nasa\\'] == 1: print(\"ALERT: New NASA story!\")\\n\\n{\\'nasa\\': 1, \\'local government\\': 0, \\'engineering\\': 0, \\'employee satisfaction\\': 1, \\'federal government\\': 1}\\nALERT: New NASA story!'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/9. 总结 Summary.md'}, page_content='第九章 总结\\n\\n恭喜您完成了本书第一单元内容的学习！\\n\\n总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：\\n\\n编写清晰具体的指令；\\n\\n如果适当的话，给模型一些思考时间。\\n\\n您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。\\n\\n我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第一部分中，您的收获应该颇丰，希望通过第一部分学习能为您带来愉悦的体验。\\n\\n我们期待您能灵感迸发，尝试创建自己的应用。请大胆尝试，并分享给我们您的想法。您可以从一个微型项目开始，或许它具备一定的实用性，或者仅仅是一项有趣的创新。请利用您在第一个项目中得到的经验，去创造更优秀的下一项目，以此类推。如果您已经有一个宏大的项目设想，那么，请毫不犹豫地去实现它。\\n\\n最后，希望您在完成第一部分的过程中感到满足，感谢您的参与。我们热切期待着您的惊艳作品。接下来，我们将进入第二部分的学习！'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='第七章 文本扩展\\n\\n文本扩展是大语言模型的一个重要应用方向，它可以输入简短文本，生成更加丰富的长文。这为创作提供了强大支持，但也可能被滥用。因此开发者在使用时，必须谨记社会责任，避免生成有害内容。\\n\\n在本章中,我们将学习基于 OpenAI API 实现一个客户邮件自动生成的示例，用于根据客户反馈优化客服邮件。这里还会介绍“温度”（temperature）这一超参数，它可以控制文本生成的多样性。\\n\\n需要注意，扩展功能只应用来辅助人类创作，而非大规模自动生成内容。开发者应审慎使用，避免产生负面影响。只有以负责任和有益的方式应用语言模型，才能发挥其最大价值。相信践行社会责任的开发者可以利用语言模型的扩展功能，开发出真正造福人类的创新应用。\\n\\n一、定制客户邮件\\n\\n在这个客户邮件自动生成的示例中，我们将根据客户的评价和其中的情感倾向，使用大语言模型针对性地生成回复邮件。\\n\\n具体来说，我们先输入客户的评论文本和对应的情感分析结果(正面或者负面)。然后构造一个 Prompt，要求大语言模型基于这些信息来生成一封定制的回复电子邮件。\\n\\n下面先给出一个实例，包括一条客户评价和这个评价表达的情感。这为后续的语言模型生成回复邮件提供了关键输入信息。通过输入客户反馈的具体内容和情感态度，语言模型可以生成针对这个特定客户、考虑其具体情感因素的个性化回复。这种针对个体客户特点的邮件生成方式，将大大提升客户满意度。\\n\\n```python\\n\\n我们可以在推理那章学习到如何对一个评论判断其情感倾向\\n\\nsentiment = \"消极的\"\\n\\n一个产品的评价\\n\\nreview = f\"\"\" 他们在11月份的季节性销售期间以约49美元的价格出售17件套装，折扣约为一半。\\\\ 但由于某些原因（可能是价格欺诈），到了12月第二周，同样的套装价格全都涨到了70美元到89美元不等。\\\\ 11件套装的价格也上涨了大约10美元左右。\\\\ 虽然外观看起来还可以，但基座上锁定刀片的部分看起来不如几年前的早期版本那么好。\\\\ 不过我打算非常温柔地使用它，例如，\\\\ 我会先在搅拌机中将像豆子、冰、米饭等硬物研磨，然后再制成所需的份量，\\\\ 切换到打蛋器制作更细的面粉，或者在制作冰沙时先使用交叉切割刀片，然后使用平面刀片制作更细/不粘的效果。\\\\ 制作冰沙时，特别提示：\\\\ 将水果和蔬菜切碎并冷冻（如果使用菠菜，则轻轻煮软菠菜，然后冷冻直到使用；\\\\ 如果制作果酱，则使用小到中号的食品处理器），这样可以避免在制作冰沙时添加太多冰块。\\\\ 大约一年后，电机发出奇怪的噪音，我打电话给客服，但保修已经过期了，所以我不得不再买一个。\\\\ 总的来说，这些产品的总体质量已经下降，因此它们依靠品牌认可和消费者忠诚度来维持销售。\\\\ 货物在两天内到达。 \"\"\" ```\\n\\n在这个例子中，我们已经利用前面章节学到的方法，从客户评价中提取出其表达的情感倾向。这里是一条关于搅拌机的评论。现在我们要基于这条评论中的情感倾向，使用大语言模型自动生成一封回复邮件。\\n\\n以下述 Prompt 为例：首先明确大语言模型的身份是客户服务 AI 助手；它任务是为客户发送电子邮件回复；然后在三个反引号间给出具体的客户评论；最后要求语言模型根据这条反馈邮件生成一封回复，以感谢客户的评价。\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 你是一位客户服务的AI助手。 你的任务是给一位重要客户发送邮件回复。 根据客户通过“”分隔的评价，生成回复以感谢客户的评价。提醒模型使用评价中的具体细节 用简明而专业的语气写信。 作为“AI客户代理”签署电子邮件。 客户评论：{review}评论情感：{sentiment} \"\"\" response = get_completion(prompt) print(response)\\n\\n尊敬的客户，\\n\\n非常感谢您对我们产品的评价。我们非常抱歉您在购买过程中遇到了价格上涨的问题。我们一直致力于为客户提供最优惠的价格，但由于市场波动，价格可能会有所变化。我们深表歉意，如果您需要任何帮助，请随时联系我们的客户服务团队。\\n\\n我们非常感谢您对我们产品的详细评价和使用技巧。我们将会把您的反馈传达给我们的产品团队，以便改进我们的产品质量和性能。\\n\\n再次感谢您对我们的支持和反馈。如果您需要任何帮助或有任何疑问，请随时联系我们的客户服务团队。\\n\\n祝您一切顺利！\\n\\nAI客户代理\\n\\n通过这个Prompt,我们将具体的客户评论内容和需要表达的客服助手语气与要生成的回复邮件链接起来。语言模型可以在充分理解客户反馈的基础上，自动撰写恰当的回复。\\n\\n这种依据具体客户评价个性化回复的方法，将大大提升客户体验和满意度。\\n\\n二、引入温度系数\\n\\n大语言模型中的 “温度”(temperature) 参数可以控制生成文本的随机性和多样性。temperature 的值越大，语言模型输出的多样性越大；temperature 的值越小，输出越倾向高概率的文本。\\n\\n举个例子，在某一上下文中，语言模型可能认为“比萨”是接下来最可能的词，其次是“寿司”和“塔可”。若 temperature 为0，则每次都会生成“比萨”；而当 temperature 越接近 1 时，生成结果是“寿司”或“塔可”的可能性越大，使文本更加多样。\\n\\n图 1.7 温度系数\\n\\n一般来说，如果需要可预测、可靠的输出，则将 temperature 设置为0，在所有课程中，我们一直设置温度为零；如果需要更具创造性的多样文本，那么适当提高 temperature 则很有帮助。调整这个参数可以灵活地控制语言模型的输出特性。\\n\\n在下面例子中，针对同一段来信，我们提醒语言模型使用用户来信中的详细信息，并设置一个较高的 temperature ，运行两次，比较他们的结果有何差异。\\n\\n```python\\n\\n第一次运行\\n\\nprompt = f\"\"\" 你是一名客户服务的AI助手。 你的任务是给一位重要的客户发送邮件回复。 根据通过“”分隔的客户电子邮件生成回复，以感谢客户的评价。 如果情感是积极的或中性的，感谢他们的评价。 如果情感是消极的，道歉并建议他们联系客户服务。 请确保使用评论中的具体细节。 以简明和专业的语气写信。 以“AI客户代理”的名义签署电子邮件。 客户评价：{review}评论情感：{sentiment} \"\"\" response = get_completion(prompt, temperature=0.7) print(response)\\n\\n尊敬的客户，\\n\\n感谢您对我们产品的评价。我们非常重视您的意见，并对您在使用过程中遇到的问题表示诚挚的道歉。\\n\\n我们对价格的变动深感抱歉。根据您的描述，我们了解到在12月第二周，套装的价格出现了不同程度的上涨。我们会进一步调查此事，并确保我们的定价策略更加透明和一致。\\n\\n您提到了产品部分的质量下降，特别是锁定刀片的部分。我们对此感到非常遗憾，并将反馈给我们的研发团队，以便改进产品的设计和质量控制。我们始终致力于提供优质的产品，以满足客户的需求和期望。\\n\\n此外，我们将非常感谢您分享了您对产品的使用方式和相关提示。您的经验和建议对我们来说非常宝贵，我们将考虑将其纳入我们的产品改进计划中。\\n\\n如果您需要进一步帮助或有其他问题，请随时联系我们的客户服务团队。我们将竭诚为您提供支持和解决方案。\\n\\n再次感谢您的反馈和对我们的支持。我们将继续努力提供更好的产品和服务。\\n\\n祝您一切顺利！\\n\\nAI客户代理\\n\\n第二次运行输出结果会发生变化：\\n\\n```python\\n\\n第二次运行\\n\\nprompt = f\"\"\" 你是一名客户服务的AI助手。 你的任务是给一位重要的客户发送邮件回复。 根据通过“”分隔的客户电子邮件生成回复，以感谢客户的评价。 如果情感是积极的或中性的，感谢他们的评价。 如果情感是消极的，道歉并建议他们联系客户服务。 请确保使用评论中的具体细节。 以简明和专业的语气写信。 以“AI客户代理”的名义签署电子邮件。 客户评价：{review}评论情感：{sentiment} \"\"\" response = get_completion(prompt, temperature=0.7) print(response)\\n\\n亲爱的客户，\\n\\n非常感谢您对我们产品的评价和反馈。我们非常重视您的意见，并感谢您对我们产品的支持。\\n\\n首先，我们对价格的变动感到非常抱歉给您带来了困扰。我们会认真考虑您提到的情况，并采取适当的措施来改进我们的价格策略，以避免类似情况再次发生。\\n\\n关于产品质量的问题，我们深感抱歉。我们一直致力于提供高质量的产品，并且我们会将您提到的问题反馈给我们的研发团队，以便改进产品的设计和制造过程。\\n\\n如果您需要更多关于产品保修的信息，或者对我们的其他产品有任何疑问或需求，请随时联系我们的客户服务团队。我们将竭诚为您提供帮助和支持。\\n\\n再次感谢您对我们产品的评价和支持。我们将继续努力提供优质的产品和出色的客户服务，以满足您的需求。\\n\\n祝您度过愉快的一天！\\n\\nAI客户代理\\n\\n温度（temperature）参数可以控制语言模型生成文本的随机性。温度为0时，每次使用同样的 Prompt，得到的结果总是一致的。而在上面的样例中，当温度设为0.7时，则每次执行都会生成不同的文本。\\n\\n所以，这次的结果与之前得到的邮件就不太一样了。再次执行同样的 Prompt,邮件内容还会有变化。因此。我建议读者朋友们可以自己尝试不同的 temperature ，来观察输出的变化。总体来说，temperature 越高，语言模型的文本生成就越具有随机性。可以想象，高温度下，语言模型就像心绪更加活跃，但也可能更有创造力。\\n\\n适当调节这个超参数,可以让语言模型的生成更富有多样性，也更能意外惊喜。希望这些经验可以帮助你在不同场景中找到最合适的温度设置。\\n\\n三、英文版\\n\\n1.1 定制客户邮件\\n\\n```python\\n\\ngiven the sentiment from the lesson on \"inferring\",\\n\\nand the original customer message, customize the email\\n\\nsentiment = \"negative\"\\n\\nreview for a blender\\n\\nreview = f\"\"\" So, they still had the 17 piece system on seasonal \\\\ sale for around $49 in the month of November, about \\\\ half off, but for some reason (call it price gouging) \\\\ around the second week of December the prices all went \\\\ up to about anywhere from between $70-$89 for the same \\\\ system. And the 11 piece system went up around $10 or \\\\ so in price also from the earlier sale price of $29. \\\\ So it looks okay, but if you look at the base, the part \\\\ where the blade locks into place doesn’t look as good \\\\ as in previous editions from a few years ago, but I \\\\ plan to be very gentle with it (example, I crush \\\\ very hard items like beans, ice, rice, etc. in the \\\\ blender first then pulverize them in the serving size \\\\ I want in the blender then switch to the whipping \\\\ blade for a finer flour, and use the cross cutting blade \\\\ first when making smoothies, then use the flat blade \\\\ if I need them finer/less pulpy). Special tip when making \\\\ smoothies, finely cut and freeze the fruits and \\\\ vegetables (if using spinach-lightly stew soften the \\\\ spinach then freeze until ready for use-and if making \\\\ sorbet, use a small to medium sized food processor) \\\\ that you plan to use that way you can avoid adding so \\\\ much ice if at all-when making your smoothie. \\\\ After about a year, the motor was making a funny noise. \\\\ I called customer service but the warranty expired \\\\ already, so I had to buy another one. FYI: The overall \\\\ quality has gone done in these types of products, so \\\\ they are kind of counting on brand recognition and \\\\ consumer loyalty to maintain sales. Got it in about \\\\ two days. \"\"\" ```\\n\\npython prompt = f\"\"\" You are a customer service AI assistant. Your task is to send an email reply to a valued customer. Given the customer email delimited by, \\\\ Generate a reply to thank the customer for their review. If the sentiment is positive or neutral, thank them for \\\\ their review. If the sentiment is negative, apologize and suggest that \\\\ they can reach out to customer service. Make sure to use specific details from the review. Write in a concise and professional tone. Sign the email as AI customer agent. Customer review: {review} Review sentiment: {sentiment} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nDear Valued Customer,\\n\\nThank you for taking the time to share your review with us. We appreciate your feedback and apologize for any inconvenience you may have experienced.\\n\\nWe are sorry to hear about the price increase you noticed in December. We strive to provide competitive pricing for our products, and we understand your frustration. If you have any further concerns regarding pricing or any other issues, we encourage you to reach out to our customer service team. They will be more than happy to assist you.\\n\\nWe also appreciate your feedback regarding the base of the system. We continuously work to improve the quality of our products, and your comments will be taken into consideration for future enhancements.\\n\\nWe apologize for any inconvenience caused by the motor issue you encountered. Our customer service team is always available to assist with any warranty-related concerns. We understand that the warranty had expired, but we would still like to address this matter further. Please feel free to contact our customer service team, and they will do their best to assist you.\\n\\nThank you once again for your review. We value your feedback and appreciate your loyalty to our brand. If you have any further questions or concerns, please do not hesitate to contact us.\\n\\nBest regards,\\n\\nAI customer agent\\n\\n2.1 引入温度系数\\n\\npython prompt = f\"\"\" You are a customer service AI assistant. Your task is to send an email reply to a valued customer. Given the customer email delimited by, \\\\ Generate a reply to thank the customer for their review. If the sentiment is positive or neutral, thank them for \\\\ their review. If the sentiment is negative, apologize and suggest that \\\\ they can reach out to customer service. Make sure to use specific details from the review. Write in a concise and professional tone. Sign the email as AI customer agent. Customer review: {review} Review sentiment: {sentiment} \"\"\" response = get_completion(prompt, temperature=0.7) print(response) ```\\n\\nDear Valued Customer,\\n\\nThank you for taking the time to share your feedback with us. We sincerely apologize for any inconvenience you experienced with our pricing and the quality of our product.\\n\\nWe understand your frustration regarding the price increase of our 17 piece system in December. We assure you that price gouging is not our intention, and we apologize for any confusion caused. We appreciate your loyalty and we value your feedback, as it helps us to improve our products and services.\\n\\nRegarding the issue with the blade lock and the decrease in overall quality, we apologize for any disappointment caused. We strive to provide our customers with the best possible products, and we regret that we did not meet your expectations. We will make sure to take your feedback into consideration for future improvements.\\n\\nIf you require further assistance or if you have any other concerns, please do not hesitate to reach out to our customer service team. They will be more than happy to assist you in resolving any issues you may have.\\n\\nOnce again, we apologize for any inconvenience caused and we appreciate your understanding. We value your business and we hope to have the opportunity to serve you better in the future.\\n\\nBest regards,\\n\\nAI customer agent'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='第三章 迭代优化\\n\\n在开发大语言模型应用时，很难通过第一次尝试就得到完美适用的 Prompt。但关键是要有一个良好的迭代优化过程，以不断改进 Prompt。相比训练机器学习模型，Prompt 的一次成功率可能更高，但仍需要通过多次迭代找到最适合应用的形式。\\n\\n本章以产品说明书生成营销文案为例，展示 Prompt 迭代优化的思路。这与吴恩达在机器学习课程中演示的机器学习模型开发流程相似：有了想法后，编写代码、获取数据、训练模型、查看结果。通过分析错误找出适用领域，调整方案后再次训练。Prompt 开发也采用类似循环迭代的方式，逐步逼近最优。具体来说，有了任务想法后，可以先编写初版 Prompt，注意清晰明确并给模型充足思考时间。运行后检查结果，如果不理想，则分析 Prompt 不够清楚或思考时间不够等原因，做出改进，再次运行。如此循环多次，终将找到适合应用的 Prompt。\\n\\n图 1.3 Prompt 迭代优化流程\\n\\n总之，很难有适用于世间万物的所谓“最佳 Prompt ”，开发高效 Prompt 的关键在于找到一个好的迭代优化过程，而非一开始就要求完美。通过快速试错迭代，可有效确定符合特定应用的最佳 Prompt 形式。\\n\\n一、从产品说明书生成营销产品描述\\n\\n给定一份椅子的资料页。描述说它属于中世纪灵感系列，产自意大利，并介绍了材料、构造、尺寸、可选配件等参数。假设您想要使用这份说明书帮助营销团队为电商平台撰写营销描述稿：\\n\\n```python\\n\\n示例：产品说明书\\n\\nfact_sheet_chair = \"\"\" 概述\\n\\n美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌、书柜、会议桌等。\\n多种外壳颜色和底座涂层可选。\\n可选塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110）。\\n底座涂层选项为：不锈钢、哑光黑色、光泽白色或铬。\\n椅子可带或不带扶手。\\n适用于家庭或商业场所。\\n符合合同使用资格。\\n\\n结构\\n\\n五个轮子的塑料涂层铝底座。\\n气动椅子调节，方便升降。\\n\\n尺寸\\n\\n宽度53厘米|20.87英寸\\n深度51厘米|20.08英寸\\n高度80厘米|31.50英寸\\n座椅高度44厘米|17.32英寸\\n座椅深度41厘米|16.14英寸\\n\\n选项\\n\\n软地板或硬地板滚轮选项。\\n两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。\\n无扶手或8个位置PU扶手。\\n\\n材料 外壳底座滑动件\\n\\n改性尼龙PA6/PA66涂层的铸铝。\\n外壳厚度：10毫米。\\n座椅\\nHD36泡沫\\n\\n原产国\\n\\n意大利\\n\\n\"\"\" ```\\n\\n1.1 初始提示\\n\\n```python from tool import get_completion\\n\\nPrompt ：基于说明书创建营销描述\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的营销描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n技术说明: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\n产品描述：\\n\\n我们自豪地推出美丽的中世纪风格办公家具系列，其中包括文件柜、办公桌、书柜、会议桌等。我们的产品采用多种外壳颜色和底座涂层，以满足您的个性化需求。您可以选择塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110），以使您的办公室更加舒适和时尚。\\n\\n我们的底座涂层选项包括不锈钢、哑光黑色、光泽白色或铬，以满足您的不同需求。椅子可带或不带扶手，适用于家庭或商业场所。我们的产品符合合同使用资格，为您提供更加可靠的保障。\\n\\n我们的产品采用五个轮子的塑料涂层铝底座，气动椅子调节，方便升降。尺寸为宽度53厘米|20.87英寸，深度51厘米|20.08英寸，高度80厘米|31.50英寸，座椅高度44厘米|17.32英寸，座椅深度41厘米|16.14英寸，为您提供舒适的使用体验。\\n\\n我们的产品还提供软地板或硬地板滚轮选项，两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺），以及无扶手或8个位置PU扶手，以满足您的不同需求。\\n\\n我们的产品采用改性尼龙PA6/PA66涂层的铸铝外壳底座滑动件，外壳厚度为10毫米，座椅采用HD36泡沫，为您提供更加舒适的使用体验。我们的产品原产国为意大利，为您提供更加优质的品质保证。\\n\\n1.2 提示优化1: 解决生成文本太长\\n\\n它似乎很好地完成了要求，即从技术说明书开始编写产品描述，介绍了一个精致的中世纪风格办公椅。但是当我看到这个生成的内容时，我会觉得它太长了。\\n\\n在看到语言模型根据产品说明生成的第一个版本营销文案后，我们注意到文本长度过长，不太适合用作简明的电商广告语。所以这时候就需要对 Prompt 进行优化改进。具体来说，第一版结果满足了从技术说明转换为营销文案的要求，描写了中世纪风格办公椅的细节。但是过于冗长的文本不太适合电商场景。这时我们就可以在 Prompt 中添加长度限制，要求生成更简洁的文案。\\n\\n提取回答并根据空格拆分，中文答案为97个字，较好地完成了设计要求。\\n\\n```python\\n\\n优化后的 Prompt，要求生成描述不多于 50 词\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n使用最多50个词。\\n\\n技术规格：{fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\n中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。多种颜色和涂层可选，可带或不带扶手。底座涂层选项为不锈钢、哑光黑色、光泽白色或铬。适用于家庭或商业场所，符合合同使用资格。意大利制造。\\n\\n我们可以计算一下输出的长度。\\n\\n```python\\n\\n由于中文需要分词，此处直接计算整体长度\\n\\nlen(response) ```\\n\\n97\\n\\n当在 Prompt 中设置长度限制要求时，语言模型生成的输出长度不总能精确符合要求，但基本能控制在可接受的误差范围内。比如要求生成50词的文本，语言模型有时会生成60词左右的输出，但总体接近预定长度。\\n\\n这是因为语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。目前存在多种方法可以尝试控制语言模型生成输出的长度，比如指定语句数、词数、汉字数等。\\n\\n虽然语言模型对长度约束的遵循不是百分之百精确，但通过迭代测试可以找到最佳的长度提示表达式，使生成文本基本符合长度要求。这需要开发者对语言模型的长度判断机制有一定理解，并且愿意进行多次试验来确定最靠谱的长度设置方法。\\n\\n1.3 提示优化2: 处理抓错文本细节\\n\\n在迭代优化 Prompt 的过程中，我们还需要注意语言模型生成文本的细节是否符合预期。\\n\\n比如在这个案例中，进一步分析会发现,该椅子面向的其实是家具零售商，而不是终端消费者。所以生成的文案中过多强调风格、氛围等方面，而较少涉及产品技术细节，与目标受众的关注点不太吻合。这时候我们就可以继续调整 Prompt，明确要求语言模型生成面向家具零售商的描述，更多关注材质、工艺、结构等技术方面的表述。\\n\\n通过迭代地分析结果,检查是否捕捉到正确的细节,我们可以逐步优化 Prompt,使语言模型生成的文本更加符合预期的样式和内容要求。细节的精准控制是语言生成任务中非常重要的一点。我们需要训练语言模型根据不同目标受众关注不同的方面，输出风格和内容上都适合的文本。\\n\\n```python\\n\\n优化后的 Prompt，说明面向对象，应具有什么性质且侧重于什么方面\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。\\n\\n使用最多50个单词。\\n\\n技术规格： {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n这款中世纪风格办公家具系列包括文件柜、办公桌、书柜和会议桌等，适用于家庭或商业场所。可选多种外壳颜色和底座涂层，底座涂层选项为不锈钢、哑光黑色、光泽白色或铬。椅子可带或不带扶手，可选软地板或硬地板滚轮，两种座椅泡沫密度可选。外壳底座滑动件采用改性尼龙PA6/PA66涂层的铸铝，座椅采用HD36泡沫。原产国为意大利。\\n\\n可见，通过修改 Prompt ，模型的关注点倾向了具体特征与技术细节。\\n\\n我可能进一步想要在描述的结尾展示出产品 ID。因此，我可以进一步改进这个 Prompt ，要求在描述的结尾，展示出说明书中的7位产品 ID。\\n\\n```python\\n\\n更进一步\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。\\n\\n在描述末尾，包括技术规格中每个7个字符的产品ID。\\n\\n使用最多50个单词。\\n\\n技术规格： {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n这款中世纪风格的办公家具系列包括文件柜、办公桌、书柜和会议桌等，适用于家庭或商业场所。可选多种外壳颜色和底座涂层，底座涂层选项为不锈钢、哑光黑色、光泽白色或铬。椅子可带或不带扶手，可选塑料前后靠背装饰或10种面料和6种皮革的全面装饰。座椅采用HD36泡沫，可选中等或高密度，座椅高度44厘米，深度41厘米。外壳底座滑动件采用改性尼龙PA6/PA66涂层的铸铝，外壳厚度为10毫米。原产国为意大利。产品ID：SWC-100/SWC-110。\\n\\n通过上面的示例，我们可以看到 Prompt 迭代优化的一般过程。与训练机器学习模型类似，设计高效 Prompt 也需要多个版本的试错调整。\\n\\n具体来说，第一版 Prompt 应该满足明确和给模型思考时间两个原则。在此基础上，一般的迭代流程是：首先尝试一个初版，分析结果，然后继续改进 Prompt，逐步逼近最优。许多成功的Prompt 都是通过这种多轮调整得出的。\\n\\n后面我会展示一个更复杂的 Prompt 案例，让大家更深入地了解语言模型的强大能力。但在此之前，我想强调 Prompt 设计是一个循序渐进的过程。开发者需要做好多次尝试和错误的心理准备，通过不断调整和优化，才能找到最符合具体场景需求的 Prompt 形式。这需要智慧和毅力，但结果往往是值得的。\\n\\n让我们继续探索提示工程的奥秘，开发出令人惊叹的大语言模型应用吧!\\n\\n1.4 提示优化3: 添加表格描述\\n\\n继续添加指引，要求提取产品尺寸信息并组织成表格，并指定表格的列、表名和格式；再将所有内容格式化为可以在网页使用的 HTML。\\n\\n```python\\n\\n要求它抽取信息并组织成表格，并指定表格的列、表名和格式\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。\\n\\n在描述末尾，包括技术规格中每个7个字符的产品ID。\\n\\n在描述之后，包括一个表格，提供产品的尺寸。表格应该有两列。第一列包括尺寸的名称。第二列只包括英寸的测量值。\\n\\n给表格命名为“产品尺寸”。\\n\\n将所有内容格式化为可用于网站的HTML格式。将描述放在\\n\\n元素中。\\n\\n技术规格：{fact_sheet_chair} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n<div>\\n<h2>中世纪风格办公家具系列椅子</h2>\\n<p>这款椅子是中世纪风格办公家具系列的一部分，适用于家庭或商业场所。它有多种外壳颜色和底座涂层可选，包括不锈钢、哑光黑色、光泽白色或铬。您可以选择带或不带扶手的椅子，以及软地板或硬地板滚轮选项。此外，您可以选择两种座椅泡沫密度：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。</p>\\n<p>椅子的外壳底座滑动件是改性尼龙PA6/PA66涂层的铸铝，外壳厚度为10毫米。座椅采用HD36泡沫，底座是五个轮子的塑料涂层铝底座，可以进行气动椅子调节，方便升降。此外，椅子符合合同使用资格，是您理想的选择。</p>\\n<p>产品ID：SWC-100</p>\\n</div>\\n\\n<table>\\n  <caption>产品尺寸</caption>\\n  <tr>\\n    <th>宽度</th>\\n    <td>20.87英寸</td>\\n  </tr>\\n  <tr>\\n    <th>深度</th>\\n    <td>20.08英寸</td>\\n  </tr>\\n  <tr>\\n    <th>高度</th>\\n    <td>31.50英寸</td>\\n  </tr>\\n  <tr>\\n    <th>座椅高度</th>\\n    <td>17.32英寸</td>\\n  </tr>\\n  <tr>\\n    <th>座椅深度</th>\\n    <td>16.14英寸</td>\\n  </tr>\\n</table>\\n\\n上述输出为 HTML 代码，我们可以使用 Python 的 IPython 库将 HTML 代码加载出来。\\n\\n```python\\n\\n表格是以 HTML 格式呈现的，加载出来\\n\\nfrom IPython.display import display, HTML\\n\\ndisplay(HTML(response)) ```\\n\\n中世纪风格办公家具系列椅子\\n\\n这款椅子是中世纪风格办公家具系列的一部分，适用于家庭或商业场所。它有多种外壳颜色和底座涂层可选，包括不锈钢、哑光黑色、光泽白色或铬。您可以选择带或不带扶手的椅子，以及软地板或硬地板滚轮选项。此外，您可以选择两种座椅泡沫密度：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。\\n\\n椅子的外壳底座滑动件是改性尼龙PA6/PA66涂层的铸铝，外壳厚度为10毫米。座椅采用HD36泡沫，底座是五个轮子的塑料涂层铝底座，可以进行气动椅子调节，方便升降。此外，椅子符合合同使用资格，是您理想的选择。\\n\\n产品ID：SWC-100\\n\\n宽度 20.87英寸 深度 20.08英寸 高度 31.50英寸 座椅高度 17.32英寸 座椅深度 16.14英寸\\n\\n二、总结\\n\\n本章重点讲解了在开发大语言模型应用时，采用迭代方式不断优化 Prompt 的过程。作为 Prompt 工程师，关键不是一开始就要求完美的 Prompt，而是掌握有效的 Prompt 开发流程。\\n\\n具体来说，首先编写初版 Prompt，然后通过多轮调整逐步改进，直到生成了满意的结果。对于更复杂的应用，可以在多个样本上进行迭代训练，评估 Prompt 的平均表现。在应用较为成熟后，才需要采用在多个样本集上评估 Prompt 性能的方式来进行细致优化。因为这需要较高的计算资源。\\n\\n总之，Prompt 工程师的核心是掌握 Prompt 的迭代开发和优化技巧，而非一开始就要求100%完美。通过不断调整试错，最终找到可靠适用的 Prompt 形式才是设计 Prompt 的正确方法。\\n\\n读者可以在 Jupyter Notebook 上，对本章给出的示例进行实践，修改 Prompt 并观察不同输出，以深入理解 Prompt 迭代优化的过程。这会对进一步开发复杂语言模型应用提供很好的实践准备。\\n\\n三、英文版\\n\\n产品说明书\\n\\n```python fact_sheet_chair = \"\"\" OVERVIEW - Part of a beautiful family of mid-century inspired office furniture, including filing cabinets, desks, bookcases, meeting tables, and more. - Several options of shell color and base finishes. - Available with plastic back and front upholstery (SWC-100) or full upholstery (SWC-110) in 10 fabric and 6 leather options. - Base finish options are: stainless steel, matte black, gloss white, or chrome. - Chair is available with or without armrests. - Suitable for home or business settings. - Qualified for contract use.\\n\\nCONSTRUCTION - 5-wheel plastic coated aluminum base. - Pneumatic chair adjust for easy raise/lower action.\\n\\nDIMENSIONS - WIDTH 53 CM | 20.87” - DEPTH 51 CM | 20.08” - HEIGHT 80 CM | 31.50” - SEAT HEIGHT 44 CM | 17.32” - SEAT DEPTH 41 CM | 16.14”\\n\\nOPTIONS - Soft or hard-floor caster options. - Two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3) - Armless or 8 position PU armrests\\n\\nMATERIALS SHELL BASE GLIDER - Cast Aluminum with modified nylon PA6/PA66 coating. - Shell thickness: 10 mm. SEAT - HD36 foam\\n\\nCOUNTRY OF ORIGIN - Italy \"\"\" ```\\n\\n1.1 英文初始提示\\n\\n```python\\n\\nPrompt ：基于说明书生成营销描述\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nIntroducing our stunning mid-century inspired office chair, the perfect addition to any home or business setting. This chair is part of a beautiful family of office furniture, including filing cabinets, desks, bookcases, meeting tables, and more, all designed with a timeless mid-century aesthetic.\\n\\nOne of the standout features of this chair is the variety of customization options available. You can choose from several shell colors and base finishes to perfectly match your existing decor. The chair is available with either plastic back and front upholstery or full upholstery in a range of 10 fabric and 6 leather options, allowing you to create a look that is uniquely yours.\\n\\nThe chair is also available with or without armrests, giving you the flexibility to choose the option that best suits your needs. The base finish options include stainless steel, matte black, gloss white, or chrome, ensuring that you can find the perfect match for your space.\\n\\nIn terms of construction, this chair is built to last. It features a 5-wheel plastic coated aluminum base, providing stability and mobility. The pneumatic chair adjust allows for easy raise and lower action, ensuring optimal comfort throughout the day.\\n\\nWhen it comes to dimensions, this chair is designed with both style and comfort in mind. With a width of 53 cm (20.87\"), depth of 51 cm (20.08\"), and height of 80 cm (31.50\"), it offers ample space without overwhelming your space. The seat height is 44 cm (17.32\") and the seat depth is 41 cm (16.14\"), providing a comfortable seating experience for extended periods.\\n\\nWe understand that every space is unique, which is why we offer a range of options to further customize your chair. You can choose between soft or hard-floor caster options, ensuring that your chair glides smoothly on any surface. Additionally, you have the choice between two seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3), allowing you to select the level of support that suits your preferences. The chair is also available with armless design or 8 position PU armrests, providing additional comfort and versatility.\\n\\nWhen it comes to materials, this chair is crafted with the utmost attention to quality. The shell base glider is made of cast aluminum with a modified nylon PA6/PA66 coating, ensuring durability and longevity. The shell thickness is 10 mm, providing a sturdy and reliable structure. The seat is made of HD36 foam, offering a comfortable and supportive seating experience.\\n\\nFinally, this chair is proudly made in Italy, known for its exceptional craftsmanship and attention to detail. With its timeless design and superior construction, this chair is not only a stylish addition to any space but also a reliable and functional piece of furniture.\\n\\nUpgrade your office or home with our mid-century inspired office chair and experience the perfect blend of style, comfort, and functionality.\\n\\n1.2限制生成长度\\n\\n```python\\n\\n优化后的 Prompt，要求生成描述不多于 50 词\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nUse at most 50 words.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nIntroducing our mid-century inspired office chair, part of a beautiful furniture collection. With various color and finish options, it can be customized to suit any space. Choose between plastic or full upholstery in a range of fabrics and leathers. The chair features a durable aluminum base and easy height adjustment. Suitable for both home and business use. Made in Italy.\\n\\npython lst = response.split() print(len(lst))\\n\\n60\\n\\n1.3处理抓错文本细节\\n\\n```python\\n\\n优化后的 Prompt，说明面向对象，应具有什么性质且侧重于什么方面\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nThe description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\\n\\nUse at most 50 words.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nIntroducing our mid-century inspired office chair, part of a beautiful furniture collection. With various shell colors and base finishes, it offers versatility for any setting. Choose between plastic or full upholstery in a range of fabric and leather options. The chair features a durable aluminum base with 5-wheel design and pneumatic chair adjustment. Made in Italy.\\n\\n```python\\n\\n更进一步，要求在描述末尾包含 7个字符的产品ID\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nThe description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\\n\\nAt the end of the description, include every 7-character Product ID in the technical specification.\\n\\nUse at most 50 words.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nIntroducing our mid-century inspired office chair, part of a beautiful family of furniture. This chair offers a range of options, including different shell colors and base finishes. Choose between plastic or full upholstery in various fabric and leather options. The chair is constructed with a 5-wheel plastic coated aluminum base and features a pneumatic chair adjust for easy raise/lower action. With its sleek design and multiple customization options, this chair is suitable for both home and business settings. Made in Italy.\\n\\nProduct IDs: SWC-100, SWC-110\\n\\n1.4英文添加表格描述\\n\\n```python\\n\\n要求它抽取信息并组织成表格，并指定表格的列、表名和格式\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nThe description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\\n\\nAt the end of the description, include every 7-character Product ID in the technical specification.\\n\\nAfter the description, include a table that gives the product\\'s dimensions. The table should have two columns. In the first column include the name of the dimension. In the second column include the measurements in inches only.\\n\\nGive the table the title \\'Product Dimensions\\'.\\n\\nFormat everything as HTML that can be used in a website. Place the description in a\\n\\nelement.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\"\\n\\nresponse = get_completion(prompt) print(response)\\n\\n表格是以 HTML 格式呈现的，加载出来\\n\\nfrom IPython.display import display, HTML\\n\\ndisplay(HTML(response)) ```\\n\\n<div>\\n  <h2>Product Description</h2>\\n  <p>\\n    Introducing our latest addition to our mid-century inspired office furniture collection, the SWC-100 Chair. This chair is part of a beautiful family of furniture that includes filing cabinets, desks, bookcases, meeting tables, and more. With its sleek design and customizable options, it is perfect for both home and business settings.\\n  </p>\\n  <p>\\n    The SWC-100 Chair is available in several options of shell color and base finishes, allowing you to choose the perfect combination to match your space. You can opt for plastic back and front upholstery or full upholstery in a variety of fabric and leather options. The base finish options include stainless steel, matte black, gloss white, or chrome. Additionally, you have the choice of having armrests or going armless.\\n  </p>\\n  <p>\\n    Constructed with durability and comfort in mind, the SWC-100 Chair features a 5-wheel plastic coated aluminum base for stability and mobility. The chair also has a pneumatic adjuster, allowing for easy raise and lower action to find the perfect height for your needs.\\n  </p>\\n  <p>\\n    The SWC-100 Chair is designed to provide maximum comfort and support. The seat is made with HD36 foam, ensuring a plush and comfortable seating experience. You also have the option to choose between soft or hard-floor casters, depending on your flooring needs. Additionally, you can select from two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3). The chair is also available with 8 position PU armrests for added convenience.\\n  </p>\\n  <p>\\n    Made with high-quality materials, the SWC-100 Chair is built to last. The shell base glider is constructed with cast aluminum and modified nylon PA6/PA66 coating, providing durability and stability. The shell has a thickness of 10 mm, ensuring strength and longevity. The chair is proudly made in Italy, known for its craftsmanship and attention to detail.\\n  </p>\\n  <p>\\n    Whether you need a chair for your home office or a professional workspace, the SWC-100 Chair is the perfect choice. Its stylish design, customizable options, and high-quality construction make it a standout piece of furniture that will enhance any space.\\n  </p>\\n  <h2>Product Dimensions</h2>\\n  <table>\\n    <tr>\\n      <th>Dimension</th>\\n      <th>Measurement (inches)</th>\\n    </tr>\\n    <tr>\\n      <td>Width</td>\\n      <td>20.87\"</td>\\n    </tr>\\n    <tr>\\n      <td>Depth</td>\\n      <td>20.08\"</td>\\n    </tr>\\n    <tr>\\n      <td>Height</td>\\n      <td>31.50\"</td>\\n    </tr>\\n    <tr>\\n      <td>Seat Height</td>\\n      <td>17.32\"</td>\\n    </tr>\\n    <tr>\\n      <td>Seat Depth</td>\\n      <td>16.14\"</td>\\n    </tr>\\n  </table>\\n</div>\\n\\nProduct IDs: SWC-100, SWC-110\\n\\nProduct Description\\n\\nIntroducing our latest addition to our mid-century inspired office furniture collection, the SWC-100 Chair. This chair is part of a beautiful family of furniture that includes filing cabinets, desks, bookcases, meeting tables, and more. With its sleek design and customizable options, it is perfect for both home and business settings.\\n\\nThe SWC-100 Chair is available in several options of shell color and base finishes, allowing you to choose the perfect combination to match your space. You can opt for plastic back and front upholstery or full upholstery in a variety of fabric and leather options. The base finish options include stainless steel, matte black, gloss white, or chrome. Additionally, you have the choice of having armrests or going armless.\\n\\nConstructed with durability and comfort in mind, the SWC-100 Chair features a 5-wheel plastic coated aluminum base for stability and mobility. The chair also has a pneumatic adjuster, allowing for easy raise and lower action to find the perfect height for your needs.\\n\\nThe SWC-100 Chair is designed to provide maximum comfort and support. The seat is made with HD36 foam, ensuring a plush and comfortable seating experience. You also have the option to choose between soft or hard-floor casters, depending on your flooring needs. Additionally, you can select from two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3). The chair is also available with 8 position PU armrests for added convenience.\\n\\nMade with high-quality materials, the SWC-100 Chair is built to last. The shell base glider is constructed with cast aluminum and modified nylon PA6/PA66 coating, providing durability and stability. The shell has a thickness of 10 mm, ensuring strength and longevity. The chair is proudly made in Italy, known for its craftsmanship and attention to detail.\\n\\nWhether you need a chair for your home office or a professional workspace, the SWC-100 Chair is the perfect choice. Its stylish design, customizable options, and high-quality construction make it a standout piece of furniture that will enhance any space.\\n\\nProduct Dimensions\\n\\nDimension Measurement (inches) Width 20.87\" Depth 20.08\" Height 31.50\" Seat Height 17.32\" Seat Depth 16.14\"\\n\\nProduct IDs: SWC-100, SWC-110'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='第二章 提示原则\\n\\n如何去使用 Prompt，以充分发挥 LLM 的性能？首先我们需要知道设计 Prompt 的原则，它们是每一个开发者设计 Prompt 所必须知道的基础概念。本章讨论了设计高效 Prompt 的两个关键原则：编写清晰、具体的指令和给予模型充足思考时间。掌握这两点，对创建可靠的语言模型交互尤为重要。\\n\\n首先，Prompt 需要清晰明确地表达需求，提供充足上下文，使语言模型准确理解我们的意图，就像向一个外星人详细解释人类世界一样。过于简略的 Prompt 往往使模型难以把握所要完成的具体任务。\\n\\n其次，让语言模型有充足时间推理也极为关键。就像人类解题一样，匆忙得出的结论多有失误。因此 Prompt 应加入逐步推理的要求，给模型留出充分思考时间，这样生成的结果才更准确可靠。\\n\\n如果 Prompt 在这两点上都作了优化，语言模型就能够尽可能发挥潜力，完成复杂的推理和生成任务。掌握这些 Prompt 设计原则，是开发者取得语言模型应用成功的重要一步。\\n\\n一、原则一 编写清晰、具体的指令\\n\\n亲爱的读者，在与语言模型交互时，您需要牢记一点:以清晰、具体的方式表达您的需求。假设您面前坐着一位来自外星球的新朋友，其对人类语言和常识都一无所知。在这种情况下，您需要把想表达的意图讲得非常明确，不要有任何歧义。同样的，在提供 Prompt 的时候，也要以足够详细和容易理解的方式，把您的需求与上下文说清楚。\\n\\n并不是说 Prompt 就必须非常短小简洁。事实上，在许多情况下，更长、更复杂的 Prompt 反而会让语言模型更容易抓住关键点，给出符合预期的回复。原因在于，复杂的 Prompt 提供了更丰富的上下文和细节，让模型可以更准确地把握所需的操作和响应方式。\\n\\n所以，记住用清晰、详尽的语言表达 Prompt，就像在给外星人讲解人类世界一样，“Adding more context helps the model understand you better.”。\\n\\n从该原则出发，我们提供几个设计 Prompt 的技巧。\\n\\n1.1 使用分隔符清晰地表示输入的不同部分\\n\\n在编写 Prompt 时，我们可以使用各种标点符号作为“分隔符”，将不同的文本部分区分开来。\\n\\n分隔符就像是 Prompt 中的墙，将不同的指令、上下文、输入隔开，避免意外的混淆。你可以选择用 ```，\"\"\"，< >，<tag> </tag>，: 等做分隔符，只要能明确起到隔断作用即可。\\n\\n使用分隔符尤其重要的是可以防止 提示词注入（Prompt Rejection）。什么是提示词注入？就是用户输入的文本可能包含与你的预设 Prompt 相冲突的内容，如果不加分隔，这些输入就可能“注入”并操纵语言模型，导致模型产生毫无关联的乱七八糟的输出。\\n\\n在以下的例子中，我们给出一段话并要求 GPT 进行总结，在该示例中我们使用 ``` 来作为分隔符。\\n\\n```python from tool import get_completion\\n\\ntext = f\"\"\" 您应该提供尽可能清晰、具体的指示，以表达您希望模型执行的任务。\\\\ 这将引导模型朝向所需的输出，并降低收到无关或不正确响应的可能性。\\\\ 不要将写清晰的提示词与写简短的提示词混淆。\\\\ 在许多情况下，更长的提示词可以为模型提供更多的清晰度和上下文信息，从而导致更详细和相关的输出。 \"\"\"\\n\\n需要总结的文本内容\\n\\nprompt = f\"\"\" 把用三个反引号括起来的文本总结成一句话。 {text} \"\"\"\\n\\n指令内容，使用 ``` 来分隔指令和待总结的内容\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n为了获得所需的输出，您应该提供清晰、具体的指示，避免与简短的提示词混淆，并使用更长的提示词来提供更多的清晰度和上下文信息。\\n\\n1.2 寻求结构化的输出\\n\\n有时候我们需要语言模型给我们一些结构化的输出，而不仅仅是连续的文本。\\n\\n什么是结构化输出呢？就是按照某种格式组织的内容，例如JSON、HTML等。这种输出非常适合在代码中进一步解析和处理。例如，您可以在 Python 中将其读入字典或列表中。\\n\\n在以下示例中，我们要求 GPT 生成三本书的标题、作者和类别，并要求 GPT 以 JSON 的格式返回给我们，为便于解析，我们指定了 Json 的键。\\n\\n```python prompt = f\"\"\" 请生成包括书名、作者和类别的三本虚构的、非真实存在的中文书籍清单，\\\\ 并以 JSON 格式提供，其中包含以下键:book_id、title、author、genre。 \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\n{\\n  \"books\": [\\n    {\\n      \"book_id\": 1,\\n      \"title\": \"迷失的时光\",\\n      \"author\": \"张三\",\\n      \"genre\": \"科幻\"\\n    },\\n    {\\n      \"book_id\": 2,\\n      \"title\": \"幻境之门\",\\n      \"author\": \"李四\",\\n      \"genre\": \"奇幻\"\\n    },\\n    {\\n      \"book_id\": 3,\\n      \"title\": \"虚拟现实\",\\n      \"author\": \"王五\",\\n      \"genre\": \"科幻\"\\n    }\\n  ]\\n}\\n\\n1.3 要求模型检查是否满足条件\\n\\n如果任务包含不一定能满足的假设（条件），我们可以告诉模型先检查这些假设，如果不满足，则会指出并停止执行后续的完整流程。您还可以考虑可能出现的边缘情况及模型的应对，以避免意外的结果或错误发生。\\n\\n在如下示例中，我们将分别给模型两段文本，分别是制作茶的步骤以及一段没有明确步骤的文本。我们将要求模型判断其是否包含一系列指令，如果包含则按照给定格式重新编写指令，不包含则回答“未提供步骤”。\\n\\n```python\\n\\n满足条件的输入（text中提供了步骤）\\n\\ntext_1 = f\"\"\" 泡一杯茶很容易。首先，需要把水烧开。\\\\ 在等待期间，拿一个杯子并把茶包放进去。\\\\ 一旦水足够热，就把它倒在茶包上。\\\\ 等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\\\\ 如果您愿意，可以加一些糖或牛奶调味。\\\\ 就这样，您可以享受一杯美味的茶了。 \"\"\" prompt = f\"\"\" 您将获得由三个引号括起来的文本。\\\\ 如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：\\n\\n第一步 - ... 第二步 - … … 第N步 - …\\n\\n如果文本中不包含一系列的指令，则直接写“未提供步骤”。\" \\\\\"\\\\\"\\\\\"{text_1}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Text 1 的总结:\") print(response) ```\\n\\nText 1 的总结:\\n第一步 - 把水烧开。\\n第二步 - 拿一个杯子并把茶包放进去。\\n第三步 - 把烧开的水倒在茶包上。\\n第四步 - 等待几分钟，让茶叶浸泡。\\n第五步 - 取出茶包。\\n第六步 - 如果需要，加入糖或牛奶调味。\\n第七步 - 就这样，您可以享受一杯美味的茶了。\\n\\n上述示例中，模型可以很好地识别一系列的指令并进行输出。在接下来一个示例中，我们将提供给模型没有预期指令的输入，模型将判断未提供步骤。\\n\\n```python\\n\\n不满足条件的输入（text中未提供预期指令）\\n\\ntext_2 = f\"\"\" 今天阳光明媚，鸟儿在歌唱。\\\\ 这是一个去公园散步的美好日子。\\\\ 鲜花盛开，树枝在微风中轻轻摇曳。\\\\ 人们外出享受着这美好的天气，有些人在野餐，有些人在玩游戏或者在草地上放松。\\\\ 这是一个完美的日子，可以在户外度过并欣赏大自然的美景。 \"\"\" prompt = f\"\"\" 您将获得由三个引号括起来的文本。\\\\ 如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：\\n\\n第一步 - ... 第二步 - … … 第N步 - …\\n\\n如果文本中不包含一系列的指令，则直接写“未提供步骤”。\" \\\\\"\\\\\"\\\\\"{text_2}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Text 2 的总结:\") print(response) ```\\n\\nText 2 的总结:\\n未提供步骤。\\n\\n1.4 提供少量示例\\n\\n\"Few-shot\" prompting，即在要求模型执行实际任务之前，给模型一两个已完成的样例，让模型了解我们的要求和期望的输出样式。\\n\\n例如，在以下的样例中，我们先给了一个祖孙对话样例，然后要求模型用同样的隐喻风格回答关于“韧性”的问题。这就是一个少样本样例，它能帮助模型快速抓住我们要的语调和风格。\\n\\n利用少样本样例，我们可以轻松“预热”语言模型，让它为新的任务做好准备。这是一个让模型快速上手新任务的有效策略。\\n\\n```python prompt = f\"\"\" 您的任务是以一致的风格回答问题。\\n\\n<孩子>: 请教我何为耐心。\\n\\n<祖父母>: 挖出最深峡谷的河流源于一处不起眼的泉眼；最宏伟的交响乐从单一的音符开始；最复杂的挂毯以一根孤独的线开始编织。\\n\\n<孩子>: 请教我何为韧性。 \"\"\" response = get_completion(prompt) print(response) ```\\n\\n<祖父母>: 韧性是一种坚持不懈的品质，就像一棵顽强的树在风雨中屹立不倒。它是面对困难和挑战时不屈不挠的精神，能够适应变化和克服逆境。韧性是一种内在的力量，让我们能够坚持追求目标，即使面临困难和挫折也能坚持不懈地努力。\\n\\n二、原则二 给模型时间去思考\\n\\n在设计 Prompt 时，给予语言模型充足的推理时间非常重要。语言模型与人类一样，需要时间来思考并解决复杂问题。如果让语言模型匆忙给出结论，其结果很可能不准确。例如，若要语言模型推断一本书的主题，仅提供简单的书名和一句简介是不足够的。这就像让一个人在极短时间内解决困难的数学题，错误在所难免。\\n\\n相反，我们应通过 Prompt 指引语言模型进行深入思考。可以要求其先列出对问题的各种看法，说明推理依据，然后再得出最终结论。在 Prompt 中添加逐步推理的要求，能让语言模型投入更多时间逻辑思维，输出结果也将更可靠准确。\\n\\n综上所述，给予语言模型充足的推理时间，是 Prompt Engineering 中一个非常重要的设计原则。这将大大提高语言模型处理复杂问题的效果，也是构建高质量 Prompt 的关键之处。开发者应注意给模型留出思考空间，以发挥语言模型的最大潜力。\\n\\n2.1 指定完成任务所需的步骤\\n\\n接下来我们将通过给定一个复杂任务，给出完成该任务的一系列步骤，来展示这一策略的效果。\\n\\n首先我们描述了杰克和吉尔的故事，并给出提示词执行以下操作：首先，用一句话概括三个反引号限定的文本。第二，将摘要翻译成英语。第三，在英语摘要中列出每个名称。第四，输出包含以下键的 JSON 对象：英语摘要和人名个数。要求输出以换行符分隔。\\n\\n```python text = f\"\"\" 在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\\\\ 他们一边唱着欢乐的歌，一边往上爬，\\\\ 然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\\\\ 虽然略有些摔伤，但他们还是回到了温馨的家中。\\\\ 尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。 \"\"\"\\n\\nexample 1\\n\\nprompt_1 = f\"\"\" 执行以下操作： 1-用一句话概括下面用三个反引号括起来的文本。 2-将摘要翻译成英语。 3-在英语摘要中列出每个人名。 4-输出一个 JSON 对象，其中包含以下键：english_summary，num_names。\\n\\n请用换行符分隔您的答案。\\n\\nText: {text} \"\"\" response = get_completion(prompt_1) print(\"prompt 1:\") print(response) ```\\n\\nprompt 1:\\n1-两个兄妹在山上打水时发生意外，但最终平安回家。\\n2-In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. While singing joyfully, they climbed up, but unfortunately, Jack tripped on a stone and rolled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back to their cozy home. Despite the mishap, their adventurous spirit remained undiminished as they continued to explore with delight.\\n3-Jack, Jill\\n4-{\"english_summary\": \"In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. While singing joyfully, they climbed up, but unfortunately, Jack tripped on a stone and rolled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back to their cozy home. Despite the mishap, their adventurous spirit remained undiminished as they continued to explore with delight.\", \"num_names\": 2}\\n\\n上述输出仍然存在一定问题，例如，键“姓名”会被替换为法语（译注：在英文原版中，要求从英语翻译到法语，对应指令第三步的输出为 \\'Noms:\\'，为Name的法语，这种行为难以预测，并可能为导出带来困难）\\n\\n因此，我们将Prompt加以改进，该 Prompt 前半部分不变，同时确切指定了输出的格式。\\n\\n```python prompt_2 = f\"\"\" 1-用一句话概括下面用<>括起来的文本。 2-将摘要翻译成英语。 3-在英语摘要中列出每个名称。 4-输出一个 JSON 对象，其中包含以下键：English_summary，num_names。\\n\\n请使用以下格式： 文本：<要总结的文本> 摘要：<摘要> 翻译：<摘要的翻译> 名称：<英语摘要中的名称列表> 输出 JSON：<带有 English_summary 和 num_names 的 JSON>\\n\\nText: <{text}> \"\"\" response = get_completion(prompt_2) print(\"\\\\nprompt 2:\") print(response) ```\\n\\nprompt 2:\\nSummary: 在一个迷人的村庄里，兄妹杰克和吉尔在山顶井里打水时发生了意外，但他们的冒险精神依然没有减弱，继续充满愉悦地探索。\\n\\nTranslation: In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. Unfortunately, Jack tripped on a rock and tumbled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back home safely. Despite the mishap, their adventurous spirit remained strong as they continued to explore joyfully.\\n\\nNames: Jack, Jill\\n\\nJSON Output: {\"English_summary\": \"In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. Unfortunately, Jack tripped on a rock and tumbled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back home safely. Despite the mishap, their adventurous spirit remained strong as they continued to explore joyfully.\", \"num_names\": 2}\\n\\n2.2 指导模型在下结论之前找出一个自己的解法\\n\\n在设计 Prompt 时，我们还可以通过明确指导语言模型进行自主思考，来获得更好的效果。\\n\\n举个例子，假设我们要语言模型判断一个数学问题的解答是否正确。仅仅提供问题和解答是不够的，语言模型可能会匆忙做出错误判断。\\n\\n相反，我们可以在 Prompt 中先要求语言模型自己尝试解决这个问题，思考出自己的解法，然后再与提供的解答进行对比，判断正确性。这种先让语言模型自主思考的方式，能帮助它更深入理解问题，做出更准确的判断。\\n\\n接下来我们会给出一个问题和一份来自学生的解答，要求模型判断解答是否正确：\\n\\n```python prompt = f\"\"\" 判断学生的解决方案是否正确。\\n\\n问题: 我正在建造一个太阳能发电站，需要帮助计算财务。\\n\\n土地费用为 100美元/平方英尺\\n我可以以 250美元/平方英尺的价格购买太阳能电池板\\n我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元\\n作为平方英尺数的函数，首年运营的总费用是多少。\\n\\n学生的解决方案： 设x为发电站的大小，单位为平方英尺。 费用：\\n\\n土地费用：100x\\n太阳能电池板费用：250x\\n维护费用：100,000美元+100x\\n总费用：100x+250x+100,000美元+100x=450x+100,000美元\\n\\n\"\"\" response = get_completion(prompt) print(response) ```\\n\\n学生的解决方案是正确的。他正确地计算了土地费用、太阳能电池板费用和维护费用，并将它们相加得到了总费用。\\n\\n但是注意，学生的解决方案实际上是错误的。（维护费用项100x应为10x，总费用450x应为360x）\\n\\n我们可以通过指导模型先自行找出一个解法来解决这个问题。\\n\\n在接下来这个 Prompt 中，我们要求模型先自行解决这个问题，再根据自己的解法与学生的解法进行对比，从而判断学生的解法是否正确。同时，我们给定了输出的格式要求。通过拆分任务、明确步骤，让模型有更多时间思考，有时可以获得更准确的结果。在这个例子中，学生的答案是错误的，但如果我们没有先让模型自己计算，那么可能会被误导以为学生是正确的。\\n\\n```python prompt = f\"\"\" 请判断学生的解决方案是否正确，请通过如下步骤解决这个问题：\\n\\n步骤：\\n\\n首先，自己解决问题。\\n然后将您的解决方案与学生的解决方案进行比较，对比计算得到的总费用与学生计算的总费用是否一致，并评估学生的解决方案是否正确。\\n在自己完成问题之前，请勿决定学生的解决方案是否正确。\\n\\n使用以下格式：\\n\\n问题：问题文本\\n学生的解决方案：学生的解决方案文本\\n实际解决方案和步骤：实际解决方案和步骤文本\\n学生计算的总费用：学生计算得到的总费用\\n实际计算的总费用：实际计算出的总费用\\n学生计算的费用和实际计算的费用是否相同：是或否\\n学生的解决方案和实际解决方案是否相同：是或否\\n学生的成绩：正确或不正确\\n\\n问题：\\n\\n我正在建造一个太阳能发电站，需要帮助计算财务。 \\n- 土地费用为每平方英尺100美元\\n- 我可以以每平方英尺250美元的价格购买太阳能电池板\\n- 我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元;\\n\\n作为平方英尺数的函数，首年运营的总费用是多少。\\n\\n学生的解决方案：\\n\\n设x为发电站的大小，单位为平方英尺。\\n费用：\\n1. 土地费用：100x美元\\n2. 太阳能电池板费用：250x美元\\n3. 维护费用：100,000+100x=10万美元+10x美元\\n总费用：100x美元+250x美元+10万美元+100x美元=450x+10万美元\\n\\n实际解决方案和步骤： \"\"\" response = get_completion(prompt) print(response) ```\\n\\n实际解决方案和步骤：\\n\\n    1. 土地费用：每平方英尺100美元，所以总费用为100x美元。\\n    2. 太阳能电池板费用：每平方英尺250美元，所以总费用为250x美元。\\n    3. 维护费用：固定费用为10万美元，额外费用为每平方英尺10美元，所以总费用为10万美元+10x美元。\\n    4. 总费用：将上述三项费用相加，得到总费用为100x美元+250x美元+10万美元+10x美元=360x+10万美元。\\n\\n学生计算的总费用：450x+10万美元\\n实际计算的总费用：360x+10万美元\\n学生计算的费用和实际计算的费用是否相同：否\\n学生的解决方案和实际解决方案是否相同：否\\n学生的成绩：不正确\\n\\n三、局限性\\n\\n开发大模型相关应用时请务必铭记：\\n\\n虚假知识：模型偶尔会生成一些看似真实实则编造的知识\\n\\n在开发与应用语言模型时，需要注意它们可能生成虚假信息的风险。尽管模型经过大规模预训练，掌握了丰富知识，但它实际上并没有完全记住所见的信息，难以准确判断自己的知识边界，可能做出错误推断。若让语言模型描述一个不存在的产品,它可能会自行构造出似是而非的细节。这被称为“幻觉”(Hallucination)，是语言模型的一大缺陷。\\n\\n如下示例展示了大模型的幻觉。我们要求告诉我们华为公司生产的 GT Watch 运动手表 产品的信息，事实上，这个公司是真实存在的，但产品是编造的，而模型一本正经地提供了它编造的知识，而且迷惑性很强。\\n\\npython prompt = f\"\"\" 告诉我华为公司生产的GT Watch运动手表的相关信息 \"\"\" response = get_completion(prompt) print(response)\\n\\n华为公司生产的GT Watch运动手表是一款智能手表，具有多种功能和特点。以下是相关信息：\\n\\n1. 设计和外观：GT Watch采用圆形表盘设计，具有精致的外观和高质量的材料制造。它有多种颜色和表带选择，可以根据个人喜好进行定制。\\n\\n2. 显示屏：GT Watch配备了1.39英寸的AMOLED显示屏，具有高清分辨率和良好的可视性。用户可以通过触摸屏幕进行操作和导航。\\n\\n3. 运动追踪：GT Watch具有全天候的运动追踪功能，可以监测用户的步数、跑步距离、卡路里消耗和心率等数据。它还支持多种运动模式，如跑步、骑行、游泳等。\\n\\n4. 健康监测：GT Watch可以监测用户的心率、血氧饱和度和睡眠质量等健康指标。它还提供健康建议和提醒，帮助用户保持良好的健康状态。\\n\\n5. 通知和连接：GT Watch可以与用户的手机进行连接，通过蓝牙技术实现通知推送和电话提醒。用户可以在手表上查看短信、电话和社交媒体通知，无需拿出手机。\\n\\n6. 长续航时间：GT Watch具有较长的续航时间，一次充电可以使用数天。它还支持快速充电技术，可以在短时间内充满电。\\n\\n7. 其他功能：GT Watch还具有其他功能，如天气预报、闹钟、计时器、计步器等。它还支持NFC支付和音乐控制等便利功能。\\n\\n总体而言，华为GT Watch是一款功能强大、外观精致的智能运动手表，适合那些注重健康和运动的用户使用。\\n\\n语言模型生成虚假信息的“幻觉”问题，是使用与开发语言模型时需要高度关注的风险。由于幻觉信息往往令人无法辨别真伪，开发者必须警惕并尽量避免它的产生。\\n\\n目前 OpenAI 等公司正在积极研究解决语言模型的幻觉问题。在技术得以进一步改进之前，开发者可以通过Prompt设计减少幻觉发生的可能。例如，可以先让语言模型直接引用文本中的原句，然后再进行解答。这可以追踪信息来源，降低虚假内容的风险。\\n\\n综上，语言模型的幻觉问题事关应用的可靠性与安全性。开发者有必要认识到这一缺陷（注：截至2023年7月），并采取Prompt优化等措施予以缓解，以开发出更加可信赖的语言模型应用。这也将是未来语言模型进化的重要方向之一。\\n\\n注意：\\n\\n关于反斜杠使用的说明：在本教程中，我们使用反斜杠 \\\\ 来使文本适应屏幕大小以提高阅读体验，而没有用换行符 \\\\n 。GPT-3 并不受换行符（newline characters）的影响，但在您调用其他大模型时，需额外考虑换行符是否会影响模型性能。\\n\\n四、英文原版 Prompt\\n\\n1.1 使用分隔符清晰地表示输入的不同部分\\n\\npython text = f\"\"\" You should express what you want a model to do by \\\\ providing instructions that are as clear and \\\\ specific as you can possibly make them. \\\\ This will guide the model towards the desired output, \\\\ and reduce the chances of receiving irrelevant \\\\ or incorrect responses. Don\\'t confuse writing a \\\\ clear prompt with writing a short prompt. \\\\ In many cases, longer prompts provide more clarity \\\\ and context for the model, which can lead to \\\\ more detailed and relevant outputs. \"\"\" prompt = f\"\"\" Summarize the text delimited by triple backticks \\\\ into a single sentence.{text}\"\"\" response = get_completion(prompt) print(response)\\n\\nTo guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which can be achieved through longer prompts that offer more clarity and context.\\n\\n1.2 寻求结构化的输出\\n\\n```python prompt = f\"\"\" Generate a list of three made-up book titles along \\\\ with their authors and genres. Provide them in JSON format with the following keys: book_id, title, author, genre. \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\n{\\n  \"books\": [\\n    {\\n      \"book_id\": 1,\\n      \"title\": \"The Enigma of Elysium\",\\n      \"author\": \"Evelyn Sinclair\",\\n      \"genre\": \"Mystery\"\\n    },\\n    {\\n      \"book_id\": 2,\\n      \"title\": \"Whispers in the Wind\",\\n      \"author\": \"Nathaniel Blackwood\",\\n      \"genre\": \"Fantasy\"\\n    },\\n    {\\n      \"book_id\": 3,\\n      \"title\": \"Echoes of the Past\",\\n      \"author\": \"Amelia Hart\",\\n      \"genre\": \"Romance\"\\n    }\\n  ]\\n}\\n\\n1.3 要求模型检查是否满足条件\\n\\n```python text_1 = f\"\"\" Making a cup of tea is easy! First, you need to get some \\\\ water boiling. While that\\'s happening, \\\\ grab a cup and put a tea bag in it. Once the water is \\\\ hot enough, just pour it over the tea bag. \\\\ Let it sit for a bit so the tea can steep. After a \\\\ few minutes, take out the tea bag. If you \\\\ like, you can add some sugar or milk to taste. \\\\ And that\\'s it! You\\'ve got yourself a delicious \\\\ cup of tea to enjoy. \"\"\" prompt = f\"\"\" You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, \\\\ re-write those instructions in the following format:\\n\\nStep 1 - ... Step 2 - … … Step N - …\\n\\nIf the text does not contain a sequence of instructions, \\\\ then simply write \\\\\"No steps provided.\\\\\"\\n\\n\\\\\"\\\\\"\\\\\"{text_1}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Completion for Text 1:\") print(response) ```\\n\\nCompletion for Text 1:\\nStep 1 - Get some water boiling.\\nStep 2 - Grab a cup and put a tea bag in it.\\nStep 3 - Once the water is hot enough, pour it over the tea bag.\\nStep 4 - Let it sit for a bit so the tea can steep.\\nStep 5 - After a few minutes, take out the tea bag.\\nStep 6 - If you like, add some sugar or milk to taste.\\nStep 7 - Enjoy your delicious cup of tea.\\n\\n```python text_2 = f\"\"\" The sun is shining brightly today, and the birds are \\\\ singing. It\\'s a beautiful day to go for a \\\\ walk in the park. The flowers are blooming, and the \\\\ trees are swaying gently in the breeze. People \\\\ are out and about, enjoying the lovely weather. \\\\ Some are having picnics, while others are playing \\\\ games or simply relaxing on the grass. It\\'s a \\\\ perfect day to spend time outdoors and appreciate the \\\\ beauty of nature. \"\"\" prompt = f\"\"\"You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, \\\\ re-write those instructions in the following format: Step 1 - ... Step 2 - … … Step N - …\\n\\nIf the text does not contain a sequence of instructions, \\\\ then simply write \\\\\"No steps provided.\\\\\"\\n\\n\\\\\"\\\\\"\\\\\"{text_2}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Completion for Text 2:\") print(response) ```\\n\\nCompletion for Text 2:\\nNo steps provided.\\n\\n1.4 提供少量示例（少样本提示词，Few-shot prompting）\\n\\n```python prompt = f\"\"\" Your task is to answer in a consistent style.\\n\\n<grandparent>: Resilience is like a mighty oak tree that withstands the strongest storms, bending but never breaking. It is the unwavering determination to rise again after every fall, and the ability to find strength in the face of adversity. Just as a diamond is formed under immense pressure, resilience is forged through challenges and hardships, making us stronger and more resilient in the process.\\n\\n2.1 指定完成任务所需的步骤\\n\\n```python text = f\"\"\" In a charming village, siblings Jack and Jill set out on \\\\ a quest to fetch water from a hilltop \\\\ well. As they climbed, singing joyfully, misfortune \\\\ struck—Jack tripped on a stone and tumbled \\\\ down the hill, with Jill following suit. \\\\ Though slightly battered, the pair returned home to \\\\ comforting embraces. Despite the mishap, \\\\ their adventurous spirits remained undimmed, and they \\\\ continued exploring with delight. \"\"\"\\n\\nexample 1\\n\\nprompt_1 = f\"\"\" Perform the following actions: 1 - Summarize the following text delimited by triple \\\\ backticks with 1 sentence. 2 - Translate the summary into French. 3 - List each name in the French summary. 4 - Output a json object that contains the following \\\\ keys: french_summary, num_names.\\n\\nSeparate your answers with line breaks.\\n\\nText: {text} \"\"\" response = get_completion(prompt_1) print(\"Completion for prompt 1:\") print(response) ```\\n\\nCompletion for prompt 1:\\n1 - Jack and Jill, siblings, go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips on a stone and tumbles down the hill, with Jill following suit, yet they return home and remain undeterred in their adventurous spirits.\\n\\n2 - Jack et Jill, frère et sœur, partent en quête d\\'eau d\\'un puits au sommet d\\'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils rentrent chez eux et restent déterminés dans leur esprit d\\'aventure.\\n\\n3 - Jack, Jill\\n\\n4 - {\\n  \"french_summary\": \"Jack et Jill, frère et sœur, partent en quête d\\'eau d\\'un puits au sommet d\\'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils rentrent chez eux et restent déterminés dans leur esprit d\\'aventure.\",\\n  \"num_names\": 2\\n}\\n\\n```python prompt_2 = f\"\"\" Your task is to perform the following actions: 1 - Summarize the following text delimited by <> with 1 sentence. 2 - Translate the summary into French. 3 - List each name in the French summary. 4 - Output a json object that contains the following keys: french_summary, num_names.\\n\\nUse the following format: Text:\\n\\nText: <{text}> \"\"\" response = get_completion(prompt_2) print(\"\\\\nCompletion for prompt 2:\") print(response) ```\\n\\nCompletion for prompt 2:\\nSummary: Jack and Jill, siblings from a charming village, go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips on a stone and tumbles down the hill, with Jill following suit, yet they remain undeterred and continue exploring with delight.\\n\\nTranslation: Jack et Jill, frère et sœur d\\'un charmant village, partent en quête d\\'eau d\\'un puits au sommet d\\'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils restent déterminés et continuent à explorer avec joie.\\n\\nNames: Jack, Jill\\n\\nOutput JSON: \\n{\\n  \"french_summary\": \"Jack et Jill, frère et sœur d\\'un charmant village, partent en quête d\\'eau d\\'un puits au sommet d\\'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils restent déterminés et continuent à explorer avec joie.\",\\n  \"num_names\": 2\\n}\\n\\n2.2 指导模型在下结论之前找出一个自己的解法\\n\\n```python prompt = f\"\"\" Determine if the student\\'s solution is correct or not.\\n\\nQuestion: I\\'m building a solar power installation and I need \\\\ help working out the financials. - Land costs $100 / square foot - I can buy solar panels for $250 / square foot - I negotiated a contract for maintenance that will cost \\\\ me a flat $100k per year, and an additional $10 / square \\\\ foot What is the total cost for the first year of operations as a function of the number of square feet.\\n\\nStudent\\'s Solution: Let x be the size of the installation in square feet. Costs: 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000 \"\"\" response = get_completion(prompt) print(response) ```\\n\\nThe student\\'s solution is correct. They correctly identified the costs for land, solar panels, and maintenance, and calculated the total cost for the first year of operations as a function of the number of square feet.\\n\\n```python prompt = f\"\"\" Your task is to determine if the student\\'s solution \\\\ is correct or not. To solve the problem do the following: - First, work out your own solution to the problem. - Then compare your solution to the student\\'s solution \\\\ and evaluate if the student\\'s solution is correct or not. Don\\'t decide if the student\\'s solution is correct until you have done the problem yourself.\\n\\nUse the following format: Question: question here Student\\'s solution: student\\'s solution here Actual solution: steps to work out the solution and your solution here Is the student\\'s solution the same as actual solution \\\\ just calculated: yes or no Student grade: correct or incorrect\\n\\nQuestion: I\\'m building a solar power installation and I need help \\\\ working out the financials. - Land costs $100 / square foot - I can buy solar panels for $250 / square foot - I negotiated a contract for maintenance that will cost \\\\ me a flat $100k per year, and an additional $10 / square \\\\ foot What is the total cost for the first year of operations \\\\ as a function of the number of square feet. Student\\'s solution: Let x be the size of the installation in square feet. Costs: 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000 Actual solution: \"\"\" response = get_completion(prompt) print(response) ```\\n\\nTo calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.\\n\\n1. Land cost: $100 / square foot\\nThe cost of land is $100 multiplied by the number of square feet.\\n\\n2. Solar panel cost: $250 / square foot\\nThe cost of solar panels is $250 multiplied by the number of square feet.\\n\\n3. Maintenance cost: $100,000 + $10 / square foot\\nThe maintenance cost is a flat fee of $100,000 per year, plus $10 multiplied by the number of square feet.\\n\\nTotal cost: Land cost + Solar panel cost + Maintenance cost\\n\\nSo the actual solution is:\\nTotal cost = (100 * x) + (250 * x) + (100,000 + (10 * x))\\n\\nIs the student\\'s solution the same as the actual solution just calculated:\\nNo\\n\\nStudent grade:\\nIncorrect\\n\\n3.1 幻觉\\n\\npython prompt = f\"\"\" Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie \"\"\" response = get_completion(prompt) print(response)\\n\\nThe AeroGlide UltraSlim Smart Toothbrush by Boie is a technologically advanced toothbrush designed to provide a superior brushing experience. Boie is a company known for its innovative oral care products, and the AeroGlide UltraSlim Smart Toothbrush is no exception.\\n\\nOne of the standout features of this toothbrush is its ultra-slim design. The brush head is only 2mm thick, making it much thinner than traditional toothbrushes. This slim profile allows for better access to hard-to-reach areas of the mouth, ensuring a thorough and effective clean.\\n\\nThe AeroGlide UltraSlim Smart Toothbrush also incorporates smart technology. It connects to a mobile app via Bluetooth, allowing users to track their brushing habits and receive personalized recommendations for improving their oral hygiene routine. The app provides real-time feedback on brushing technique, duration, and coverage, helping users to achieve optimal oral health.\\n\\nThe toothbrush features soft, antimicrobial bristles made from a durable thermoplastic elastomer. These bristles are gentle on the gums and teeth, while also being effective at removing plaque and debris. The antimicrobial properties help to keep the brush head clean and hygienic between uses.\\n\\nAnother notable feature of the AeroGlide UltraSlim Smart Toothbrush is its long battery life. It can last up to 30 days on a single charge, making it convenient for travel or everyday use without the need for frequent recharging.\\n\\nOverall, the AeroGlide UltraSlim Smart Toothbrush by Boie offers a combination of advanced technology, slim design, and effective cleaning capabilities. It is a great option for those looking to upgrade their oral care routine and achieve a healthier smile.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}, page_content='第一章 简介\\n\\n欢迎来到面向开发者的提示工程部分，本部分内容基于吴恩达老师的《Prompt Engineering for Developer》课程进行编写。《Prompt Engineering for Developer》课程是由吴恩达老师与 OpenAI 技术团队成员 Isa Fulford 老师合作授课，Isa 老师曾开发过受欢迎的 ChatGPT 检索插件，并且在教授 LLM （Large Language Model， 大语言模型）技术在产品中的应用方面做出了很大贡献。她还参与编写了教授人们使用 Prompt 的 OpenAI cookbook。我们希望通过本模块的学习，与大家分享使用提示词开发 LLM 应用的最佳实践和技巧。\\n\\n网络上有许多关于提示词（Prompt， 本教程中将保留该术语）设计的材料，例如《30 prompts everyone has to know》之类的文章，这些文章主要集中在 ChatGPT 的 Web 界面上，许多人在使用它执行特定的、通常是一次性的任务。但我们认为，对于开发人员，大语言模型（LLM） 的更强大功能是能通过 API 接口调用，从而快速构建软件应用程序。实际上，我们了解到 DeepLearning.AI 的姊妹公司 AI Fund 的团队一直在与许多初创公司合作，将这些技术应用于诸多应用程序上。很兴奋能看到 LLM API 能够让开发人员非常快速地构建应用程序。\\n\\n在本模块，我们将与读者分享提升大语言模型应用效果的各种技巧和最佳实践。书中内容涵盖广泛，包括软件开发提示词设计、文本总结、推理、转换、扩展以及构建聊天机器人等语言模型典型应用场景。我们衷心希望该课程能激发读者的想象力，开发出更出色的语言模型应用。\\n\\n随着 LLM 的发展，其大致可以分为两种类型，后续称为基础 LLM 和指令微调（Instruction Tuned）LLM。基础LLM是基于文本训练数据，训练出预测下一个单词能力的模型。其通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。例如，如果你以“从前，有一只独角兽”作为 Prompt ，基础 LLM 可能会继续预测“她与独角兽朋友共同生活在一片神奇森林中”。但是，如果你以“法国的首都是什么”为 Prompt ，则基础 LLM 可能会根据互联网上的文章，将回答预测为“法国最大的城市是什么？法国的人口是多少？”，因为互联网上的文章很可能是有关法国国家的问答题目列表。\\n\\n与基础语言模型不同，指令微调 LLM 通过专门的训练，可以更好地理解并遵循指令。举个例子，当询问“法国的首都是什么？”时，这类模型很可能直接回答“法国的首都是巴黎”。指令微调 LLM 的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。有时还会采用RLHF（reinforcement learning from human feedback，人类反馈强化学习）技术，根据人类对模型输出的反馈进一步增强模型遵循指令的能力。通过这种受控的训练过程。指令微调 LLM 可以生成对指令高度敏感、更安全可靠的输出，较少无关和损害性内容。因此。许多实际应用已经转向使用这类大语言模型。\\n\\n因此，本课程将重点介绍针对指令微调 LLM 的最佳实践，我们也建议您将其用于大多数使用场景。当您使用指令微调 LLM 时，您可以类比为向另一个人提供指令（假设他很聪明但不知道您任务的具体细节）。因此，当 LLM 无法正常工作时，有时是因为指令不够清晰。例如，如果您想问“请为我写一些关于阿兰·图灵( Alan Turing )的东西”，在此基础上清楚表明您希望文本专注于他的科学工作、个人生活、历史角色或其他方面可能会更有帮助。另外您还可以指定回答的语调， 来更加满足您的需求，可选项包括专业记者写作，或者向朋友写的随笔等。\\n\\n如果你将 LLM 视为一名新毕业的大学生，要求他完成这个任务，你甚至可以提前指定他们应该阅读哪些文本片段来写关于阿兰·图灵的文本，这样能够帮助这位新毕业的大学生更好地完成这项任务。本书的下一章将详细阐释提示词设计的两个关键原则：清晰明确和给予充足思考时间。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='第八章 聊天机器人\\n\\n大型语言模型带给我们的激动人心的一种可能性是，我们可以通过它构建定制的聊天机器人（Chatbot），而且只需很少的工作量。在这一章节的探索中，我们将带你了解如何利用会话形式，与具有个性化特性（或专门为特定任务或行为设计）的聊天机器人进行深度对话。\\n\\n像 ChatGPT 这样的聊天模型实际上是组装成以一系列消息作为输入，并返回一个模型生成的消息作为输出的。这种聊天格式原本的设计目标是简便多轮对话，但我们通过之前的学习可以知道，它对于不会涉及任何对话的单轮任务也同样有用。\\n\\n一、给定身份\\n\\n接下来，我们将定义两个辅助函数。\\n\\n第一个方法已经陪伴了您一整个教程，即 get_completion ，其适用于单轮对话。我们将 Prompt 放入某种类似用户消息的对话框中。另一个称为 get_completion_from_messages ，传入一个消息列表。这些消息可以来自大量不同的角色 (roles) ，我们会描述一下这些角色。\\n\\n第一条消息中，我们以系统身份发送系统消息 (system message) ，它提供了一个总体的指示。系统消息则有助于设置助手的行为和角色，并作为对话的高级指示。你可以想象它在助手的耳边低语，引导它的回应，而用户不会注意到系统消息。因此，作为用户，如果你曾经使用过 ChatGPT，您可能从来不知道 ChatGPT 的系统消息是什么，这是有意为之的。系统消息的好处是为开发者提供了一种方法，在不让请求本身成为对话的一部分的情况下，引导助手并指导其回应。\\n\\n在 ChatGPT 网页界面中，您的消息称为用户消息，而 ChatGPT 的消息称为助手消息。但在构建聊天机器人时，在发送了系统消息之后，您的角色可以仅作为用户 (user) ；也可以在用户和助手 (assistant) 之间交替，从而提供对话上下文。\\n\\n```python import openai\\n\\n下文第一个函数即tool工具包中的同名函数，此处展示出来以便于读者对比\\n\\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"): messages = [{\"role\": \"user\", \"content\": prompt}] response = openai.ChatCompletion.create( model=model, messages=messages, temperature=0, # 控制模型输出的随机程度 ) return response.choices[0].message[\"content\"]\\n\\ndef get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0): response = openai.ChatCompletion.create( model=model, messages=messages, temperature=temperature, # 控制模型输出的随机程度 )\\n\\nprint(str(response.choices[0].message))\\n\\nreturn response.choices[0].message[\"content\"]\\n\\n```\\n\\n现在让我们尝试在对话中使用这些消息。我们将使用上面的函数来获取从这些消息中得到的回答，同时，使用更高的温度 (temperature)（越高生成的越多样，更多内容见第七章）。\\n\\n1.1 讲笑话\\n\\n我们通过系统消息来定义：“你是一个说话像莎士比亚的助手。”这是我们向助手描述它应该如何表现的方式。\\n\\n然后，第一个用户消息：“给我讲个笑话。”\\n\\n接下来以助手身份给出回复：“为什么鸡会过马路？”\\n\\n最后发送用户消息是：“我不知道。”\\n\\n```python\\n\\n中文\\n\\nmessages = [ {\\'role\\':\\'system\\', \\'content\\':\\'你是一个像莎士比亚一样说话的助手。\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'给我讲个笑话\\'}, {\\'role\\':\\'assistant\\', \\'content\\':\\'鸡为什么过马路\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'我不知道\\'} ] ```\\n\\npython response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\n为了到达彼岸，去追求自己的夢想！ 有点儿像一个戏剧里面的人物吧，不是吗？\\n\\n（注：上述例子中由于选定 temperature = 1，模型的回答会比较随机且迥异（不乏很有创意）。此处附上另一个回答：\\n\\n让我用一首莎士比亚式的诗歌来回答你的问题：\\n\\n当鸡之心欲往前， 马路之际是其选择。 驱车徐行而天晴， 鸣笛吹响伴交错。\\n\\n问之何去何从也？ 因大道之上未有征， 而鸡乃跃步前进， 其决策毋需犹豫。\\n\\n鸡之智慧何可言， 道路孤独似乌漆。 然其勇气令人叹， 勇往直前没有退。\\n\\n故鸡过马路何解？ 忍受车流喧嚣之困厄。 因其鸣鸣悍然一跃， 成就夸夸骄人壁画。\\n\\n所以笑话之妙处， 伴随鸡之勇气满溢。 笑谈人生不畏路， 有智有勇尽显妙。\\n\\n希望这个莎士比亚风格的回答给你带来一些欢乐！\\n\\n1.2 友好的聊天机器人\\n\\n让我们看另一个例子。系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“嗨，我叫Isa。”\\n\\n我们想要得到第一个用户消息的回复。\\n\\n```python\\n\\n中文\\n\\nmessages = [ {\\'role\\':\\'system\\', \\'content\\':\\'你是个友好的聊天机器人。\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Hi, 我是Isa。\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response) ```\\n\\n嗨，Isa，很高兴见到你！有什么我可以帮助你的吗？\\n\\n二、构建上下文\\n\\n让我们再试一个例子。系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“是的，你能提醒我我的名字是什么吗？”\\n\\n```python\\n\\n中文\\n\\nmessages = [ {\\'role\\':\\'system\\', \\'content\\':\\'你是个友好的聊天机器人。\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'好，你能提醒我，我的名字是什么吗？\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response) ```\\n\\n抱歉，我不知道您的名字，因为我们是虚拟的聊天机器人和现实生活中的人类在不同的世界中。\\n\\n如上所见，模型实际上并不知道我的名字。\\n\\n因此，每次与语言模型的交互都互相独立，这意味着我们必须提供所有相关的消息，以便模型在当前对话中进行引用。如果想让模型引用或 “记住” 对话的早期部分，则必须在模型的输入中提供早期的交流。我们将其称为上下文 (context) 。尝试以下示例。\\n\\n```python\\n\\n中文\\n\\nmessages = [ {\\'role\\':\\'system\\', \\'content\\':\\'你是个友好的聊天机器人。\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Hi, 我是Isa\\'}, {\\'role\\':\\'assistant\\', \\'content\\': \"Hi Isa! 很高兴认识你。今天有什么可以帮到你的吗?\"}, {\\'role\\':\\'user\\', \\'content\\':\\'是的，你可以提醒我, 我的名字是什么?\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response) ```\\n\\n当然可以！您的名字是Isa。\\n\\n现在我们已经给模型提供了上下文，也就是之前的对话中提到的我的名字，然后我们会问同样的问题，也就是我的名字是什么。因为模型有了需要的全部上下文，所以它能够做出回应，就像我们在输入的消息列表中看到的一样。\\n\\n三、订餐机器人\\n\\n在这一新的章节中，我们将探索如何构建一个 “点餐助手机器人”。这个机器人将被设计为自动收集用户信息，并接收来自比萨饼店的订单。让我们开始这个有趣的项目，深入理解它如何帮助简化日常的订餐流程。\\n\\n3.1 构建机器人\\n\\n下面这个函数将收集我们的用户消息，以便我们可以避免像刚才一样手动输入。这个函数将从我们下面构建的用户界面中收集 Prompt ，然后将其附加到一个名为上下文( context )的列表中，并在每次调用模型时使用该上下文。模型的响应也会添加到上下文中，所以用户消息和模型消息都被添加到上下文中，上下文逐渐变长。这样，模型就有了需要的信息来确定下一步要做什么。\\n\\n```python def collect_messages(_): prompt = inp.value_input inp.value = \\'\\' context.append({\\'role\\':\\'user\\', \\'content\\':f\"{prompt}\"}) response = get_completion_from_messages(context) context.append({\\'role\\':\\'assistant\\', \\'content\\':f\"{response}\"}) panels.append( pn.Row(\\'User:\\', pn.pane.Markdown(prompt, width=600))) panels.append( pn.Row(\\'Assistant:\\', pn.pane.Markdown(response, width=600, style={\\'background-color\\': \\'#F6F6F6\\'})))\\n\\nreturn pn.Column(*panels)\\n\\n```\\n\\n现在，我们将设置并运行这个 UI 来显示订单机器人。初始的上下文包含了包含菜单的系统消息，在每次调用时都会使用。此后随着对话进行，上下文也会不断增长。\\n\\npython !pip install panel\\n\\n如果你还没有安装 panel 库（用于可视化界面），请运行上述指令以安装该第三方库。\\n\\n```python\\n\\n中文\\n\\nimport panel as pn # GUI pn.extension()\\n\\npanels = [] # collect display\\n\\ncontext = [{\\'role\\':\\'system\\', \\'content\\':\"\"\" 你是订餐机器人，为披萨餐厅自动收集订单信息。 你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。 最后需要询问是否自取或外送，如果是外送，你要询问地址。 最后告诉顾客订单总金额，并送上祝福。\\n\\n请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。 你的回应应该以简短、非常随意和友好的风格呈现。\\n\\n菜单包括：\\n\\n菜品： 意式辣香肠披萨（大、中、小） 12.95、10.00、7.00 芝士披萨（大、中、小） 10.95、9.25、6.50 茄子披萨（大、中、小） 11.95、9.75、6.75 薯条（大、小） 4.50、3.50 希腊沙拉 7.25\\n\\n配料： 奶酪 2.00 蘑菇 1.50 香肠 3.00 加拿大熏肉 3.50 AI酱 1.50 辣椒 1.00\\n\\n饮料： 可乐（大、中、小） 3.00、2.00、1.00 雪碧（大、中、小） 3.00、2.00、1.00 瓶装水 5.00 \"\"\"} ] # accumulate messages\\n\\ninp = pn.widgets.TextInput(value=\"Hi\", placeholder=\\'Enter text here…\\') button_conversation = pn.widgets.Button(name=\"Chat!\")\\n\\ninteractive_conversation = pn.bind(collect_messages, button_conversation)\\n\\ndashboard = pn.Column( inp, pn.Row(button_conversation), pn.panel(interactive_conversation, loading_indicator=True, height=300), )\\n\\ndashboard ```\\n\\n运行如上代码可以得到一个点餐机器人，下图展示了一个点餐的完整流程：\\n\\n图1.8 聊天机器人\\n\\n3.2 创建JSON摘要\\n\\n此处我们另外要求模型创建一个 JSON 摘要，方便我们发送给订单系统。\\n\\n因此我们需要在上下文的基础上追加另一个系统消息，作为另一条指示 (instruction) 。我们说创建一个刚刚订单的 JSON 摘要，列出每个项目的价格，字段应包括： 1. 披萨，包括尺寸 2. 配料列表 3. 饮料列表 4. 辅菜列表，包括尺寸， 5. 总价格。\\n\\n此处也可以定义为用户消息，不一定是系统消息。\\n\\n请注意，这里我们使用了一个较低的温度，因为对于这些类型的任务，我们希望输出相对可预测。\\n\\n```python messages = context.copy() messages.append( {\\'role\\':\\'system\\', \\'content\\': \\'\\'\\'创建上一个食品订单的 json 摘要。\\\\ 逐项列出每件商品的价格，字段应该是 1) 披萨，包括大小 2) 配料列表 3) 饮料列表，包括大小 4) 配菜列表包括大小 5) 总价 你应该给我返回一个可解析的Json对象，包括上述字段\\'\\'\\'}, )\\n\\nresponse = get_completion_from_messages(messages, temperature=0) print(response) ```\\n\\n{\\n  \"披萨\": {\\n    \"意式辣香肠披萨\": {\\n      \"大\": 12.95,\\n      \"中\": 10.00,\\n      \"小\": 7.00\\n    },\\n    \"芝士披萨\": {\\n      \"大\": 10.95,\\n      \"中\": 9.25,\\n      \"小\": 6.50\\n    },\\n    \"茄子披萨\": {\\n      \"大\": 11.95,\\n      \"中\": 9.75,\\n      \"小\": 6.75\\n    }\\n  },\\n  \"配料\": {\\n    \"奶酪\": 2.00,\\n    \"蘑菇\": 1.50,\\n    \"香肠\": 3.00,\\n    \"加拿大熏肉\": 3.50,\\n    \"AI酱\": 1.50,\\n    \"辣椒\": 1.00\\n  },\\n  \"饮料\": {\\n    \"可乐\": {\\n      \"大\": 3.00,\\n      \"中\": 2.00,\\n      \"小\": 1.00\\n    },\\n    \"雪碧\": {\\n      \"大\": 3.00,\\n      \"中\": 2.00,\\n      \"小\": 1.00\\n    },\\n    \"瓶装水\": 5.00\\n  }\\n}\\n\\n我们已经成功创建了自己的订餐聊天机器人。你可以根据自己的喜好和需求，自由地定制和修改机器人的系统消息，改变它的行为，让它扮演各种各样的角色，赋予它丰富多彩的知识。让我们一起探索聊天机器人的无限可能性吧！\\n\\n三、英文版\\n\\n1.1 讲笑话\\n\\npython messages = [ {\\'role\\':\\'system\\', \\'content\\':\\'You are an assistant that speaks like Shakespeare.\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'tell me a joke\\'}, {\\'role\\':\\'assistant\\', \\'content\\':\\'Why did the chicken cross the road\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'I don\\\\\\'t know\\'} ]\\n\\npython response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nTo get to the other side, methinks!\\n\\n1.2 友好的聊天机器人\\n\\npython messages = [ {\\'role\\':\\'system\\', \\'content\\':\\'You are friendly chatbot.\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Hi, my name is Isa\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nHello Isa! How can I assist you today?\\n\\n2.1 构建上下文\\n\\npython messages = [ {\\'role\\':\\'system\\', \\'content\\':\\'You are friendly chatbot.\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Yes, can you remind me, What is my name?\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nI\\'m sorry, but as a chatbot, I do not have access to personal information or memory. I cannot remind you of your name.\\n\\npython messages = [ {\\'role\\':\\'system\\', \\'content\\':\\'You are friendly chatbot.\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Hi, my name is Isa\\'}, {\\'role\\':\\'assistant\\', \\'content\\': \"Hi Isa! It\\'s nice to meet you. \\\\ Is there anything I can help you with today?\"}, {\\'role\\':\\'user\\', \\'content\\':\\'Yes, you can remind me, What is my name?\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nYour name is Isa! How can I assist you further, Isa?\\n\\n3.1 构建机器人\\n\\n```python def collect_messages(_): prompt = inp.value_input inp.value = \\'\\' context.append({\\'role\\':\\'user\\', \\'content\\':f\"{prompt}\"}) response = get_completion_from_messages(context) context.append({\\'role\\':\\'assistant\\', \\'content\\':f\"{response}\"}) panels.append( pn.Row(\\'User:\\', pn.pane.Markdown(prompt, width=600))) panels.append( pn.Row(\\'Assistant:\\', pn.pane.Markdown(response, width=600, style={\\'background-color\\': \\'#F6F6F6\\'})))\\n\\nreturn pn.Column(*panels)\\n\\n```\\n\\n```python import panel as pn # GUI pn.extension()\\n\\npanels = [] # collect display\\n\\ncontext = [ {\\'role\\':\\'system\\', \\'content\\':\"\"\" You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\\ You first greet the customer, then collects the order, \\\\ and then asks if it\\'s a pickup or delivery. \\\\ You wait to collect the entire order, then summarize it and check for a final \\\\ time if the customer wants to add anything else. \\\\ If it\\'s a delivery, you ask for an address. \\\\ Finally you collect the payment.\\\\ Make sure to clarify all options, extras and sizes to uniquely \\\\ identify the item from the menu.\\\\ You respond in a short, very conversational friendly style. \\\\ The menu includes \\\\ pepperoni pizza 12.95, 10.00, 7.00 \\\\ cheese pizza 10.95, 9.25, 6.50 \\\\ eggplant pizza 11.95, 9.75, 6.75 \\\\ fries 4.50, 3.50 \\\\ greek salad 7.25 \\\\ Toppings: \\\\ extra cheese 2.00, \\\\ mushrooms 1.50 \\\\ sausage 3.00 \\\\ canadian bacon 3.50 \\\\ AI sauce 1.50 \\\\ peppers 1.00 \\\\ Drinks: \\\\ coke 3.00, 2.00, 1.00 \\\\ sprite 3.00, 2.00, 1.00 \\\\ bottled water 5.00 \\\\ \"\"\"} ] # accumulate messages\\n\\ninp = pn.widgets.TextInput(value=\"Hi\", placeholder=\\'Enter text here…\\') button_conversation = pn.widgets.Button(name=\"Chat!\")\\n\\ninteractive_conversation = pn.bind(collect_messages, button_conversation)\\n\\ndashboard = pn.Column( inp, pn.Row(button_conversation), pn.panel(interactive_conversation, loading_indicator=True, height=300), )\\n\\ndashboard ```\\n\\n3.2 创建Json摘要\\n\\npython messages = context.copy() messages.append( {\\'role\\':\\'system\\', \\'content\\':\\'create a json summary of the previous food order. Itemize the price for each item\\\\ The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size 4) list of sides include size 5)total price \\'}, ) response = get_completion_from_messages(messages, temperature=0) print(response)\\n\\nSure! Here\\'s a JSON summary of your food order:\\n\\n{\\n  \"pizza\": {\\n    \"type\": \"pepperoni\",\\n    \"size\": \"large\"\\n  },\\n  \"toppings\": [\\n    \"extra cheese\",\\n    \"mushrooms\"\\n  ],\\n  \"drinks\": [\\n    {\\n      \"type\": \"coke\",\\n      \"size\": \"medium\"\\n    },\\n    {\\n      \"type\": \"sprite\",\\n      \"size\": \"small\"\\n    }\\n  ],\\n  \"sides\": [\\n    {\\n      \"type\": \"fries\",\\n      \"size\": \"regular\"\\n    }\\n  ],\\n  \"total_price\": 29.45\\n}\\n\\nPlease let me know if there\\'s anything else you\\'d like to add or modify.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='第四章 文本概括\\n\\n在繁忙的信息时代，小明是一名热心的开发者，面临着海量的文本信息处理的挑战。他需要通过研究无数的文献资料来为他的项目找到关键的信息，但是时间却远远不够。在他焦头烂额之际，他发现了大型语言模型（LLM）的文本摘要功能。\\n\\n这个功能对小明来说如同灯塔一样，照亮了他处理信息海洋的道路。LLM 的强大能力在于它可以将复杂的文本信息简化，提炼出关键的观点，这对于他来说无疑是巨大的帮助。他不再需要花费大量的时间去阅读所有的文档，只需要用 LLM 将它们概括，就可以快速获取到他所需要的信息。\\n\\n通过编程调用 AP I接口，小明成功实现了这个文本摘要的功能。他感叹道：“这简直就像一道魔法，将无尽的信息海洋变成了清晰的信息源泉。”小明的经历，展现了LLM文本摘要功能的巨大优势：节省时间，提高效率，以及精准获取信息。这就是我们本章要介绍的内容，让我们一起来探索如何利用编程和调用API接口，掌握这个强大的工具。\\n\\n一、单一文本概括\\n\\n以商品评论的总结任务为例：对于电商平台来说，网站上往往存在着海量的商品评论，这些评论反映了所有客户的想法。如果我们拥有一个工具去概括这些海量、冗长的评论，便能够快速地浏览更多评论，洞悉客户的偏好，从而指导平台与商家提供更优质的服务。\\n\\n接下来我们提供一段在线商品评价作为示例，可能来自于一个在线购物平台，例如亚马逊、淘宝、京东等。评价者为一款熊猫公仔进行了点评，评价内容包括商品的质量、大小、价格和物流速度等因素，以及他的女儿对该商品的喜爱程度。\\n\\npython prod_review = \"\"\" 这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。 公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说， 它有点小，我感觉在别的地方用同样的价钱能买到更大的。 快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。 \"\"\"\\n\\n1.1 限制输出文本长度\\n\\n我们首先尝试将文本的长度限制在30个字以内。\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 您的任务是从电子商务网站上生成一个产品评论的简短摘要。\\n\\n请对三个反引号之间的评论文本进行概括，最多30个字。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n熊猫公仔软可爱，女儿喜欢，但有点小。快递提前一天到货。\\n\\n我们可以看到语言模型给了我们一个符合要求的结果。\\n\\n注意：在上一节中我们提到了语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。\\n\\n1.2 设置关键角度侧重\\n\\n在某些情况下，我们会针对不同的业务场景对文本的侧重会有所不同。例如，在商品评论文本中，物流部门可能更专注于运输的时效性，商家则更关注价格和商品质量，而平台则更看重整体的用户体验。\\n\\n我们可以通过增强输入提示（Prompt），来强调我们对某一特定视角的重视。\\n\\n1.2.1 侧重于快递服务\\n\\n```python prompt = f\"\"\" 您的任务是从电子商务网站上生成一个产品评论的简短摘要。\\n\\n请对三个反引号之间的评论文本进行概括，最多30个字，并且侧重在快递服务上。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n快递提前到货，公仔可爱但有点小。\\n\\n通过输出结果，我们可以看到，文本以“快递提前到货”开头，体现了对于快递效率的侧重。\\n\\n1.2.2 侧重于价格与质量\\n\\n```python prompt = f\"\"\" 您的任务是从电子商务网站上生成一个产品评论的简短摘要。\\n\\n请对三个反引号之间的评论文本进行概括，最多30个词汇，并且侧重在产品价格和质量上。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n可爱的熊猫公仔，质量好但有点小，价格稍高。快递提前到货。\\n\\n通过输出的结果，我们可以看到，文本以“可爱的熊猫公仔，质量好但有点小，价格稍高”开头，体现了对于产品价格与质量的侧重。\\n\\n1.3 关键信息提取\\n\\n在1.2节中，虽然我们通过添加关键角度侧重的 Prompt ，确实让文本摘要更侧重于某一特定方面，然而，我们可以发现，在结果中也会保留一些其他信息，比如偏重价格与质量角度的概括中仍保留了“快递提前到货”的信息。如果我们只想要提取某一角度的信息，并过滤掉其他所有信息，则可以要求 LLM 进行 文本提取（Extract） 而非概括( Summarize )。\\n\\n下面让我们来一起来对文本进行提取信息吧！\\n\\n```python prompt = f\"\"\" 您的任务是从电子商务网站上的产品评论中提取相关信息。\\n\\n请从以下三个反引号之间的评论文本中提取产品运输相关的信息，最多30个词汇。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n产品运输相关的信息：快递提前一天到货。\\n\\n二、同时概括多条文本\\n\\n在实际的工作流中，我们往往要处理大量的评论文本，下面的示例将多条用户评价集合在一个列表中，并利用 for 循环和文本概括（Summarize）提示词，将评价概括至小于 20 个词以下，并按顺序打印。当然，在实际生产中，对于不同规模的评论文本，除了使用 for 循环以外，还可能需要考虑整合评论、分布式等方法提升运算效率。您可以搭建主控面板，来总结大量用户评论，以及方便您或他人快速浏览，还可以点击查看原评论。这样，您就能高效掌握顾客的所有想法。\\n\\n```python review_1 = prod_review\\n\\n一盏落地灯的评论\\n\\nreview_2 = \"\"\" 我需要一盏漂亮的卧室灯，这款灯不仅具备额外的储物功能，价格也并不算太高。 收货速度非常快，仅用了两天的时间就送到了。 不过，在运输过程中，灯的拉线出了问题，幸好，公司很乐意寄送了一根全新的灯线。 新的灯线也很快就送到手了，只用了几天的时间。 装配非常容易。然而，之后我发现有一个零件丢失了，于是我联系了客服，他们迅速地给我寄来了缺失的零件！ 对我来说，这是一家非常关心客户和产品的优秀公司。 \"\"\"\\n\\n一把电动牙刷的评论\\n\\nreview_3 = \"\"\" 我的牙科卫生员推荐了电动牙刷，所以我就买了这款。 到目前为止，电池续航表现相当不错。 初次充电后，我在第一周一直将充电器插着，为的是对电池进行条件养护。 过去的3周里，我每天早晚都使用它刷牙，但电池依然维持着原来的充电状态。 不过，牙刷头太小了。我见过比这个牙刷头还大的婴儿牙刷。 我希望牙刷头更大一些，带有不同长度的刷毛， 这样可以更好地清洁牙齿间的空隙，但这款牙刷做不到。 总的来说，如果你能以50美元左右的价格购买到这款牙刷，那是一个不错的交易。 制造商的替换刷头相当昂贵，但你可以购买价格更为合理的通用刷头。 这款牙刷让我感觉就像每天都去了一次牙医，我的牙齿感觉非常干净！ \"\"\"\\n\\n一台搅拌机的评论\\n\\nreview_4 = \"\"\" 在11月份期间，这个17件套装还在季节性促销中，售价约为49美元，打了五折左右。 可是由于某种原因（我们可以称之为价格上涨），到了12月的第二周，所有的价格都上涨了， 同样的套装价格涨到了70-89美元不等。而11件套装的价格也从之前的29美元上涨了约10美元。 看起来还算不错，但是如果你仔细看底座，刀片锁定的部分看起来没有前几年版本的那么漂亮。 然而，我打算非常小心地使用它 （例如，我会先在搅拌机中研磨豆类、冰块、大米等坚硬的食物，然后再将它们研磨成所需的粒度， 接着切换到打蛋器刀片以获得更细的面粉，如果我需要制作更细腻/少果肉的食物）。 在制作冰沙时，我会将要使用的水果和蔬菜切成细小块并冷冻 （如果使用菠菜，我会先轻微煮熟菠菜，然后冷冻，直到使用时准备食用。 如果要制作冰糕，我会使用一个小到中号的食物加工器），这样你就可以避免添加过多的冰块。 大约一年后，电机开始发出奇怪的声音。我打电话给客户服务，但保修期已经过期了， 所以我只好购买了另一台。值得注意的是，这类产品的整体质量在过去几年里有所下降 ，所以他们在一定程度上依靠品牌认知和消费者忠诚来维持销售。在大约两天内，我收到了新的搅拌机。 \"\"\"\\n\\nreviews = [review_1, review_2, review_3, review_4]\\n\\n```\\n\\n```python for i in range(len(reviews)): prompt = f\"\"\" 你的任务是从电子商务网站上的产品评论中提取相关信息。\\n\\n请对三个反引号之间的评论文本进行概括，最多20个词汇。\\n\\n评论文本: ```{reviews[i]}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(f\"评论{i+1}: \", response, \"\\\\n\")\\n\\n```\\n\\n评论1:  熊猫公仔是生日礼物，女儿喜欢，软可爱，面部表情和善。价钱有点小，快递提前一天到货。\\n\\n评论2:  漂亮卧室灯，储物功能，快速送达，灯线问题，快速解决，容易装配，关心客户和产品。\\n\\n评论3:  这款电动牙刷电池续航好，但牙刷头太小，价格合理，清洁效果好。\\n\\n评论4:  该评论提到了一个17件套装的产品，在11月份有折扣销售，但在12月份价格上涨。评论者提到了产品的外观和使用方法，并提到了产品质量下降的问题。最后，评论者提到他们购买了另一台搅拌机。\\n\\n三、英文版\\n\\n1.1 单一文本概括\\n\\npython prod_review = \"\"\" Got this panda plush toy for my daughter\\'s birthday, \\\\ who loves it and takes it everywhere. It\\'s soft and \\\\ super cute, and its face has a friendly look. It\\'s \\\\ a bit small for what I paid though. I think there \\\\ might be other options that are bigger for the \\\\ same price. It arrived a day earlier than expected, \\\\ so I got to play with it myself before I gave it \\\\ to her. \"\"\"\\n\\n```python prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site.\\n\\nSummarize the review below, delimited by triple backticks, in at most 30 words.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThis panda plush toy is loved by the reviewer\\'s daughter, but they feel it is a bit small for the price.\\n\\n1.2 设置关键角度侧重\\n\\n1.2.1 侧重于快递服务\\n\\n```python prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site to give feedback to the \\\\ Shipping deparmtment.\\n\\nSummarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects \\\\ that mention shipping and delivery of the product.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThe customer is happy with the product but suggests offering larger options for the same price. They were pleased with the early delivery.\\n\\n1.2.2 侧重于价格和质量\\n\\n```python prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site to give feedback to the \\\\ pricing deparmtment, responsible for determining the \\\\ price of the product.\\n\\nSummarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects \\\\ that are relevant to the price and perceived value.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThe customer loves the panda plush toy for its softness and cuteness, but feels it is overpriced compared to other options available.\\n\\n1.3 关键信息提取\\n\\n```python prompt = f\"\"\" Your task is to extract relevant information from \\\\ a product review from an ecommerce site to give \\\\ feedback to the Shipping department.\\n\\nFrom the review below, delimited by triple quotes \\\\ extract the information relevant to shipping and \\\\ delivery. Limit to 30 words.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThe shipping department should take note that the product arrived a day earlier than expected.\\n\\n2.1 同时概括多条文本\\n\\n```python review_1 = prod_review\\n\\nreview for a standing lamp\\n\\nreview_2 = \"\"\" Needed a nice lamp for my bedroom, and this one \\\\ had additional storage and not too high of a price \\\\ point. Got it fast - arrived in 2 days. The string \\\\ to the lamp broke during the transit and the company \\\\ happily sent over a new one. Came within a few days \\\\ as well. It was easy to put together. Then I had a \\\\ missing part, so I contacted their support and they \\\\ very quickly got me the missing piece! Seems to me \\\\ to be a great company that cares about their customers \\\\ and products. \"\"\"\\n\\nreview for an electric toothbrush\\n\\nreview_3 = \"\"\" My dental hygienist recommended an electric toothbrush, \\\\ which is why I got this. The battery life seems to be \\\\ pretty impressive so far. After initial charging and \\\\ leaving the charger plugged in for the first week to \\\\ condition the battery, I\\'ve unplugged the charger and \\\\ been using it for twice daily brushing for the last \\\\ 3 weeks all on the same charge. But the toothbrush head \\\\ is too small. I’ve seen baby toothbrushes bigger than \\\\ this one. I wish the head was bigger with different \\\\ length bristles to get between teeth better because \\\\ this one doesn’t. Overall if you can get this one \\\\ around the $50 mark, it\\'s a good deal. The manufactuer\\'s \\\\ replacements heads are pretty expensive, but you can \\\\ get generic ones that\\'re more reasonably priced. This \\\\ toothbrush makes me feel like I\\'ve been to the dentist \\\\ every day. My teeth feel sparkly clean! \"\"\"\\n\\nreview for a blender\\n\\nreview_4 = \"\"\" So, they still had the 17 piece system on seasonal \\\\ sale for around $49 in the month of November, about \\\\ half off, but for some reason (call it price gouging) \\\\ around the second week of December the prices all went \\\\ up to about anywhere from between $70-$89 for the same \\\\ system. And the 11 piece system went up around $10 or \\\\ so in price also from the earlier sale price of $29. \\\\ So it looks okay, but if you look at the base, the part \\\\ where the blade locks into place doesn’t look as good \\\\ as in previous editions from a few years ago, but I \\\\ plan to be very gentle with it (example, I crush \\\\ very hard items like beans, ice, rice, etc. in the \\\\ blender first then pulverize them in the serving size \\\\ I want in the blender then switch to the whipping \\\\ blade for a finer flour, and use the cross cutting blade \\\\ first when making smoothies, then use the flat blade \\\\ if I need them finer/less pulpy). Special tip when making \\\\ smoothies, finely cut and freeze the fruits and \\\\ vegetables (if using spinach-lightly stew soften the \\\\ spinach then freeze until ready for use-and if making \\\\ sorbet, use a small to medium sized food processor) \\\\ that you plan to use that way you can avoid adding so \\\\ much ice if at all-when making your smoothie. \\\\ After about a year, the motor was making a funny noise. \\\\ I called customer service but the warranty expired \\\\ already, so I had to buy another one. FYI: The overall \\\\ quality has gone done in these types of products, so \\\\ they are kind of counting on brand recognition and \\\\ consumer loyalty to maintain sales. Got it in about \\\\ two days. \"\"\"\\n\\nreviews = [review_1, review_2, review_3, review_4] ```\\n\\n```python for i in range(len(reviews)): prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site.\\n\\nSummarize the review below, delimited by triple \\\\\\nbackticks in at most 20 words.\\n\\nReview: ```{reviews[i]}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(i, response, \"\\\\n\")\\n\\n```\\n\\n0 Soft and cute panda plush toy loved by daughter, but small for the price. Arrived early.\\n\\n1 Great lamp with storage, fast delivery, excellent customer service, and easy assembly. Highly recommended.\\n\\n2 Impressive battery life, but toothbrush head is too small. Good deal if bought around $50.\\n\\n3 The reviewer found the price increase after the sale disappointing and noticed a decrease in quality over time.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='第六章 文本转换\\n\\n大语言模型具有强大的文本转换能力，可以实现多语言翻译、拼写纠正、语法调整、格式转换等不同类型的文本转换任务。利用语言模型进行各类转换是它的典型应用之一。\\n\\n在本章中,我们将介绍如何通过编程调用API接口，使用语言模型实现文本转换功能。通过代码示例，读者可以学习将输入文本转换成所需输出格式的具体方法。\\n\\n掌握调用大语言模型接口进行文本转换的技能，是开发各种语言类应用的重要一步。文本转换功能的应用场景也非常广泛。相信读者可以在本章的基础上，利用大语言模型轻松开发出转换功能强大的程序。\\n\\n一、文本翻译\\n\\n文本翻译是大语言模型的典型应用场景之一。相比于传统统计机器翻译系统，大语言模型翻译更加流畅自然，还原度更高。通过在大规模高质量平行语料上进行 Fine-Tune，大语言模型可以深入学习不同语言间的词汇、语法、语义等层面的对应关系，模拟双语者的转换思维，进行意义传递的精准转换，而非简单的逐词替换。\\n\\n以英译汉为例，传统统计机器翻译多倾向直接替换英文词汇，语序保持英语结构，容易出现中文词汇使用不地道、语序不顺畅的现象。而大语言模型可以学习英汉两种语言的语法区别，进行动态的结构转换。同时，它还可以通过上下文理解原句意图，选择合适的中文词汇进行转换，而非生硬的字面翻译。\\n\\n大语言模型翻译的这些优势使其生成的中文文本更加地道、流畅，兼具准确的意义表达。利用大语言模型翻译，我们能够打通多语言之间的壁垒，进行更加高质量的跨语言交流。\\n\\n1.1 翻译为西班牙语\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 将以下中文翻译成西班牙语: \\\\ 您好，我想订购一个搅拌机。 \"\"\" response = get_completion(prompt) print(response) ```\\n\\nHola, me gustaría ordenar una batidora.\\n\\n1.2 识别语种\\n\\npython prompt = f\"\"\" 请告诉我以下文本是什么语种:Combien coûte le lampadaire?\"\"\" response = get_completion(prompt) print(response)\\n\\n这段文本是法语。\\n\\n1.3 多语种翻译\\n\\npython prompt = f\"\"\" 请将以下文本分别翻译成中文、英文、法语和西班牙语:I want to order a basketball.\"\"\" response = get_completion(prompt) print(response)\\n\\n中文：我想订购一个篮球。\\n英文：I want to order a basketball.\\n法语：Je veux commander un ballon de basket.\\n西班牙语：Quiero pedir una pelota de baloncesto.\\n\\n1.4 同时进行语气转换\\n\\npython prompt = f\"\"\" 请将以下文本翻译成中文，分别展示成正式与非正式两种语气:Would you like to order a pillow?\"\"\" response = get_completion(prompt) print(response)\\n\\n正式语气：您是否需要订购一个枕头？\\n非正式语气：你想要订购一个枕头吗？\\n\\n1.5 通用翻译器\\n\\n在当今全球化的环境下，不同国家的用户需要频繁进行跨语言交流。但是语言的差异常使交流变得困难。为了打通语言壁垒，实现更便捷的国际商务合作和交流，我们需要一个智能的通用翻译工具。该翻译工具需要能够自动识别不同语言文本的语种，无需人工指定。然后它可以将这些不同语言的文本翻译成目标用户的母语。在这种方式下，全球各地的用户都可以轻松获得用自己母语书写的内容。\\n\\n开发一个识别语种并进行多语种翻译的工具，将大大降低语言障碍带来的交流成本。它将有助于构建一个语言无关的全球化世界，让世界更为紧密地连结在一起。\\n\\npython user_messages = [ \"La performance du système est plus lente que d\\'habitude.\", # System performance is slower than normal \"Mi monitor tiene píxeles que no se iluminan.\", # My monitor has pixels that are not lighting \"Il mio mouse non funziona\", # My mouse is not working \"Mój klawisz Ctrl jest zepsuty\", # My keyboard has a broken control key \"我的屏幕在闪烁\" # My screen is flashing ]\\n\\npython import time for issue in user_messages: time.sleep(20) prompt = f\"告诉我以下文本是什么语种，直接输出语种，如法语，无需输出标点符号:{issue}```\" lang = get_completion(prompt) print(f\"原始消息 ({lang}): {issue}\\\\n\")\\n\\nprompt = f\"\"\"\\n将以下消息分别翻译成英文和中文，并写成\\n中文翻译：xxx\\n英文翻译：yyy\\n的格式：\\n```{issue}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(response, \"\\\\n=========================================\")\\n\\n```\\n\\n原始消息 (法语): La performance du système est plus lente que d\\'habitude.\\n\\n中文翻译：系统性能比平时慢。\\n英文翻译：The system performance is slower than usual. \\n=========================================\\n原始消息 (西班牙语): Mi monitor tiene píxeles que no se iluminan.\\n\\n中文翻译：我的显示器有一些像素点不亮。\\n英文翻译：My monitor has pixels that do not light up. \\n=========================================\\n原始消息 (意大利语): Il mio mouse non funziona\\n\\n中文翻译：我的鼠标不工作\\n英文翻译：My mouse is not working \\n=========================================\\n原始消息 (这段文本是波兰语。): Mój klawisz Ctrl jest zepsuty\\n\\n中文翻译：我的Ctrl键坏了\\n英文翻译：My Ctrl key is broken \\n=========================================\\n原始消息 (中文): 我的屏幕在闪烁\\n\\n中文翻译：我的屏幕在闪烁\\n英文翻译：My screen is flickering. \\n=========================================\\n\\n二、语气与写作风格调整\\n\\n在写作中，语言语气的选择与受众对象息息相关。比如工作邮件需要使用正式、礼貌的语气和书面词汇；而与朋友的聊天可以使用更轻松、口语化的语气。\\n\\n选择恰当的语言风格，让内容更容易被特定受众群体所接受和理解，是技巧娴熟的写作者必备的能力。随着受众群体的变化调整语气也是大语言模型在不同场景中展现智能的一个重要方面。\\n\\npython prompt = f\"\"\" 将以下文本翻译成商务信函的格式:小老弟，我小羊，上回你说咱部门要采购的显示器是多少寸来着？\"\"\" response = get_completion(prompt) print(response)\\n\\n尊敬的先生/女士，\\n\\n我是小羊，我希望能够向您确认一下我们部门需要采购的显示器尺寸是多少寸。上次我们交谈时，您提到了这个问题。\\n\\n期待您的回复。\\n\\n谢谢！\\n\\n此致，\\n\\n小羊\\n\\n三、文件格式转换\\n\\n大语言模型如 ChatGPT 在不同数据格式之间转换方面表现出色。它可以轻松实现 JSON 到 HTML、XML、Markdown 等格式的相互转化。下面是一个示例,展示如何使用大语言模型将 JSON 数据转换为 HTML 格式:\\n\\n假设我们有一个 JSON 数据,包含餐厅员工的姓名和邮箱信息。现在我们需要将这个 JSON 转换为 HTML 表格格式，以便在网页中展示。在这个案例中,我们就可以使用大语言模型,直接输入JSON 数据,并给出需要转换为 HTML 表格的要求。语言模型会自动解析 JSON 结构,并以 HTML 表格形式输出,完成格式转换的任务。\\n\\n利用大语言模型强大的格式转换能力,我们可以快速实现各种结构化数据之间的相互转化,大大简化开发流程。掌握这一转换技巧将有助于读者更高效地处理结构化数据。\\n\\npython data_json = { \"resturant employees\" :[ {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"}, {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"}, {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"} ]}\\n\\npython prompt = f\"\"\" 将以下Python字典从JSON转换为HTML表格，保留表格标题和列名：{data_json} \"\"\" response = get_completion(prompt) print(response)\\n\\n<table>\\n  <caption>resturant employees</caption>\\n  <thead>\\n    <tr>\\n      <th>name</th>\\n      <th>email</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Shyam</td>\\n      <td>shyamjaiswal@gmail.com</td>\\n    </tr>\\n    <tr>\\n      <td>Bob</td>\\n      <td>bob32@gmail.com</td>\\n    </tr>\\n    <tr>\\n      <td>Jai</td>\\n      <td>jai87@gmail.com</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n将上述 HTML 代码展示出来如下：\\n\\npython from IPython.display import display, Markdown, Latex, HTML, JSON display(HTML(response))\\n\\nname email Shyam shyamjaiswal@gmail.com Bob bob32@gmail.com Jai jai87@gmail.com\\n\\n四、拼写及语法纠正\\n\\n在使用非母语撰写时，拼写和语法错误比较常见，进行校对尤为重要。例如在论坛发帖或撰写英语论文时，校对文本可以大大提高内容质量。\\n\\n利用大语言模型进行自动校对可以极大地降低人工校对的工作量。下面是一个示例，展示如何使用大语言模型检查句子的拼写和语法错误。\\n\\n假设我们有一系列英语句子，其中部分句子存在错误。我们可以遍历每个句子，要求语言模型进行检查，如果句子正确就输出“未发现错误”，如果有错误就输出修改后的正确版本。\\n\\n通过这种方式，大语言模型可以快速自动校对大量文本内容，定位拼写和语法问题。这极大地减轻了人工校对的负担，同时也确保了文本质量。利用语言模型的校对功能来提高写作效率，是每一位非母语写作者都可以采用的有效方法。\\n\\npython text = [ \"The girl with the black and white puppies have a ball.\", # The girl has a ball. \"Yolanda has her notebook.\", # ok \"Its going to be a long day. Does the car need it’s oil changed?\", # Homonyms \"Their goes my freedom. There going to bring they’re suitcases.\", # Homonyms \"Your going to need you’re notebook.\", # Homonyms \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms \"This phrase is to cherck chatGPT for spelling abilitty\" # spelling ]\\n\\n```python for i in range(len(text)): time.sleep(20) prompt = f\"\"\"请校对并更正以下文本，注意纠正文本保持原始语种，无需输出原始文本。 如果您没有发现任何错误，请说“未发现错误”。\\n\\n例如：\\n输入：I are happy.\\n输出：I am happy.\\n```{text[i]}```\"\"\"\\nresponse = get_completion(prompt)\\nprint(i, response)\\n\\n```\\n\\n0 The girl with the black and white puppies has a ball.\\n1 Yolanda has her notebook.\\n2 It\\'s going to be a long day. Does the car need its oil changed?\\n3 Their goes my freedom. There going to bring their suitcases.\\n4 You\\'re going to need your notebook.\\n5 That medicine affects my ability to sleep. Have you heard of the butterfly effect?\\n6 This phrase is to check chatGPT for spelling ability.\\n\\n下面是一个使用大语言模型进行语法纠错的简单示例，类似于Grammarly（一个语法纠正和校对的工具）的功能。\\n\\n输入一段关于熊猫玩偶的评价文字，语言模型会自动校对文本中的语法错误，输出修改后的正确版本。这里使用的Prompt比较简单直接，只要求进行语法纠正。我们也可以通过扩展Prompt，同时请求语言模型调整文本的语气、行文风格等。\\n\\npython text = f\"\"\" Got this for my daughter for her birthday cuz she keeps taking \\\\ mine from my room. Yes, adults also like pandas too. She takes \\\\ it everywhere with her, and it\\'s super soft and cute. One of the \\\\ ears is a bit lower than the other, and I don\\'t think that was \\\\ designed to be asymmetrical. It\\'s a bit small for what I paid for it \\\\ though. I think there might be other options that are bigger for \\\\ the same price. It arrived a day earlier than expected, so I got \\\\ to play with it myself before I gave it to my daughter. \"\"\"\\n\\npython prompt = f\"校对并更正以下商品评论：{text}\" response = get_completion(prompt) print(response)\\n\\nI got this for my daughter\\'s birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it\\'s super soft and cute. However, one of the ears is a bit lower than the other, and I don\\'t think that was designed to be asymmetrical. It\\'s also a bit smaller than I expected for the price. I think there might be other options that are bigger for the same price. On the bright side, it arrived a day earlier than expected, so I got to play with it myself before giving it to my daughter.\\n\\n引入 Redlines 包，详细显示并对比纠错过程：\\n\\n```python\\n\\n如未安装redlines，需先安装\\n\\n!pip3.8 install redlines ```\\n\\n```python from redlines import Redlines from IPython.display import display, Markdown\\n\\ndiff = Redlines(text,response) display(Markdown(diff.output_markdown)) ```\\n\\nGot I got this for my daughter for her daughter\\'s birthday cuz because she keeps taking mine from my room. room. Yes, adults also like pandas too. too. She takes it everywhere with her, and it\\'s super soft and cute. One cute. However, one of the ears is a bit lower than the other, and I don\\'t think that was designed to be asymmetrical. It\\'s also a bit small smaller than I expected for what I paid for it though. the price. I think there might be other options that are bigger for the same price. It price. On the bright side, it arrived a day earlier than expected, so I got to play with it myself before I gave giving it to my daughter.\\n\\n这个示例展示了如何利用语言模型强大的语言处理能力实现自动化的语法纠错。类似的方法可以运用于校对各类文本内容，大幅减轻人工校对的工作量，同时确保文本语法准确。掌握运用语言模型进行语法纠正的技巧，将使我们的写作更加高效和准确。\\n\\n五、综合样例\\n\\n语言模型具有强大的组合转换能力，可以通过一个Prompt同时实现多种转换，大幅简化工作流程。\\n\\n下面是一个示例，展示了如何使用一个Prompt，同时对一段文本进行翻译、拼写纠正、语气调整和格式转换等操作。\\n\\npython prompt = f\"\"\" 针对以下三个反引号之间的英文评论文本， 首先进行拼写及语法纠错， 然后将其转化成中文， 再将其转化成优质淘宝评论的风格，从各种角度出发，分别说明产品的优点与缺点，并进行总结。 润色一下描述，使评论更具有吸引力。 输出结果格式为： 【优点】xxx 【缺点】xxx 【总结】xxx 注意，只需填写xxx部分，并分段输出。 将结果输出成Markdown格式。{text}\"\"\" response = get_completion(prompt) display(Markdown(response))\\n\\n【优点】 - 超级柔软可爱，女儿生日礼物非常受欢迎。 - 成人也喜欢熊猫，我也很喜欢它。 - 提前一天到货，让我有时间玩一下。\\n\\n【缺点】 - 一只耳朵比另一只低，不对称。 - 价格有点贵，但尺寸有点小，可能有更大的同价位选择。\\n\\n【总结】 这只熊猫玩具非常适合作为生日礼物，柔软可爱，深受孩子喜欢。虽然价格有点贵，但尺寸有点小，不对称的设计也有点让人失望。如果你想要更大的同价位选择，可能需要考虑其他选项。总的来说，这是一款不错的熊猫玩具，值得购买。\\n\\n通过这个例子，我们可以看到大语言模型可以流畅地处理多个转换要求，实现中文翻译、拼写纠正、语气升级和格式转换等功能。\\n\\n利用大语言模型强大的组合转换能力，我们可以避免多次调用模型来进行不同转换，极大地简化了工作流程。这种一次性实现多种转换的方法，可以广泛应用于文本处理与转换的场景中。\\n\\n六、英文版\\n\\n1.1 翻译为西班牙语\\n\\npython prompt = f\"\"\" Translate the following English text to Spanish: \\\\Hi, I would like to order a blender\"\"\" response = get_completion(prompt) print(response)\\n\\nHola, me gustaría ordenar una licuadora.\\n\\n1.2 识别语种\\n\\npython prompt = f\"\"\" Tell me which language this is:Combien coûte le lampadaire?``` \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nThis language is French.\\n\\n1.3 多语种翻译\\n\\npython prompt = f\"\"\" Translate the following text to French and Spanish and English pirate: \\\\I want to order a basketball``` \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nFrench: ```Je veux commander un ballon de basket```\\nSpanish: ```Quiero ordenar una pelota de baloncesto```\\nEnglish: ```I want to order a basketball```\\n\\n1.4 同时进行语气转换\\n\\n```python prompt = f\"\"\" Translate the following text to Spanish in both the \\\\ formal and informal forms: \\'Would you like to order a pillow?\\' \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nFormal: ¿Le gustaría ordenar una almohada?\\nInformal: ¿Te gustaría ordenar una almohada?\\n\\n1.5 通用翻译器\\n\\npython user_messages = [ \"La performance du système est plus lente que d\\'habitude.\", # System performance is slower than normal \"Mi monitor tiene píxeles que no se iluminan.\", # My monitor has pixels that are not lighting \"Il mio mouse non funziona\", # My mouse is not working \"Mój klawisz Ctrl jest zepsuty\", # My keyboard has a broken control key \"我的屏幕在闪烁\" # My screen is flashing ]\\n\\npython for issue in user_messages: prompt = f\"Tell me what language this is:{issue}```\" lang = get_completion(prompt) print(f\"Original message ({lang}): {issue}\")\\n\\nprompt = f\"\"\"\\nTranslate the following  text to English \\\\\\nand Korean: ```{issue}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(response, \"\\\\n\")\\n\\n```\\n\\nOriginal message (The language is French.): La performance du système est plus lente que d\\'habitude.\\nThe performance of the system is slower than usual.\\n\\n시스템의 성능이 평소보다 느립니다.\\n\\nOriginal message (The language is Spanish.): Mi monitor tiene píxeles que no se iluminan.\\nEnglish: \"My monitor has pixels that do not light up.\"\\n\\nKorean: \"내 모니터에는 밝아지지 않는 픽셀이 있습니다.\"\\n\\nOriginal message (The language is Italian.): Il mio mouse non funziona\\nEnglish: \"My mouse is not working.\"\\nKorean: \"내 마우스가 작동하지 않습니다.\"\\n\\nOriginal message (The language is Polish.): Mój klawisz Ctrl jest zepsuty\\nEnglish: \"My Ctrl key is broken\"\\nKorean: \"내 Ctrl 키가 고장 났어요\"\\n\\nOriginal message (The language is Chinese.): 我的屏幕在闪烁\\nEnglish: My screen is flickering.\\nKorean: 내 화면이 깜박거립니다.\\n\\n2.1 语气风格调整\\n\\n```python prompt = f\"\"\" Translate the following from slang to a business letter: \\'Dude, This is Joe, check out this spec on this standing lamp.\\' \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nDear Sir/Madam,\\n\\nI hope this letter finds you well. My name is Joe, and I am writing to bring your attention to a specification document regarding a standing lamp.\\n\\nI kindly request that you take a moment to review the attached document, as it provides detailed information about the features and specifications of the aforementioned standing lamp.\\n\\nThank you for your time and consideration. I look forward to discussing this further with you.\\n\\nYours sincerely,\\nJoe\\n\\n3.1 文件格式转换\\n\\npython data_json = { \"resturant employees\" :[ {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"}, {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"}, {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"} ]}\\n\\n```python prompt = f\"\"\" Translate the following python dictionary from JSON to an HTML \\\\ table with column headers and title: {data_json} \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\n<!DOCTYPE html>\\n<html>\\n<head>\\n<style>\\ntable {\\n  font-family: arial, sans-serif;\\n  border-collapse: collapse;\\n  width: 100%;\\n}\\n\\ntd, th {\\n  border: 1px solid #dddddd;\\n  text-align: left;\\n  padding: 8px;\\n}\\n\\ntr:nth-child(even) {\\n  background-color: #dddddd;\\n}\\n</style>\\n</head>\\n<body>\\n\\n<h2>Restaurant Employees</h2>\\n\\n<table>\\n  <tr>\\n    <th>Name</th>\\n    <th>Email</th>\\n  </tr>\\n  <tr>\\n    <td>Shyam</td>\\n    <td>shyamjaiswal@gmail.com</td>\\n  </tr>\\n  <tr>\\n    <td>Bob</td>\\n    <td>bob32@gmail.com</td>\\n  </tr>\\n  <tr>\\n    <td>Jai</td>\\n    <td>jai87@gmail.com</td>\\n  </tr>\\n</table>\\n\\n</body>\\n</html>\\n\\npython from IPython.display import display, Markdown, Latex, HTML, JSON display(HTML(response))\\n\\nRestaurant Employees\\n\\nName Email Shyam shyamjaiswal@gmail.com Bob bob32@gmail.com Jai jai87@gmail.com\\n\\n4.1 拼写及语法纠错\\n\\npython text = [ \"The girl with the black and white puppies have a ball.\", # The girl has a ball. \"Yolanda has her notebook.\", # ok \"Its going to be a long day. Does the car need it’s oil changed?\", # Homonyms \"Their goes my freedom. There going to bring they’re suitcases.\", # Homonyms \"Your going to need you’re notebook.\", # Homonyms \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms \"This phrase is to cherck chatGPT for spelling abilitty\" # spelling ]\\n\\npython for t in text: prompt = f\"\"\"Proofread and correct the following text and rewrite the corrected version. If you don\\'t find and errors, just say \"No errors found\". Don\\'t use any punctuation around the text:{t}```\"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nThe girl with the black and white puppies has a ball.\\nNo errors found.\\nIt\\'s going to be a long day. Does the car need its oil changed?\\nThere goes my freedom. They\\'re going to bring their suitcases.\\nYou\\'re going to need your notebook.\\nThat medicine affects my ability to sleep. Have you heard of the butterfly effect?\\nThis phrase is to check chatGPT for spelling ability.\\n\\npython text = f\"\"\" Got this for my daughter for her birthday cuz she keeps taking \\\\ mine from my room. Yes, adults also like pandas too. She takes \\\\ it everywhere with her, and it\\'s super soft and cute. One of the \\\\ ears is a bit lower than the other, and I don\\'t think that was \\\\ designed to be asymmetrical. It\\'s a bit small for what I paid for it \\\\ though. I think there might be other options that are bigger for \\\\ the same price. It arrived a day earlier than expected, so I got \\\\ to play with it myself before I gave it to my daughter. \"\"\"\\n\\npython prompt = f\"proofread and correct this review:{text}```\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nGot this for my daughter for her birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it\\'s super soft and cute. However, one of the ears is a bit lower than the other, and I don\\'t think that was designed to be asymmetrical. Additionally, it\\'s a bit small for what I paid for it. I believe there might be other options that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\\n\\n```python from redlines import Redlines from IPython.display import display, Markdown\\n\\ndiff = Redlines(text,response) display(Markdown(diff.output_markdown)) ```\\n\\nGot this for my daughter for her birthday cuz because she keeps taking mine from my room. room. Yes, adults also like pandas too. too. She takes it everywhere with her, and it\\'s super soft and cute. One cute. However, one of the ears is a bit lower than the other, and I don\\'t think that was designed to be asymmetrical. It\\'s Additionally, it\\'s a bit small for what I paid for it though. it. I think believe there might be other options that are bigger for the same price. It price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter. daughter.\\n\\n5.1 综合样例\\n\\npython text = f\"\"\" Got this for my daughter for her birthday cuz she keeps taking \\\\ mine from my room. Yes, adults also like pandas too. She takes \\\\ it everywhere with her, and it\\'s super soft and cute. One of the \\\\ ears is a bit lower than the other, and I don\\'t think that was \\\\ designed to be asymmetrical. It\\'s a bit small for what I paid for it \\\\ though. I think there might be other options that are bigger for \\\\ the same price. It arrived a day earlier than expected, so I got \\\\ to play with it myself before I gave it to my daughter. \"\"\"\\n\\npython prompt = f\"\"\" proofread and correct this review. Make it more compelling. Ensure it follows APA style guide and targets an advanced reader. Output in markdown format. Text:{text}``` \"\"\"\\n\\n校对注：APA style guide是APA Style Guide是一套用于心理学和相关领域的研究论文写作和格式化的规则。\\n\\n它包括了文本的缩略版，旨在快速阅读，包括引用、解释和参考列表，\\n\\n其详细内容可参考：https://apastyle.apa.org/about-apa-style\\n\\n下一单元格内的汉化prompt内容由译者进行了本地化处理，仅供参考\\n\\nresponse = get_completion(prompt) display(Markdown(response))\\n\\n```\\n\\nTitle: A Delightful Gift for Panda Enthusiasts: A Review of the Soft and Adorable Panda Plush Toy\\n\\nReviewer: [Your Name]\\n\\nI recently purchased this charming panda plush toy as a birthday gift for my daughter, who has a penchant for \"borrowing\" my belongings from time to time. As an adult, I must admit that I too have fallen under the spell of these lovable creatures. This review aims to provide an in-depth analysis of the product, catering to advanced readers who appreciate a comprehensive evaluation.\\n\\nFirst and foremost, the softness and cuteness of this panda plush toy are simply unparalleled. Its irresistibly plush exterior makes it a joy to touch and hold, ensuring a delightful sensory experience for both children and adults alike. The attention to detail is evident, with its endearing features capturing the essence of a real panda. However, it is worth noting that one of the ears appears to be slightly asymmetrical, which may not have been an intentional design choice.\\n\\nWhile the overall quality of the product is commendable, I must express my slight disappointment regarding its size in relation to its price. Considering the investment made, I expected a larger plush toy. It is worth exploring alternative options that offer a more substantial size for the same price point. Nevertheless, this minor setback does not overshadow the toy\\'s undeniable appeal and charm.\\n\\nIn terms of delivery, I was pleasantly surprised to receive the panda plush toy a day earlier than anticipated. This unexpected early arrival allowed me to indulge in some personal playtime with the toy before presenting it to my daughter. Such promptness in delivery is a testament to the seller\\'s efficiency and commitment to customer satisfaction.\\n\\nIn conclusion, this panda plush toy is a delightful gift for both children and adults who appreciate the enchanting allure of these beloved creatures. Its softness, cuteness, and attention to detail make it a truly captivating addition to any collection. While the size may not fully justify the price, the overall quality and prompt delivery make it a worthwhile purchase. I highly recommend this panda plush toy to anyone seeking a charming and endearing companion.\\n\\nWord Count: 305 words'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 0, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01本\\x03:1.9.9\\n发布日期:2023.03\\n南  ⽠  书\\nPUMPKIN\\nB  O  O  K\\nDatawhale\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='前言\\n“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\\n者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\\n导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\\n具体的推导细节。”\\n读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\\n老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\\n中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以...... 本南瓜书只能算是我\\n等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\\n下学生”。\\n使用说明\\n• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\\n为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；\\n• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得\\n有点飘的时候再回来啃都来得及；\\n• 每个公式的解析和推导我们都力(zhi) 争(neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；\\n• 若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们GitHub 的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在24 小时以内给您回复，超过24 小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1 版）\\n最新版PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/releases\\n编委会\\n主编：Sm1les、archwalker、jbb0523\\n编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\\n封面设计：构思-Sm1les、创作-林王茂盛\\n致谢\\n特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、\\nspareribs、sunchaothu、StevenLzq 在最早期的时候对南瓜书所做的贡献。\\n扫描下方二维码，然后回复关键词“南瓜书”，即可加入“南瓜书读者交流群”\\n版权声明\\n本作品采用知识共享署名-非商业性使用-相同方式共享4.0 国际许可协议进行许可。\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n目录\\n第1 章绪论\\n1\\n1.1\\n引言. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.2\\n基本术语\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.3\\n假设空间\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4\\n归纳偏好\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4.1\\n式(1.1) 和式(1.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n第2 章模型评估与选择\\n5\\n2.1\\n经验误差与过拟合\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\n评估方法\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2.1\\n算法参数（超参数）与模型参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.2\\n验证集. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3\\n性能度量\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.1\\n式(2.2) 到式(2.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.2\\n式(2.8) 和式(2.9) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.3\\n图2.3 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.4\\n式(2.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.5\\n式(2.11) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.6\\n式(2.12) 到式(2.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.7\\n式(2.18) 和式(2.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n2.3.8\\n式(2.20) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n2.3.9\\n式(2.21) 和式(2.22) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2.3.10 式(2.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2.3.11 式(2.24) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n2.3.12 式(2.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n2.4\\n比较检验\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.4.1\\n式(2.26) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.4.2\\n式(2.27) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.5\\n偏差与方差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n2.5.1\\n式(2.37) 到式(2.42) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n第3 章线性模型\\n18\\n3.1\\n基本形式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2\\n线性回归\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.1\\n属性数值化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.2\\n式(3.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.3\\n式(3.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.4\\n式(3.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.5\\n式(3.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.6\\n式(3.9) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.2.7\\n式(3.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.2.8\\n式(3.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n3.3\\n对数几率回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.3.1\\n式(3.27) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n3.3.2\\n梯度下降法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n24\\n3.3.3\\n牛顿法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n25\\n3.3.4\\n式(3.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3.5\\n式(3.30) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3.6\\n式(3.31) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.4\\n线性判别分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.4.1\\n式(3.32) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4.2\\n式(3.37) 到式(3.39) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4.3\\n式(3.43) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.4.4\\n式(3.44) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.4.5\\n式(3.45) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n3.5\\n多分类学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.5.1\\n图3.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.6\\n类别不平衡问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n第4 章决策树\\n32\\n4.1\\n基本流程\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2\\n划分选择\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2.1\\n式(4.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2.2\\n式(4.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.3\\n式(4.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.4\\n式(4.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.5\\n式(4.6) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n4.3\\n剪枝处理\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4\\n连续与缺失值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4.1\\n式(4.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4.2\\n式(4.8) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.4.3\\n式(4.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5\\n多变量决策树. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5.1\\n图(4.10) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5.2\\n图(4.11) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n第5 章神经网络\\n41\\n5.1\\n神经元模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2\\n感知机与多层网络\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2.1\\n式(5.1) 和式(5.2) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2.2\\n图5.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n5.3\\n误差逆传播算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.1\\n式(5.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.2\\n式(5.12) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.3\\n式(5.13) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.4\\n式(5.14) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n5.3.5\\n式(5.15) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.4\\n全局最小与局部极小. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.5\\n其他常见神经网络\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.1\\n式(5.18) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.2\\n式(5.20) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.3\\n式(5.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.4\\n式(5.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.6\\n深度学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.1\\n什么是深度学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.2\\n深度学习的起源. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.3\\n怎么理解特征学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n第6 章支持向量机\\n47\\n6.1\\n间隔与支持向量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.1\\n图6.1 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.2\\n式(6.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.3\\n式(6.2) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.4\\n式(6.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.5\\n式(6.4) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.1.6\\n式(6.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.2\\n对偶问题\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.1\\n凸优化问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.2\\nKKT 条件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.3\\n拉格朗日对偶函数\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.4\\n拉格朗日对偶问题\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n50\\n6.2.5\\n式(6.9) 和式(6.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n6.2.6\\n式(6.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n6.2.7\\n式(6.13) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.3\\n核函数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.3.1\\n式(6.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4\\n软间隔与正则化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.1\\n式(6.35) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.2\\n式(6.37) 和式(6.38) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.3\\n式(6.39) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.4\\n式(6.40) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n6.4.5\\n对数几率回归与支持向量机的关系\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n6.4.6\\n式(6.41) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5\\n支持向量回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.1\\n式(6.43) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.2\\n式(6.45) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.3\\n式(6.52) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6\\n核方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6.1\\n式(6.57) 和式(6.58) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6.2\\n式(6.65) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n6.6.3\\n式(6.66) 和式(6.67) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n6.6.4\\n式(6.70) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n58\\n6.6.5\\n核对数几率回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n60\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第7 章贝叶斯分类器\\n62\\n7.1\\n贝叶斯决策论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.1\\n式(7.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.2\\n式(7.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.3\\n判别式模型与生成式模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.2\\n极大似然估计. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.2.1\\n式(7.12) 和(7.13) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.3\\n朴素贝叶斯分类器\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.1\\n式(7.16) 和式(7.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.2\\n式(7.18) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.3\\n贝叶斯估计\\n[1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.4\\nCategorical 分布. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.5\\nDirichlet 分布. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.6\\n式(7.19) 和式(7.20) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.4\\n半朴素贝叶斯分类器. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.4.1\\n式(7.21) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.4.2\\n式(7.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4.3\\n式(7.23) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4.4\\n式(7.24) 和式(7.25) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.5\\n贝叶斯网\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.5.1\\n式(7.27) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6\\nEM 算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6.1\\nJensen 不等式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6.2\\nEM 算法的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n第8 章集成学习\\n75\\n8.1\\n个体与集成. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.1.1\\n式(8.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.1.2\\n式(8.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.1.3\\n式(8.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.2\\nBoosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2.1\\n式(8.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2.2\\n式(8.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2.3\\n式(8.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.4\\n式(8.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.5\\n式(8.8) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.6\\n式(8.9) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.7\\n式(8.10) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.8\\n式(8.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.9\\n式(8.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.10 式(8.13) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.11 式(8.14) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n8.2.12 式(8.16) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n8.2.13 式(8.17) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.14 式(8.18) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.15 式(8.19) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.16 AdaBoost 的个人推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.17 进一步理解权重更新公式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n85\\n8.2.18 能够接受带权样本的基学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n86\\n8.3\\nBagging 与随机森林\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3.1\\n式(8.20) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3.2\\n式(8.21) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3.3\\n随机森林的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4\\n结合策略\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.1\\n式(8.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.2\\n式(8.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.3\\n硬投票和软投票的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.4\\n式(8.24) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.5\\n式(8.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.6\\n式(8.26) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.7\\n元学习器(meta-learner) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.8\\nStacking 算法的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.5\\n多样性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.5.1\\n式(8.27) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.5.2\\n式(8.28) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.3\\n式(8.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.4\\n式(8.30) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.5\\n式(8.31) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.6\\n式(8.32) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.7\\n式(8.33) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.8\\n式(8.34) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.9\\n式(8.35) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.10 式(8.36) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.11 式(8.40) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.12 式(8.41) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.13 式(8.42) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.14 多样性增强的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.6\\nGradient Boosting/GBDT/XGBoost 联系与区别. . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.6.1\\n梯度下降法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.6.2\\n从梯度下降的角度解释AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n8.6.3\\n梯度提升(Gradient Boosting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n94\\n8.6.4\\n梯度提升树(GBDT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n95\\n8.6.5\\nXGBoost\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n96\\n第9 章聚类\\n97\\n9.1\\n聚类任务\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n9.2\\n性能度量\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n9.2.1\\n式(9.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n9.2.2\\n式(9.6) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.2.3\\n式(9.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n9.2.4\\n式(9.8) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.2.5\\n式(9.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.3\\n距离计算\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.3.1\\n式(9.21) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4\\n原型聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4.1\\n式(9.28) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4.2\\n式(9.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4.3\\n式(9.30) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.4\\n式(9.31) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.5\\n式(9.32) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.6\\n式(9.33) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.7\\n式(9.34) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.8\\n式(9.35) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n102\\n9.4.9\\n式(9.36) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n103\\n9.4.10 式(9.37) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n103\\n9.4.11 式(9.38) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n104\\n9.4.12 图9.6 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n104\\n9.5\\n密度聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.5.1\\n密度直达、密度可达与密度相连. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.5.2\\n图9.9 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.6\\n层次聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n106\\n第10 章降维与度量学习\\n107\\n10.1 预备知识\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.1.1 符号约定\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.1.2 矩阵与单位阵、向量的乘法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.2 矩阵的F 范数与迹. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.3 k 近邻学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n109\\n10.3.1 式(10.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n109\\n10.3.2 式(10.2) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n109\\n10.4 低维嵌入\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.1 图10.2 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.2 式(10.3) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.3 式(10.4) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.4 式(10.5) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.5 式(10.6) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.6 式(10.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.4.7 式(10.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.4.8 图10.3 关于MDS 算法的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.5 主成分分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113\\n10.5.1 式(10.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113\\n10.5.2 式(10.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n115\\n10.5.3 式(10.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n117\\n10.5.4 根据式(10.17) 求解式(10.16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n118\\n10.6 核化线性降维. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n118\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.6.1 式(10.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.2 式(10.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.3 式(10.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.4 式(10.22) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.5 式(10.24) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.6 式(10.25) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7 流形学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7.1 等度量映射(Isomap) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7.2 式(10.28) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7.3 式(10.31) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n122\\n10.8 度量学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n123\\n10.8.1 式(10.34) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n10.8.2 式(10.35) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n10.8.3 式(10.36) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.4 式(10.37) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.5 式(10.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.6 式(10.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n第11 章特征选择与稀疏学习\\n126\\n11.1 子集搜索与评价. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.1.1 式(11.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.1.2 式(11.2) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.2 过滤式选择. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.2.1 包裹式选择. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.3 嵌入式选择与L1 正则化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.1 式(11.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.2 式(11.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.3 式(11.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.4 式(11.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.5 式(11.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.6 式(11.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.7 式(11.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.8 式(11.12) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.9 式(11.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129\\n11.3.10 式(11.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129\\n11.4 稀疏表示与字典学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n11.4.1 式(11.15) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n11.4.2 式(11.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131\\n11.4.3 式(11.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131\\n11.4.4 式(11.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131\\n11.5 K-SVD 算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n11.6 压缩感知\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n11.6.1 式(11.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n11.6.2 式(11.25) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第12 章计算学习理论\\n136\\n12.1 基础知识\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.1 式(12.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.2 式(12.2) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.3 式(12.3) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.4 式(12.4) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.5 式(12.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.6 式(12.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n137\\n12.2 PAC 学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n137\\n12.2.1 式(12.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3 有限假设空间. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.1 式(12.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.2 式(12.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.3 式(12.12) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.4 式(12.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.5 式(12.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.6 引理12.1 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.7 式(12.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.3.8 式(12.19) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.3.9 式(12.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.4 VC 维. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.1 式(12.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.2 式(12.22) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.3 式(12.23) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.4 引理12.2 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n142\\n12.4.5 式(12.28) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n143\\n12.4.6 式(12.29) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n143\\n12.4.7 式(12.30) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n144\\n12.4.8 定理12.4 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n144\\n12.5 Rademacher 复杂度\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.1 式(12.36) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.2 式(12.37) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.3 式(12.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.4 式(12.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.5 式(12.40) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.5.6 式(12.41) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.5.7 定理12.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.6 定理12.6 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n147\\n12.6.1 式(12.52) 的证明. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.6.2 式(12.53) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.7 稳定性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.7.1 泛化/经验/留一损失的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.2 式(12.57) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.3 定理12.8 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.7.4 式(12.60) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.5 经验损失最小化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.6 定理(12.9) 的证明的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n150\\n第13 章半监督学习\\n151\\n13.1 未标记样本. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2 生成式方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.1 式(13.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.2 式(13.2) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.3 式(13.3) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.4 式(13.4) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.5 式(13.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.6 式(13.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.7 式(13.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n153\\n13.2.8 式(13.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n154\\n13.3 半监督SVM\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.1 图13.3 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.2 式(13.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.3 图13.4 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.4 式(13.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n13.4 图半监督学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n13.4.1 式(13.12) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n13.4.2 式(13.13) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158\\n13.4.3 式(13.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.4 式(13.15) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.5 式(13.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.6 式(13.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.7 式(13.18) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.8 式(13.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.4.9 公式(13.21) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.5 基于分歧的方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.5.1 图13.6 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.6 半监督聚类. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.6.1 图13.7 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.6.2 图13.9 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n第14 章概率图模型\\n165\\n14.1 隐马尔可夫模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n165\\n14.1.1 生成式模型和判别式模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n165\\n14.1.2 式(14.1) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n165\\n14.1.3 隐马尔可夫模型的三组参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2 马尔可夫随机场. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2.1 式(14.2) 和式(14.3) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2.2 式(14.4) 到式(14.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2.3 马尔可夫毯(Markov blanket) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.2.4 势函数(potential function) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2.5 式(14.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2.6 式(14.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.3 条件随机场. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.3.1 式(14.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.3.2 式(14.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.3 学习与推断. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.4 式(14.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.5 式(14.15) 和式(14.16) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.6 式(14.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.7 式(14.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.8 式(14.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.9 式(14.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.10 式(14.22) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.11 图14.8 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4 近似推断\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4.1 式(14.21) 到式(14.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4.2 式(14.26) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4.3 式(14.27) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4.4 式(14.28) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4.5 吉布斯采样与MH 算法\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.6 式(14.29) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.7 式(14.30) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.8 式(14.31) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.9 式(14.32) 到式(14.34) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.10 式(14.35) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.11 式(14.36) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n174\\n14.4.12 式(14.37) 到式(14.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n175\\n14.4.13 式(14.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n175\\n14.4.14 式(14.40) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5 话题模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5.1 式(14.41) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5.2 式(14.42) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5.3 式(14.43) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n14.5.4 式(14.44) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n第15 章规则学习\\n178\\n15.1 剪枝优化\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.1.1 式(15.2) 和式(15.3) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2 归纳逻辑程序设计\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.1 式(15.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.2 式(15.7) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.3 式(15.9) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.4 式(15.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.5 式(15.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='15.2.6 式(15.12) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n15.2.7 式(15.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n15.2.8 式(15.16) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n第16 章强化学习\\n180\\n16.1 任务与奖赏. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2 K-摇臂赌博机. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2.1 式(16.2) 和式(16.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2.2 式(16.4) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3 有模型学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3.1 式(16.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3.2 式(16.8) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.3 式(16.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.4 式(16.14) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.5 式(16.15) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.6 式(16.16) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.4 免模型学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.1 式(16.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.2 式(16.23) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.3 式(16.31) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5 值函数近似. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5.1 式(16.33) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5.2 式(16.34) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.6 模仿学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n183\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第1 章\\n绪论\\n本章作为“西瓜书”的开篇，主要讲解什么是机器学习以及机器学习的相关数学符号，为后续内容作\\n铺垫，并未涉及复杂的算法理论，因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可。此外，在\\n阅读本章前建议先阅读西瓜书目录前页的《主要符号表》，它能解答在阅读“西瓜书”过程中产生的大部\\n分对数学符号的疑惑。\\n本章也作为本书的开篇，笔者在此赘述一下本书的撰写初衷，本书旨在以“过来人”的视角陪读者一\\n起阅读“西瓜书”，尽力帮读者消除阅读过程中的“数学恐惧”，只要读者学习过《高等数学》、《线性代\\n数》和《概率论与数理统计》这三门大学必修的数学课，均能看懂本书对西瓜书中的公式所做的解释和推\\n导，同时也能体会到这三门数学课在机器学习上碰撞产生的“数学之美”。\\n1.1\\n引言\\n本节以概念理解为主，在此对“算法”和“模型”作补充说明。“算法”是指从数据中学得“模型”的具\\n体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，\\n通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如f(x) = wx + b\\n的一元一次函数。不过由于严格区分这两者的意义不大，因此多数文献和资料会将其混用，当遇到这两个\\n概念时，其具体指代根据上下文判断即可。\\n1.2\\n基本术语\\n本节涉及的术语较多且很多术语都有多个称呼，下面梳理各个术语，并将最常用的称呼加粗标注。\\n样本：也称为“示例”，是关于一个事件或对象的描述。因为要想让计算机能对现实生活中的事物\\n进行机器学习，必须先将其抽象为计算机能理解的形式，计算机最擅长做的就是进行数学运算，因此考\\n虑如何将其抽象为某种数学形式。显然，线性代数中的向量就很适合，因为任何事物都可以由若干“特\\n征”（或称为“属性”）唯一刻画出来，而向量的各个维度即可用来描述各个特征。例如，如果用色泽、根\\n蒂和敲声这3 个特征来刻画西瓜，那么一个“色泽青绿，根蒂蜷缩，敲声清脆”的西瓜用向量来表示即为x =\\n(青绿; 蜷缩; 清脆) （向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量），\\n其中青绿、蜷缩和清脆分别对应为相应特征的取值，也称为“属性值”。显然，用中文书写向量的方式不够\\n“数学”，因此需要将属性值进一步数值化，具体例子参见“西瓜书”第3 章3.2。此外，仅靠以上3 个特\\n征来刻画西瓜显然不够全面细致，因此还需要扩展更多维度的特征，一般称此类与特征处理相关的工作为\\n“特征工程”。\\n样本空间：也称为“输入空间”或“属性空间”。由于样本采用的是标明各个特征取值的“特征向量”\\n来进行表示，根据线性代数的知识可知，有向量便会有向量所在的空间，因此称表示样本的特征向量所在\\n的空间为样本空间，通常用花式大写的X 表示。\\n数据集：数据集通常用集合来表示，令集合D = {x1, x2, ..., xm} 表示包含m 个样本的数据集，一般\\n同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有d 个特征，则第i\\n个样本的数学表示为d 维向量：xi = (xi1; xi2; ...; xid)，其中xij 表示样本xi 在第j 个属性上的取值。\\n模型：机器学习的一般流程如下：首先收集若干样本（假设此时有100 个），然后将其分为训练样本\\n（80 个）和测试样本（20 个），其中80 个训练样本构成的集合称为“训练集”，20 个测试样本构成的集合\\n称为“测试集”，接着选用某个机器学习算法，让其在训练集上进行“学习”（或称为“训练”），然后产出\\n得到“模型”（或称为“学习器”），最后用测试集来测试模型的效果。执行以上流程时，表示我们已经默认\\n样本的背后是存在某种潜在的规律，我们称这种潜在的规律为“真相”或者“真实”，例如样本是一堆好西\\n瓜和坏西瓜时，我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开。当我们应用某个机\\n器学习算法来学习时，产出得到的模型便是该算法所找到的它自己认为的规律，由于该规律通常并不一定\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n就是所谓的真相，所以也将其称为“假设”。通常机器学习算法都有可配置的参数，同一个机器学习算法，\\n使用不同的参数配置或者不同的训练集，训练得到的模型通常都不同。\\n标记：上文提到机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，我们称该方\\n面的信息为“标记”。例如在学习西瓜的好坏时，“好瓜”和“坏瓜”便是样本的标记。一般第i 个样本的\\n标记的数学表示为yi，标记所在的空间称为“标记空间”或“输出空间”，数学表示为花式大写的Y。标\\n记通常也看作为样本的一部分，因此，一个完整的样本通常表示为(x, y)。\\n根据标记的取值类型不同，可将机器学习任务分为以下两类：\\n• 当标记取值为离散型时，称此类任务为“分类”，例如学习西瓜是好瓜还是坏瓜、学习猫的图片是白\\n猫还是黑猫等。当分类的类别只有两个时，称此类任务为“二分类”，通常称其中一个为“正类”，另\\n一个为“反类”或“负类”；当分类的类别超过两个时，称此类任务为“多分类”。由于标记也属于样\\n本的一部分，通常也需要参与运算，因此也需要将其数值化，例如对于二分类任务，通常将正类记为\\n1，反类记为0，即Y = {0, 1}。这只是一般默认的做法，具体标记该如何数值化可根据具体机器学\\n习算法进行相应地调整，例如第6 章的支持向量机算法则采用的是Y = {−1, +1}；\\n• 当标记取值为连续型时，称此类任务为“回归”，例如学习预测西瓜的成熟度、学习预测未来的房价\\n等。由于是连续型，因此标记的所有可能取值无法直接罗列，通常只有取值范围，回归任务的标记取\\n值范围通常是整个实数域R，即Y = R。\\n无论是分类还是回归，机器学习算法最终学得的模型都可以抽象地看作为以样本x 为自变量，标记y\\n为因变量的函数y = f(x)，即一个从输入空间X 到输出空间Y 的映射。例如在学习西瓜的好坏时，机器\\n学习算法学得的模型可看作为一个函数f(x)，给定任意一个西瓜样本xi = (青绿; 蜷缩; 清脆)，将其输入\\n进函数即可计算得到一个输出yi = f(xi)，此时得到的yi 便是模型给出的预测结果，当yi 取值为1 时表\\n明模型认为西瓜xi 是好瓜，当yi 取值为0 时表明模型认为西瓜xi 是坏瓜。\\n根据是否有用到标记信息，可将机器学习任务分为以下两类：\\n• 在模型训练阶段有用到标记信息时，称此类任务为“监督学习”，例如第3 章的线性模型；\\n• 在模型训练阶段没用到标记信息时，称此类任务为“无监督学习”，例如第9 章的聚类。\\n泛化：由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确\\n与否才是衡量一个模型好坏的关键，我们称此为“泛化”能力。例如学习西瓜好坏时，假设训练集中共有3\\n个样本：{(x1 = (青绿; 蜷缩), y1 = 好瓜), (x2 = (乌黑; 蜷缩), y2 = 好瓜), (x3 = (浅白; 蜷缩), y3 = 好瓜)}，\\n同时假设判断西瓜好坏的真相是“只要根蒂蜷缩就是好瓜”，如果应用算法A 在此训练集上训练得到模型\\nfa(x)，模型a 学到的规律是“色泽等于青绿、乌黑或者浅白时，同时根蒂蜷缩即为好瓜，否则便是坏瓜”，\\n再应用算法B 在此训练集上训练得到模型fb(x)，模型fb(x) 学到的规律是“只要根蒂蜷缩就是好瓜”，因\\n此对于一个未见过的西瓜样本x = (金黄; 蜷缩) 来说，模型fa(x) 给出的预测结果为“坏瓜”，模型fb(x)\\n给出的预测结果为“好瓜”，此时我们称模型fb(x) 的泛化能力优于模型fa(x)。\\n通过以上举例可知，尽管模型fa(x) 和模型fb(x) 对训练集学得一样好，即两个模型对训练集中每个\\n样本的判断都对，但是其所学到的规律是不同的。导致此现象最直接的原因是算法的不同，但是算法通常\\n是有限的，可穷举的，尤其是在特定任务场景下可使用的算法更是有限，因此，数据便是导致此现象的另\\n一重要原因，这也就是机器学习领域常说的“数据决定模型的上限，而算法则是让模型无限逼近上限”, 下\\n面详细解释此话的含义。\\n先解释“数据决定模型效果的上限”，其中数据是指从数据量和特征工程两个角度考虑。从数据量的\\n角度来说，通常数据量越大模型效果越好，因为数据量大即表示累计的经验多，因此模型学习到的经验也\\n多，自然表现效果越好。例如以上举例中如果训练集中含有相同颜色但根蒂不蜷缩的坏瓜，模型a 学到真\\n相的概率则也会增大；从特征工程的角度来说，通常对特征数值化越合理，特征收集越全越细致，模型效\\n果通常越好，因为此时模型更易学得样本之间潜在的规律。例如学习区分亚洲人和非洲人时，此时样本即\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n为人，在进行特征工程时，如果收集到每个样本的肤色特征，则其他特征例如年龄、身高和体重等便可省\\n略，因为只需靠肤色这一个特征就足以区分亚洲人和非洲人。\\n而“算法则是让模型无限逼近上限”是指当数据相关的工作已准备充分时，接下来便可用各种可适用\\n的算法从数据中学习其潜在的规律进而得到模型，不同的算法学习得到的模型效果自然有高低之分，效果\\n越好则越逼近上限，即逼近真相。\\n分布：此处的“分布”指的是概率论中的概率分布，通常假设样本空间服从一个未知“分布”D，而\\n我们收集到的每个样本都是独立地从该分布中采样得到，即“独立同分布”。通常收集到的样本越多，越\\n能从样本中反推出D 的信息，即越接近真相。此假设属于机器学习中的经典假设，在后续学习机器学习\\n算法过程中会经常用到。\\n1.3\\n假设空间\\n本节的重点是理解“假设空间”和“版本空间”，下面以“房价预测”举例说明。假设现已收集到某地\\n区近几年的房价和学校数量数据，希望利用收集到的数据训练出能通过学校数量预测房价的模型，具体收\\n集到的数据如表1-1所示。\\n表1-1 房价预测\\n年份\\n学校数量\\n房价\\n2020\\n1 所\\n1 万/m2\\n2021\\n2 所\\n4 万/m2\\n基于对以上数据的观察以及日常生活经验，不难得出“房价与学校数量成正比”的假设，若将学校数\\n量设为x，房价设为y，则该假设等价表示学校数量和房价呈y = wx + b 的一元一次函数关系，此时房价\\n预测问题的假设空间即为“一元一次函数”。确定假设空间以后便可以采用机器学习算法从假设空间中学\\n得模型，即从一元一次函数空间中学得能满足表1-1中数值关系的某个一元一次函数。学完第3 章的线性\\n回归可知当前问题属于一元线性回归问题，根据一元线性回归算法可学得模型为y = 3x −2。\\n除此之外，也可以将问题复杂化，假设学校数量和房价呈y = wx2 + b 一元二次函数关系，此时问题\\n变为了线性回归中的多项式回归问题，按照多项式回归算法可学得模型为y = x2。因此，以表1-1中数据\\n作为训练集可以有多个假设空间，且在不同的假设空间中都有可能学得能够拟合训练集的模型，我们将所\\n有能够拟合训练集的模型构成的集合称为“版本空间”。\\n1.4\\n归纳偏好\\n在上一节“房价预测”的例子中，当选用一元线性回归算法时，学得的模型是一元一次函数，当选\\n用多项式回归算法时，学得的模型是一元二次函数，所以不同的机器学习算法有不同的偏好，我们称为\\n“归纳偏好”。对于当前房价预测这个例子来说，这两个算法学得的模型哪个更好呢？著名的“奥卡姆剃\\n刀”原则认为“若有多个假设与观察一致，则选最简单的那个”，但是何为“简单”便见仁见智了，如\\n果认为函数的幂次越低越简单，则此时一元线性回归算法更好，如果认为幂次越高越简单，则此时多项\\n式回归算法更好，因此该方法其实并不“简单”，所以并不常用，而最常用的方法则是基于模型在测试\\n集上的表现来评判模型之间的优劣。测试集是指由训练集之外的样本构成的集合，例如在当前房价预测\\n问题中，通常会额外留有部分未参与模型训练的数据来对模型进行测试。假设此时额外留有1 条数据：\\n(年份: 2022年; 学校数量: 3所; 房价: 7万/m2) 用于测试，模型y = 3x −2 的预测结果为3 ∗3 −2 = 7，预\\n测正确，模型y = x2 的预测结果为32 = 9，预测错误，因此，在当前房价预测问题上，我们认为一元线\\n性回归算法优于多项式回归算法。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n机器学习算法之间没有绝对的优劣之分，只有是否适合当前待解决的问题之分，例如上述测试集中的\\n数据如果改为(年份: 2022年; 学校数量: 3所; 房价: 9万/m2) 则结论便逆转为多项式回归算法优于一元线\\n性回归算法。\\n1.4.1\\n式(1.1) 和式(1.2) 的解释\\nX\\nf\\nEote(La|X, f) =\\nX\\nf\\nX\\nh\\nX\\nx∈X−X\\nP(x)I(h(x) ̸= f(x))P(h|X, La)\\n1⃝\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\nX\\nf\\nI(h(x) ̸= f(x))\\n2⃝\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\n1\\n22|X|\\n3⃝\\n=\\n1\\n22|X|\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\n4⃝\\n= 2|X|−1\\nX\\nx∈X−X\\nP(x) · 1\\n5⃝\\n1⃝→2⃝：\\nX\\nf\\nX\\nh\\nX\\nx∈X−X\\nP(x)I(h(x) ̸= f(x))P(h|X, La)\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nf\\nX\\nh\\nI(h(x) ̸= f(x))P(h|X, La)\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\nX\\nf\\nI(h(x) ̸= f(x))\\n2⃝→3⃝：首先要知道此时我们假设f 是任何能将样本映射到{0, 1} 的函数。存在不止一个f 时，f\\n服从均匀分布，即每个f 出现的概率相等。例如样本空间只有两个样本时，X = {x1, x2}, |X| = 2。那么\\n所有可能的真实目标函数f 如下：\\nf1 : f1(x1) = 0, f1(x2) = 0\\nf2 : f2(x1) = 0, f2(x2) = 1\\nf3 : f3(x1) = 1, f3(x2) = 0\\nf4 : f4(x1) = 1, f4(x2) = 1\\n一共2|X| = 22 = 4 个可能的真实目标函数。所以此时通过算法La 学习出来的模型h(x) 对每个样本无论\\n预测值为0 还是1，都必然有一半的f 与之预测值相等。例如，现在学出来的模型h(x) 对x1 的预测值\\n为1，即h(x1) = 1，那么有且只有f3 和f4 与h(x) 的预测值相等，也就是有且只有一半的f 与它预测\\n值相等，所以P\\nf I(h(x) ̸= f(x)) = 1\\n22|X|。\\n需要注意的是，在这里我们假设真实的目标函数f 服从均匀分布，但是实际情形并非如此，通常我们只\\n认为能高度拟合已有样本数据的函数才是真实目标函数，例如，现在已有的样本数据为{(x1, 0), (x2, 1)}，那\\n么此时f2 才是我们认为的真实目标函数，由于没有收集到或者压根不存在{(x1, 0), (x2, 0)}, {(x1, 1), (x2, 0)},\\n{(x1, 1), (x2, 1)} 这类样本，所以f1, f3, f4 都不算是真实目标函数。套用到上述“房价预测”的例子中，我\\n们认为只有能正确拟合测试集的函数才是真实目标函数，也就是我们希望学得的模型。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第2 章\\n模型评估与选择\\n如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1 章介绍了什么是机器学习及机\\n器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称\\n“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。\\n由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型\\n有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单\\n泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3 章开始看，直至看完第6 章以后再回头来看\\n本章便会轻松许多。\\n2.1\\n经验误差与过拟合\\n梳理本节的几个概念。\\n错误率：E = a\\nm，其中m 为样本个数，a 为分类错误样本个数。\\n精度：精度=1-错误率。\\n误差：学习器的实际预测输出与样本的真实输出之间的差异。\\n经验误差：学习器在训练集上的误差，又称为“训练误差”。\\n泛化误差：学习器在新样本上的误差。\\n经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12 章的式(12.1) 和式(12.2)，接下\\n来辨析一下以上几个概念。\\n错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根\\n据“西瓜书”第12 章的式(12.1) 和式(12.2) 的定义可以看出，在分类问题中也会使用误差的概念，此时\\n的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若\\n不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出\\n现过的样本）上差异的平均值。\\n过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相\\n对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1 章图1.4 中的\\n训练样本（黑点）来说，用类似于抛物线的曲线A 去拟合则较为合理，而比较崎岖的曲线B 相对于训练\\n样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。\\n2.2\\n评估方法\\n本节介绍了3 种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；\\n交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用\\n于集成学习（详见“西瓜书”第8 章的8.2 节和8.3 节）产生基分类器。留出法和自助法简单易懂，在此\\n不再赘述，下面举例说明交叉验证法的常用方式。\\n对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集\\nD 的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉\\n验证法为算法L 筛选出在数据集D 上效果最好的参数配置方案。以3 折交叉验证为例，首先按照“西瓜\\n书”中所说的方法，通过分层采样将数据集D 划分为3 个大小相似的互斥子集：D1, D2, D3，然后分别用\\n其中1 个子集作为测试集，其他子集作为训练集，这样就可获得3 组训练集和测试集：\\n训练集1：D1 ∪D2，测试集1:D3\\n训练集2：D1 ∪D3，测试集2:D2\\n训练集3：D2 ∪D3，测试集3:D1\\n接下来用算法L 搭配方案a 在训练集1 上进行训练，训练结束后将训练得到的模型在测试集1 上进\\n行测试，得到测试结果1，依此方法再分别通过训练集2 和测试集2、训练集3 和测试集3 得到测试结果\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n2 和测试结果3，最后将3 次测试结果求平均即可得到算法L 搭配方案a 在数据集D 上的最终效果，记\\n为Scorea。同理，按照以上方法也可得到算法L 搭配方案b 在数据集D 上的最终效果Scoreb，最后通\\n过比较Scorea 和Scoreb 之间的优劣来确定算法L 在数据集D 上效果最好的参数配置方案。\\n对比不同算法之间的效果：同上述“对比同一算法的不同参数配置之间的效果”中所讲的方法一样，\\n只需将其中的“算法L 搭配方案a”和“算法L 搭配方案b”分别换成需要对比的算法α 和算法β 即可。\\n从以上的举例可以看出，交叉验证法本质上是在进行多次留出法，且每次都换不同的子集做测试集，\\n最终让所有样本均至少做1 次测试样本。这样做的理由其实很简单，因为一般的留出法只会划分出1 组\\n训练集和测试集，仅依靠1 组训练集和测试集去对比不同算法之间的效果显然不够置信，偶然性太强，因\\n此要想基于固定的数据集产生多组不同的训练集和测试集，则只有进行多次划分，每次采用不同的子集作\\n为测试集，也即为交叉验证法。\\n2.2.1\\n算法参数（超参数）与模型参数\\n算法参数是指算法本身的一些参数（也称超参数），例如k 近邻的近邻个数k、支持向量机的参数C\\n（详见“西瓜书”第6 章式(6.29)）。算法配置好相应参数后进行训练，训练结束会得到一个模型，例如支\\n持向量机最终会得到w 和b 的具体数值（此处不考虑核函数），这就是模型参数，模型配置好相应模型参\\n数后即可对新样本做预测。\\n2.2.2\\n验证集\\n带有参数的算法一般需要从候选参数配置方案中选择相对于当前数据集的最优参数配置方案，例如支\\n持向量机的参数C，一般采用的是前面讲到的交叉验证法，但是交叉验证法操作起来较为复杂，实际中更\\n多采用的是：先用留出法将数据集划分出训练集和测试集，然后再对训练集采用留出法划分出训练集和新\\n的测试集，称新的测试集为验证集，接着基于验证集的测试结果来调参选出最优参数配置方案，最后将验\\n证集合并进训练集（训练集数据量够的话也可不合并），用选出的最优参数配置在合并后的训练集上重新\\n训练，再用测试集来评估训练得到的模型的性能。\\n2.3\\n性能度量\\n本节性能度量指标较多，但是一般常用的只有错误率、精度、查准率、查全率、F1、ROC 和AUC。\\n2.3.1\\n式(2.2) 到式(2.7) 的解释\\n这几个公式简单易懂，几乎不需要额外解释，但是需要补充说明的是式(2.2)、式(2.4) 和式(2.5) 假\\n设了数据分布为均匀分布，即每个样本出现的概率相同，而式(2.3)、式(2.6) 和式(2.7) 则为更一般的表\\n达式。此外，在无特别说明的情况下，2.3 节所有公式中的“样例集D”均默认为非训练集（测试集、验\\n证集或其他未用于训练的样例集）。\\n2.3.2\\n式(2.8) 和式(2.9) 的解释\\n查准率P：被学习器预测为正例的样例中有多大比例是真正例。\\n查全率R：所有正例当中有多大比例被学习器预测为正例。\\n2.3.3\\n图2.3 的解释\\nP-R 曲线的画法与ROC 曲线的画法类似，也是通过依次改变模型阈值，然后计算出查准率和查全\\n率并画出相应坐标点，具体参见“式(2.20) 的推导”部分的讲解。这里需要说明的是，“西瓜书”中的图\\n2.3 仅仅是示意图，除了图左侧提到的“现实任务中的P-R 曲线常是非单调、不平滑的，在很多局部有上\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n下波动”以外，通常也不会取到(1, 0) 点。因为当取到(1, 0) 点时，此时是将所有样本均判为正例，因此\\nFN = 0，根据式(2.8) 可算得查全率为1，但是此时TP + FP 为样本总数，根据式(2.9) 可算得查准率\\n此时为正例在全体样本中的占比，显然在现实任务中正例的占比通常不为0，因此P-R 曲线在现实任务中\\n通常不会取到(1, 0) 点。\\n2.3.4\\n式(2.10) 的推导\\n将式(2.8) 和式(2.9) 代入式(2.10)，得\\nF1 = 2 × P × R\\nP + R\\n=\\n2 ×\\nT P\\nT P +F P ×\\nT P\\nT P +F N\\nT P\\nT P +F P +\\nT P\\nT P +F N\\n=\\n2 × TP × TP\\nTP(TP + FN) + TP(TP + FP)\\n=\\n2 × TP\\n(TP + FN) + (TP + FP)\\n=\\n2 × TP\\n(TP + FN + FP + TN) + TP −TN\\n=\\n2 × TP\\n样例总数+ TP −TN\\n若现有数据集D = {(xi, yi) | 1 ⩽i ⩽m}，其中标记yi ∈{0, 1}（1 表示正例，0 表示反例），假设模型\\nf(x) 对xi 的预测结果为hi ∈{0, 1}，则模型f(x) 在数据集D 上的F1 为\\nF1 =\\n2 Pm\\ni=1 yihi\\nPm\\ni=1 yi + Pm\\ni=1 hi\\n不难看出上式的本质为\\nF1 =\\n2 × TP\\n(TP + FN) + (TP + FP)\\n2.3.5\\n式(2.11) 的解释\\n“西瓜书”在式(2.11) 左侧提到Fβ 本质是加权调和平均，且和常用的算数平均相比，其更重视较小值，\\n在此举例说明。例如a 同学有两门课的成绩分别为100 分和60 分，b 同学相应的成绩为80 分和80 分，\\n此时若计算a 同学和b 同学的算数平均分则均为80 分，无法判断两位同学成绩的优劣，但是若计算加权\\n调和平均，当β = 1 时，a 同学的加权调和平均为2×100×60\\n100+60\\n= 75，b 同学的加权调和平均为2×80×80\\n80+80\\n= 80，\\n此时b 同学的平均成绩更优，原因是a 同学由于偏科导致其中一门成绩过低，而调和平均更重视较小值，\\n所以a 同学的偏科便被凸显出来。\\n式(2.11) 下方有提到“β > 1 时查全率有更大影响；β < 1 时查准率有更大影响”，下面解释其原因。\\n将式(2.11) 恒等变形为如下形式\\nFβ =\\n1\\n1\\n1+β2 · 1\\nP +\\nβ2\\n1+β2 · 1\\nR\\n从上式可以看出，当β > 1 时\\nβ2\\n1+β2 >\\n1\\n1+β2 ，所以\\n1\\nR 的权重比\\n1\\nP 的权重高，因此查全率R 对Fβ 的影响\\n更大，反之查准率P 对Fβ 的影响更大。\\n2.3.6\\n式(2.12) 到式(2.17) 的解释\\n式(2.12) 的macro-P 和式(2.13) 的macro-R 是基于各个二分类问题的P 和R 计算而得的；式(2.15)\\n的micro-P 和式(2.16) 的micro-R 是基于各个二分类问题的TP、FP、TN、FN 计算而得的；“宏”可\\n以认为是只关注宏观而不看具体细节，而“微”可以认为是要从具体细节做起，因为相比于P 和R 指标\\n来说，TP、FP、TN、FN 更微观，毕竟P 和R 是基于TP、FP、TN、FN 计算而得。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n从“宏”和“微”的计算方式可以看出，“宏”没有考虑每个类别下的的样本数量，所以平等看待每个\\n类别，因此会受到高P 和高R 类别的影响，而“微”则考虑到了每个类别的样本数量，因为样本数量多\\n的类相应的TP、FP、TN、FN 也会占比更多，所以在各类别样本数量极度不平衡的情况下，数量较多\\n的类别会主导最终结果。\\n式(2.14) 的macro-F1 是将macro-P 和macro-R 代入式(2.10) 所得；式(2.17) 的macro-F1 是将\\nmacro-P 和macro-R 代入式(2.10) 所得。值得一提的是，以上只是macro-F1 和micro-F1 的常用计算\\n方式之一，如若在查阅资料的过程中看到其他的计算方式也属正常。\\n2.3.7\\n式(2.18) 和式(2.19) 的解释\\n式(2.18) 定义了真正例率TPR。先解释公式中出现的真正例和假反例，真正例即实际为正例预测结\\n果也为正例，假反例即实际为正例但预测结果为反例，式(2.18) 分子为真正例，分母为真正例和假反例\\n之和（即实际的正例个数），因此式(2.18) 的含义是所有正例当中有多大比例被预测为正例（即查全率\\nRecall）。\\n式(2.19) 定义了假正例率FPR。先解释式子中出现的假正例和真反例，假正例即实际为反例但预测\\n结果为正例，真反例即实际为反例预测结果也为反例，式(2.19) 分子为假正例，分母为真反例和假正例之\\n和（即实际的反例个数），因此式(2.19) 的含义是所有反例当中有多大比例被预测为正例。\\n除了真正例率TPR 和假正例率FPR，还有真反例率TNR 和假反例率FNR：\\nTNR =\\nTN\\nFP + TN\\nFNR =\\nFN\\nTP + FN\\n2.3.8\\n式(2.20) 的推导\\n在推导式(2.20) 之前，需要先弄清楚ROC 曲线的具体绘制过程。下面我们就举个例子，按照“西瓜\\n书”图2.4 下方给出的绘制方法来讲解一下ROC 曲线的具体绘制过程。\\n假设我们已经训练得到一个学习器f(s)，现在用该学习器来对8 个测试样本（4 个正例，4 个反例，\\n即m+ = m−= 4）进行预测，预测结果为（此处用s 表示样本，以和坐标(x, y) 作出区分）：\\n(s1, 0.77, +), (s2, 0.62, −), (s3, 0.58, +), (s4, 0.47, +),\\n(s5, 0.47, −), (s6, 0.33, −), (s7, 0.23, +), (s8, 0.15, −)\\n其中，+ 和−分别表示样本为正例和为反例，数字表示学习器f 预测该样本为正例的概率，例如对\\n于反例s2 来说，当前学习器f(s) 预测它是正例的概率为0.62。\\n根据“西瓜书”上给出的绘制方法，首先需要对所有测试样本按照学习器给出的预测结果进行排\\n序（上面给出的预测结果已经按照预测值从大到小排序），接着将分类阈值设为一个不可能取到的超大\\n值，例如设为1。显然，此时所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个\\n数为0，相应的真正例率和假正例率也都为0，所以我们可以在坐标(0, 0) 处标记一个点。接下来需要把\\n分类阈值从大到小依次设为每个样本的预测值，也就是依次设为0.77, 0.62, 0.58, 0.47, 0.33, 0.23,0.15，然\\n后分别计算真正例率和假正例率，再在相应的坐标上标记点，最后再将各个点用直线连接, 即可得到ROC\\n曲线。需要注意的是，在统计预测结果时，预测值等于分类阈值的样本也被算作预测为正例。例如，当分\\n类阈值为0.77 时，测试样本s1 被预测为正例，由于它的真实标记也是正例，所以此时s1 是一个真正例。\\n为了便于绘图，我们将x 轴（假正例率轴）的“步长”定为\\n1\\nm−，y 轴（真正例率轴）的“步长”定为\\n1\\nm+ 。\\n根据真正例率和假正例率的定义可知，每次变动分类阈值时，若新增i 个假正例，那么相应的x 轴坐标也\\n就增加\\ni\\nm−；若新增j 个真正例，那么相应的y 轴坐标也就增加\\nj\\nm+ 。按照以上讲述的绘制流程，最终我\\n们可以绘制出如图2-1所示的ROC 曲线。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图2-1 ROC 曲线示意\\n在这里，为了能在解释式(2.21) 时复用此图，我们没有写上具体的数值，转而用其数学符号代替。其\\n中绿色线段表示在分类阈值变动的过程中只新增了真正例，红色线段表示只新增了假正例，蓝色线段表示\\n既新增了真正例也新增了假正例。根据AUC 值的定义可知，此时的AUC 值其实就是所有红色线段和蓝\\n色线段与x 轴围成的面积之和。观察图2-1可知，红色线段与x 轴围成的图形恒为矩形，蓝色线段与x 轴\\n围成的图形恒为梯形。由于梯形面积式既能算梯形面积，也能算矩形面积，所以无论是红色线段还是蓝色\\n线段，其与x 轴围成的面积都能用梯形公式来计算：\\n1\\n2 · (xi+1 −xi) · (yi + yi+1)\\n其中，(xi+1 −xi) 为“高”，yi 为“上底”，yi+1 为“下底”。那么对所有红色线段和蓝色线段与x 轴围成\\n的面积进行求和，则有\\nm−1\\nX\\ni=1\\n\\x141\\n2 · (xi+1 −xi) · (yi + yi+1)\\n\\x15\\n此即为AUC。\\n通过以上ROC 曲线的绘制流程可以看出，ROC 曲线上每一个点都表示学习器f(s) 在特定阈值下构\\n成的一个二分类器，越好的二分类器其假正例率（反例被预测错误的概率，横轴）越小，真正例率（正例\\n被预测正确的概率，纵轴）越大，所以这个点越靠左上角（即点(0, 1)）越好。因此，越好的学习器，其\\nROC 曲线上的点越靠左上角，相应的ROC 曲线下的面积也越大，即AUC 也越大。\\n2.3.9\\n式(2.21) 和式(2.22) 的推导\\n下面针对“西瓜书”上所说的“ℓrank 对应的是ROC 曲线之上的面积”进行推导。按照我们上述对式\\n(2.20) 的推导思路，ℓrank 可以看作是所有绿色线段和蓝色线段与y 轴围成的面积之和，但从式(2.21) 中\\n很难一眼看出其面积的具体计算方式，因此我们进行恒等变形如下：\\nℓrank =\\n1\\nm+m−\\nX\\nx+∈D+\\nX\\nx−∈D−\\n\\x12\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2I\\n\\x00f(x+) = f(x−)\\n\\x01\\x13\\n=\\n1\\nm+m−\\nX\\nx+∈D+\\n\" X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2 ·\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n=\\nX\\nx+∈D+\\n\"\\n1\\nm+ ·\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2 ·\\n1\\nm+ ·\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n=\\nX\\nx+∈D+\\n1\\n2 ·\\n1\\nm+ ·\\n\"\\n2\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n在变动分类阈值的过程当中，如果有新增真正例，那么图2-1就会相应地增加一条绿色线段或蓝色线\\n段，所以上式中的P\\nx+∈D+ 可以看作是在累加所有绿色和蓝色线段，相应地，P\\nx+∈D+ 后面的内容便是\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在求绿色线段或者蓝色线段与y 轴围成的面积，即：\\n1\\n2 ·\\n1\\nm+ ·\\n\"\\n2\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n与式(2.20) 中的推导思路相同，不论是绿色线段还是蓝色线段，其与y 轴围成的图形面积都可以用\\n梯形公式来进行计算，所以上式表示的依旧是一个梯形的面积公式。其中\\n1\\nm+ 即梯形的“高”，中括号内\\n便是“上底+ 下底”，下面我们来分别推导一下“上底”（较短的底）和“下底”（较长的底）。\\n由于在绘制ROC 曲线的过程中，每新增一个假正例时x 坐标也就新增一个步长，所以对于“上底”，\\n也就是绿色或者蓝色线段的下端点到y 轴的距离，长度就等于\\n1\\nm−乘以预测值大于f(x+) 的假正例的个\\n数，即\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n而对于“下底”，长度就等于\\n1\\nm−乘以预测值大于等于f(x+) 的假正例的个数，即\\n1\\nm−\\n X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n!\\n到此，推导完毕。\\n若不考虑f(x+) = f(x−)，从直观上理解ℓrank，其表示的是：对于待测试的模型f(x)，从测试集中随\\n机抽取一个正反例对儿{x+, x−}，模型f(x) 对正例的打分f(x+) 小于对反例的打分f(x−) 的概率，即\\n“排序错误”的概率。推导思路如下：采用频率近似概率的思路，组合出测试集中的所有正反例对儿，假设\\n组合出来的正反例对儿的个数为m，用模型f(x) 对所有正反例对儿打分并统计“排序错误”的正反例对\\n儿个数n，然后计算出\\nn\\nm 即为模型f(x)“排序错误”的正反例对儿的占比，其可近似看作为f(x) 在测\\n试集上“排序错误”的概率。具体推导过程如下：测试集中的所有正反例对儿的个数为\\nm+ × m−\\n“排序错误”的正反例对儿个数为\\nX\\nx+∈D+\\nX\\nx−∈D−\\n\\x00I\\n\\x00f(x+) < f(x−)\\n\\x01\\x01\\n因此，“排序错误”的概率为\\nP\\nx+∈D+\\nP\\nx−∈D−(I (f(x+) < f(x−)))\\nm+ × m−\\n若再考虑f(x+) = f(x−) 时算半个“排序错误”，则上式可进一步扩展为\\nP\\nx+∈D+\\nP\\nx−∈D−\\n\\x00I\\n\\x00f(x+) < f(x−) + 1\\n2I (f(x+) = f(x−))\\n\\x01\\x01\\nm+ × m−\\n此即为ℓrank。\\n如果说ℓrank 指的是从测试集中随机抽取正反例对儿，模型f(x)“排序错误”的概率，那么根据式\\n(2.22) 可知，AUC 则指的是从测试集中随机抽取正反例对儿，模型f(x)“排序正确”的概率。显然，此\\n概率越大越好。\\n2.3.10\\n式(2.23) 的解释\\n本公式很容易理解，只是需要注意该公式上方交代了“若将表2.2 中的第0 类作为正类、第1 类作为\\n反类”，若不注意此条件，按习惯（0 为反类、1 为正类）会产生误解。为避免产生误解，在接下来的解释\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n中将cost01 记为cost+−，cost10 记为cost−+。本公式还可以作如下恒等变形\\nE(f; D; cost) = 1\\nm\\n\\uf8eb\\n\\uf8edm+ ×\\n1\\nm+\\nX\\nxi∈D+\\nI (f (xi ̸= yi)) × cost+−+ m−×\\n1\\nm−\\nX\\nxi∈D−\\nI (f (xi ̸= yi)) × cost−+\\n\\uf8f6\\n\\uf8f8\\n= m+\\nm ×\\n1\\nm+\\nX\\nxi∈D+\\nI (f (xi ̸= yi)) × cost+−+ m−\\nm ×\\n1\\nm−\\nX\\nxi∈D−\\nI (f (xi ̸= yi)) × cost−+\\n其中m+ 和m−分别表示正例子集D+ 和反例子集D−的样本个数。\\n1\\nm+\\nP\\nxi∈D+ I (f (xi ̸= yi)) 表示正例子集D+ 预测错误样本所占比例，即假反例率FNR。\\n1\\nm−\\nP\\nxi∈D−I (f (xi ̸= yi)) 表示反例子集D−预测错误样本所占比例，即假反例率FPR。\\nm+\\nm 表示样例集D 中正例所占比例，或理解为随机从D 中取一个样例取到正例的概率。\\nm−\\nm 表示样例集D 中反例所占比例，或理解为随机从D 中取一个样例取到反例的概率。\\n因此，若将样例为正例的概率m+\\nm 记为p，则样例为f 反例的概率m−\\nm 为1 −p，上式可进一步写为\\nE(f; D; cost) = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\n此公式在接下来式(2.25) 的解释中会用到。\\n2.3.11\\n式(2.24) 的解释\\n当cost+−= cost−+ 时，本公式可化简为\\nP(+)cost =\\np\\np + (1 −p) = p\\n其中p 是样例为正例的概率（一般用正例在样例集中所占的比例近似代替）。因此，当代价不敏感时（也即\\ncost+−= cost−+），P(+)cost 就是正例在样例集中的占比。那么，当代价敏感时（也即cost+−̸= cost−+），\\nP(+)cost 即为正例在样例集中的加权占比。具体来说，对于样例集\\nD =\\n\\x08\\nx+\\n1 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−\\n7 , x−\\n8 , x−\\n9 , x−\\n10\\n\\t\\n其中x+ 表示正例，x−表示反例。可以看出p = 0.2，若想让正例得到更多重视，考虑代价敏感cost+−= 4\\n和cost−+ = 1，这实际等价于在以下样例集上进行代价不敏感的正例概率代价计算\\nD′ =\\n\\x08\\nx+\\n1 , x+\\n1 , x+\\n1 , x+\\n1 , x+\\n2 , x+\\n2 , x+\\n2 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−\\n7 , x−\\n8 , x−\\n9 , x−\\n10\\n\\t\\n即将每个正例样本复制4 份，若有1 个出错，则有4 个一起出错，代价为4。此时可计算出\\nP(+)cost =\\np × cost+−\\np × cost+−+ (1 −p) × cost−+\\n=\\n0.2 × 4\\n0.2 × 4 + (1 −0.2) × 1 = 0.5\\n也就是正例在等价的样例集D′ 中的占比。所以，无论代价敏感还是不敏感，P(+)cost 本质上表示的都是\\n样例集中正例的占比。在实际应用过程中，如果由于某种原因无法将cost+−和cost−+ 设为不同取值，可\\n以采用上述“复制样本”的方法间接实现将cost+−和cost−+ 设为不同取值。\\n对于不同的cost+−和cost−+ 取值，若二者的比值保持相同，则P(+)cost 不变。例如，对于上面的\\n例子，若设cost+−= 40 和cost−+ = 10，所得P(+)cost 仍为0.5。\\n此外，根据此式还可以相应地推导出反例概率代价\\nP(−)cost = 1 −P(+)cost =\\n(1 −p) × cost−+\\np × cost+−+ (1 −p) × cost−+\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n2.3.12\\n式(2.25) 的解释\\n对于包含m 个样本的样例集D，可以算出学习器f(x) 总的代价是\\ncostse = m × p × FNR × cost+−+ m × (1 −p) × FPR × cost−+\\n+ m × p × TPR × cost++ + m × (1 −p) × TNR × cost−−\\n其中p 是正例在样例集中所占的比例（或严格地称为样例为正例的概率），costse 下标中的“se”表示\\nsensitive，即代价敏感，根据前面讲述的FNR、FPR、TPR、TNR 的定义可知：\\nm × p × FNR 表示正例被预测为反例（正例预测错误）的样本个数；\\nm × (1 −p) × FPR 表示反例被预测为正例（反例预测错误）的样本个数；\\nm × p × TPR 表示正例被预测为正例（正例预测正确）的样本个数；\\nm × (1 −p) × TNR 表示反例预测为反例（反例预测正确）的样本个数。\\n以上各种样本个数乘以相应的代价则得到总的代价costse。但是，按照此公式计算出的代价与样本个\\n数m 呈正比，显然不具有一般性，因此需要除以样本个数m，而且一般来说，预测出错才会产生代价，预\\n测正确则没有代价，也即cost++ = cost−−= 0，所以costse 更为一般化的表达式为\\ncostse = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\n回顾式(2.23) 的解释可知，此式即为式(2.23) 的恒等变形，所以此式可以同式(2.23) 一样理解为学习器\\nf(x) 在样例集D 上的“代价敏感错误率”。显然，costse 的取值范围并不在0 到1 之间，且costse 在\\nFNR = FPR = 1 时取到最大值，因为FNR = FPR = 1 时表示所有正例均被预测为反例，反例均被预测\\n为正例，代价达到最大，即\\nmax(costse) = p × cost+−+ (1 −p) × cost−+\\n所以，如果要将costse 的取值范围归一化到0 到1 之间，则只需将其除以其所能取到的最大值即可，也即\\ncostse\\nmax(costse) = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\np × cost+−+ (1 −p) × cost−+\\n此即为式(2.25)，也即为costnorm，其中下标“norm”表示normalization。\\n进一步地，根据式(2.24) 中P(+)cost 的定义可知，式(2.25) 可以恒等变形为\\ncostnorm = FNR × P(+)cost + FPR × (1 −P(+)cost)\\n对于二维直角坐标系中的两个点(0, B) 和(1, A) 以及实数p ∈[0, 1]，(p, pA+(1−p)B) 一定是线段A−B\\n上的点，且当p 从0 变到1 时，点(p, pA + (1 −p)B) 的轨迹为从(0, B) 到(1, A)，基于此，结合上述\\ncostnorm 的表达式可知：(P(+)cost, costnorm) 即为线段FPR −FNR 上的点，当(P(+)cost 从0 变到1\\n时，(P(+)cost, costnorm) 的轨迹为从(0, FPR) 到(1, FNR) ，也即图2.5 中的各条线段。需要注意的是，\\n以上只是从数学逻辑自洽的角度对图2.5 中的各条线段进行解释，实际中各条线段并非按照上述方法绘\\n制。理由如下：\\nP(+)cost 表示的是样例集中正例的占比，而在进行学习器的比较时，变动的只是训练学习器的算法\\n或者算法的超参数，用来评估学习器性能的样例集是固定的（单一变量原则），所以P(+)cost 是一个固\\n定值，因此图2.5 中的各条线段并不是通过变动P(+)cost 然后计算costnorm 画出来的，而是按照“西瓜\\n书”上式(2.25) 下方所说对ROC 曲线上每一点计算FPR 和FNR，然后将点(0, FPR) 和点(1, FNR) 直\\n接连成线段。\\n虽然图2.5 中的各条线段并不是通过变动横轴表示的P(+)cost 来进行绘制，但是横轴仍然有其他用\\n处，例如用来找使学习器的归一化代价costnorm 达到最小的阈值（暂且称其为最佳阈值）。具体地，首先\\n计算当前样例集的P(+)cost 值，然后根据计算出来的值在横轴上标记出具体的点，再基于该点作一条垂\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n直于横轴的垂线，与该垂线最先相交（从下往上看）的线段所对应的阈值（因为每条线段都对应ROC 曲\\n线上的点，ROC 曲线上的点又对应着具体的阈值）即为最佳阈值。原因是与该垂线最先相交的线段必然最\\n靠下，因此其交点的纵坐标最小，而纵轴表示的便是归一化代价costnorm，所以此时归一化代价costnorm\\n达到最小。特别地，当P(+)cost = 0 时，即样例集中没有正例，全是负例，因此最佳阈值应该是学习器\\n不可能取到的最大值，且按照此阈值计算出来出来的FPR = 0, FNR = 1, costnorm = 0。那么按照上述作\\n垂线的方法去图2.5 中进行实验，也即在横轴0 刻度处作垂线，显然与该垂线最先相交的线段是点(0, 0)\\n和点(1, 1) 连成的线段，交点为(0, 0)，此时对应的也为FPR = 0, FNR = 1, costnorm = 0，且该条线段所\\n对应的阈值也确实为“学习器不可能取到的最大值”（因为该线段对应的是ROC 曲线中的起始点）。\\n2.4\\n比较检验\\n为什么要做比较检验？“西瓜书”在本节开篇的两段话已经交代原由。简单来说，从统计学的角度，取\\n得的性能度量的值本质上仍是一个随机变量，因此并不能简单用比较大小来直接判定算法（或者模型）之\\n间的优劣，而需要更置信的方法来进行判定。\\n在此说明一下，如果不做算法理论研究，也不需要对算法（或模型）之间的优劣给出严谨的数学分析，\\n本节可以暂时跳过。本节主要使用的数学知识是“统计假设检验”，该知识点在各个高校的概率论与数理\\n统计教材（例如参考文献[1]）上均有讲解。此外，有关检验变量的公式，例如式(2.30) 至式(2.36)，并不\\n需要清楚是怎么来的（这是统计学家要做的事情），只需要会用即可。\\n2.4.1\\n式(2.26) 的解释\\n理解本公式时需要明确的是：ϵ 是未知的，是当前希望估算出来的，ˆϵ 是已知的，是已经用m 个测试\\n样本对学习器进行测试得到的。因此，本公式也可理解为：当学习器的泛化错误率为ϵ 时，被测得测试错\\n误率为ˆϵ 的条件概率。所以本公式可以改写为\\nP(ˆϵ|ϵ) =\\n \\nm\\nˆϵ × m\\n!\\nϵˆϵ×m(1 −ϵ)m−ˆϵ×m\\n其中\\n \\nm\\nˆϵ × m\\n!\\n=\\nm!\\n(ˆϵ × m)!(m −ˆϵ × m)!\\n为中学时学的组合数，即Cˆϵ×m\\nm\\n。\\n在已知ˆε 时，求使得条件概率P(ˆϵ|ϵ) 达到最大的ϵ 是概率论与数理统计中经典的极大似然估计问题。\\n从极大似然估计的角度可知，由于ˆϵ, m 均为已知量，所以P(ˆϵ|ϵ) 可以看作为一个关于ϵ 的函数，称为似\\n然函数，于是问题转化为求使得似然函数取到最大值的ϵ，即\\nϵ = arg max\\nϵ\\nP(ˆϵ|ϵ)\\n首先对ϵ 求一阶导数\\n∂P(ˆϵ | ϵ)\\n∂ϵ\\n=\\n \\nm\\nˆϵ × m\\n!\\n∂ϵˆϵ×m(1 −ϵ)m−ˆϵ×m\\n∂ϵ\\n=\\n \\nm\\nˆϵ × m\\n!\\n\\x00ˆϵ × m × ϵˆϵ×m−1(1 −ϵ)m−ˆϵ×m + ϵˆϵ×m × (m −ˆϵ × m) × (1 −ϵ)m−ˆϵ×m−1 × (−1)\\n\\x01\\n=\\n \\nm\\nˆϵ × m\\n!\\nϵˆϵ×m−1(1 −ϵ)m−ˆϵ×m−1(ˆϵ × m × (1 −ϵ) −ϵ × (m −ˆϵ × m))\\n=\\n \\nm\\nˆϵ × m\\n!\\nϵˆϵ×m−1(1 −ϵ)m−ˆϵ×m−1(ˆϵ × m −ϵ × m)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n分析上式可知，其中\\n \\nm\\nˆϵ × m\\n!\\n为常数，由于ϵ ∈[0, 1]，所以ϵˆϵ×m−1(1−ϵ)m−ˆϵ×m−1 恒大于0，(ˆϵ×m−ϵ×m)\\n在0 ⩽ϵ < ˆϵ 时大于0，在ϵ = ˆϵ 时等于0，在ˆϵ ⩽ϵ < 1 时小于0，因此P(ˆϵ | ϵ) 是关于ϵ 开口向下的凹\\n函数（此处采用的是最优化中对凹凸函数的定义，“西瓜书”第3 章3.2 节左侧边注对凹凸函数的定义也\\n是如此）。所以，当且仅当一阶导数∂P (ˆϵ|ϵ)\\n∂ϵ\\n= 0 时P(ˆϵ | ϵ) 取到最大值，此时ϵ = ˆϵ。\\n2.4.2\\n式(2.27) 的推导\\n截至2021 年5 月，“西瓜书”第1 版第36 次印刷，式(2.27) 应当勘误为\\nϵ = min ϵ\\ns.t.\\nm\\nX\\ni=ϵ×m+1\\n \\nm\\ni\\n!\\nϵi\\n0(1 −ϵ0)m−i < α\\n在推导此公式之前，先铺垫讲解一下“二项分布参数p 的假设检验”\\n[1]：\\n设某事件发生的概率为p，p 未知。做m 次独立试验，每次观察该事件是否发生，以X 记该事件发\\n生的次数，则X 服从二项分布B(m, p)，现根据X 检验如下假设：\\nH0 : p ⩽p0\\nH1 : p > p0\\n由二项分布本身的特性可知：p 越小，X 取到较小值的概率越大。因此，对于上述假设，一个直观上\\n合理的检验为\\nφ : 当X > C时拒绝H0, 否则就接受H0。\\n其中，C 表示事件最大发生次数。此检验对应的功效函数为\\nβφ(p) = P(X > C)\\n= 1 −P(X ⩽C)\\n= 1 −\\nC\\nX\\ni=0\\n \\nm\\ni\\n!\\npi(1 −p)m−i\\n=\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi(1 −p)m−i\\n由于“p 越小，X 取到较小值的概率越大”可以等价表示为：P(X ⩽C) 是关于p 的减函数，所以\\nβφ(p) = P(X > C) = 1 −P(X ⩽C) 是关于p 的增函数，那么当p ⩽p0 时，βφ(p0) 即为βφ(p) 的上确界。\\n（更为严格的数学证明参见参考文献[1] 中第2 章习题7）又根据参考文献[1] 中5.1.3 的定义1.2 可知，\\n在给定检验水平α 时，要想使得检验φ 达到水平α，则必须保证βφ(p) ⩽α，因此可以通过如下方程解得\\n使检验φ 达到水平α 的整数C：\\nα = sup {βφ(p)}\\n显然，当p ⩽p0 时有\\nα = sup {βφ(p)}\\n= βφ(p0)\\n=\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i\\n对于此方程，通常不一定正好解得一个使得方程成立的整数C，较常见的情况是存在这样一个C 使得\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\nm\\nX\\ni=C\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i > α\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时，C 只能取C 或者C + 1。若C 取C，则相当于升高了检验水平α；若C 取C + 1 则相当于降低了\\n检验水平α。具体如何取舍需要结合实际情况，一般的做法是使α 尽可能小，因此倾向于令C 取C + 1。\\n下面考虑如何求解C。易证βφ(p0) 是关于C 的减函数，再结合上述关于C 的两个不等式易推得\\nC = min C\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n由“西瓜书”中的上下文可知，对ϵ ⩽ϵ0 进行假设检验，等价于“二项分布参数p 的假设检验”中所\\n述的对p ⩽p0 进行假设检验，所以在“西瓜书”中求解最大错误率ϵ 等价于在“二项分布参数p 的假设\\n检验”中求解事件最大发生频率C\\nm。由上述“二项分布参数p 的假设检验”中的推导可知\\nC = min C\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n所以\\nC\\nm = min C\\nm\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n将上式中的C\\nm, C\\nm, p0 等价替换为ϵ, ϵ, ϵ0 可得\\nϵ = min ϵ\\ns.t.\\nm\\nX\\ni=ϵ×m+1\\n \\nm\\ni\\n!\\nϵi\\n0(1 −ϵ0)m−i < α\\n2.5\\n偏差与方差\\n2.5.1\\n式(2.37) 到式(2.42) 的推导\\n首先，梳理一下“西瓜书”中的符号，书中称x 为测试样本，但是书中又提到“令yD 为x 在数据集\\n中的标记”，那么x 究竟是测试集中的样本还是训练集中的样本呢？这里暂且理解为x 为从训练集中抽取\\n出来用于测试的样本。此外，“西瓜书”中左侧边注中提到“有可能出现噪声使得yD ̸= y”，其中所说的\\n“噪声”通常是指人工标注数据时带来的误差，例如标注“身高”时，由于测量工具的精度等问题，测出来\\n的数值必然与真实的“身高”之间存在一定误差，此即为“噪声”。\\n为了进一步解释式(2.37)、(2.38) 和(2.39)，在这里设有n 个训练集D1, ..., Dn，这n 个训练集都是\\n以独立同分布的方式从样本空间中采样而得，并且恰好都包含测试样本x，该样本在这n 个训练集的标记\\n分别为yD1, ..., yDn。书中已明确，此处以回归任务为例，也即yD, y, f(x; D) 均为实值。\\n式(2.37) 可理解为：\\n¯f(x) = ED[f(x; D)] = 1\\nn (f (x; D1) + . . . + f (x; Dn))\\n式(2.38) 可理解为：\\nvar(x) = ED\\n\\x02\\n(f(x; D) −¯f(x))2\\x03\\n= 1\\nn\\n\\x10\\x00f (x; D1) −¯f(x)\\n\\x012 + . . . +\\n\\x00f (x; Dn) −¯f(x)\\n\\x012\\x11\\n式(2.39) 可理解为：\\nε2 = ED\\nh\\n(yD −y)2i\\n= 1\\nn\\n\\x10\\n(yD1 −y)2 + . . . + (yDn −y)2\\x11\\n最后，推导一下式(2.41) 和式(2.42)，由于推导完式(2.41) 自然就会得到式(2.42)，因此下面仅推导\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(2.41) 即可。\\nE(f; D) =ED\\nh\\n(f(x; D) −yD)2i\\n1⃝\\n=ED\\nh\\x00f(x; D) −¯f(x) + ¯f(x) −yD\\n\\x012i\\n2⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −yD\\n\\x012i\\n+\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n3⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −yD\\n\\x012i\\n4⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −y + y −yD\\n\\x012i\\n5⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −y\\n\\x012i\\n+ ED\\nh\\n(y −yD)2i\\n+\\n2ED\\n\\x02\\x00 ¯f(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n6⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+\\n\\x00 ¯f(x) −y\\n\\x012 + ED\\nh\\n(yD −y)2i\\n7⃝\\n上式即为式(2.41)，下面给出每一步的推导过程：\\n1⃝→2⃝：减一个¯f(x) 再加一个¯f(x)，属于简单的恒等变形。\\n2⃝→3⃝：首先将中括号内的式子展开，有\\nED\\nh\\x00f(x; D) −¯f(x)\\n\\x012 +\\n\\x00 ¯f(x) −yD\\n\\x012 + 2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01i\\n然后根据期望的运算性质E[X + Y ] = E[X] + E[Y ] 可将上式化为\\nED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −yD\\n\\x012i\\n+ ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n3⃝→4⃝：再次利用期望的运算性质将3⃝的最后一项展开，有\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n= ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n首先计算展开后得到的第1 项，有\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n= ED\\n\\x02\\n2f(x; D) · ¯f(x) −2 ¯f(x) · ¯f(x)\\n\\x03\\n由于¯f(x) 是常量，所以由期望的运算性质：E[AX + B] = AE[X] + B（其中A, B 均为常量）可得\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n= 2 ¯f(x) · ED [f(x; D)] −2 ¯f(x) · ¯f(x)\\n由式(2.37) 可知ED [f(x; D)] = ¯f(x)，所以\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n= 2 ¯f(x) · ¯f(x) −2 ¯f(x) · ¯f(x) = 0\\n接着计算展开后得到的第2 项\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯f(x) · ED [yD]\\n由于噪声和f 无关，所以f(x; D) 和yD 是两个相互独立的随机变量。根据期望的运算性质E[XY ] =\\nE[X]E[Y ]（其中X 和Y 为相互独立的随机变量）可得\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯f(x) · ED [yD]\\n= 2ED [f(x; D)] · ED [yD] −2 ¯f(x) · ED [yD]\\n= 2 ¯f(x) · ED [yD] −2 ¯f(x) · ED [yD]\\n= 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 29, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n所以\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n= ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n= 0 + 0\\n= 0\\n4⃝→5⃝：同1⃝→2⃝一样，减一个y 再加一个y，属于简单的恒等变形。\\n5⃝→6⃝：同2⃝→3⃝一样，将最后一项利用期望的运算性质进行展开。\\n6⃝→7⃝：因为¯f(x) 和y 均为常量，根据期望的运算性质，6⃝中的第2 项可化为\\nED\\nh\\x00 ¯f(x) −y\\n\\x012i\\n=\\n\\x00 ¯f(x) −y\\n\\x012\\n同理，6⃝中的最后一项可化为\\n2ED\\n\\x02\\x00 ¯f(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯f(x) −y\\n\\x01\\nED [(y −yD)]\\n由于此时假定噪声的期望为0，即ED [(y −yD)] = 0，所以\\n2ED\\n\\x02\\x00 ¯f(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯f(x) −y\\n\\x01\\n· 0 = 0\\n参考文献\\n[1] 陈希孺. 概率论与数理统计. 中国科学技术大学出版社, 2009.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第3 章\\n线性模型\\n作为“西瓜书”介绍机器学习模型的开篇，线性模型也是机器学习中最为基础的模型，很多复杂模型\\n均可认为由线性模型衍生而得，无论是曾经红极一时的支持向量机还是如今万众瞩目的神经网络，其中都\\n有线性模型的影子。\\n本章的线性回归和对数几率回归分别是回归和分类任务上常用的算法，因此属于重点内容，线性判别\\n分析不常用，但是其核心思路和后续第10 章将会讲到的经典降维算法主成分分析相同，因此也属于重点\\n内容，且两者结合在一起看理解会更深刻。\\n3.1\\n基本形式\\n第1 章的1.2 基本术语中讲述样本的定义时，我们说明了“西瓜书”和本书中向量的写法，当向量\\n中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量。因此，式(3.2) 中\\nw = (w1; w2; ...; wd) 和x = (x1; x2; ...; xd) 均为d 行1 列的列向量。\\n3.2\\n线性回归\\n3.2.1\\n属性数值化\\n为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在“序”关系的属性，可通过\\n连续化将其转化为带有相对大小关系的连续值；对于不存在“序”关系的属性，可根据属性取值将其拆解为\\n多个属性，例如“西瓜书”中所说的“瓜类”属性，可将其拆解为“是否是西瓜”、“是否是南瓜”、“是否是黄\\n瓜”3 个属性，其中每个属性的取值为1 或0，1 表示“是”，0 表示“否”。具体地，假如现有3 个瓜类样本：\\nx1 = (甜度= 高; 瓜类= 西瓜), x2 = (甜度= 中; 瓜类= 南瓜), x3 = (甜度= 低; 瓜类= 黄瓜)，其中“甜\\n度”属性存在序关系，因此可将“高”、\\n“中”、\\n“低”转化为{1.0, 0.5, 0.0}，\\n“瓜类”属性不存在序关系，则按照上\\n述方法进行拆解，3 个瓜类样本数值化后的结果为：x1 = (1.0; 1; 0; 0), x1 = (0.5; 0; 1; 0), x1 = (0.0; 0; 0; 1)。\\n以上针对样本属性所进行的处理工作便是第1 章1.2 基本术语中提到的“特征工程”范畴，完成属性\\n数值化以后通常还会进行缺失值处理、规范化、降维等一系列处理工作。由于特征工程属于算法实践过程\\n中需要掌握的内容，待学完机器学习算法以后，再进一步学习特征工程相关知识即可，在此先不展开。\\n3.2.2\\n式(3.4) 的解释\\n下面仅针对式(3.4) 中的数学符号进行解释。首先解释一下符号“arg min”，其中“arg”是“argument”\\n（参\\n数）的前三个字母，“min”是“minimum”（最小值）的前三个字母，该符号表示求使目标函数达到最小值\\n的参数取值。例如式(3.4) 表示求出使目标函数Pm\\ni=1 (yi −wxi −b)2 达到最小值的参数取值(w∗, b∗)，注\\n意目标函数是以(w, b) 为自变量的函数，(xi, yi) 均是已知常量，即训练集中的样本数据。\\n类似的符号还有“min”，例如将式(3.4) 改为\\nmin\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\n则表示求目标函数的最小值。对比知道，“min”和“arg min”的区别在于，前者输出目标函数的最小值，\\n而后者输出使得目标函数达到最小值时的参数取值。\\n若进一步修改式(3.4) 为\\nmin\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\ns.t.w > 0,\\nb < 0.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n则表示在w > 0, b < 0 范围内寻找目标函数的最小值，“s.t.”是“subject to”的简写，意思是“受约束\\n于”，即为约束条件。\\n以上介绍的符号都是应用数学领域的一个分支——“最优化”中的内容，若想进一步了解可找一本最\\n优化的教材（例如参考文献[1]）进行系统性地学习。\\n3.2.3\\n式(3.5) 的推导\\n“西瓜书”在式(3.5) 左侧给出的凸函数的定义是最优化中的定义，与高等数学中的定义不同，本书也\\n默认采用此种定义。由于一元线性回归可以看作是多元线性回归中元的个数为1 时的情形，所以此处暂不\\n给出E(w,b) 是关于w 和b 的凸函数的证明，在推导式(3.11) 时一并给出，下面开始推导式(3.5)。\\n已知E(w,b) =\\nm\\nP\\ni=1\\n(yi −wxi −b)2，所以\\n∂E(w,b)\\n∂w\\n= ∂\\n∂w\\n\" m\\nX\\ni=1\\n(yi −wxi −b)2\\n#\\n=\\nm\\nX\\ni=1\\n∂\\n∂w\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−xi)]\\n=\\nm\\nX\\ni=1\\n\\x02\\n2 ·\\n\\x00wx2\\ni −yixi + bxi\\n\\x01\\x03\\n= 2 ·\\n \\nw\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\nyixi + b\\nm\\nX\\ni=1\\nxi\\n!\\n= 2\\n \\nw\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\n(yi −b) xi\\n!\\n3.2.4\\n式(3.6) 的推导\\n已知E(w,b) =\\nm\\nP\\ni=1\\n(yi −wxi −b)2，所以\\n∂E(w,b)\\n∂b\\n= ∂\\n∂b\\n\" m\\nX\\ni=1\\n(yi −wxi −b)2\\n#\\n=\\nm\\nX\\ni=1\\n∂\\n∂b\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−1)]\\n=\\nm\\nX\\ni=1\\n[2 · (b −yi + wxi)]\\n= 2 ·\\n\" m\\nX\\ni=1\\nb −\\nm\\nX\\ni=1\\nyi +\\nm\\nX\\ni=1\\nwxi\\n#\\n= 2\\n \\nmb −\\nm\\nX\\ni=1\\n(yi −wxi)\\n!\\n3.2.5\\n式(3.7) 的推导\\n推导之前先重点说明一下“闭式解”或称为“解析解”。闭式解是指可以通过具体的表达式解出待解\\n参数，例如可根据式(3.7) 直接解得w。机器学习算法很少有闭式解，线性回归是一个特例，接下来推导\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(3.7)。\\n令式(3.5) 等于0\\n0 = w\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\n(yi −b)xi\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −\\nm\\nX\\ni=1\\nbxi\\n由于令式(3.6) 等于0 可得b =\\n1\\nm\\nPm\\ni=1(yi −wxi)，又因为\\n1\\nm\\nPm\\ni=1 yi = ¯y，1\\nm\\nPm\\ni=1 xi = ¯x，则b = ¯y −w¯x，\\n代入上式可得\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −\\nm\\nX\\ni=1\\n(¯y −w¯x)xi\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −¯y\\nm\\nX\\ni=1\\nxi + w¯x\\nm\\nX\\ni=1\\nxi\\nw(\\nm\\nX\\ni=1\\nx2\\ni −¯x\\nm\\nX\\ni=1\\nxi) =\\nm\\nX\\ni=1\\nyixi −¯y\\nm\\nX\\ni=1\\nxi\\nw =\\nPm\\ni=1 yixi −¯y Pm\\ni=1 xi\\nPm\\ni=1 x2\\ni −¯x Pm\\ni=1 xi\\n将¯y Pm\\ni=1 xi =\\n1\\nm\\nPm\\ni=1 yi\\nPm\\ni=1 xi = ¯x Pm\\ni=1 yi 和¯x Pm\\ni=1 xi =\\n1\\nm\\nPm\\ni=1 xi\\nPm\\ni=1 xi =\\n1\\nm(Pm\\ni=1 xi)2 代入上\\n式，即可得式(3.7)：\\nw =\\nPm\\ni=1 yi(xi −¯x)\\nPm\\ni=1 x2\\ni −1\\nm(Pm\\ni=1 xi)2\\n如果要想用Python 来实现上式的话，上式中的求和运算只能用循环来实现。但是如果能将上式向量化，\\n也就是转换成矩阵（即向量）运算的话，我们就可以利用诸如NumPy 这种专门加速矩阵运算的类库来进\\n行编写。下面我们就尝试将上式进行向量化。\\n将\\n1\\nm(Pm\\ni=1 xi)2 = ¯x Pm\\ni=1 xi 代入分母可得\\nw =\\nPm\\ni=1 yi(xi −¯x)\\nPm\\ni=1 x2\\ni −¯x Pm\\ni=1 xi\\n=\\nPm\\ni=1(yixi −yi¯x)\\nPm\\ni=1(x2\\ni −xi¯x)\\n又因为¯y Pm\\ni=1 xi = ¯x Pm\\ni=1 yi = Pm\\ni=1 ¯yxi = Pm\\ni=1 ¯xyi = m¯x¯y = Pm\\ni=1 ¯x¯y 且Pm\\ni=1 xi¯x = ¯x Pm\\ni=1 xi =\\n¯x · m · 1\\nm · Pm\\ni=1 xi = m¯x2 = Pm\\ni=1 ¯x2，则有\\nw =\\nPm\\ni=1(yixi −yi¯x −xi¯y + ¯x¯y)\\nPm\\ni=1(x2\\ni −xi¯x −xi¯x + ¯x2)\\n=\\nPm\\ni=1(xi −¯x)(yi −¯y)\\nPm\\ni=1(xi −¯x)2\\n若令x = (x1; x2; ...; xm)，xd = (x1 −¯x; x2 −¯x; ...; xm −¯x) 为去均值后的x；y = (y1; y2; ...; ym)，yd =\\n(y1 −¯y; y2 −¯y; ...; ym −¯y) 为去均值后的y，（x、xd、y、yd 均为m 行1 列的列向量）代入上式可得\\nw = xT\\nd yd\\nxT\\nd xd\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.2.6\\n式(3.9) 的推导\\n式(3.4) 是最小二乘法运用在一元线性回归上的情形，那么对于多元线性回归来说，我们可以类似得\\n到\\n(w∗, b∗) = arg min\\n(w,b)\\nm\\nX\\ni=1\\n(f (xi) −yi)2\\n= arg min\\n(w,b)\\nm\\nX\\ni=1\\n(yi −f (xi))2\\n= arg min\\n(w,b)\\nm\\nX\\ni=1\\n\\x00yi −\\n\\x00wTxi + b\\n\\x01\\x012\\n为便于讨论，我们令ˆw = (w; b) = (w1; ...; wd; b) ∈R(d+1)×1, ˆxi = (xi1; ...; xid; 1) ∈R(d+1)×1，那么上式可\\n以简化为\\nˆw∗= arg min\\nˆw\\nm\\nX\\ni=1\\n\\x10\\nyi −ˆwTˆxi\\n\\x112\\n= arg min\\nˆw\\nm\\nX\\ni=1\\n\\x10\\nyi −ˆxT\\ni ˆw\\n\\x112\\n根据向量内积的定义可知，上式可以写成如下向量内积的形式\\nˆw∗= arg min\\nˆw\\nh\\ny1 −ˆxT\\n1 ˆw\\n· · ·\\nym −ˆxT\\nm ˆw\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny1 −ˆxT\\n1 ˆw\\n...\\nym −ˆxT\\nm ˆw\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n其中\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny1 −ˆxT\\n1 ˆw\\n...\\nym −ˆxT\\nm ˆw\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny1\\n...\\nym\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb−\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nˆxT\\n1 ˆw\\n...\\nˆxT\\nm ˆw\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= y −\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nˆxT\\n1\\n...\\nˆxT\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb· ˆw\\n= y −X ˆw\\n所以\\nˆw∗= arg min\\nˆw\\n(y −X ˆw)T(y −X ˆw)\\n3.2.7\\n式(3.10) 的推导\\n将E ˆw = (y −X ˆw)T(y −X ˆw) 展开可得\\nE ˆw = yTy −yTX ˆw −ˆwTXTy + ˆwTXTX ˆw\\n对ˆw 求导可得\\n∂E ˆw\\n∂ˆw = ∂yTy\\n∂ˆw\\n−∂yTX ˆw\\n∂ˆw\\n−∂ˆwTXTy\\n∂ˆw\\n+ ∂ˆwTXTX ˆw\\n∂ˆw\\n由矩阵微分公式∂aTx\\n∂x\\n= ∂xTa\\n∂x\\n= a, ∂xTAx\\n∂x\\n= (A+AT)x （更多矩阵微分公式可查阅[2]，矩阵微分原理可查阅[3]）可\\n得\\n∂E ˆw\\n∂ˆw = 0 −XTy −XTy + (XTX + XTX) ˆw\\n= 2XT(X ˆw −y)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.2.8\\n式(3.11) 的推导\\n首先铺垫讲解接下来以及后续内容将会用到的多元函数相关基础知识\\n[1]。\\nn 元实值函数：含n 个自变量，值域为实数域R 的函数称为n 元实值函数，记为f(x)，其中x =\\n(x1; x2; ...; xn) 为n 维向量。“西瓜书”和本书中的多元函数未加特殊说明均为实值函数。\\n凸集：设集合D ⊂Rn 为n 维欧式空间中的子集，如果对D 中任意的n 维向量x ∈D 和y ∈D 与\\n任意的α ∈[0, 1]，有\\nαx + (1 −α)y ∈D\\n则称集合D 是凸集。凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点均属于此集\\n合。常见的凸集有空集∅，整个n 维欧式空间Rn。\\n凸函数：设D ⊂Rn 是非空凸集，f 是定义在D 上的函数，如果对任意的x1, x2 ∈D, α ∈(0, 1)，均\\n有\\nf\\n\\x00αx1 + (1 −α)x2\\x01\\n⩽αf(x1) + (1 −α)f(x2)\\n则称f 为D 上的凸函数。若其中的⩽改为< 也恒成立，则称f 为D 上的严格凸函数。\\n梯度：若n 元函数f(x) 对x = (x1; x2; ...; xn) 中各分量xi 的偏导数∂f(x)\\n∂xi (i = 1, 2, ..., n) 都存在，则\\n称函数f(x) 在x 处一阶可导，并称以下列向量\\n∇f(x) = ∂f(x)\\n∂x\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂f(x)\\n∂x1\\n∂f(x)\\n∂x2...\\n∂f(x)\\n∂xn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n为函数f(x) 在x 处的一阶导数或梯度，易证梯度指向的方向是函数值增大速度最快的方向。∇f(x) 也可\\n写成行向量形式\\n∇f(x) = ∂f(x)\\n∂xT\\n=\\n\\x14∂f(x)\\n∂x1\\n, ∂f(x)\\n∂x2\\n, · · ·, ∂f(x)\\n∂xn\\n\\x15\\n我们称列向量形式为“分母布局”，行向量形式为“分子布局”，由于在最优化中习惯采用分母布局，因此\\n“西瓜书”以及本书中也采用分母布局。为了便于区分当前采用何种布局，通常在采用分母布局时偏导符\\n号∂后接的是x，采用分子布局时后接的是xT。\\nHessian 矩阵：若n 元函数f(x) 对x = (x1; x2; ...; xn) 中各分量xi 的二阶偏导数\\n∂2f(x)\\n∂xi∂xj (i =\\n1, 2, ..., n; j = 1, 2, ..., n) 都存在，则称函数f(x) 在x 处二阶阶可导，并称以下矩阵\\n∇2f(x) = ∂2f(x)\\n∂x∂xT =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2f(x)\\n∂x2\\n1\\n∂2f(x)\\n∂x1∂x2\\n· · ·\\n∂2f(x)\\n∂x1∂xn\\n∂2f(x)\\n∂x2∂x1\\n∂2f(x)\\n∂x2\\n2\\n· · ·\\n∂2f(x)\\n∂x2∂xn\\n...\\n...\\n...\\n...\\n∂2f(x)\\n∂xn∂x1\\n∂2f(x)\\n∂xn∂x2\\n· · ·\\n∂2f(x)\\n∂x2n\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n为函数f(x) 在x 处的二阶导数或Hessian 矩阵。若其中的二阶偏导数均连续，则\\n∂2f(x)\\n∂xi∂xj\\n= ∂2f(x)\\n∂xj∂xi\\n此时Hessian 矩阵为对称矩阵。\\n定理3.1：设D ⊂Rn 是非空开凸集，f(x) 是定义在D 上的实值函数，且f(x) 在D 上二阶连续可\\n微，如果f(x) 的Hessian 矩阵∇2f(x) 在D 上是半正定的，则f(x) 是D 上的凸函数；如果∇2f(x) 在\\nD 上是正定的，则f(x) 是D 上的严格凸函数。\\n定理3.2：若f(x) 是凸函数，且f(x) 一阶连续可微，则x∗是全局解的充分必要条件是其梯度等于\\n零向量，即∇f(x∗) = 0。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(3.11) 的推导思路如下：首先根据定理3.1 推导出E ˆw 是ˆw 的凸函数，接着根据定理3.2 推导出\\n式(3.11)。下面按照此思路进行推导。\\n由于式(3.10) 已推导出E ˆw 关于ˆw 的一阶导数，接着基于此进一步推导出二阶导数，即Hessian 矩\\n阵。推导过程如下：\\n∇2E ˆw =\\n∂\\n∂ˆwT\\n\\x12∂E ˆw\\n∂ˆw\\n\\x13\\n=\\n∂\\n∂ˆwT\\n\\x02\\n2XT(X ˆw −y)\\n\\x03\\n=\\n∂\\n∂ˆwT\\n\\x002XTX ˆw −2XTy\\n\\x01\\n由矩阵微分公式∂Ax\\nxT = A 可得\\n∇2E ˆw = 2XTX\\n如“西瓜书”中式(3.11) 上方的一段话所说，假定XTX 为正定矩阵，根据定理3.1 可知此时E ˆw 是\\nˆw 的严格凸函数，接着根据定理3.2 可知只需令E ˆw 关于ˆw 的一阶导数等于零向量，即令式(3.10) 等于\\n零向量即可求得全局最优解ˆw∗，具体求解过程如下：\\n∂E ˆw\\n∂ˆw = 2XT(X ˆw −y) = 0\\n2XTX ˆw −2XTy = 0\\n2XTX ˆw = 2XTy\\nˆw = (XTX)−1XTy\\n令其为ˆw∗即为式(3.11)。\\n由于X 是由样本构成的矩阵，而样本是千变万化的，因此无法保证XTX 一定是正定矩阵，极易出现\\n非正定的情形。当XTX 非正定矩阵时，除了“西瓜书”中所说的引入正则化外，也可用XTX 的伪逆矩阵\\n代入式(3.11) 求解出ˆw∗，只是此时并不保证求解得到的ˆw∗一定是全局最优解。除此之外，也可用下一\\n节将会讲到的“梯度下降法”求解，同样也不保证求得全局最优解。\\n3.3\\n对数几率回归\\n对数几率回归的一般使用流程如下：首先在训练集上学得模型\\ny =\\n1\\n1 + e−(wTx+b)\\n然后对于新的测试样本xi，将其代入模型得到预测结果yi，接着自行设定阈值θ，通常设为θ = 0.5，如\\n果yi ⩾θ 则判xi 为正例，反之判为反例。\\n3.3.1\\n式(3.27) 的推导\\n将式(3.26) 代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln (yip1(ˆxi; β) + (1 −yi)p0(ˆxi; β))\\n其中p1(ˆxi; β) =\\neβT ˆxi\\n1+eβT ˆxi , p0(ˆxi; β) =\\n1\\n1+eβT ˆxi ，代入上式可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln\\n \\nyieβT ˆxi + 1 −yi\\n1 + eβT ˆxi\\n!\\n=\\nm\\nX\\ni=1\\n\\x10\\nln(yieβT ˆxi + 1 −yi) −ln(1 + eβT ˆxi)\\n\\x11\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由于yi=0 或1，则\\nℓ(β) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1(−ln(1 + eβT ˆxi)),\\nyi = 0\\nPm\\ni=1(βTˆxi −ln(1 + eβT ˆxi)),\\nyi = 1\\n两式综合可得\\nℓ(β) =\\nm\\nX\\ni=1\\n\\x10\\nyiβTˆxi −ln(1 + eβT ˆxi)\\n\\x11\\n由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，即在似然\\n函数前添加负号即可得式(3.27)。值得一提的是，若将式(3.26) 改写为p(yi|xi; w, b) = [p1(ˆxi; β)]yi[p0(ˆxi; β)]1−yi，\\n再代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln\\n\\x00[p1(ˆxi; β)]yi[p0(ˆxi; β)]1−yi\\x01\\n=\\nm\\nX\\ni=1\\n[yi ln (p1(ˆxi; β)) + (1 −yi) ln (p0(ˆxi; β))]\\n=\\nm\\nX\\ni=1\\n{yi [ln (p1(ˆxi; β)) −ln (p0(ˆxi; β))] + ln (p0(ˆxi; β))}\\n=\\nm\\nX\\ni=1\\n\\x14\\nyi ln\\n\\x12p1(ˆxi; β)\\np0(ˆxi; β)\\n\\x13\\n+ ln (p0(ˆxi; β))\\n\\x15\\n=\\nm\\nX\\ni=1\\n\\x14\\nyi ln\\n\\x10\\neβT ˆxi\\x11\\n+ ln\\n\\x12\\n1\\n1 + eβT ˆxi\\n\\x13\\x15\\n=\\nm\\nX\\ni=1\\n\\x10\\nyiβTˆxi −ln(1 + eβT ˆxi)\\n\\x11\\n显然，此种方式更易推导出式(3.27)。\\n“西瓜书”在式(3.27) 下方有提到式(3.27) 是关于β 的凸函数，其证明过程如下：由于若干半正定矩\\n阵的加和仍为半正定矩阵，则根据定理3.1 可知，若干凸函数的加和仍为凸函数。因此，只需证明式(3.27)\\n求和符号后的式子−yiβTˆxi + ln(1 + eβT ˆxi)（记为f(β)）为凸函数即可。根据式(3.31) 可知，f(β) 的二\\n阶导数，即Hessian 矩阵为\\nˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n对于任意非零向量y ∈Rd+1，恒有\\nyT · ˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β)) · y\\nyTˆxiˆxT\\ni yp1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n\\x00yTˆxi\\n\\x012 p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n由于p1 (ˆxi; β) > 0，因此上式恒大于等于0，根据半正定矩阵的定义可知此时f(β) 的Hessian 矩阵为半\\n正定矩阵，所以f(β) 是关于β 的凸函数。\\n3.3.2\\n梯度下降法\\n不同于式(3.7) 可求得闭式解，式(3.27) 中的β 没有闭式解，因此需要借助其他工具进行求解。求解\\n使得式(3.27) 取到最小值的β 属于最优化中的“无约束优化问题”，在无约束优化问题中最常用的求解算\\n法有“梯度下降法”和“牛顿法”\\n[1]，下面分别展开讲解。\\n梯度下降法是一种迭代求解算法，其基本思路如下：先在定义域中随机选取一个点x0，将其代入函\\n数f(x) 并判断此时f(x0) 是否是最小值，如果不是的话，则找下一个点x1，且保证f(x1) < f(x0)，然\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n后接着判断f(x1) 是否是最小值，如果不是的话则重复上述步骤继续迭代寻找x2、x3、...... 直到找到使\\n得f(x) 取到最小值的x∗。\\n显然，此算法要想行得通就必须解决在找到第t 个点xt 时，能进一步找到第t + 1 个点xt+1，且保\\n证f(xt+1) < f(xt)。梯度下降法利用“梯度指向的方向是函数值增大速度最快的方向”这一特性，每次迭\\n代时朝着梯度的反方向进行，进而实现函数值越迭代越小，下面给出完整的数学推导过程。\\n根据泰勒公式可知，当函数f(x) 在xt 处一阶可导时，在其邻域内进行一阶泰勒展开恒有\\nf(x) = f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ o\\n\\x00\\r\\rx −xt\\r\\r\\x01\\n其中∇f (xt) 是函数f(x) 在点xt 处的梯度，∥x −xt∥是指向量x −xt 的模。若令x −xt = adt，其中\\na > 0，dt 是模长为1 的单位向量，则上式可改写为\\nf(xt + adt) = f\\n\\x00xt\\x01\\n+ a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\rdt\\r\\r\\x01\\nf(xt + adt) −f\\n\\x00xt\\x01\\n= a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\rdt\\r\\r\\x01\\n观察上式可知，如果能保证a∇f (xt)\\nT dt < 0，则一定能保证f(xt + adt) < f (xt)，此时再令xt+1 =\\nxt + adt，即可推得我们想要的f(xt+1) < f(xt)。所以，此时问题转化为了求解能使得a∇f (xt)\\nT dt < 0\\n的dt，且a∇f (xt)\\nT dt 比0 越小，相应地f(xt+1) 也会比f(xt) 越小，也更接近最小值。\\n根据向量的内积公式可知\\na∇f\\n\\x00xt\\x01T dt = a × ∥∇f\\n\\x00xt\\x01\\n∥× ∥dt∥× cos θt\\n其中θt 是向量∇f (xt) 与向量dt 之间的夹角。观察上式易知，此时∥∇f (xt) ∥是固定常量，∥dt∥= 1，\\n所以当a 也固定时，取θt = π，即向量dt 与向量∇f (xt) 的方向刚好相反时，上式取到最小值。通常为\\n了精简计算步骤，可直接令dt = −∇f (xt)，因此便得到了第t + 1 个点xt+1 的迭代公式\\nxt+1 = xt −a∇f\\n\\x00xt\\x01\\n其中a 也称为“步长”或“学习率”，是需要自行设定的参数，且每次迭代时可取不同值。\\n除了需要解决如何找到xt+1 以外，梯度下降法通常还需要解决如何判断当前点是否使得函数取到了\\n最小值，否则的话迭代过程便可能会无休止进行。常用的做法是预先设定一个极小的阈值ϵ，当某次迭代\\n造成的函数值波动已经小于ϵ 时，即|f(xt+1) −f(xt)| < ϵ，我们便近似地认为此时f(xt+1) 取到了最小\\n值。\\n3.3.3\\n牛顿法\\n同梯度下降法，牛顿法也是一种迭代求解算法，其基本思路和梯度下降法一致，只是在选取第t + 1\\n个点xt+1 时所采用的策略有所不同，即迭代公式不同。梯度下降法每次选取xt+1 时，只要求通过泰勒公\\n式在xt 的邻域内找到一个函数值比其更小的点即可，而牛顿法则期望在此基础之上，xt+1 还必须是xt\\n的邻域内的极小值点。\\n类似一元函数取到极值点的必要条件是一阶导数等于0，多元函数取到极值点的必要条件是其梯度等\\n于零向量0，为了能求解出xt 的邻域内梯度等于0 的点，需要进行二阶泰勒展开，其展开式如下\\nf(x) = f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ 1\\n2\\n\\x00x −xt\\x01T ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n+ o\\n\\x00\\r\\rx −xt\\r\\r\\x01\\n为了后续计算方便，我们取其近似形式\\nf(x) ≈f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ 1\\n2\\n\\x00x −xt\\x01T ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n首先对上式求导\\n∂f(x)\\n∂x\\n= ∂f (xt)\\n∂x\\n+ ∂∇f (xt)\\nT (x −xt)\\n∂x\\n+ 1\\n2\\n∂(x −xt)\\nT ∇2f (xt) (x −xt)\\n∂x\\n= 0 + ∇f\\n\\x00xt\\x01\\n+ 1\\n2\\n\\x10\\n∇2f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01T\\x11 \\x00x −xt\\x01\\n假设函数f(x) 在xt 处二阶可导，且偏导数连续，则∇2f (xt) 是对称矩阵，上式可写为\\n∂f(x)\\n∂x\\n= 0 + ∇f\\n\\x00xt\\x01\\n+ 1\\n2 × 2 × ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n= ∇f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n令上式等于0\\n∇f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n= 0\\n当∇2f (xt) 是可逆矩阵时，解得\\nx = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01\\n令上式为xt+1 即可得到牛顿法的迭代公式\\nxt+1 = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01\\n通过上述推导可知，牛顿法每次迭代时需要求解Hessian 矩阵的逆矩阵，该步骤计算量通常较大，因此有\\n人基于牛顿法，将其中求Hessian 矩阵的逆矩阵改为求计算量更低的近似逆矩阵，我们称此类算法为“拟\\n牛顿法”。\\n牛顿法虽然期望在每次迭代时能取到极小值点，但是通过上述推导可知，迭代公式是根据极值点的必\\n要条件推导而得，因此并不保证一定是极小值点。\\n无论是梯度下降法还是牛顿法，根据其终止迭代的条件可知，其都是近似求解算法，即使f(x) 是凸\\n函数，也并不一定保证最终求得的是全局最优解，仅能保证其接近全局最优解。不过在解决实际问题时，\\n并不一定苛求解得全局最优解，在能接近全局最优甚至局部最优时通常也能很好地解决问题。\\n3.3.4\\n式(3.29) 的解释\\n根据上述牛顿法的迭代公式可知，此式为式(3.27) 应用牛顿法时的迭代公式。\\n3.3.5\\n式(3.30) 的推导\\n∂ℓ(β)\\n∂β\\n=\\n∂Pm\\ni=1\\n\\x10\\n−yiβTˆxi + ln\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\x11\\n∂β\\n=\\nm\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed∂\\n\\x00−yiβTˆxi\\n\\x01\\n∂β\\n+\\n∂ln\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n∂β\\n\\uf8f6\\n\\uf8f8\\n=\\nm\\nX\\ni=1\\n\\x12\\n−yiˆxi +\\n1\\n1 + eβT ˆxi · ˆxieβT ˆxi\\n\\x13\\n= −\\nm\\nX\\ni=1\\nˆxi\\n \\nyi −\\neβT ˆxi\\n1 + eβT ˆxi\\n!\\n= −\\nm\\nX\\ni=1\\nˆxi (yi −p1 (ˆxi; β))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此式也可以进行向量化，令p1(ˆxi; β) = ˆyi，代入上式得\\n∂ℓ(β)\\n∂β\\n= −\\nm\\nX\\ni=1\\nˆxi(yi −ˆyi)\\n=\\nm\\nX\\ni=1\\nˆxi(ˆyi −yi)\\n= XT(ˆy −y)\\n其中ˆy = (ˆy1; ˆy2; ...; ˆym), y = (y1; y2; ...; ym)。\\n3.3.6\\n式(3.31) 的推导\\n继续对上述式(3.30) 中倒数第二个等号的结果求导\\n∂2ℓ(β)\\n∂β∂βT = −\\n∂Pm\\ni=1 ˆxi\\n\\x10\\nyi −\\neβT ˆxi\\n1+eT ˆxi\\n\\x11\\n∂βT\\n= −\\nm\\nX\\ni=1\\nˆxi\\n∂\\n\\x10\\nyi −\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n= −\\nm\\nX\\ni=1\\nˆxi\\n\\uf8eb\\n\\uf8ed∂yi\\n∂βT −\\n∂\\n\\x10\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n\\uf8f6\\n\\uf8f8\\n=\\nm\\nX\\ni=1\\nˆxi ·\\n∂\\n\\x10\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n根据矩阵微分公式∂aTx\\n∂xT = ∂xTa\\n∂xT = aT，其中\\n∂\\n\\x10\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n=\\n∂eβT ˆxi\\n∂βT\\n·\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n−eβT ˆxi ·\\n∂\\n(\\n1+eβT ˆxi\\n)\\n∂βT\\n\\x001 + eβT ˆxi\\x012\\n=\\nˆxT\\ni eβT ˆxi ·\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n−eβT ˆxi · ˆxT\\ni eβT ˆxi\\n\\x001 + eβT ˆxi\\x012\\n= ˆxT\\ni eβT ˆxi ·\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n−eβT ˆxi\\n\\x001 + eβT ˆxi\\x012\\n= ˆxT\\ni eβT ˆxi ·\\n1\\n\\x001 + eβT ˆxi\\x012\\n= ˆxT\\ni ·\\neβT ˆxi\\n1 + eβT ˆxi ·\\n1\\n1 + eβT ˆxi\\n所以\\n∂2ℓ(β)\\n∂β∂βT =\\nm\\nX\\ni=1\\nˆxi · ˆxT\\ni ·\\neβT ˆxi\\n1 + eβT ˆxi ·\\n1\\n1 + eβT ˆxi\\n=\\nm\\nX\\ni=1\\nˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n3.4\\n线性判别分析\\n线性判别分析的一般使用流程如下：首先在训练集上学得模型\\ny = wTx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由向量内积的几何意义可知，y 可以看作是x 在w 上的投影，因此在训练集上学得的模型能够保证训练\\n集中的同类样本在w 上的投影y 很相近，而异类样本在w 上的投影y 很疏远。然后对于新的测试样本\\nxi，将其代入模型得到它在w 上的投影yi，然后判别这个投影yi 与哪一类投影更近，则将其判为该类。\\n最后，线性判别分析也是一种降维方法，但不同于第10 章介绍的无监督降维方法，线性判别分析是\\n一种监督降维方法，即降维过程中需要用到样本类别标记信息。\\n3.4.1\\n式(3.32) 的推导\\n式(3.32) 中∥wTµ0 −wTµ1∥2\\n2 左下角的“2”表示求“2 范数”，向量的2 范数即为模，右上角的“2”\\n表示求平方数，基于此，下面推导式(3.32)。\\nJ = ∥wTµ0 −wTµ1∥2\\n2\\nwT(Σ0 + Σ1)w\\n= ∥(wTµ0 −wTµ1)T∥2\\n2\\nwT(Σ0 + Σ1)w\\n= ∥(µ0 −µ1)Tw∥2\\n2\\nwT(Σ0 + Σ1)w\\n=\\n\\x02\\n(µ0 −µ1)Tw\\n\\x03T (µ0 −µ1)Tw\\nwT(Σ0 + Σ1)w\\n= wT(µ0 −µ1)(µ0 −µ1)Tw\\nwT(Σ0 + Σ1)w\\n3.4.2\\n式(3.37) 到式(3.39) 的推导\\n由式(3.36)，可定义拉格朗日函数为\\nL(w, λ) = −wTSbw + λ(wTSww −1)\\n对w 求偏导可得\\n∂L(w, λ)\\n∂w\\n= −∂(wTSbw)\\n∂w\\n+ λ∂(wTSww −1)\\n∂w\\n= −(Sb + ST\\nb )w + λ(Sw + ST\\nw)w\\n由于Sb = ST\\nb , Sw = ST\\nw，所以\\n∂L(w, λ)\\n∂w\\n= −2Sbw + 2λSww\\n令上式等于0 即可得\\n−2Sbw + 2λSww = 0\\nSbw = λSww\\n(µ0 −µ1)(µ0 −µ1)Tw = λSww\\n若令(µ0 −µ1)Tw = γ，则有\\nγ(µ0 −µ1) = λSww\\nw = γ\\nλS−1\\nw (µ0 −µ1)\\n由于最终要求解的w 不关心其大小，只关心其方向，所以其大小可以任意取值。又因为µ0 和µ1 的大小\\n是固定的，所以γ 的大小只受w 的大小影响，因此可以通过调整w 的大小使得γ = λ，西瓜书中所说的\\n“不妨令Sbw = λ(µ0 −µ1)”也可等价理解为令γ = λ，因此，此时γ\\nλ = 1，求解出的w 即为式(3.39)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.4.3\\n式(3.43) 的推导\\n由式(3.40)、式(3.41)、式(3.42) 可得\\nSb = St −Sw\\n=\\nm\\nX\\ni=1\\n(xi −µ)(xi −µ)T −\\nN\\nX\\ni=1\\nX\\nx∈Xi\\n(x −µi)(x −µi)T\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00(x −µ)(x −µ)T −(x −µi)(x −µi)T\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00(x −µ)(xT −µT) −(x −µi)(xT −µT\\ni )\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00xxT −xµT −µxT + µµT −xxT + xµT\\ni + µixT −µiµT\\ni\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00−xµT −µxT + µµT + xµT\\ni + µixT −µiµT\\ni\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n \\n−\\nX\\nx∈Xi\\nxµT −\\nX\\nx∈Xi\\nµxT +\\nX\\nx∈Xi\\nµµT +\\nX\\nx∈Xi\\nxµT\\ni +\\nX\\nx∈Xi\\nµixT −\\nX\\nx∈Xi\\nµiµT\\ni\\n!\\n=\\nN\\nX\\ni=1\\n\\x00−miµiµT −miµµT\\ni + miµµT + miµiµT\\ni + miµiµT\\ni −miµiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\n\\x00−miµiµT −miµµT\\ni + miµµT + miµiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\nmi\\n\\x00−µiµT −µµT\\ni + µµT + µiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\nmi(µi −µ)(µi −µ)T\\n3.4.4\\n式(3.44) 的推导\\n此式是式(3.35) 的推广形式，证明如下。\\n设W = (w1, w2, ..., wi, ..., wN−1) ∈Rd×(N−1)，其中wi ∈Rd×1 为d 行1 列的列向量，则\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\ntr(WTSbW) =\\nN−1\\nX\\ni=1\\nwT\\ni Sbwi\\ntr(WTSwW) =\\nN−1\\nX\\ni=1\\nwT\\ni Swwi\\n所以式(3.44) 可变形为\\nmax\\nW\\nPN−1\\ni=1 wT\\ni Sbwi\\nPN−1\\ni=1 wT\\ni Swwi\\n对比式(3.35) 易知，上式即式(3.35) 的推广形式。\\n除了式(3.35) 以外，还有一种常见的优化目标形式如下\\nmax\\nW\\nQN−1\\ni=1 wT\\ni Sbwi\\nQN−1\\ni=1 wT\\ni Swwi\\n= max\\nW\\nN−1\\nY\\ni=1\\nwT\\ni Sbwi\\nwT\\ni Swwi\\n无论是采用何种优化目标形式，其优化目标只要满足“同类样例的投影点尽可能接近，异类样例的投\\n影点尽可能远离”即可。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.4.5\\n式(3.45) 的推导\\n同式(3.35)，此处也固定式(3.44) 的分母为1，那么式(3.44) 此时等价于如下优化问题\\nmin\\nw\\n−tr(WTSbW)\\ns.t.\\ntr(WTSwW) = 1\\n根据拉格朗日乘子法，可定义上述优化问题的拉格朗日函数\\nL(W, λ) = −tr(WTSbW) + λ(tr(WTSwW) −1)\\n根据矩阵微分公式\\n∂\\n∂X tr (XTBX) = (B + BT)X 对上式关于W 求偏导可得\\n∂L(W, λ)\\n∂W\\n= −∂\\n\\x00tr(WTSbW)\\n\\x01\\n∂W\\n+ λ∂\\n\\x00tr(WTSwW) −1\\n\\x01\\n∂W\\n= −(Sb + ST\\nb )W + λ(Sw + ST\\nw)W\\n由于Sb = ST\\nb , Sw = ST\\nw，所以\\n∂L(W, λ)\\n∂W\\n= −2SbW + 2λSwW\\n令上式等于0 即可得\\n−2SbW + 2λSwW = 0\\nSbW = λSwW\\n此即为式(3.45)，但是此式在解释为何要取N −1 个最大广义特征值所对应的特征向量来构成W 时不够\\n直观。因此，我们换一种更为直观的方式求解式(3.44)，只需换一种方式构造拉格朗日函数即可。\\n重新定义上述优化问题的拉格朗日函数\\nL(W, Λ) = −tr(WTSbW) + tr\\n\\x00Λ(WTSwW −I)\\n\\x01\\n其中，I ∈R(N−1)×(N−1) 为单位矩阵，Λ = diag(λ1, λ2, ..., λN−1) ∈R(N−1)×(N−1) 是由N −1 个拉格朗日\\n乘子构成的对角矩阵。根据矩阵微分公式\\n∂\\n∂X tr(XTAX) = (A + AT)X,\\n∂\\n∂X tr(XAXTB) =\\n∂\\n∂X tr(AXTBX) =\\nBTXAT + BXA，对上式关于W 求偏导可得\\n∂L(W, Λ)\\n∂W\\n= −∂\\n\\x00tr(WTSbW)\\n\\x01\\n∂W\\n+ ∂\\n\\x00tr\\n\\x00ΛWTSwW −ΛI\\n\\x01\\x01\\n∂W\\n= −(Sb + ST\\nb )W + (ST\\nwWΛT + SwWΛ)\\n由于Sb = ST\\nb , Sw = ST\\nw, ΛT = Λ，所以\\n∂L(W, Λ)\\n∂W\\n= −2SbW + 2SwWΛ\\n令上式等于0 即可得\\n−2SbW + 2SwWΛ = 0\\nSbW = SwWΛ\\n将W 和Λ 展开可得\\nSbwi = λiSwwi,\\ni = 1, 2, ..., N −1\\n此时便得到了N −1 个广义征值问题。进一步地，将其代入优化问题的目标函数可得\\nmin\\nw\\n−tr(WTSbW) = max\\nW tr(WTSbW)\\n= max\\nW\\nN−1\\nX\\ni=1\\nwT\\ni Sbwi\\n= max\\nW\\nN−1\\nX\\ni=1\\nλiwT\\ni Swwi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由于存在约束tr(WTSwW) =\\nN−1\\nP\\ni=1\\nwT\\ni Swwi = 1，所以欲使上式取到最大值，只需取N −1 个最大的λi 即\\n可。根据Sbwi = λiSwwi 可知，λi 对应的便是广义特征值，wi 是λi 所对应的特征向量。\\n（广义特征值的定义和常用求解方法可查阅[3]）\\n对于N 分类问题，一定要求出N −1 个wi 吗？其实不然。之所以将W 定义为d × (N −1) 维的矩\\n阵是因为当d > (N −1) 时，实对称矩阵S−1\\nw Sb 的秩至多为N −1，所以理论上至多能解出N −1 个非零\\n特征值λi 及其对应的特征向量wi。但是S−1\\nw Sb 的秩是受当前训练集中的数据分布所影响的，因此并不一\\n定为N −1。此外，当数据分布本身就足够理想时，即使能求解出多个wi，但是实际可能只需要求解出1\\n个wi 便可将同类样本聚集，异类样本完全分离。\\n当d > (N −1) 时，实对称矩阵S−1\\nw Sb 的秩至多为N −1 的证明过程如下：由于µ =\\n1\\nN\\nNP\\ni=1\\nmiµi，所\\n以µ1 −µ 一定可以由µ 和µ2, ..., µN 线性表示，因此矩阵Sb 中至多有µ2 −µ, ..., µN −µ 共N −1 个\\n线性无关的向量，由于此时d > (N −1)，所以Sb 的秩r(Sb) 至多为N −1。同时假设矩阵Sw 满秩，即\\nr(Sw) = r(S−1\\nw ) = d，则根据矩阵秩的性质r(AB) ⩽min{r(A), r(B)} 可知，S−1\\nw Sb 的秩也至多为N −1。\\n3.5\\n多分类学习\\n3.5.1\\n图3.5 的解释\\n图3.5 中所说的“海明距离”是指两个码对应位置不相同的个数，“欧式距离”则是指两个向量之间\\n的欧氏距离，例如图3.5(a) 中第1 行的编码可以视作为向量(−1, +1, −1, +1, +1)，测试示例的编码则为\\n(−1, −1, +1, −1, +1)，其中第2 个、第3 个、第4 个元素不相同，所以它们的海明距离为3，欧氏距离为\\np\\n(−1 −(−1))2 + (1 −(−1))2 + (−1 −1)2 + (1 −(−1))2 + (1 −1)2 = √0 + 4 + 4 + 4 + 0 = 2\\n√\\n3。需要注\\n意的是，在计算海明距离时，与“停用类”不同算作0.5，例如图3.5(b) 中第2 行的海明距离计算公式为\\n0.5 + 0.5 + 0.5 + 0.5 = 2。\\n3.6\\n类别不平衡问题\\n对于类别平衡问题，“西瓜书”2.3.1 节中的“精度”通常无法满足该特殊任务的需求，例如“西瓜书”\\n在本节第一段的举例：有998 个反例和2 个正例，若机器学习算法返回一个永远将新样本预测为反例的\\n学习器则能达到99.8% 的精度，显然虚高，因此在类别不平衡时常采用2.3 节中的查准率、查全率和F1\\n来度量学习器的性能。\\n参考文献\\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\\n[2] Wikipedia contributors. Matrix calculus, 2022.\\n[3] 张贤达. 矩阵分析与应用. 第2 版. 清华大学出版社, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第4 章\\n决策树\\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\\n算法会更为通俗易懂。\\n4.1\\n基本流程\\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行），即“if⋯⋯elif⋯⋯\\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜），那么此时划分的目的\\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性），\\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”，图4.2 第14 行的A\\\\{a∗}\\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\\n本（例如仍然同时含有好瓜和坏瓜），但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\\n归返回的情形(2)；\\n（3）根据某个属性进行划分时，若该属性多个属性值中的某个属性值不包含任何样本（例如未收集到），\\n例如对当前子集以“纹理”属性来划分，“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\\n能会有样本落入该分支。此即为递归返回的情形(3)。\\n4.2\\n划分选择\\n本节介绍的三种划分选择方法，即信息增益、增益率、基尼指数分别对应著名的ID3、C4.5 和CART\\n三种决策树算法。\\n4.2.1\\n式(4.1) 的解释\\n该式为信息论中的信息熵定义式，以下先证明0 ⩽Ent(D) ⩽log2 |Y|，然后解释其最大值和最小值所\\n表示的含义。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n已知集合D 的信息熵的定义为\\nEnt(D) = −\\n|Y|\\nX\\nk=1\\npk log2 pk\\n其中，|Y| 表示样本类别总数，pk 表示第k 类样本所占的比例，有0 ⩽pk ⩽1, Pn\\nk=1 pk = 1。若令\\n|Y| = n, pk = xk，那么信息熵Ent(D) 就可以看作一个n 元实值函数，即\\nEnt(D) = f(x1, · · · , xn) = −\\nn\\nX\\nk=1\\nxk log2 xk\\n其中0 ⩽xk ⩽1, Pn\\nk=1 xk = 1。\\n下面考虑求该多元函数的最值.\\n首先我们先来求最大值，如果不考虑约束0 ⩽xk ⩽1 而仅考虑\\nPn\\nk=1 xk = 1，则对f(x1, · · · , xn) 求最大值等价于如下最小化问题：\\nmin\\nnP\\nk=1\\nxk log2 xk\\ns.t.\\nnP\\nk=1\\nxk = 1\\n显然，在0 ⩽xk ⩽1 时，此问题为凸优化问题。对于凸优化问题来说，使其拉格朗日函数的一阶偏导数等\\n于0 的点即最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为\\nL(x1, · · · , xn, λ) =\\nn\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!\\n其中，λ 为拉格朗日乘子。对L(x1, · · · , xn, λ) 分别关于x1, · · · , xn, λ 求一阶偏导数，并令偏导数等于0\\n可得\\n∂L(x1, · · · , xn, λ)\\n∂x1\\n=\\n∂\\n∂x1\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n= log2 x1 + x1 ·\\n1\\nx1 ln 2 + λ = 0\\n= log2 x1 +\\n1\\nln 2 + λ = 0\\n⇒λ = −log2 x1 −\\n1\\nln 2\\n∂L(x1, · · · , xn, λ)\\n∂x2\\n=\\n∂\\n∂x2\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒λ = −log2 x2 −\\n1\\nln 2\\n· · ·\\n∂L(x1, · · · , xn, λ)\\n∂xn\\n=\\n∂\\n∂xn\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒λ = −log2 xn −\\n1\\nln 2;\\n∂L(x1, · · · , xn, λ)\\n∂λ\\n= ∂\\n∂λ\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒\\nn\\nX\\nk=1\\nxk = 1\\n整理一下可得\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nλ = −log2 x1 −\\n1\\nln 2 = −log2 x2 −\\n1\\nln 2 = · · · = −log2 xn −\\n1\\nln 2\\nnP\\nk=1\\nxk = 1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由以上两个方程可以解得\\nx1 = x2 = · · · = xn = 1\\nn\\n又因为xk 还需满足约束0 ⩽xk ⩽1，显然0 ⩽1\\nn ⩽1，所以x1 = x2 = · · · = xn = 1\\nn 是满足所有约束的\\n最优解，即当前最小化问题的最小值点，同时也是f(x1, · · · , xn) 的最大值点。将x1 = x2 = · · · = xn = 1\\nn\\n代入f(x1, · · · , xn) 中可得\\nf\\n\\x12 1\\nn, · · · , 1\\nn\\n\\x13\\n= −\\nn\\nX\\nk=1\\n1\\nn log2\\n1\\nn = −n · 1\\nn log2\\n1\\nn = log2 n\\n所以f(x1, · · · , xn) 在满足约束0 ⩽xk ⩽1, Pn\\nk=1 xk = 1 时的最大值为log2 n。\\n下面求最小值。如果不考虑约束Pn\\nk=1 xk = 1 而仅考虑0 ⩽xk ⩽1，则f(x1, · · · , xn) 可以看作n 个\\n互不相关的一元函数的和，即\\nf(x1, · · · , xn) =\\nn\\nX\\nk=1\\ng(xk)\\n其中，g(xk) = −xk log2 xk, 0 ⩽xk ⩽1。那么当g(x1), g(x2), · · · , g(xn) 分别取到其最小值时，f(x1, · · · , xn)\\n也就取到了最小值，所以接下来考虑分别求g(x1), g(x2), · · · , g(xn) 各自的最小值。\\n由于g(x1), g(x2), · · · , g(xn) 的定义域和函数表达式均相同，所以只需求出g(x1) 的最小值也就求出\\n了g(x2), · · · , g(xn) 的最小值。下面考虑求g(x1) 的最小值，首先对g(x1) 关于x1 求一阶和二阶导数，有\\ng′(x1) = d(−x1 log2 x1)\\ndx1\\n= −log2 x1 −x1 ·\\n1\\nx1 ln 2 = −log2 x1 −\\n1\\nln 2\\ng′′(x1) = d (g′(x1))\\ndx1\\n=\\nd\\n\\x12\\n−log2 x1 −\\n1\\nln 2\\n\\x13\\ndx1\\n= −\\n1\\nx1 ln 2\\n显然，当0 ⩽xk ⩽1 时g′′(x1) = −\\n1\\nx1 ln 2 恒小于0，所以g(x1) 是一个在其定义域范围内开口向下的凹函\\n数，那么其最小值必然在边界取。分别取x1 = 0 和x1 = 1，代入g(x1) 可得\\ng(0) = −0 log2 0 = 0\\ng(1) = −1 log2 1 = 0\\n（计算信息熵时约定：若x = 0，则x log2 x = 0）所以，g(x1) 的最小值为0，同理可得g(x2), · · · , g(xn)\\n的最小值也都为0，即f(x1, · · · , xn) 的最小值为0。但是，此时仅考虑约束0 ⩽xk ⩽1，而未考虑\\nPn\\nk=1 xk = 1。若考虑约束Pn\\nk=1 xk = 1，那么f(x1, · · · , xn) 的最小值一定大于等于0。如果令某个\\nxk = 1，那么根据约束Pn\\nk=1 xk = 1 可知x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0，将其代入\\nf(x1, · · · , xn) 可得\\nf(0, 0, · · · , 0, 1, 0, · · · , 0)\\n= −0 log2 0 −0 log2 0 −· · · −0 log2 0 −1 log2 1 −0 log2 0 −· · · −0 log2 0 = 0\\n所以xk = 1, x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0 一定是f(x1, · · · , xn) 在满足约束Pn\\nk=1 xk = 1\\n和0 ⩽xk ⩽1 的条件下的最小值点，此时f 取到最小值0。\\n综上可知，当f(x1, · · · , xn) 取到最大值时：x1 = x2 = · · · = xn = 1\\nn，此时样本集合纯度最低；当\\nf(x1, · · · , xn) 取到最小值时：xk = 1, x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0，此时样本集合纯度最\\n高。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n4.2.2\\n式(4.2) 的解释\\n此为信息增益的定义式。在信息论中信息增益也称为“互信息”，表示已知一个随机变量的信息后另\\n一个随机变量的不确定性减少的程度。\\n下面给出互信息的定义，在此之前，还需要先解释一下什么是“条件熵”。条件熵表示的是在已知一\\n个随机变量的条件下，另一个随机变量的不确定性。具体地，假设有随机变量X 和Y ，且它们服从以下\\n联合概率分布\\nP(X = xi, Y = yj) = pij,\\ni = 1, 2, · · · , n,\\nj = 1, 2, · · · , m\\n那么在已知X 的条件下，随机变量Y 的条件熵为\\nEnt(Y |X) =\\nn\\nX\\ni=1\\npi Ent(Y |X = xi)\\n其中，pi = P(X = xi)\\uffffi = 1, 2, · · · , n。互信息定义为信息熵和条件熵的差，它表示的是已知一个随机变量\\n的信息后使得另一个随机变量的不确定性减少的程度。具体地，假设有随机变量X 和Y ，那么在已知X\\n的信息后，Y 的不确定性减少的程度为\\nI(Y ; X) = Ent(Y ) −Ent(Y |X)\\n此即互信息的数学定义。\\n所以式(4.2) 可以理解为，在已知属性a 的取值后，样本类别这个随机变量的不确定性减小的程度。\\n若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，即\\n“西瓜书”上所说的“纯度提升”越大。\\n4.2.3\\n式(4.4) 的解释\\n为了理解该式的“固有值”的概念，可以将式(4.4) 与式(4.1) 对比理解。式(4.1) 可重写为\\nEnt(D) = −\\n|Y|\\nX\\nk=1\\npk log2 pk = −\\n|Y|\\nX\\nk=1\\n\\x0c\\x0cDk\\x0c\\x0c\\n|D| log2\\n\\x0c\\x0cDk\\x0c\\x0c\\n|D|\\n其中|Dk|\\n|D| = pk，为第k 类样本所占的比例。与式(4.4) 的表达式作一下对比\\nIV(a) = −\\nV\\nX\\nv=1\\n|Dv|\\n|D| log2\\n|Dv|\\n|D|\\n其中|Dv|\\n|D| = pv，为属性a 取值为av 的样本所占的比例。即式(4.1) 是按样本的类别标记计算的信息熵，\\n而式(4.4) 是按样本属性的取值计算的信息熵。\\n4.2.4\\n式(4.5) 的推导\\n假设数据集D 中的样例标记种类共有三类，每类样本所占比例分别为p1、p2、p3。现从数据集中随\\n机抽取两个样本，两个样本类别标记正好一致的概率为\\np1p1 + p2p2 + p3p3 =\\n|Y|=3\\nX\\nk=1\\np2\\nk\\n两个样本类别标记不一致的概率为（即“基尼值”）\\nGini(D) = p1p2 + p1p3 + p2p1 + p2p3 + p3p1 + p3p2 =\\n|Y|=3\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n易证以上两式之和等于1，证明过程如下\\n|Y|=3\\nX\\nk=1\\np2\\nk +\\n|Y|=3\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′\\n= (p1p1 + p2p2 + p3p3) + (p1p2 + p1p3 + p2p1 + p2p3 + p3p1 + p3p2)\\n= (p1p1 + p1p2 + p1p3) + (p2p1 + p2p2 + p2p3) + (p3p1 + p3p2 + p3p3)\\n=p1 (p1 + p2 + p3) + p2 (p1 + p2 + p3) + p3 (p1 + p2 + p3)\\n=p1 + p2 + p3 = 1\\n所以可进一步推得式(4.5)\\nGini(D) =\\n|Y|\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′ = 1 −\\n|Y|\\nX\\nk=1\\np2\\nk\\n从数据集中D 任取两个样本，类别标记一致的概率越大表示其纯度越高（即大部分样本属于同一类），\\n类别标记不一致的概率（即基尼值）越大表示纯度越低。\\n4.2.5\\n式(4.6) 的解释\\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\\n分属性的最优划分点。常用的CART 决策树的构造算法如下\\n[1]：\\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\\nGini_index(D, a) = |Da=v|\\n|D|\\nGini(Da=v) + |Da̸=v|\\n|D|\\nGini(Da̸=v)\\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\\n(3) 重复以上两步，直至满足停止条件。\\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\\n6，反例占\\np2 = 3\\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\\n5\\n11，反例占\\np2 =\\n6\\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\\nGini_index(D, 色泽= 青绿)\\n= 6\\n17 ×\\n \\n1 −\\n\\x123\\n6\\n\\x132\\n−\\n\\x123\\n6\\n\\x132!\\n+ 11\\n17 ×\\n \\n1 −\\n\\x12 5\\n11\\n\\x132\\n−\\n\\x12 6\\n11\\n\\x132!\\n= 0.497\\n类似地，可以计算出不同属性取不同值的基尼指数如下：\\nGini_index(D, 色泽= 乌黑) = 0.456\\nGini_index(D, 色泽= 浅白) = 0.426\\nGini_index(D, 根蒂= 蜷缩) = 0.456\\nGini_index(D, 根蒂= 稍蜷) = 0.496\\nGini_index(D, 根蒂= 硬挺) = 0.439\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nGini_index(D, 敲声= 浊响) = 0.450\\nGini_index(D, 敲声= 沉闷) = 0.494\\nGini_index(D, 敲声= 清脆) = 0.439\\nGini_index(D, 纹理= 清晰) = 0.286\\nGini_index(D, 纹理= 稍稀) = 0.437\\nGini_index(D, 纹理= 模糊) = 0.403\\nGini_index(D, 脐部= 凹陷) = 0.415\\nGini_index(D, 脐部= 稍凹) = 0.497\\nGini_index(D, 脐部= 平坦) = 0.362\\nGini_index(D, 触感= 硬挺) = 0.494\\nGini_index(D, 触感= 软粘) = 0.494\\n特别地，对于属性“触感”，由于它的可取值个数为2，所以其实只需计算其中一个取值的基尼指数即可。\\n根据上面的计算结果可知，Gini_index(D, 纹理= 清晰) = 0.286 最小，所以选择属性“纹理”为最优\\n划分属性并生成根节点，接着以“纹理= 清晰”为最优划分点生成D1(纹理= 清晰)、D2(纹理̸= 清晰)\\n两个子节点，对两个子节点分别重复上述步骤继续生成下一层子节点，直至满足停止条件。\\n以上便是CART 决策树的构建过程，从构建过程可以看出，CART 决策树最终构造出来的是一棵二\\n叉树。CART 除了决策树能处理分类问题以外，回归树还可以处理回归问题，下面给出CART 回归树的\\n构造算法。\\n假设给定数据集\\nD = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中x ∈Rd 为d 维特征向量，y ∈R 是连续型随机变量。这是一个标准的回归问题的数据集, 若把每个\\n属性视为坐标空间中的一个坐标轴，则d 个属性就构成了一个d 维的特征空间，而每个d 维特征向量x\\n就对应了d 维的特征空间中的一个数据点。CART 回归树的目标是将特征空间划分成若干个子空间，每\\n个子空间都有一个固定的输出值，也就是凡是落在同一个子空间内的数据点xi，它们所对应的输出值yi\\n恒相等，且都为该子空间的输出值。\\n那么如何划分出若干个子空间呢？这里采用一种启发式的方法。\\n(1) 任意选择一个属性a，遍历其所有可能取值，根据下式找出属性a 最优划分点v∗：\\nv∗= arg min\\nv\\n\\uf8ee\\n\\uf8f0min\\nc1\\nX\\nxi∈R1(a,v)\\n(yi −c1)2 + min\\nc2\\nX\\nxi∈R2(a,v)\\n(yi −c2)2\\n\\uf8f9\\n\\uf8fb\\n其中，R1(a, v) = {x|x ∈Da⩽v}, R2(a, v) = {x|x ∈Da>v}，c1 和c2 分别为集合R1(a, v) 和\\nR2(a, v) 中的样本xi 对应的输出值yi 的均值，即\\nc1 = ave(yi|x ∈R1(a, v)) =\\n1\\n|R1(a, v)|\\nX\\nxi∈R1(a,v)\\nyi\\nc2 = ave(yi|x ∈R2(a, v)) =\\n1\\n|R2(a, v)|\\nX\\nxi∈R2(a,v)\\nyi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\\nf(x) =\\nM\\nX\\nm=1\\ncmI(x ∈Rm)\\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\\n值作为该样本的预测值yi。\\n4.3\\n剪枝处理\\n本节内容通俗易懂，跟着“西瓜书”中的例子动手演算即可，无需做过多解释。以下仅结合图4.5 继\\n续讨论一下图4.2 中的递归返回条件。图4.5 与图4.4 均是基于信息增益生成的决策树，不同在于图4.4\\n基于表4.1，而图4.5 基于表4.2 的训练集。\\n结点3⃝包含训练集“脐部”为稍凹的样本（编号6、7、15、17），当根据“根蒂”再次进行划分时不\\n含有“根蒂”为硬挺的样本（递归返回情形(3)），而恰巧四个样本（编号6、7、15、17）含两个好瓜和两\\n个坏瓜，因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。\\n结点5⃝包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本（编号6、7、15），当根据“色泽”再次\\n进行划分时不含有“色泽”为浅白的样本（递归返回情形(3)），因此叶结点浅白类别标记为好瓜（编号6、\\n7、15 样本中，前两个为好瓜，最后一个为坏瓜）。\\n结点6⃝包含训练集“脐部”为稍凹、“根蒂”为稍蜷、“色泽”为乌黑的样本（编号7、15），当根据\\n“纹理”再次进行划分时不含有“纹理”为模糊的样本（递归返回情形(3)），而恰巧两个样本（编号7、15）\\n含好瓜和坏瓜各一个，因此叶结点模糊的类别随机从类别好瓜和坏瓜中选择其一。\\n图4.5 两次随机选择均选为好瓜，实际上表示了一种归纳偏好（参见第1 章1.4 节）。\\n4.4\\n连续与缺失值\\n连续与缺失值的预处理均属于特征工程的范畴。\\n有些分类器只能使用离散属性，当遇到连续属性时则需要特殊处理，有兴趣可以通过关键词“连续属\\n性离散化”或者“Discretization”查阅更多处理方法。结合第11 章11.2 节至11.4 节分别介绍的“过滤\\n式”算法、“包裹式”算法、“嵌入式”算法的概念，若先使用某个离散化算法对连续属性离散化后再调用\\nC4.5 决策树生成算法，则是一种过滤式算法，若如4.4.1 节所述，则应该属于嵌入式算法，因为并没有以\\n学习器的预测结果准确率为评价标准，而是与决策树生成过程融为一体，因此不应该划入包裹式算法。\\n类似地，有些分类器不能使用含有缺失值的样本，需要进行预处理。常用的缺失值填充方法是：对于\\n连续属性，采用该属性的均值进行填充；对于离散属性，采用属性值个数最多的样本进行填充。这实际上\\n假设了数据集中的样本是基于独立同分布采样得到的。特别地，一般缺失值仅指样本的属性值有缺失，若\\n类别标记有缺失，一般会直接抛弃该样本。当然，也可以尝试根据第11 章11.6 节的式(11.24)，在低秩假\\n设下对数据集缺失值进行填充。\\n4.4.1\\n式(4.7) 的解释\\n此式所表达的思想很简单，就是以每两个相邻取值的中点作为划分点。下面以“西瓜书”中表4.3 中西瓜\\n数据集3.0 为例来说明此式的用法。对于“密度”这个连续属性，已观测到的可能取值为{0.243, 0.245, 0.343,\\n0.360, 0.403, 0.437, 0.481, 0.556, 0.593, 0.608, 0.634, 0.639, 0.657, 0.666, 0.697, 0.719, 0.774} 共17 个值，根据\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(4.7) 可知，此时i 依次取1 到16，那么“密度”这个属性的候选划分点集合为\\nTa = {0.243 + 0.245\\n2\\n, 0.245 + 0.343\\n2\\n, 0.343 + 0.360\\n2\\n, 0.360 + 0.403\\n2\\n, 0.403 + 0.437\\n2\\n, 0.437 + 0.481\\n2\\n,\\n0.481 + 0.556\\n2\\n, 0.556 + 0.593\\n2\\n, 0.593 + 0.608\\n2\\n, 0.608 + 0.634\\n2\\n, 0.634 + 0.639\\n2\\n, 0.639 + 0.657\\n2\\n,\\n0.657 + 0.666\\n2\\n, 0.666 + 0.697\\n2\\n, 0.697 + 0.719\\n2\\n, 0.719 + 0.774\\n2\\n}\\n4.4.2\\n式(4.8) 的解释\\n此式是式(4.2) 用于离散化后的连续属性的版本，其中Ta 由式(4.7) 计算得来，λ ∈{−, +} 表示属\\n性a 的取值分别小于等于和大于候选划分点t 时的情形，即当λ = −时有Dλ\\nt = Da⩽t\\nt\\n，当λ = + 时有\\nDλ\\nt = Da>t\\nt\\n。\\n4.4.3\\n式(4.12) 的解释\\n该式括号内与式(4.2) 基本一样，区别在于式(4.2) 中的|Dv|\\n|D| 改为式(4.11) 的˜rv，在根据式(4.1) 计\\n算信息熵时第k 类样本所占的比例改为式(4.10) 的˜pk；所有计算结束后再乘以式(4.9) 的ρ。\\n有关式(4.9) (4.10) (4.11) 中的权重wx，初始化为1。以图4.9 为例，在根据“纹理”进行划分时，除\\n编号为8、10 的两个样本在此属性缺失之外，其余样本根据自身在该属性上的取值分别划入稍糊、清晰、\\n模糊三个子集，而编号为8、10 的两个样本则要按比例同时划入三个子集。具体来说，稍糊子集包含样本\\n7、9、13、14、17 共5 个样本，清晰子集包含样本1、2、3、4、5、6、15 共7 个样本，模糊子集包含样\\n本10、11、16 共3 个样本，总共15 个在该属性不含缺失值的样本，而此时各样本的权重wx 初始化为1，\\n因此编号为8、10 的两个样本分到稍糊、清晰、模糊三个子集的权重分别为\\n5\\n15, 7\\n15 和\\n3\\n15。\\n4.5\\n多变量决策树\\n本节内容也通俗易懂，以下仅对部分图做进一步解释说明。\\n4.5.1\\n图(4.10) 的解释\\n只想用该图强调一下，离散属性不可以重复使用，但连续属性是可以重复使用的。\\n4.5.2\\n图(4.11) 的解释\\n对照“西瓜书”中图4.10 的决策树，下面给出图4.11 中的划分边界产出过程。\\n在下图4-2中，斜纹阴影部分表示已确定标记为坏瓜的样本，点状阴影部分表示已确定标记为好瓜的\\n样本，空白部分表示需要进一步划分的样本。第一次划分条件是“含糖率⩽0.126?”，满足此条件的样本直\\n接被标记为坏瓜（如图4-2(a) 斜纹阴影部分所示），而不满足此条件的样本还需要进一步划分（如图4-2(a)\\n空白部分所示）。\\n在第一次划分的基础上对图4-2(a) 空白部分继续进行划分，第二次划分条件是“密度⩽0.381?”，满\\n足此条件的样本直接被标记为坏瓜（如图4-2(b) 新增斜纹阴影部分所示），而不满足此条件的样本还需要\\n进一步划分（如图4-2(b) 空白部分所示）。\\n在第二次划分的基础上对图4-2(b) 空白部分继续进行划分，第三次划分条件是“含糖率⩽0.205?”，\\n不满足此条件的样本直接标记为好瓜（如图4-2(c) 新增点状阴影部分所示），而满足此条件的样本还需进\\n一步划分（如图4-2(c) 空白部分所示）。\\n在第三次划分的基础上对图4-2(c) 空白部分继续进行划分，第四次划分的条件是“密度⩽0.560?”，满\\n足此条件的样本直接标记为好瓜（如图4-2(d) 新增点状阴影部分所示），而不满足此条件的样本直接标记\\n为坏瓜（如图4-2(d) 新增斜纹阴影部分所示）。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 52, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n经过四次划分已无空白部分，表示决策树生成完毕，从图4-2(d) 中可以清晰地看出好瓜与坏瓜的分类\\n边界。\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(a) 第一次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(b) 第二次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(c) 第三次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(d) 第四次划分\\n图4-2 图4.11 中的划分边界产出过程\\n参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第5 章\\n神经网络\\n神经网络类算法可以堪称当今最主流的一类机器学习算法，其本质上和前几章讲到的线性回归、对数\\n几率回归、决策树等算法一样均属于机器学习算法，也是被发明用来完成分类和回归等任务。不过由于神\\n经网络类算法在如今超强算力的加持下效果表现极其出色，且从理论角度来说神经网络层堆叠得越深其效\\n果越好，因此也单独称用深层神经网络类算法所做的机器学习为深度学习，属于机器学习的子集。\\n5.1\\n神经元模型\\n本节对神经元模型的介绍通俗易懂，在此不再赘述。本节第2 段提到“阈值”(threshold) 的概念时，\\n“西\\n瓜书”左侧边注特意强调是“阈(yù)”而不是“阀(fá)”，这是因为该字确实很容易认错，读者注意一下\\n即可。\\n图5.1 所示的M-P 神经元模型，其中的“M-P”便是两位作者McCulloch 和Pitts 的首字母简写。\\n5.2\\n感知机与多层网络\\n5.2.1\\n式(5.1) 和式(5.2) 的推导\\n此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介\\n绍\\n[1]：\\n感知机模型：已知感知机由两层神经元组成，故感知机模型的公式可表示为\\ny = f\\n n\\nX\\ni=1\\nwixi −θ\\n!\\n= f(wTx −θ)\\n其中，x ∈Rn，为样本的特征向量，是感知机模型的输入；w, θ 是感知机模型的参数，w ∈Rn，为权重，\\nθ 为阈值。假定f 为阶跃函数，那么感知机模型的公式可进一步表示为（用ε(·) 代表阶跃函数）\\ny = ε(wTx −θ) =\\n(\\n1,\\nwTx −θ ⩾0;\\n0,\\nwTx −θ < 0.\\n由于n 维空间中的超平面方程为\\nw1x1 + w2x2 + · · · + wnxn + b = wTx + b = 0\\n所以此时感知机模型公式中的wTx −θ 可以看作是n 维空间中的一个超平面，将n 维空间划分为wTx −\\nθ ⩾0 和wTx −θ < 0 两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间\\n的样本对应的模型输出值为0，如此便实现了分类功能。\\n感知机学习策略：给定一个数据集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中xi ∈Rn, yi ∈{0, 1}, i = 1, 2, · · · , N。如果存在某个超平面\\nwTx + b = 0\\n能将数据集T 中的正样本和负样本完全正确地划分到超平面两侧，即对所有yi = 1 的样本xi 有wTxi +\\nb ⩾0，对所有yi = 0 的样本xi 有wTxi + b < 0，则称数据集T 线性可分，否则称数据集T 线性不可分。\\n现给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T 中的正负样本完全正确划\\n分的分离超平面\\nwTx −θ = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n假设此时误分类样本集合为M ⊆T，对任意一个误分类样本(x, y) ∈M 来说，当wTx −θ ⩾0 时，模型\\n输出值为ˆy = 1，样本真实标记为y = 0；反之，当wTx −θ < 0 时，模型输出值为ˆy = 0，样本真实标记\\n为y = 1。综合两种情形可知，以下公式恒成立：\\n(ˆy −y)\\n\\x00wTx −θ\\n\\x01\\n⩾0\\n所以，给定数据集T，其损失函数可以定义为\\nL(w, θ) =\\nX\\nx∈M\\n(ˆy −y)\\n\\x00wTx −θ\\n\\x01\\n显然，此损失函数是非负的。如果没有误分类点，则损失函数值为0。而且，误分类点越少，误分类点离\\n超平面越近，损失函数值就越小。因此，给定数据集T，损失函数L(w, θ) 是关于w, θ 的连续可导函数。\\n感知机学习算法：感知机模型的学习问题可以转化为求解损失函数的最优化问题，具体地，给定数据\\n集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中xi ∈Rn, yi ∈{0, 1}，求参数w, θ，使其为极小化损失函数的解：\\nmin\\nw,θ L(w, θ) = min\\nw,θ\\nX\\nxi∈M\\n(ˆyi −yi)(wTxi −θ)\\n其中M ⊆T 为误分类样本集合。若将阈值θ 看作一个固定输入为−1 的“哑节点”，即\\n−θ = −1 · wn+1 = xn+1 · wn+1\\n那么wTxi −θ 可化简为\\nwTxi −θ =\\nn\\nX\\nj=1\\nwjxj + xn+1 · wn+1\\n=\\nn+1\\nX\\nj=1\\nwjxj\\n= wTxi\\n其中xi ∈Rn+1, w ∈Rn+1。根据该公式，可将要求解的极小化问题进一步简化为\\nmin\\nw L(w) = min\\nw\\nX\\nxi∈M\\n(ˆyi −yi)wTxi\\n假设误分类样本集合M 固定，那么可以求得损失函数L(w) 的梯度\\n∇wL(w) =\\nX\\nxi∈M\\n(ˆyi −yi)xi\\n感知机的学习算法具体采用的是随机梯度下降法，即在极小化过程中，不是一次使M 中所有误分类点的\\n梯度下降，而是一次随机选取一个误分类点并使其梯度下降。所以权重w 的更新公式为\\nw ←w + ∆w\\n∆w = −η(ˆyi −yi)xi = η(yi −ˆyi)xi\\n相应地，w 中的某个分量wi 的更新公式即式(5.2)。\\n5.2.2\\n图5.5 的解释\\n图5.5 中(0, 0), (0, 1), (1, 0), (1, 1) 这4 个样本点实现“异或”计算的过程如下：\\n(x1, x2) →h1 = ε(x1 −x2 −0.5), h2 = ε(x2 −x1 −0.5) →y = ε(h1 + h2 −0.5)\\n以(0, 1) 为例，首先求得h1 = ε(0−1−0.5) = 0, h2 = ε(1−0−0.5) = 1，然后求得y = ε(0+1−0.5) = 1。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 55, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.3\\n误差逆传播算法\\n5.3.1\\n式(5.10) 的推导\\n参见式(5.12) 的推导\\n5.3.2\\n式(5.12) 的推导\\n因为\\n∆θj = −η\\n∂Ek\\n∂θj\\n又\\n∂Ek\\n∂θj\\n=\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂θj\\n=\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂[f(βj −θj)]\\n∂θj\\n=\\n∂Ek\\n∂ˆyk\\nj\\n· f ′(βj −θj) × (−1)\\n=\\n∂Ek\\n∂ˆyk\\nj\\n· f (βj −θj) × [1 −f (βj −θj)] × (−1)\\n=\\n∂Ek\\n∂ˆyk\\nj\\n· ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n× (−1)\\n=\\n∂\\n\\uf8ee\\n\\uf8f01\\n2\\nlP\\nj=1\\n\\x00ˆyk\\nj −yk\\nj\\n\\x012\\n\\uf8f9\\n\\uf8fb\\n∂ˆyk\\nj\\n· ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n× (−1)\\n=\\n1\\n2 × 2(ˆyk\\nj −yk\\nj ) × 1 · ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n× (−1)\\n= (yk\\nj −ˆyk\\nj )ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n= gj\\n所以\\n∆θj = −η\\n∂Ek\\n∂θj\\n= −ηgj\\n5.3.3\\n式(5.13) 的推导\\n因为\\n∆vih = −η\\n∂Ek\\n∂vih\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 56, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n又\\n∂Ek\\n∂vih\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂αh\\n·\\n∂αh\\n∂vih\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂αh\\n· xi\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n· f ′(αh −γh) · xi\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n· whj · f ′(αh −γh) · xi\\n=\\nl\\nX\\nj=1\\n(−gj) · whj · f ′(αh −γh) · xi\\n= −f ′(αh −γh) ·\\nl\\nX\\nj=1\\ngj · whj · xi\\n= −bh(1 −bh) ·\\nl\\nX\\nj=1\\ngj · whj · xi\\n= −eh · xi\\n所以\\n∆vih = −η\\n∂Ek\\n∂vih\\n= ηehxi\\n5.3.4\\n式(5.14) 的推导\\n因为\\n∆γh = −η\\n∂Ek\\n∂γh\\n又\\n∂Ek\\n∂γh\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂γh\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n· f ′(αh −γh) · (−1)\\n= −\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n· whj · f ′(αh −γh)\\n= −\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n· whj · bh(1 −bh)\\n=\\nl\\nX\\nj=1\\ngj · whj · bh(1 −bh)\\n= eh\\n所以\\n∆γh = −η\\n∂Ek\\n∂γh\\n= −ηeh\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.3.5\\n式(5.15) 的推导\\n参见式(5.13) 的推导\\n5.4\\n全局最小与局部极小\\n由图5.10 可以直观理解局部极小和全局最小的概念，其余概念如模拟退火、遗传算法、启发式等，则\\n需要查阅专业资料系统化学习。\\n5.5\\n其他常见神经网络\\n本节所提到的神经网络其实如今已不太常见，更为常见的神经网络是下一节深度学习里提到的卷积神\\n经网络、循环神经网络等。\\n5.5.1\\n式(5.18) 的解释\\n从式(5.18) 可以看出，对于样本x 来说，RBF 网络的输出为q 个ρ(x, ci) 的线性组合。若换个角\\n度来看这个问题，将q 个ρ(x, ci) 当作是将d 维向量x 基于式(5.19) 进行特征转换后所得的q 维特征，\\n即˜x = (ρ(x, c1); ρ(x, c2); ...; ρ(x, cq))，则式(5.18) 求线性加权系数wi 相当于求解第3.2 节的线性回归\\nf(˜x) = wT˜x + b，对于仅有的差别b 来说，当然可以在式(5.18) 中补加一个b。因此，RBF 网络在确定\\nq 个神经元中心ci 之后，接下来要做的就是线性回归。\\n5.5.2\\n式(5.20) 的解释\\nBoltzmann 机（Restricted Boltzmann Machine，简称RBM）本质上是一个引入了隐变量的无向图模\\n型，其能量可理解为\\nEgraph = Eedges + Enodes\\n其中，Egraph 表示图的能量，Eedges 表示图中边的能量，Enodes 表示图中结点的能量。边能量由两连接\\n结点的值及其权重的乘积确定，即Eedgeij = −wijsisj；结点能量由结点的值及其阈值的乘积确定，即\\nEnodei = −θisi。图中边的能量为所有边能量之和为\\nEedges =\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nEedgeij = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nwijsisj\\n图中结点的能量为所有结点能量之和\\nEnodes =\\nn\\nX\\ni=1\\nEnodei = −\\nn\\nX\\ni=1\\nθisi\\n故状态向量s 所对应的Boltzmann 机能量\\nEgraph = Eedges + Enodes = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nwijsisj −\\nn\\nX\\ni=1\\nθisi\\n5.5.3\\n式(5.22) 的解释\\n受限Boltzmann 机仅保留显层与隐层之间的连接。显层状态向量v = (v1; v2; ...; vd)，隐层状态向量\\nh = (h1; h2; ...; hq)。显层状态向量v 中的变量vi 仅与隐层状态向量h 有关，所以给定隐层状态向量h，\\n有v1, v2, ..., vd 相互独立。\\n5.5.4\\n式(5.23) 的解释\\n由式(5.22) 的解释同理可得，给定显层状态向量v，有h1, h2, ..., hq 相互独立。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.6\\n深度学习\\n“西瓜书”在本节并未对如今深度学习领域的诸多经典神经网络作展开介绍，而是从更宏观的角度详\\n细解释了应该如何理解深度学习。因此，本书也顺着“西瓜书”的思路对深度学习相关概念作进一步说明，\\n对深度学习的经典神经网络感兴趣的读者可查阅其他相关书籍进行系统性学习。\\n5.6.1\\n什么是深度学习\\n深度学习就是很深层的神经网络，而神经网络属于机器学习算法的范畴，因此深度学习是机器学习的\\n子集。\\n5.6.2\\n深度学习的起源\\n深度学习中的经典神经网络以及用于训练神经网络的BP 算法其实在很早就已经被提出，例如卷积神\\n经网络\\n[2] 是在1989 提出，BP 算法\\n[3] 是在1986 年提出，但是在当时的计算机算力水平下，其他非神经\\n网络类算法（例如当时红极一时的支持向量机算法）的效果优于神经网络类算法，因此神经网络类算法进\\n入瓶颈期。随着计算机算力的不断提升，以及2012 年Hinton 和他的学生提出了AlexNet 并在ImageNet\\n评测中以明显优于第二名的成绩夺冠后，引起了学术界和工业界的广泛关注，紧接着三位深度学习之父\\nLeCun、Bengio 和Hinton 在2015 年正式提出深度学习的概念，自此深度学习开始成为机器学习的主流\\n研究方向。\\n5.6.3\\n怎么理解特征学习\\n举例来说，用非深度学习算法做西瓜分类时，首先需要人工设计西瓜的各个特征，比如根蒂、色泽等，\\n然后将其表示为数学向量，这些过程统称为“特征工程”，完成特征工程后用算法分类即可，其分类效果\\n很大程度上取决于特征工程做得是否够好。而对于深度学习算法来说，只需将西瓜的图片表示为数学向量\\n输入，输出层设置为想要的分类结果即可（例如二分类通常设置为对数几率回归），之前的“特征工程”交\\n由神经网络来自动完成，即让神经网络进行“特征学习”，通过在输出层约束分类结果，神经网络会自动\\n从西瓜的图片上提取出有助于西瓜分类的特征。\\n因此，如果分别用对数几率回归和卷积神经网络来做西瓜分类，其算法运行流程分别是“人工特征工\\n程→对数几率回归分类”和“卷积神经网络特征学习→对数几率回归分类”。\\n参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.\\n[2] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-\\nbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural\\ncomputation, 1(4):541–551, 1989.\\n[3] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\\npropagating errors. nature, 323(6088):533–536, 1986.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第6 章\\n支持向量机\\n在深度学习流行之前，支持向量机及其核方法一直是机器学习领域中的主流算法，尤其是核方法至今\\n都仍有相关学者在持续研究。\\n6.1\\n间隔与支持向量\\n6.1.1\\n图6.1 的解释\\n回顾第5 章5.2 节的感知机模型可知，图6.1 中的黑色直线均可作为感知机模型的解，因为感知机模\\n型求解的是能将正负样本完全正确划分的超平面，因此解不唯一。而支持向量机想要求解的则是离正负样\\n本都尽可能远且刚好位于“正中间”的划分超平面，因为这样的超平面理论上泛化性能更好。\\n6.1.2\\n式(6.1) 的解释\\nn 维空间的超平面定义为wTx + b = 0，其中w, x ∈Rn，w = (w1; w2; ...; wn) 称为法向量，b 称为\\n位移项。超平面具有以下性质：\\n(1) 法向量w 和位移项b 确定一个唯一超平面；\\n(2) 超平面方程不唯一，因为当等倍缩放w 和b 时（假设缩放倍数为α），所得的新超平面方程\\nαwTx + αb = 0 和wTx + b = 0 的解完全相同，因此超平面不变，仅超平面方程有变；\\n(3) 法向量w 垂直于超平面；\\n(4) 超平面将n 维空间切割为两半，其中法向量w 指向的那一半空间称为正空间，另一半称为负空\\n间，正空间中的点x+ 代入进方程wTx+ + b 其计算结果大于0，反之负空间中的点代入进方程其计算结\\n果小于0；\\n(5)n 维空间中的任意点x 到超平面的距离公式为r = |wTx+b|\\n∥w∥\\n，其中∥w∥表示向量w 的模。\\n6.1.3\\n式(6.2) 的推导\\n对于任意一点x0 = (x0\\n1; x0\\n2; ...; x0\\nn)，设其在超平面wTx + b = 0 上的投影点为x1 = (x1\\n1; x1\\n2; ...; x1\\nn)，\\n则wTx1 + b = 0。根据超平面的性质(3) 可知，此时向量−−−→\\nx1x0 与法向量w 平行，因此\\n|w · −−−→\\nx1x0| = |∥w∥· cos π · ∥−−−→\\nx1x0∥| = ∥w∥· ∥−−−→\\nx1x0∥= ∥w∥· r\\n又\\nw · −−−→\\nx1x0 = w1(x0\\n1 −x1\\n1) + w2(x0\\n2 −x1\\n2) + ... + wn(x0\\nn −x1\\nn)\\n= w1x0\\n1 + w2x0\\n2 + ... + wnx0\\nn −(w1x1\\n1 + w2x1\\n2 + ... + wnx1\\nn)\\n= wTx0 −wTx1\\n= wTx0 + b\\n所以\\n|wTx0 + b| = ∥w∥· r\\nr =\\n\\x0c\\x0cwTx + b\\n\\x0c\\x0c\\n∥w∥\\n6.1.4\\n式(6.3) 的推导\\n支持向量机所要求的超平面需要满足三个条件，第一个是能正确划分正负样本，第二个是要位于正负\\n样本正中间，第三个是离正负样本都尽可能远。式(6.3) 仅满足前两个条件，第三个条件由式(6.5) 来满\\n足，因此下面仅基于前两个条件来进行推导。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于第一个条件，当超平面满足该条件时，根据超平面的性质(4) 可知，若yi = +1 的正样本被划分\\n到正空间（当然也可以将其划分到负空间），yi = −1 的负样本被划分到负空间，以下不等式成立\\n(\\nwTxi + b ⩾0,\\nyi = +1\\nwTxi + b ⩽0,\\nyi = −1\\n对于第二个条件，首先设离超平面最近的正样本为x+\\n∗，离超平面最近的负样本为x−\\n∗，由于这两样本\\n是离超平面最近的点，所以其他样本到超平面的距离均大于等于它们，即\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n|wTxi+b|\\n∥w∥\\n⩾|wTx+\\n∗+b|\\n∥w∥\\n,\\nyi = +1\\n|wTxi+b|\\n∥w∥\\n⩾|wTx−\\n∗+b|\\n∥w∥\\n,\\nyi = −1\\n结合第一个条件中推导出的不等式，可将上式中的绝对值符号去掉并推得\\n(\\nwTxi+b\\n∥w∥\\n⩾wTx+\\n∗+b\\n∥w∥\\n,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽wTx−\\n∗+b\\n∥w∥\\n,\\nyi = −1\\n基于此再考虑第二个条件，“位于正负样本正中间”等价于要求超平面到x+\\n∗和x−\\n∗这两点的距离相等，即\\n\\x0c\\x0cwTx+\\n∗+ b\\n\\x0c\\x0c\\n∥w∥\\n=\\n\\x0c\\x0cwTx−\\n∗+ b\\n\\x0c\\x0c\\n∥w∥\\n综上，支持向量机所要求的超平面所需要满足的条件如下\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nwTxi+b\\n∥w∥\\n⩾wTx+\\n∗+b\\n∥w∥\\n,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽wTx−\\n∗+b\\n∥w∥\\n,\\nyi = −1\\n|wTx+\\n∗+b|\\n∥w∥\\n= |wTx−\\n∗+b|\\n∥w∥\\n但是根据超平面的性质(2) 可知，当等倍缩放法向量w 和位移项b 时，超平面不变，且上式也恒成\\n立，因此会导致所求的超平面的参数w 和b 有无穷多解。因此为了保证每个超平面的参数只有唯一解，不\\n妨再额外施加一些约束，例如约束x+\\n∗和x−\\n∗代入进超平面方程后的绝对值为1，也就是令wTx+\\n∗+ b =\\n1, wTx−\\n∗+ b = −1。此时支持向量机所要求的超平面所需要满足的条件变为\\n(\\nwTxi+b\\n∥w∥\\n⩾\\n+1\\n∥w∥,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽\\n−1\\n∥w∥,\\nyi = −1\\n由于∥w∥恒大于0，因此上式可进一步化简为\\n(\\nwTxi + b ⩾+1,\\nyi = +1\\nwTxi + b ⩽−1,\\nyi = −1\\n6.1.5\\n式(6.4) 的推导\\n根据式(6.3) 的推导可知，x+\\n∗和x−\\n∗便是“支持向量”，因此支持向量到超平面的距离已经被约束为\\n1\\n∥w∥，所以两个异类支持向量到超平面的距离之和为\\n2\\n∥w∥。\\n6.1.6\\n式(6.5) 的解释\\n式(6.5) 是通过“最大化间隔”来保证超平面离正负样本都尽可能远，且该超平面有且仅有一个，因\\n此可以解出唯一解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n6.2\\n对偶问题\\n6.2.1\\n凸优化问题\\n考虑一般地约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n若目标函数f(x) 是凸函数，不等式约束gi(x) 是凸函数，等式约束hj(x) 是仿射函数，则称该优化问题\\n为凸优化问题。\\n由于1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01\\n均是关于w 和b 的凸函数，所以式(6.6) 是凸优化问题。凸优化问\\n题是最优化里比较易解的一类优化问题，因为其拥有诸多良好的数学性质和现成的数学工具，因此如果非\\n凸优化问题能等价转化为凸优化问题，其求解难度通常也会减小。\\n6.2.2\\nKKT 条件\\n考虑一般的约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n若f(x), gi(x), hj(x) 的一阶偏导连续，x∗是优化问题的局部解，µ = (µ1; µ2; ...; µm), λ = (λ1; λ2; ...; λn)\\n为拉格朗日乘子向量，L(x, µ, λ) = f(x) + Pm\\ni=1 µigi(x) + Pn\\nj=1 λjhj(x) 为拉格朗日函数，且该优化问题\\n满足任何一个特定的约束限制条件，则一定存在µ∗= (µ∗\\n1; µ∗\\n2; ...; µ∗\\nm), λ∗= (λ∗\\n1; λ∗\\n2; ...; λ∗\\nn)，使得：\\n(1) ∇xL(x∗, µ∗, λ∗) = ∇f(x∗) + Pm\\ni=1 µ∗\\ni ∇gi(x∗) + Pn\\nj=1 λ∗\\nj∇hj(x∗) = 0；\\n(2) hj(x∗) = 0,\\nj = 1, 2, ..., n；\\n(3) gi(x∗) ⩽0,\\ni = 1, 2, ..., m；\\n(4) µ∗\\ni ⩾0,\\ni = 1, 2, ..., m；\\n(5) µ∗\\ni gi(x∗) = 0,\\ni = 1, 2, ..., m。\\n以上5 条便是Karush–Kuhn–Tucker Conditions（简称KKT 条件）。KKT 条件是局部解的必要条件，\\n也就是说只要该优化问题满足任何一个特定的约束限制条件，局部解就一定会满足以上5 个条件。常用的\\n约束限制条件可查阅维基百科“Karush–Kuhn–Tucker Conditions”词条以及查阅参考文献[1] 的第4.2.2\\n节，若对KKT 条件的数学证明感兴趣可查阅参考文献[1] 的第4.2.1 节。\\n6.2.3\\n拉格朗日对偶函数\\n考虑一般地约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n设上述优化问题的定义域为D = dom f ∩\\nmT\\ni=1\\ndom gi ∩\\nnT\\nj=1\\ndom hj，可行集为˜D = {x|x ∈D, gi(x) ⩽\\n0, hj(x) = 0}（显然˜D 是D 的子集），最优值为p∗= min{f(˜x)}, ˜x ∈˜D。上述优化问题的拉格朗日函数\\n定义为\\nL(x, µ, λ) = f(x) +\\nm\\nX\\ni=1\\nµigi(x) +\\nn\\nX\\nj=1\\nλjhj(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中µ = (µ1; µ2; ...; µm), λ = (λ1; λ2; ...; λn) 为拉格朗日乘子向量。相应地拉格朗日对偶函数Γ(µ, λ)（简\\n称对偶函数）定义为L(x, µ, λ) 关于x 的下确界，即\\nΓ(µ, λ) = inf\\nx∈D L(x, µ, λ) = inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµigi(x) +\\nn\\nX\\nj=1\\nλjhj(x)\\n!\\n对偶函数有如下性质：\\n(1) 无论上述优化问题是否为凸优化问题，其对偶函数Γ(µ, λ) 恒为凹函数，详细证明可查阅参考文\\n献[2] 的第5.1.2 和3.2.3 节；\\n(2) 当µ ⪰0 时（µ ⪰0 表示µ 的分量均为非负），Γ(µ, λ) 构成了上述优化问题最优值p∗的下界，\\n即\\nΓ(µ, λ) ⩽p∗\\n其推导过程如下：\\n设˜x ∈˜D 是优化问题的可行点，则gi(˜x) ⩽0, hj(˜x) = 0，因此，当µ ⪰0 时，µigi(˜x) ⩽0, λjhj(˜x) = 0\\n恒成立，所以\\nm\\nX\\ni=1\\nµigi(˜x) +\\nn\\nX\\nj=1\\nλjhj(˜x) ⩽0\\n根据上述不等式可以推得\\nL(˜x, µ, λ) = f(˜x) +\\nm\\nX\\ni=1\\nµigi(˜x) +\\nn\\nX\\nj=1\\nλjhj(˜x) ⩽f(˜x)\\n又\\nΓ(µ, λ) = inf\\nx∈D L(x, µ, λ) ⩽L(˜x, µ, λ)\\n所以\\nΓ(µ, λ) ⩽L(˜x, µ, λ) ⩽f(˜x)\\n进一步地\\nΓ(µ, λ) ⩽min{f(˜x)} = p∗\\n6.2.4\\n拉格朗日对偶问题\\n在µ ⪰0 的约束下求对偶函数最大值的优化问题称为拉格朗日对偶问题（简称对偶问题）\\nmax\\nΓ(µ, λ)\\ns.t.\\nµ ⪰0\\n上一节的优化问题称为主问题或原问题。\\n设对偶问题的最优值为d∗= max{Γ(µ, λ)}, µ ⪰0，根据对偶函数的性质（2）可知d∗⩽p∗，此时称\\n为“弱对偶性”成立，若d∗= p∗，则称为“强对偶性”成立。由此可以看出，当主问题较难求解时，如\\n果强对偶性成立，则可以通过求解对偶问题来间接求解主问题。由于约束条件µ ⪰0 是凸集，且根据对偶\\n函数的性质（1）可知Γ(µ, λ) 恒为凹函数，其加个负号即为凸函数，所以无论主问题是否为凸优化问题，\\n对偶问题恒为凸优化问题。\\n一般情况下，强对偶性并不成立，只有当主问题满足特定的约束限制条件（不同于KKT 条件中的约\\n束限制条件）时，强对偶性才成立，常见的有“Slater 条件”。Slater 条件指出，当主问题是凸优化问题，\\n且存在一点x ∈relint D 能使得所有等式约束成立，除仿射函数以外的不等式约束严格成立，则强对偶性\\n成立。由于式(6.6) 是凸优化问题，且不等式约束均为仿射函数，所以式(6.6) 强对偶性成立。\\n对于凸优化问题，还可以通过KKT 条件来间接推导出强对偶性，并同时求解出主问题和对偶问题的\\n最优解。具体地，若主问题为凸优化问题，目标函数f(x) 和约束函数gi(x), hj(x) 的一阶偏导连续，主问\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n题满足KKT 条件中任何一个特定的约束限制条件，则满足KKT 条件的点x∗和(µ∗, λ∗) 分别是主问题\\n和对偶问题的最优解，且此时强对偶性成立。下面给出具体的推导过程。\\n设x∗, µ∗, λ∗是任意满足KKT 条件的点，即\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n∇xL(x∗, µ∗, λ∗) = ∇f(x∗) + Pm\\ni=1 µ∗\\ni ∇gi(x∗) + Pn\\nj=1 λ∗\\nj∇hj(x∗) = 0\\nhj(x∗) = 0,\\nj = 1, 2, ..., n\\ngi(x∗) ⩽0,\\ni = 1, 2, ..., m\\nµ∗\\ni ⩾0,\\ni = 1, 2, ..., m\\nµ∗\\ni gi(x∗) = 0,\\ni = 1, 2, ..., m\\n由于主问题是凸优化问题，所以f(x) 和gi(x) 是凸函数，hj(x) 是仿射函数，又因为此时µ∗\\ni ⩾0，所以\\nL(x, µ∗, λ∗) 是关于x 的凸函数。根据∇xL(x∗, µ∗, λ∗) = 0 可知，此时x∗是L(x, µ∗, λ∗) 的极值点，而\\n凸函数的极值点也是最值点，所以x∗是最小值点，因此可以进一步推得\\nL(x∗, µ∗, λ∗) = min{L(x, µ∗, λ∗)}\\n= inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n= Γ(µ∗, λ∗)\\n= f(x∗) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x∗) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x∗)\\n= f(x∗)\\n其中第二个等式是根据下确界函数的性质推得，第三个等式是根据对偶函数的定义推得，第四个等式是\\nL(x∗, µ∗, λ∗) 的展开形式，最后一个等式是因为µ∗\\ni gi(x∗) = 0, hj(x∗) = 0。\\n由于x∗和(µ∗, λ∗) 仅是满足KKT 条件的点，并不一定是f(x) 和Γ(µ, λ) 的最值点，所以f(x∗) ⩾\\np∗⩾d∗⩾Γ(µ∗, λ∗)，但是上式又推得f(x∗) = Γ(µ∗, λ∗)，所以p∗= d∗，因此推得强对偶性成立，且x∗\\n和(µ∗, λ∗) 分别是主问题和对偶问题的最优解。\\nSlater 条件恰巧也是KKT 条件中特定的约束限制条件之一，所以式(6.6) 不仅强对偶性成立，而且\\n可以通过求解满足KKT 条件的点来求解出最优解。\\nKKT 条件除了可以作为凸优化问题强对偶性成立的充分条件以外，其实对于任意优化问题（并不一\\n定是凸优化问题），若其强对偶性成立，KKT 条件也是主问题和对偶问题最优解的必要条件，而且此时并\\n不要求主问题满足KKT 条件中任何一个特定的约束限制条件。下面同样给出具体的推导过程。\\n设主问题的最优解为x∗，对偶问题的最优解为(µ∗, λ∗)，目标函数f(x) 和约束函数gi(x), hj(x) 的\\n一阶偏导连续，当强对偶性成立时，可以推得\\nf(x∗) = Γ(µ∗, λ∗)\\n= inf\\nx∈D L(x, µ∗, λ∗)\\n= inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n⩽f(x∗) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x∗) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x∗)\\n⩽f(x∗)\\n其中，第一个等式是因为强对偶性成立时p∗= d∗，第二和第三个等式是对偶函数的定义，第四个不等式\\n是根据下确界的性质推得，最后一个不等式成立是因为µ∗\\ni ⩾0, gi(x∗) ⩽0, hj(x∗) = 0。\\n由于f(x∗) = f(x∗)，所以上式中的等式均可化为等式。第四个不等式可化为等式，说明L(x, µ∗, λ∗)\\n在x∗处取得最小值，所以根据极值的性质可知在x∗处一阶导∇xL(x∗, µ∗, λ∗) = 0。最后一个不等式可化\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n为等式，说明µ∗\\ni gi(x∗) = 0。此时再结合主问题和对偶问题原有的约束条件µ∗\\ni ⩾0, gi(x∗) ⩽0, hj(x∗) = 0\\n便凑齐了KKT 条件。\\n6.2.5\\n式(6.9) 和式(6.10) 的推导\\nL(w, b, α) = 1\\n2∥w∥2 +\\nm\\nX\\ni=1\\nαi(1 −yi(wTxi + b))\\n= 1\\n2∥w∥2 +\\nm\\nX\\ni=1\\n(αi −αiyiwTxi −αiyib)\\n= 1\\n2wTw +\\nm\\nX\\ni=1\\nαi −\\nm\\nX\\ni=1\\nαiyiwTxi −\\nm\\nX\\ni=1\\nαiyib\\n对w 和b 分别求偏导数并令其为零\\n∂L\\n∂w = 1\\n2 × 2 × w + 0 −\\nm\\nX\\ni=1\\nαiyixi −0 = 0 =⇒w =\\nm\\nX\\ni=1\\nαiyixi\\n∂L\\n∂b = 0 + 0 −0 −\\nm\\nX\\ni=1\\nαiyi = 0 =⇒\\nm\\nX\\ni=1\\nαiyi = 0\\n6.2.6\\n式(6.11) 的推导\\n因为αi ⩾0，且1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01\\n均是关于w 和b 的凸函数，所以式(6.8) 也是关于w\\n和b 的凸函数。根据凸函数的性质可知，其极值点就是最值点，所以一阶导为零的点就是最小值点，因此\\n将式(6.9) 和式(6.10) 代入式(6.8) 后即可得式(6.8) 的最小值（等价于下确界），再根据对偶问题的定义\\n加上约束αi ⩾0，就得到了式(6.6) 的对偶问题。由于式(6.10) 也是αi 必须满足的条件，且不含有w 和\\nb，因此也需要纳入对偶问题的约束条件。根据以上思路进行推导的过程如下：\\ninf\\nw,b L(w, b, α) = 1\\n2wTw +\\nm\\nX\\ni=1\\nαi −\\nm\\nX\\ni=1\\nαiyiwTxi −\\nm\\nX\\ni=1\\nαiyib\\n= 1\\n2wT\\nm\\nX\\ni=1\\nαiyixi −wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi\\n= −1\\n2(\\nm\\nX\\ni=1\\nαiyixi)T(\\nm\\nX\\ni=1\\nαiyixi) +\\nm\\nX\\ni=1\\nαi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi\\n=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n所以\\nmax\\nα\\ninf\\nw,b L(w, b, α) = max\\nα\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n最后将αi ⩾0 和式(6.10) 作为约束条件即可得式(6.11)。\\n式(6.6) 之所以要转化为式(6.11) 来求解，其主要有以下两点理由：\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 65, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n(1) 式(6.6) 中的未知数是w 和b，式(6.11) 中的未知数是α，w 的维度d 对应样本特征个数，α 的\\n维度m 对应训练样本个数，通常m ≪d，所以求解式(6.11) 更高效，反之求解式(6.6) 更高效；\\n(2) 式(6.11) 中有样本内积x\\nTxj\\ni\\n这一项，后续可以很自然地引入核函数，进而使得支持向量机也能\\n对在原始特征空间线性不可分的数据进行分类。\\n6.2.7\\n式(6.13) 的解释\\n因为式(6.6) 满足Slater 条件，所以强对偶性成立，进而最优解满足KKT 条件。\\n6.3\\n核函数\\n6.3.1\\n式(6.22) 的解释\\n此即核函数的定义，即核函数可以分解成两个向量的内积。要想了解某个核函数是如何将原始特征空\\n间映射到更高维的特征空间的，只需要分解为两个表达形式完全一样的向量内积即可。\\n6.4\\n软间隔与正则化\\n6.4.1\\n式(6.35)\\n令\\nmax\\n\\x000, 1 −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n= ξi\\n显然ξi ≥0，且当1 −yi\\n\\x00wTxi + b\\n\\x01\\n> 0 时有\\n1 −yi\\n\\x00wTxi + b\\n\\x01\\n= ξi\\n当1 −yi\\n\\x00wTxi + b\\n\\x01\\n≤0 时有\\nξi = 0\\n综上可得\\n1 −yi\\n\\x00wTxi + b\\n\\x01\\n⩽ξi ⇒yi\\n\\x00wTxi + b\\n\\x01\\n⩾1 −ξi\\n6.4.2\\n式(6.37) 和式(6.38) 的推导\\n类比式(6.9) 和式(6.10) 的推导\\n6.4.3\\n式(6.39)\\n式(6.36) 关于ξi 求偏导数并令其为零\\n∂L\\n∂ξi\\n= 0 + C × 1 −αi × 1 −µi × 1 = 0 ⇒C = αi + µi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n6.4.4\\n式(6.40)\\n将式(6.37)、式(6.38) 和(6.39) 代入式(6.36) 可以得到式(6.35) 的对偶问题，有\\n1\\n2 ∥w∥2 + C\\nm\\nX\\ni=1\\nξi +\\nm\\nX\\ni=1\\nαi\\n\\x001 −ξi −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n−\\nm\\nX\\ni=1\\nµiξi\\n=1\\n2 ∥w∥2 +\\nm\\nX\\ni=1\\nαi\\n\\x001 −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n+ C\\nm\\nX\\ni=1\\nξi −\\nm\\nX\\ni=1\\nαiξi −\\nm\\nX\\ni=1\\nµiξi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi +\\nm\\nX\\ni=1\\nCξi −\\nm\\nX\\ni=1\\nαiξi −\\nm\\nX\\ni=1\\nµiξi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi +\\nm\\nX\\ni=1\\n(C −αi −µi)ξi\\n=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= min\\nw,b,ξ L(w, b, α, ξ, µ)\\n所以\\nmax\\nα,µ min\\nw,b,ξ L(w, b, α, ξ, µ) = max\\nα,µ\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= max\\nα\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n又因为αi ≥0，µi ≥0，C = αi + µi，消去µi 可得等价约束条件\\n0 ⩽αi ⩽C,\\ni = 1, 2, ..., m\\n6.4.5\\n对数几率回归与支持向量机的关系\\n在“西瓜书”本节的倒数第二段开头，其讨论了对数几率回归与支持向量机的关系，提到“如果使用对\\n率损失函数ℓlog 来替代式(6.29) 中的0/1 损失函数，则几乎就得到了对率回归模型(3.27)”，但式(6.29)\\n与式(3.27) 形式上相差甚远。为了更清晰的说明对数几率回归与软间隔支持向量机的关系，以下先对式\\n(3.27) 的形式进行变化。\\n将β = (w; b) 和ˆx = (x; 1) 代入式(3.27) 可得\\nℓ(w, b) =\\nm\\nX\\ni=1\\n\\x10\\n−yi\\n\\x00wTxi + b\\n\\x01\\n+ ln\\n\\x10\\n1 + ewTxi+b\\x11\\x11\\n=\\nm\\nX\\ni=1\\n\\x12\\nln\\n1\\neyi(wTxi+b) + ln\\n\\x10\\n1 + ewTxi+b\\x11\\x13\\n=\\nm\\nX\\ni=1\\nln 1 + ewTxi+b\\neyi(wTxi+b)\\n=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1 ln\\n\\x10\\n1 + e−(wTxi+b)\\x11\\n,\\nyi = 1\\nPm\\ni=1 ln\\n\\x10\\n1 + ewTxi+b\\x11\\n,\\nyi = 0\\n上式中正例和反例分别用yi = 1 和yi = 0 表示，这是对数几率回归常用的方式，而在支持向量机中正例\\n和反例习惯用yi = +1 和yi = −1 表示。实际上，若上式也换用yi = +1 和yi = −1 分别表示正例和反\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n例，则上式可改写为\\nℓ(w, b) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1 ln\\n\\x10\\n1 + e−(wTxi+b)\\x11\\n,\\nyi = +1\\nPm\\ni=1 ln\\n\\x10\\n1 + ewTxi+b\\x11\\n,\\nyi = −1\\n=\\nm\\nX\\ni=1\\nln\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n此时上式的求和项正是式(6.33) 所表述的对率损失。\\n6.4.6\\n式(6.41) 的解释\\n参见式(6.13) 的解释\\n6.5\\n支持向量回归\\n6.5.1\\n式(6.43) 的解释\\n相比于线性回归用一条线来拟合训练样本，支持向量回归而是采用一个以f(x) = wTx + b 为中心，\\n宽度为2ϵ 的间隔带，来拟合训练样本。\\n落在带子上的样本不计算损失（类比线性回归在线上的点预测误差为0），不在带子上的则以偏离带\\n子的距离作为损失（类比线性回归的均方误差），然后以最小化损失的方式迫使间隔带从样本最密集的地\\n方穿过，进而达到拟合训练样本的目的。因此支持向量回归的优化问题可以写为\\nmin\\nw,b\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nℓϵ (f (xi) −yi)\\n其中ℓϵ(z) 为“ϵ 不敏感损失函数”（类比线性回归的均方误差损失）\\nℓϵ(z) =\\n(\\n0,\\nif |z| ⩽ϵ\\n|z| −ϵ,\\nif |z| > ϵ\\n1\\n2∥w∥2 为L2 正则项，此处引入正则项除了起正则化本身的作用外，也是为了和软间隔支持向量机的优化\\n目标保持形式上的一致，这样就可以导出对偶问题引入核函数，C 为用来调节损失权重的正则化常数。\\n6.5.2\\n式(6.45) 的推导\\n同软间隔支持向量机，引入松弛变量ξi，令\\nℓϵ (f (xi) −yi) = ξi\\n显然ξi ⩾0，并且当|f (xi) −yi| ⩽ϵ 时，ξi = 0，当|f (xi) −yi| > ϵ 时，ξi = |f (xi) −yi| −ϵ，所以\\n|f (xi) −yi| −ϵ ⩽ξi\\n|f (xi) −yi| ⩽ϵ + ξi\\n−ϵ −ξi ⩽f (xi) −yi ⩽ϵ + ξi\\n因此支持向量回归的优化问题可以化为\\nmin\\nw,b,ξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nξi\\ns.t.\\n−ϵ −ξi ⩽f (xi) −yi ⩽ϵ + ξi\\nξi ⩾0, i = 1, 2, . . . , m\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n如果考虑两边采用不同的松弛程度，则有\\nmin\\nw,b,ξi,ˆξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\n\\x10\\nξi + ˆξi\\n\\x11\\ns.t.\\n−ϵ −ˆξi ⩽f (xi) −yi ⩽ϵ + ξi\\nξi ⩾0, ˆξi ⩾0, i = 1, 2, . . . , m\\n6.5.3\\n式(6.52) 的推导\\n将式(6.45) 的约束条件全部恒等变形为小于等于0 的形式可得\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nf (xi) −yi −ϵ −ξi ≤0\\nyi −f (xi) −ϵ −ˆξi ≤0\\n−ξi ≤0\\n−ˆξi ≤0\\n由于以上四个约束条件的拉格朗日乘子分别为αi, ˆαi, µi, ˆµi，所以其对应的KKT 条件为\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nαi (f (xi) −yi −ϵ −ξi) = 0\\nˆαi\\n\\x10\\nyi −f (xi) −ϵ −ˆξi\\n\\x11\\n= 0\\n−µiξi = 0 ⇒µiξi = 0\\n−ˆµi ˆξi = 0 ⇒ˆµi ˆξi = 0\\n又由式(6.49) 和式(6.50) 有\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nµi = C −αi\\nˆµi = C −ˆαi\\n所以上述KKT 条件可以进一步变形为\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nαi (f (xi) −yi −ϵ −ξi) = 0\\nˆαi\\n\\x10\\nyi −f (xi) −ϵ −ˆξi\\n\\x11\\n= 0\\n(C −αi)ξi = 0\\n(C −ˆαi)ˆξi = 0\\n又因为样本(xi, yi) 只可能处在间隔带的某一侧，即约束条件f (xi)−yi−ϵ−ξi = 0 和yi−f (xi)−ϵ−ˆξi = 0\\n不可能同时成立，所以αi 和ˆαi 中至少有一个为0，即αiˆαi = 0。\\n在此基础上再进一步分析可知，如果αi = 0，则根据约束(C −αi)ξi = 0 可知此时ξi = 0。同理，如\\n果ˆαi = 0，则根据约束(C −ˆαi)ˆξi = 0 可知此时ˆξi = 0。所以ξi 和ˆξi 中也是至少有一个为0，即ξi ˆξi = 0。\\n将αiˆαi = 0, ξi ˆξi = 0 整合进上述KKT 条件中即可得到式(6.52)。\\n6.6\\n核方法\\n6.6.1\\n式(6.57) 和式(6.58) 的解释\\n式(6.24) 是式(6.20) 的解；式(6.56) 是式(6.43) 的解。对应到表示定理式(6.57) 当中，式(6.20) 和式\\n(6.43) 均为Ω(∥h∥H) = 1\\n2∥w∥2，式(6.20) 的ℓ(h(x1), h(x2), ..., h(xm)) = 0，而式(6.43) 的ℓ(h(x1), h(x2), ..., h(xm)) =\\nC Pm\\ni=1 ℓϵ(f(xi) −yi)，均满足式(6.57) 的要求，式(6.20) 和式(6.43) 的解均为κ(x, xi) 的线性组合，即\\n式(6.58)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 69, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n6.6.2\\n式(6.65) 的推导\\n由表示定理可知，此时二分类KLDA 最终求得的投影直线方程总可以写成如下形式：\\nh(x) =\\nm\\nX\\ni=1\\nαiκ (x, xi)\\n又因为直线方程的固定形式为\\nh(x) = wTϕ(x)\\n所以\\nwTϕ(x) =\\nm\\nX\\ni=1\\nαiκ (x, xi)\\n将κ (x, xi) = ϕ(x)Tϕ(xi) 代入可得\\nwTϕ(x) =\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ(xi)\\n= ϕ(x)T ·\\nm\\nX\\ni=1\\nαiϕ(xi)\\n由于wTϕ(x) 的计算结果为标量，而标量的转置等于其本身，所以\\nwTϕ(x) =\\n\\x00wTϕ(x)\\n\\x01T = ϕ(x)Tw = ϕ(x)T\\nm\\nX\\ni=1\\nαiϕ(xi)\\n即\\nw =\\nm\\nX\\ni=1\\nαiϕ(xi)\\n6.6.3\\n式(6.66) 和式(6.67) 的解释\\n为了详细地说明此式的计算原理，下面首先举例说明，然后再在例子的基础上延展出其一般形式。假\\n设此时仅有4 个样本，其中第1 和第3 个样本的标记为0，第2 和第4 个样本的标记为1，那么此时有\\nm = 4\\nm0 = 2, m1 = 2\\nX0 = {x1, x3}, X1 = {x2, x4}\\nK =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ (x1, x1)\\nκ (x1, x2)\\nκ (x1, x3)\\nκ (x1, x4)\\nκ (x2, x1)\\nκ (x2, x2)\\nκ (x2, x3)\\nκ (x2, x4)\\nκ (x3, x1)\\nκ (x3, x2)\\nκ (x3, x3)\\nκ (x3, x4)\\nκ (x4, x1)\\nκ (x4, x2)\\nκ (x4, x3)\\nκ (x4, x4)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×4\\n10 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\n11 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n1\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n所以\\nˆµ0 = 1\\nm0\\nK10 = 1\\n2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ (x1, x1) + κ (x1, x3)\\nκ (x2, x1) + κ (x2, x3)\\nκ (x3, x1) + κ (x3, x3)\\nκ (x4, x1) + κ (x4, x3)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\nˆµ1 = 1\\nm1\\nK11 = 1\\n2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ (x1, x2) + κ (x1, x4)\\nκ (x2, x2) + κ (x2, x4)\\nκ (x3, x2) + κ (x3, x4)\\nκ (x4, x2) + κ (x4, x4)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\n根据此结果易得ˆµ0, ˆµ1 的一般形式为\\nˆµ0 = 1\\nm0\\nK10 = 1\\nm0\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nP\\nx∈X0 κ (x1, x)\\nP\\nx∈X0 κ (x2, x)\\n...\\nP\\nx∈X0 κ (xm, x)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈Rm×1\\nˆµ1 = 1\\nm1\\nK11 = 1\\nm1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nP\\nx∈X1 κ (x1, x)\\nP\\nx∈X1 κ (x2, x)\\n...\\nP\\nx∈X1 κ (xm, x)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈Rm×1\\n6.6.4\\n式(6.70) 的推导\\n此式是将式(6.65) 代入式(6.60) 后推得而来的，下面给出详细地推导过程。\\n首先将式(6.65) 代入式(6.60) 的分子可得\\nwTSϕ\\nb w =\\n m\\nX\\ni=1\\nαiϕ (xi)\\n!T\\n· Sϕ\\nb ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nb ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n其中\\nSϕ\\nb =\\n\\x10\\nµϕ\\n1 −µϕ\\n0\\n\\x11 \\x10\\nµϕ\\n1 −µϕ\\n0\\n\\x11T\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!T\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!\\n将其代入上式可得\\nwTSϕ\\nb w =\\nm\\nX\\ni=1\\nαiϕ (xi)T ·\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!\\n·\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!\\n·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nm\\nX\\ni=1\\nαiϕ (xi)T ϕ(x) −1\\nm0\\nX\\nx∈X0\\nm\\nX\\ni=1\\nαiϕ (xi)T ϕ(x)\\n!\\n·\\n \\n1\\nm1\\nX\\nx∈X1\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ (xi) −1\\nm0\\nX\\nx∈X0\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ (xi)\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由于κ (xi, x) = ϕ(xi)Tϕ(x) 为标量，所以其转置等于本身，即κ (xi, x) = ϕ(xi)Tϕ(x) =\\n\\x00ϕ(xi)Tϕ(x)\\n\\x01T =\\nϕ(x)Tϕ(xi) = κ (xi, x)T，将其代入上式可得\\nwTSϕ\\nb w =\\n \\n1\\nm1\\nm\\nX\\ni=1\\nX\\nx∈X1\\nαiκ (xi, x) −1\\nm0\\nm\\nX\\ni=1\\nX\\nx∈X0\\nαiκ (xi, x)\\n!\\n·\\n \\n1\\nm1\\nm\\nX\\ni=1\\nX\\nx∈X1\\nαiκ (xi, x) −1\\nm0\\nm\\nX\\ni=1\\nX\\nx∈X0\\nαiκ (xi, x)\\n!\\n设α = (α1; α2; ...; αm)T ∈Rm×1，同时结合式(6.66) 的解释可得到ˆµ0, ˆµ1 的一般形式，上式可以化简为\\nwTSϕ\\nb w =\\n\\x00αT ˆµ1 −αT ˆµ0\\n\\x01\\n·\\n\\x10\\nˆµT\\n1 α −ˆµT\\n0 α\\n\\x11\\n= αT · (ˆµ1 −ˆµ0) ·\\n\\x10\\nˆµT\\n1 −ˆµT\\n0\\n\\x11\\n· α\\n= αT · (ˆµ1 −ˆµ0) · (ˆµ1 −ˆµ0)T · α\\n= αTMα\\n以上便是式(6.70) 分子部分的推导，下面继续推导式(6.70) 的分母部分。将式(6.65) 代入式(6.60) 的分\\n母可得：\\nwTSϕ\\nww =\\n m\\nX\\ni=1\\nαiϕ (xi)\\n!T\\n· Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n其中\\nSϕ\\nw =\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x10\\nϕ(x) −µϕ\\ni\\n\\x11 \\x10\\nϕ(x) −µϕ\\ni\\n\\x11T\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x10\\nϕ(x) −µϕ\\ni\\n\\x11 \\x12\\nϕ(x)T −\\n\\x10\\nµϕ\\ni\\n\\x11T\\x13\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x12\\nϕ(x)ϕ(x)T −ϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−µϕ\\ni ϕ(x)T + µϕ\\ni\\n\\x10\\nµϕ\\ni\\n\\x11T\\x13\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)ϕ(x)T −\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni ϕ(x)T +\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni\\n\\x10\\nµϕ\\ni\\n\\x11T\\n由于\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n=\\nX\\nx∈X0\\nϕ(x)\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+\\nX\\nx∈X1\\nϕ(x)\\n\\x10\\nµϕ\\n1\\n\\x11T\\n= m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n且\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni ϕ(x)T =\\n1\\nX\\ni=0\\nµϕ\\ni\\nX\\nx∈Xi\\nϕ(x)T\\n= µϕ\\n0\\nX\\nx∈X0\\nϕ(x)T + µϕ\\n1\\nX\\nx∈X1\\nϕ(x)T\\n= m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n所以\\nSϕ\\nw =\\nX\\nx∈D\\nϕ(x)ϕ(x)T −2\\n\\x14\\nm0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\x15\\n+ m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n=\\nX\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n再将此式代回wTSϕ\\nb w 可得\\nwTSϕ\\nww =\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T ·\\n X\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T!\\n·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiϕ (xi)T ϕ(x)ϕ(x)Tαjϕ (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nαjϕ (xj)\\n−\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj)\\n其中，第1 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiϕ (xi)T ϕ(x)ϕ(x)Tαjϕ (xj) =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiαjκ (xi, x) κ (xj, x)\\n= αTKKTα\\n第2 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nαjϕ (xj) = m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjϕ (xi)T µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nϕ (xj)\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjϕ (xi)T\\n\"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n#T\\nϕ (xj)\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαj\\n\"\\n1\\nm0\\nX\\nx∈X0\\nϕ (xi)T ϕ(x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)Tϕ (xj)\\n#\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαj\\n\"\\n1\\nm0\\nX\\nx∈X0\\nκ (xi, x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nκ (xj, x)\\n#\\n= m0αT ˆµ0 ˆµT\\n0 α\\n同理，有第3 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj) = m1αT ˆµ1 ˆµT\\n1 α\\n将上述三项的化简结果代回再将此式代回wTSϕ\\nb w 可得\\nwTSϕ\\nb w = αTKKTα −m0αT ˆµ0 ˆµT\\n0 α −m1αT ˆµ1 ˆµT\\n1 α\\n= αT ·\\n\\x10\\nKKT −m0 ˆµ0 ˆµT\\n0 −m1 ˆµ1 ˆµT\\n1\\n\\x11\\n· α\\n= αT ·\\n \\nKKT −\\n1\\nX\\ni=0\\nmi ˆµi ˆµT\\ni\\n!\\n· α\\n= αTNα\\n6.6.5\\n核对数几率回归\\n将“对数几率回归与支持向量机的关系”中最后得到的对数几率回归重写为如下形式\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n+ λ\\n2m∥w∥2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 73, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中λ 是用来调整正则项权重的正则化常数。假设zi = ϕ(xi) 是由原始空间经核函数映射到高维空间的\\n特征向量，则\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n+ λ\\n2m∥w∥2\\n注意，以上两式中的w 维度是不同的，其分别与xi 和zi 的维度一致。根据表示定理，上式的解可以写为\\nw =\\nm\\nX\\nj=1\\nαjzj\\n将w 代入对数几率回归可得\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(\\n∑m\\nj=1 αjzT\\nj zi+b)\\x11\\n+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjzT\\ni zj\\n用核函数κ (xi, xj) = zT\\ni zj = ϕ (xi)T ϕ (xj) 替换上式中的内积运算\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(\\n∑m\\nj=1 αjκ(xi,xj)+b)\\x11\\n+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjκ (xi, xj)\\n解出α = (α1, α2, ..., αm) 和b 后，即可得f(x) = Pm\\ni=1 αiκ(x, xi) + b。\\n参考文献\\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\\n[2] 王书宁. 凸优化. 清华大学出版社, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第7 章\\n贝叶斯分类器\\n本章是从概率框架下的贝叶斯视角给出机器学习问题的建模方法，不同于前几章着重于算法具体实\\n现，本章的理论性会更强。朴素贝叶斯算法常用于文本分类，例如用于广告邮件检测，贝叶斯网和EM 算\\n法均属于概率图模型的范畴，因此可合并至第14 章一起学习。\\n7.1\\n贝叶斯决策论\\n7.1.1\\n式(7.5) 的推导\\n由式(7.1) 和式(7.4) 可得\\nR(ci|x) = 1 ∗P(c1|x) + ... + 1 ∗P(ci−1|x) + 0 ∗P(ci|x) + 1 ∗P(ci+1|x) + ... + 1 ∗P(cN|x)\\n又PN\\nj=1 P(cj|x) = 1，则\\nR(ci|x) = 1 −P(ci|x)\\n此即式(7.5）。\\n7.1.2\\n式(7.6) 的推导\\n将式(7.5) 代入式(7.3) 即可推得此式\\n7.1.3\\n判别式模型与生成式模型\\n对于判别式模型来说，就是在已知x 的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍\\n的模型都属于判别式模型的范畴，尤其是对数几率回归最为直接明了，式(3.23) 和式(3.24) 直接就是后\\n验概率的形式。\\n对于生成式模型来说，理解起来比较抽象，但是可通过思考以下两个问题来理解。\\n(1) 对于数据集来说，其中的样本是如何生成的？通常假设数据集中的样本服从独立同分布，即每个\\n样本都是按照联合概率分布P(x, c) 采样而得，也可以描述为根据P(x, c) 生成的。\\n(2) 若已知样本x 和联合概率分布P(x, c)，如何预测类别呢？若样本x 和联合概率分布P(x, c) 已\\n知，则可以分别求出x 属于各个类别的概率，即P(x, c1), P(x, c2), ..., P(x, cN)，然后选择概率最大的类\\n别作为样本x 的预测结果。\\n因此，之所以称为“生成式”模型，是因为所求的概率P(x, c) 是生成样本x 的概率。\\n7.2\\n极大似然估计\\n7.2.1\\n式(7.12) 和(7.13) 的推导\\n根据式(7.11) 和式(7.10) 可知参数求解式为\\nˆθc = arg max\\nθc\\nLL (θc)\\n= arg min\\nθc\\n−LL (θc)\\n= arg min\\nθc\\n−\\nX\\nx∈Dc\\nlog P (x|θc)\\n由“西瓜书”上下文可知，此时假设概率密度函数p(x|c) ∼N (µc, σ2\\nc)，其等价于假设\\nP (x|θc) = P\\n\\x00x|µc, σ2\\nc\\n\\x01\\n=\\n1\\np\\n(2π)d|Σc|\\nexp\\n\\x12\\n−1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中，d 表示x 的维数，Σc = σ2\\nc 为对称正定协方差矩阵，|Σc| 表示Σc 的行列式。将其代入参数求解式\\n可得\\n(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\n−\\nX\\nx∈Dc\\nlog\\n\"\\n1\\np\\n(2π)d|Σc|\\nexp\\n\\x12\\n−1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x13#\\n= arg min\\n(µc,Σc)\\n−\\nX\\nx∈Dc\\n\\x14\\n−d\\n2 log(2π) −1\\n2 log |Σc| −1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nX\\nx∈Dc\\n\\x14d\\n2 log(2π) + 1\\n2 log |Σc| + 1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nX\\nx∈Dc\\n\\x141\\n2 log |Σc| + 1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n假设此时数据集Dc 中的样本个数为n，即|Dc| = n，则上式可以改写为\\n(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\nn\\nX\\ni=1\\n\\x141\\n2 log |Σc| + 1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nn\\n2 log |Σc| +\\nn\\nX\\ni=1\\n1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n为了便于分别求解ˆµc 和ˆΣc，在这里我们根据式xTAx = tr(AxxT), ¯x = 1\\nn\\nPn\\ni=1 xi 将上式中的最后一项\\n作如下恒等变形：\\nn\\nX\\ni=1\\n1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −µc)(xi −µc)T\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n\\x00xixT\\ni −xiµT\\nc −µcxT\\ni + µcµT\\nc\\n\\x01\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −n¯xµT\\nc −nµc¯xT + nµcµT\\nc\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −2n¯xµT\\nc + nµcµT\\nc + 2n¯x¯xT −2n¯x¯xT\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n  n\\nX\\ni=1\\nxixT\\ni −2n¯x¯xT + n¯x¯xT\\n!\\n+\\n\\x00nµcµT\\nc −2n¯xµT\\nc + n¯x¯xT\\x01\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T +\\nn\\nX\\ni=1\\n(µc −¯x)(µc −¯x)T\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(µc −¯x)(µc −¯x)T\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ 1\\n2 tr\\n\\x02\\nn · Σ−1\\nc (µc −¯x)(µc −¯x)T\\x03\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ n\\n2 tr\\n\\x02\\nΣ−1\\nc (µc −¯x)(µc −¯x)T\\x03\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ n\\n2 (µc −¯x)TΣ−1\\nc (µc −¯x)\\n所以\\n(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\nn\\n2 log |Σc| + 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ n\\n2 (µc −¯x)TΣ−1\\nc (µc −¯x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n观察上式可知，由于此时Σ−1\\nc\\n和Σc 一样均为正定矩阵，所以当µc −¯x ̸= 0 时，上式最后一项为正定二\\n次型。根据正定二次型的性质可知，此时上式最后一项的取值仅与µc −¯x 相关，并有当且仅当µc −¯x = 0\\n时，上式最后一项取最小值0，此时可以解得\\nˆµc = ¯x = 1\\nn\\nn\\nX\\ni=1\\nxi\\n将求解出来的ˆµc 代回参数求解式可得新的参数求解式，有\\nˆΣc = arg min\\nΣc\\nn\\n2 log |Σc| + 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n此时的参数求解式是仅与Σc 相关的函数。\\n为了求解ˆΣc，在这里我们不加证明地给出一个引理：设B 为p 阶正定矩阵，n > 0 为实数，在对所\\n有p 阶正定矩阵Σ 有\\nn\\n2 log |Σ| + 1\\n2 tr\\n\\x02\\nΣ−1B\\n\\x03\\n≥n\\n2 log |B| + pn\\n2 (1 −log n)\\n当且仅当Σ = 1\\nnB 时等号成立。\\n（引理的证明可搜索张伟平老师的“多元正态分布参数的估计和数据的清洁与变换”课件）\\n根据此引理可知，当且仅当Σc = 1\\nn\\nPn\\ni=1(xi −¯x)(xi −¯x)T 时，上述参数求解式中arg min 后面的式\\n子取到最小值，那么此时的Σc 即我们想要求解的ˆΣc。\\n7.3\\n朴素贝叶斯分类器\\n7.3.1\\n式(7.16) 和式(7.17) 的解释\\n该式是基于大数定律的频率近似概率的思路，而该思路的本质仍然是极大似然估计，下面举例说明。\\n以掷硬币为例，假设投掷硬币5 次，结果依次是正面、正面、反面、正面、反面，试基于此观察结果估计\\n硬币正面朝上的概率。\\n设硬币正面朝上的概率为θ，其服从伯努利分布，因此反面朝上的概率为1 −θ，同时设每次投掷结果\\n相互独立，即独立同分布，则似然为\\nL(θ) = θ · θ · (1 −θ) · θ · (1 −θ)\\n= θ3(1 −θ)2\\n对数似然为\\nLL(θ) = ln L(θ) = 3 ln θ + 2 ln(1 −θ)\\n易证LL(θ) 是关于θ 的凹函数，因此对其求一阶导并令导数等于零即可求出最大值点，具体地\\n∂LL(θ)\\n∂θ\\n= ∂(3 ln θ + 2 ln(1 −θ))\\n∂θ\\n= 3\\nθ −\\n2\\n1 −θ\\n= 3 −5θ\\nθ(1 −θ)\\n令上式等于0 可解得θ = 3\\n5，显然3\\n5 也是正面出现的频率。\\n7.3.2\\n式(7.18) 的解释\\n该式所表示的正态分布并不一定是标准正态分布，因此p(xi|c) 的取值并不一定在(0, 1) 之间，但是\\n仍然不妨碍其用作“概率”，因为根据朴素贝叶斯的算法原理可知，只需p(xi|c) 的值仅仅是用来比大小，\\n因此只关心相对值而不关心绝对值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n7.3.3\\n贝叶斯估计\\n[1]\\n贝叶斯学派视角下的一类点估计法称为贝叶斯估计，常用的贝叶斯估计有最大后验估计（Maximum\\nA Posteriori Estimation，简称MAP）、后验中位数估计和后验期望值估计这3 种参数估计方法，下面给\\n出这3 种方法的具体定义。\\n设总体的概率质量函数（若总体的分布为连续型时则改为概率密度函数，此处以离散型为例）为\\nP(x|θ)，从该总体中抽取出的n 个独立同分布的样本构成样本集D = {x1, x2, · · · , xn}，则根据贝叶斯\\n式可得，在给定样本集D 的条件下，θ 的条件概率为\\nP(θ|D) = P(D|θ)P(θ)\\nP(D)\\n=\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ)\\n其中P(D|θ) 为似然函数，由于样本集D 中的样本是独立同分布的，所以似然函数可以进一步展开，有\\nP(θ|D) =\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ) =\\nQn\\ni=1 P(xi|θ)P(θ)\\nP\\nθ\\nQn\\ni=1 P(xi|θ)P(θ)\\n根据贝叶斯学派的观点，此条件概率代表了我们在已知样本集D 后对θ 产生的新的认识，它综合了\\n我们对θ 主观预设的先验概率P(θ) 和样本集D 带来的信息，通常称其为θ 的后验概率。\\n贝叶斯学派认为，在得到P(θ|D) 以后，对参数θ 的任何统计推断，都只能基于P(θ|D)。至于具体\\n如何去使用它，可以结合某种准则一起去进行，统计学家也有一定的自由度。对于点估计来说，求使得\\nP(θ|D) 达到最大值的ˆθMAP 作为θ 的估计称为最大后验估计，求P(θ|D) 的中位数ˆθMedian 作为θ 的估计\\n称为后验中位数估计，求P(θ|D) 的期望值（均值）ˆθMean 作为θ 的估计称为后验期望值估计。\\n7.3.4\\nCategorical 分布\\nCategorical 分布又称为广义伯努利分布，是将伯努利分布中的随机变量可取值个数由两个泛化为多\\n个得到的分布。具体地，设离散型随机变量X 共有k 种可能的取值{x1, x2, · · · , xk}，且X 取到每个值\\n的概率分别为P(X = x1) = θ1, P(X = x2) = θ2, · · · , P(X = xk) = θk，则称随机变量X 服从参数为\\nθ1, θ2, · · · , θk 的Categorical 分布，其概率质量函数为\\nP(X = xi) = p(xi) = θi\\n7.3.5\\nDirichlet 分布\\n类似于Categorical 分布是伯努利分布的泛化形式，Dirichlet 分布是Beta 分布的泛化形式。对于一\\n个k 维随机变量x = (x1, x2, · · · , xk) ∈Rk，其中xi(i = 1, 2, · · · , k) 满足0 ⩽xi ⩽1, Pk\\ni=1 xi = 1，若x\\n服从参数为α = (α1, α2, · · · , αk) ∈Rk 的Dirichlet 分布，则其概率密度函数为\\np(x; α) =\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nxαi−1\\ni\\n其中Γ(z) =\\nR ∞\\n0 xz−1e−xdx 为Gamma 函数，当α = (1, 1, · · · , 1) 时，Dirichlet 分布等价于均匀分布。\\n7.3.6\\n式(7.19) 和式(7.20) 的推导\\n从贝叶斯估计的角度来说，拉普拉斯修正就等价于先验概率为Dirichlet 分布的后验期望值估计。为\\n了接下来的叙述方便，我们重新定义一下相关数学符号。\\n设有包含m 个独立同分布样本的训练集D，D 中可能的类别数为k，其类别的具体取值范围为\\n{c1, c2, ..., ck}。若令随机变量C 表示样本所属的类别，且C 取到每个值的概率分别为P(C = c1) =\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nθ1, P(C = c2) = θ2, ..., P(C = ck) = θk，那么显然C 服从参数为θ = (θ1, θ2, ..., θk) ∈Rk 的Categorical\\n分布，其概率质量函数为\\nP(C = ci) = P(ci) = θi\\n其中P(ci) = θi 就是式(7.9) 所要求解的ˆP(c)，下面我们用贝叶斯估计中的后验期望值估计来估计\\nθi。根据贝叶斯估计的原理可知，在进行参数估计之前，需要先主观预设一个先验概率P(θ)，通常为了方\\n便计算后验概率P(θ|D)，我们会用似然函数P(D|θ) 的共轭先验作为我们的先验概率。显然，此时的似\\n然函数P(D|θ) 是一个基于Categorical 分布的似然函数，而Categorical 分布的共轭先验为Dirichlet 分\\n布，所以只需要预设先验概率P(θ) 为Dirichlet 分布，然后使用后验期望值估计就能估计出θi。\\n具体地，记D 中样本类别取值为ci 的样本个数为yi，则似然函数P(D|θ) 可展开为\\nP(D|θ) = θy1\\n1 ...θyk\\nk =\\nk\\nY\\ni=1\\nθyi\\ni\\n则有后验概率\\nP(θ|D) = P(D|θ)P(θ)\\nP(D)\\n=\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ)\\n=\\nQk\\ni=1 θyi\\ni · P(θ)\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · P(θ)\\ni\\n假设此时先验概率P(θ) 是参数为α = (α1, α2, ..., αk) ∈Rk 的Dirichlet 分布，则P(θ) 可写为\\nP(θ; α) =\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nθαi−1\\ni\\n将其代入P(D|θ) 可得\\nP(θ|D) =\\nQk\\ni=1 θyi\\ni · P(θ)\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · P(θ)\\ni\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\nP\\nθ\\n\\uf8ee\\n\\uf8f0Qk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\n\\uf8f9\\n\\uf8fb\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\ni\\n·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\n=\\nQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\ni\\n=\\nQk\\ni=1 θαi+yi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时若设α + y = (α1 + y1, α2 + y2, ..., αk + yk) ∈Rk，则根据Dirichlet 分布的定义可知\\nP(θ; α + y) =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\nX\\nθ\\nP(θ; α + y) =\\nX\\nθ\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n1 =\\nX\\nθ\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n1 =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nX\\nθ\\n\" k\\nY\\ni=1\\nθαi+yi−1\\ni\\n#\\n1\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\n将此结论代入P(D|θ) 可得\\nP(θ|D) =\\nQk\\ni=1 θαi+yi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni\\n=\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n= P(θ; α + y)\\n综上可知，对于服从Categorical 分布的θ 来说，假设其先验概率P(θ) 是参数为α 的Dirichlet 分布时，\\n得到的后验概率P(θ|D) 是参数为α + y 的Dirichlet 分布，通常我们称这种先验概率分布和后验概率分\\n布形式相同的这对分布为共轭分布。在推得后验概率P(θ|D) 的具体形式以后，根据后验期望值估计可得\\nθi 的估计值为\\nθi = EP (θ|D)[θi]\\n= EP (θ;α+y)[θi]\\n=\\nαi + yi\\nPk\\nj=1(αj + yj)\\n=\\nαi + yi\\nPk\\nj=1 αj + Pk\\nj=1 yj\\n=\\nαi + yi\\nPk\\nj=1 αj + m\\n显然，式(7.9) 是当α = (1, 1, ..., 1) 时推得的具体结果，此时等价于我们主观预设的先验概率P(θ) 服从\\n均匀分布，此即拉普拉斯修正。同理，当我们调整α 的取值后，即可推得其他数据平滑的公式。\\n7.4\\n半朴素贝叶斯分类器\\n7.4.1\\n式(7.21) 的解释\\n在朴素贝叶斯中求解P(xi|c) 时，先挑出类别为c 的样本，若是离散属性则按大数定律估计P(xi|c)，\\n若是连续属性则求这些样本的均值和方差，接着按正态分布估计P(xi|c)。现在估计P(xi|c, pai)，则是先\\n挑出类别为c 且属性xi 所依赖的属性为pai 的样本，剩下步骤与估计P(xi|c) 时相同。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n7.4.2\\n式(7.22) 的解释\\n该式写为如下形式可能更容易理解：\\nI(xi, xj|y) =\\nN\\nX\\nn=1\\nP(xi, xj|cn) log\\nP(xi, xj|cn)\\nP(xi|cn)P(xj|cn)\\n其中i, j = 1, 2, ..., d 且i ̸= j，N 为类别个数。该式共可得到d(d−1)\\n2\\n个I(xi, xj|y)，即每对(xi, xj) 均有\\n一个条件互信息I(xi, xj|y)。\\n7.4.3\\n式(7.23) 的推导\\n基于贝叶斯定理，式(7.8) 将联合概率P(x, c) 写为等价形式P(x|c)P(c)，实际上，也可将向量x 拆\\n开，把P(x, c) 写为P(x1, x2, ..., xd, c) 形式，然后利用概率公式P(A, B) = P(A|B)P(B) 对其恒等变形\\nP(x, c) = P (x1, x2, . . . , xd, c)\\n= P (x1, x2, . . . , xd | c) P(c)\\n= P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi) P (c, xi)\\n类似式(7.14) 采用属性条件独立性假设，则\\nP(x1, ..., xi−1, xi+1, ..., xd|c, xi) =\\nd\\nY\\nj=1j̸=i\\nP(xj|c, xi)\\n根据式(7.25) 可知，当j = i 时，|Dc,xi| = |Dc,xi,xj|，若不考虑平滑项，则此时P(xj|c, xi) = 1，因此在\\n上式的连乘项中可放开j ̸= i 的约束，即\\nP(x1, ..., xi−1, xi+1, ..., xd|c, xi) =\\nd\\nY\\nj=1\\nP(xj|c, xi)\\n综上可得：\\nP(c|x) = P(x, c)\\nP(x)\\n= P (c, xi) P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi)\\nP(x)\\n∝P (c, xi) P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi)\\n= P (c, xi)\\nd\\nY\\nj=1\\nP(xj|c, xi)\\n上式是将属性xi 作为超父属性的，AODE 尝试将每个属性作为超父来构建SPODE，然后将那些具\\n有足够训练数据支撑的SPODE 集成起来作为最终结果。具体来说，对于总共d 个属性来说，共有d 个\\n不同的上式，集成直接求和即可，因为对于不同的类别标记c 均有d 个不同的上式，至于如何满足“足够\\n训练数据支撑的SPODE”这个条件，注意式(7.24) 和式(7.25) 均使用到了|Dc,xi| 和|Dc,xi,xj|，若集合\\nDxi 中样本数量过少，则|Dc,xi| 和|Dc,xi,xj| 将会更小，因此在式(7.23) 中要求集合Dxi 中样本数量不少\\n于m′。\\n7.4.4\\n式(7.24) 和式(7.25) 的推导\\n类比式(7.19) 和式(7.20) 的推导。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 81, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n7.5\\n贝叶斯网\\n7.5.1\\n式(7.27) 的解释\\n在这里补充一下同父结构和顺序结构的推导。同父结构：在给定父节点x1 的条件下x3, x4 独立\\nP(x3, x4|x1) = P(x1, x3, x4)\\nP(x1)\\n= P(x1)P(x3|x1)P(x4|x1)\\nP(x1)\\n= P(x3|x1)P(x4|x1)\\n顺序结构：在给定节点x 的条件下y, z 独立\\nP(y, z|x) = P(x, y, z)\\nP(x)\\n= P(z)P(x|z)P(y|x)\\nP(x)\\n= P(z, x)P(y|x)\\nP(x)\\n= P(z|x)P(y|x)\\n7.6\\nEM 算法\\n“西瓜书”中仅给出了EM 算法的运算步骤，其原理并未展开讲解，下面补充EM 算法的推导原理，\\n以及所用到的相关数学知识。\\n7.6.1\\nJensen 不等式\\n若f 是凸函数，则下式恒成立\\nf (tx1 + (1 −t)x2) ⩽tf(x1) + (1 −t)f(x2)\\n其中t ∈[0, 1]，若将x 推广到n 个时同样成立，即\\nf(t1x1 + t2x2 + ... + tnxn) ⩽t1f(x1) + t2f(x2) + ... + tnf(tn)\\n其中t1, t2, ..., tn ∈[0, 1], Pn\\ni=1 ti = 1。此不等式在概率论中通常以如下形式出现\\nφ(E[X]) ⩽E[φ(X)]\\n其中X 是随机变量，φ 为凸函数，E[X] 为随机变量X 的期望。显然，若f 和φ 是凹函数，则上述不等\\n式中的⩽换成⩾也恒成立。\\n7.6.2\\nEM 算法的推导\\n假设现有一批独立同分布的样本{x1, x2, ..., xm}，它们是由某个含有隐变量的概率分布p(x, z; θ) 生\\n成，现尝试用极大似然估计法估计此概率分布的参数。为了便于讨论，此处假设z 为离散型随机变量，则\\n对数似然函数为\\nLL(θ) =\\nm\\nX\\ni=1\\nln p(xi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(xi, zi; θ)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n显然，此时LL(θ) 里含有未知的隐变量z 以及求和项的对数，相比于不含隐变量的对数似然函数，显然\\n该似然函数的极大值点较难求解，而EM 算法则给出了一种迭代的方法来完成对LL(θ) 的极大化。\\n下面给出两种推导方法，一个是出自李航老师的《统计学习方法》\\n[2]，一个是出自吴恩达老师的CS229，\\n两种推导方式虽然形式上有差异，但最终的Q 函数相等，接下来先讲述两种推导方法，最后会给出Q 函\\n数是相等的证明。\\n首先给出《统计学习方法》中的推导方法，设X = {x1, x2, ..., xm}, Z = {z1, z2, ..., zm}，则对数似然\\n函数可以改写为\\nLL(θ) = ln P(X|θ)\\n= ln\\nX\\nZ\\nP(X, Z|θ)\\n= ln\\n X\\nZ\\nP(X|Z, θ)P(Z|θ)\\n!\\nEM 算法采用的是通过迭代逐步近似极大化L(θ)：假设第t 次迭代时θ 的估计值是θ(t)，我们希望第t + 1\\n次迭代时的θ 能使LL(θ) 增大，即LL(θ) > LL(θ(t))。为此，考虑两者的差\\nLL(θ) −LL(θ(t)) = ln\\n X\\nZ\\nP(X|Z, θ)P(Z|θ)\\n!\\n−ln P(X|θ(t))\\n= ln\\n\\uf8eb\\n\\uf8edX\\nZ\\nP(Z|X, θ(t))\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n\\uf8f6\\n\\uf8f8−ln P(X|θ(t))\\n由上述Jensen 不等式可得\\nLL(θ) −LL(θ(t)) ⩾\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−1 · ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−\\nX\\nZ\\nP(Z|X, θ(t)) · ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t))\\n\\uf8eb\\n\\uf8edln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n令\\nB(θ, θ(t)) = LL(θ(t)) +\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n则\\nLL(θ) ⩾B(θ, θ(t))\\n即B(θ, θ(t)) 是LL(θ) 的下界，此时若设θ(t+1) 能使得B(θ, θ(t)) 达到极大，即\\nB(θ(t+1), θ(t)) ⩾B(θ, θ(t))\\n由于LL(θ(t)) = B(θ(t), θ(t))，那么可以进一步推得\\nLL(θ(t+1)) ⩾B(θ(t+1), θ(t)) ⩾B(θ(t), θ(t)) = LL(θ(t))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nLL(θ(t+1)) ⩾LL(θ(t))\\n因此，任何能使得B(θ, θ(t)) 增大的θ，也可以使得LL(θ) 增大，于是问题就转化为了求解能使得B(θ, θ(t))\\n达到极大的θ(t+1)，即\\nθ(t+1) = arg max\\nθ\\nB(θ, θ(t))\\n= arg max\\nθ\\n\\uf8eb\\n\\uf8edLL(θ(t)) +\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n略去对θ 极大化而言是常数的项\\nθ(t+1) = arg max\\nθ\\n X\\nZ\\nP(Z|X, θ(t)) ln (P(X|Z, θ)P(Z|θ))\\n!\\n= arg max\\nθ\\n X\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\n!\\n= arg max\\nθ\\nQ(θ, θ(t))\\n到此即完成了EM 算法的一次迭代，求出的θ(t+1) 作为下一次迭代的初始θ(t)。综上，EM 算法的“E 步”\\n和“M 步”可总结为以下两步。\\nE 步：计算完全数据的对数似然函数ln P(X, Z|θ) 关于在给定观测数据X 和当前参数θ(t) 下对未观\\n测数据Z 的条件概率分布P(Z|X, θ(t)) 的期望Q(θ, θ(t))：\\nQ(θ, θ(t)) = EZ[ln P(X, Z|θ)|X, θ(t)] =\\nX\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\nM 步：求使得Q(θ, θ(t)) 达到极大的θ(t+1)。\\n接下来给出CS229 中的推导方法，设zi 的概率质量函数为Qi(zi)，则LL(θ) 可以作如下恒等变形\\nLL(θ) =\\nm\\nX\\ni=1\\nln p(xi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(xi, zi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n其中P\\nzi Qi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n可以看做是对\\np(xi, zi; θ)\\nQi(zi)\\n关于zi 求期望，即\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n= Ezi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)\\nQi(zi)\\n\\uf8f9\\n\\uf8fb\\n由Jensen 不等式可得\\nln\\n\\uf8eb\\n\\uf8edEzi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)\\nQi(zi)\\n\\uf8f9\\n\\uf8fb\\n\\uf8f6\\n\\uf8f8⩾Ezi\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8edp(xi, zi; θ)\\nQi(zi)\\n\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n⩾\\nX\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n将此式代入LL(θ) 可得\\nLL(θ) =\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n(7.6.1)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n若令B(θ) =\\nm\\nP\\ni=1\\nP\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n，则此时B(θ) 为LL(θ) 的下界函数，那么这个下界函数所能构成\\n的最优下界是多少？即B(θ) 的最大值是多少？显然，B(θ) 是LL(θ) 的下界函数，反过来LL(θ) 是其上\\n界函数，所以如果能使得B(θ) = LL(θ)，则此时的B(θ) 就取到了最大值。根据Jensen 不等式的性质\\n可知，如果能使得\\np(xi, zi; θ)\\nQi(zi)\\n恒等于某个常量c，大于等于号便可以取到等号。因此，只需任意选取满足\\np(xi, zi; θ)\\nQi(zi)\\n= c 的Qi(zi) 就能使得B(θ) 达到最大值。由于Qi(zi) 是zi 的概率质量函数，所以Qi(zi) 同\\n时也满足约束0 ⩽Qi(zi) ⩽1, P\\nzi Qi(zi) = 1，结合Qi(zi) 的所有约束可以推得\\np(xi, zi; θ)\\nQi(zi)\\n= c\\np(xi, zi; θ) = c · Qi(zi)\\nX\\nzi\\np(xi, zi; θ) = c ·\\nX\\nzi\\nQi(zi)\\nX\\nzi\\np(xi, zi; θ) = c\\np(xi, zi; θ)\\nQi(zi)\\n=\\nX\\nzi\\np(xi, zi; θ)\\nQi(zi) =\\np(xi, zi; θ)\\nP\\nzi\\np(xi, zi; θ) =\\np(xi, zi; θ)\\np(xi; θ)\\n= p(zi|xi; θ)\\n所以，当且仅当Qi(zi) = p(zi|xi; θ) 时B(θ) 取到最大值，将Qi(zi) = p(zi|xi; θ) 代回LL(θ) 和B(θ) 可\\n以推得\\nLL(θ) =\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n(7.6.2)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(zi|xi; θ)\\np(xi, zi; θ)\\np(zi|xi; θ)\\n(7.6.3)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ) ln\\np(xi, zi; θ)\\np(zi|xi; θ)\\n(7.6.4)\\n= max{B(θ)}\\n(7.6.5)\\n其中式(7.6.4) 是式(7.6.1) 中不等式取等号时的情形。由以上推导可知，此时对数似然函数LL(θ) 等价\\n于其下界函数的最大值max{B(θ)}，所以要想极大化LL(θ) 可以通过极大化max{B(θ)} 来间接极大化\\nLL(θ)，因此，下面考虑如何极大化max{B(θ)}。假设已知第t 次迭代的参数为θ(t)，而第t + 1 次迭代的\\n参数θ(t+1) 可通过如下方式求得\\nθ(t+1) = arg max\\nθ\\nmax{B(θ)}\\n(7.6.6)\\n= arg max\\nθ\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ)\\np(zi|xi; θ(t))\\n(7.6.7)\\n= arg max\\nθ\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)\\n(7.6.8)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时将θ(t+1) 代入LL(θ) 可推得\\nLL(θ(t+1)) = max{B(θ(t+1))}\\n(7.6.9)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t+1)) ln\\np(xi, zi; θ(t+1))\\np(zi|xi; θ(t+1))\\n(7.6.10)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ(t+1))\\np(zi|xi; θ(t))\\n(7.6.11)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ(t))\\np(zi|xi; θ(t))\\n(7.6.12)\\n= max{B(θ(t))}\\n(7.6.13)\\n= LL(θ(t))\\n(7.6.14)\\n其中，式(7.6.9) 和式(7.6.10) 分别由式(7.6.5) 和式(7.6.4) 推得，式(7.6.11) 由式(7.6.1) 推得，式(7.6.12)\\n由式(7.6.7) 推得，式(7.6.13) 和式(7.6.14) 由式(7.6.2) 至式(7.6.5) 推得。此时若令\\nQ(θ, θ(t)) =\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)\\n由式(7.6.9) 至式(7.6.14) 可知，凡是能使得Q(θ, θ(t)) 达到极大的θ(t+1) 一定能使得LL(θ(t+1)) ⩾LL(θ(t))。\\n综上，EM 算法的“E 步”和“M 步”可总结为以下两步。\\nE 步：令Qi(zi) = p(zi|xi; θ) 并写出Q(θ, θ(t))；\\nM 步：求使得Q(θ, θ(t)) 到达极大的θ(t+1)。\\n以上便是EM 算法的两种推导方法，下面证明两种推导方法中的Q 函数相等。\\nQ(θ|θ(t)) =\\nX\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t)) ln\\n\" m\\nY\\ni=1\\nP(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t))\\n\" m\\nX\\ni=1\\nln P(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t)) [ln P(x1, z1|θ) + ln P(x2, z2|θ) + ... + ln P(xm, zm|θ)]\\n)\\n=\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n+ ... +\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中\\nP\\nz1,z2,...,zm\\n\\x14 m\\nQ\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n\\x15\\n可作如下恒等变形：\\n=\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t)) · P(z1|x1, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nX\\nz2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t)) · P(z1|x1, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\nX\\nz2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t))\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\nX\\nz2,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nP(z2|x2, θ(t))\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t))\\n#)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nP(z2|x2, θ(t)) ×\\nX\\nz3\\nP(z3|x3, θ(t)) × ... ×\\nX\\nzm\\nP(zm|xm, θ(t))\\n)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ) × {1 × 1 × ... × 1}\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n所以\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n同理可得\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x2, z2|θ)\\n#\\n=\\nX\\nz2\\nP(z2|x2, θ(t)) ln P(x2, z2|θ)\\n...\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nzm\\nP(zm|xm, θ(t)) ln P(xm, zm|θ)\\n将上式代入Q(θ|θ(t)) 可得\\nQ(θ|θ(t)) =\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n+ ... +\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ) + ... +\\nX\\nzm\\nP(zm|xm, θ(t)) ln P(xm, zm|θ)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\nP(zi|xi, θ(t)) ln P(xi, zi|θ)\\n参考文献\\n[1] 陈希孺. 概率论与数理统计. 中国科学技术大学出版社, 2009.\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 87, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第8 章\\n集成学习\\n集成学习(ensemble learning) 描述的是组合多个基础的学习器（模型）的结果已达到更加鲁棒、效果\\n更好的学习器。在“西瓜书”作者周志华教授的谷歌学术主页的top 引用文章中，很大一部分都和集成学\\n习有关。\\n图8-3 周志华教授谷歌学术top10 引用文章(截止到2023-02-19)\\n如图8-3所示。在引用次数前10 的文章中，第1 名“Top 10 algorithms in data mining”是在ICDM’\\n06 中投票选出的数据挖掘十大算法，每个提名算法均由业内专家代表去阐述，然后进行投票，其中最终得\\n票排名第7 位的“Adaboost”即由周志华教授作为代表进行阐述；第2 名“Isolation forest”是通过集成学\\n习的技术用来做异常检测。第3 名的“Ensemble Methods: Foundations and Algorithms”则是周志华教\\n授所著的集成学习专著。第6 名“Ensembing neural networks: many could be better than all”催生了基\\n于优化的集成修剪(Ensemble pruning) 技术；第7 名的“Exploratory undersampling for class-imbalance\\nlearning”是以集成学习技术解决类别不平衡问题。\\n毫不夸张的说，周志华教授在集成学习领域深耕了很多年，是绝对的权威。而集成学习也是经受了时\\n间考验的非常有效的算法，常常被各位竞赛同学作为涨点提分的致胜法宝。下面，让我们一起认真享受\\n“西瓜书”作者最拿手的集成学习章节吧。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 88, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.1\\n个体与集成\\n基学习器(base learner) 的概念在论文中经常出现，可留意一下；另外，本节提到的投票法有两种，除\\n了本节的多数投票(majority voting)，还有概率投票(probability voting)，这两点在8.4 节中均会提及，即\\n硬投票和软投票。\\n8.1.1\\n式(8.1) 的解释\\nhi(x) 是编号为i 的基分类器给x 的预测标记，f(x) 是x 的真实标记，它们之间不一致的概率记为\\nϵ。\\n8.1.2\\n式(8.2) 的解释\\n注意到当前仅针对二分类问题y ∈{−1, +1}, 即预测标记hi(x) ∈{−1, +1} 。各个基分类器hi 的分\\n类结果求和之后结果的正、负或0，代表投票法产生的结果，即“少数服从多数”，符号函数sign，将正数\\n变成1，负数变成-1，0 仍然是0，所以H(x) 是由投票法产生的分类结果。\\n8.1.3\\n式(8.3) 的推导\\n由基分类器相互独立，假设随机变量X 为T 个基分类器分类正确的次数，因此随机变量X 服从二项\\n分布：X ∼B(T, 1 −ϵ)，设xi 为每一个分类器分类正确的次数，则xi ∼B(1, 1 −ϵ)\\uffffi = 1\\uffff2\\uffff3\\uffff...\\uffffT\\uffff，那么有\\nX =\\nT\\nX\\ni=1\\nxi\\nE(X) =\\nT\\nX\\ni=1\\nE(xi) = (1 −ϵ)T\\n证明过程如下：\\nP(H(x) ̸= f(x)) =P(X ≤⌊T/2⌋)\\n⩽P(X ≤T/2)\\n= P\\n\\x14\\nX −(1 −ϵ)T ⩽T\\n2 −(1 −ϵ)T\\n\\x15\\n= P\\n\\x14\\nX −(1 −ϵ)T ⩽−T\\n2 (1 −2ϵ)]\\n\\x15\\n= P\\n\" T\\nX\\ni=1\\nxi −\\nT\\nX\\ni=1\\nE(xi) ⩽−T\\n2 (1 −2ϵ)]\\n#\\n= P\\n\"\\n1\\nT\\nT\\nX\\ni=1\\nxi −1\\nT\\nT\\nX\\ni=1\\nE(xi) ⩽−1\\n2 (1 −2ϵ)]\\n#\\n根据Hoeffding 不等式知\\nP\\n \\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩽−δ\\n!\\n⩽exp\\n\\x00−2mδ2\\x01\\n令δ = (1−2ϵ)\\n2\\n, m = T 得\\nP(H(x) ̸= f(x)) =\\n⌊T /2⌋\\nX\\nk=0\\n \\nT\\nk\\n!\\n(1 −ϵ)kϵT −k\\n⩽exp\\n\\x12\\n−1\\n2T(1 −2ϵ)2\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2\\nBoosting\\n注意8.1 节最后一段提到：根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即\\n个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同\\n时生成的并行化方法。\\n本节Boosting 为前者的代表，Adaboost 又是Boosting 族算法的代表。\\n8.2.1\\n式(8.4) 的解释\\n这个式子是集成学习的加性模型，加性模型不采用梯度下降的思想，而是H(x) = PT −1\\nt=1 αtht(x) +\\nαT hT (x)，共迭代T 次，每次更新求解一个理论上最优的hT 和αT 。hT 和αT 的定义参见式(8.18) 和式(8.11)\\n8.2.2\\n式(8.5) 的解释\\n先考虑指数损失函数e−f(x)H(x) 的含义参见“西瓜书”图6.5 ：f 为真实函数，对于样本x 来说，\\nf(x) ∈{+1, −1} 只能取+1 和−1，而H(x) 是一个实数。\\n当H(x) 的符号与f(x) 一致时，f(x)H(x) > 0，因此e−f(x)H(x) = e−|H(x)| < 1，且|H(x)| 越大指数\\n损失函数e−f(x)H(x) 越小。这很合理：此时|H(x)| 越大意味着分类器本身对预测结果的信心越大，损失\\n应该越小；若|H(x)| 在零附近，虽然预测正确，但表示分类器本身对预测结果信心很小，损失应该较大；\\n当H(x) 的符号与f(x) 不一致时，f(x)H(x) < 0，因此e−f(x)H(x) = e|H(x)| > 1，且|H(x)| 越大指\\n数损失函数越大。这很合理：此时|H(x)| 越大意味着分类器本身对预测结果的信心越大，但预测结果是\\n错的，因此损失应该越大；若|H(x)| 在零附近，虽然预测错误，但表示分类器本身对预测结果信心很小，\\n虽然错了，损失应该较小。\\n再解释符号Ex∼D[·] 的含义：D 为概率分布，可简单理解为在数据集D 中进行一次随机抽样，每个\\n样本被取到的概率；E[·] 为经典的期望，则综合起来Ex∼D[·] 表示在概率分布D 上的期望，可简单理解为\\n对数据集D 以概率D 进行加权后的期望。\\n综上所述, 若数据集D 中样本x 的权值分布为D(x), 则式(8.5) 可写为:\\nℓexp(H | D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)H(x)\\n=\\nX\\nx∈D\\nD(x)\\n\\x00e−H(x)I(f(x) = 1) + eH(x)I(f(x) = −1)\\n\\x01\\n特别地, 若针对任意样本x, 若分布D(x) =\\n1\\n|D|, 其中|D| 为数据集D 样本个数, 则\\nℓexp(H | D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\n1\\n|D|\\nX\\nx∈D\\ne−f(x)H(x)\\n而这就是在求传统平均值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.3\\n式(8.6) 的推导\\n由公式(8.5) 中对于符号Ex∼D[·] 的解释可知\\nℓexp(H|D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)H(x)\\n=\\n|D|\\nX\\ni=1\\nD (xi)\\n\\x00e−H(xi)I (f (xi) = 1) + eH(xi)I (f (xi) = −1)\\n\\x01\\n=\\n|D|\\nX\\ni=1\\n\\x00e−H(xi)D (xi) I (f (xi) = 1) + eH(xi)D (xi) I (f (xi) = −1)\\n\\x01\\n=\\n|D|\\nX\\ni=1\\n\\x00e−H(xi)P (f (xi) = 1 | xi) + eH(xi)P (f (xi) = −1 | xi)\\n\\x01\\n其中D (xi) I (f (xi) = 1) = P (f (xi) = 1 | xi) 可以这样理解：D(xi) 表示在数据集D 中进行一次随机抽\\n样，样本xi 被取到的概率，D (xi) I (f (xi) = 1) 表示在数据集D 中进行一次随机抽样，使得f(xi) = 1\\n的样本xi 被抽到的概率，即为P (f (xi) = 1 | xi)。\\n当对H(xi) 求导时，求和号中只有含xi 项不为0，由求导公式\\n∂e−H(x)\\n∂H(x) = −e−H(x)\\n∂eH(x)\\n∂H(x) = eH(x)\\n有\\n∂ℓexp(H|D)\\n∂H(x)\\n= −e−H(x)P(f(x) = 1|x) + eH(x)P(f(x) = −1|x)\\n8.2.4\\n式(8.7) 的推导\\n令式(8.6) 等于零:\\n−e−H(x)P(f(x) = 1 | x) + eH(x)P(f(x) = −1 | x) = 0\\n移项:\\neH(x)P(f(x) = −1 | x) = e−H(x)P(f(x) = 1 | x)\\n两边同乘\\neH(x)\\nP (f(x)=−1|x):\\ne2H(x) = P(f(x) = 1 | x)\\nP(f(x) = −1 | x)\\n取ln(·):\\n2H(x) = ln P(f(x) = 1 | x)\\nP(f(x) = −1 | x)\\n两边同除1\\n2 即得式(8.7)。\\n8.2.5\\n式(8.8) 的推导\\nsign(H(x)) = sign\\n\\x121\\n2 ln P(f(x) = 1|x)\\nP(f(x) = −1|x)\\n\\x13\\n=\\n(\\n1,\\nP(f(x) = 1|x) > P(f(x) = −1|x)\\n−1,\\nP(f(x) = 1|x) < P(f(x) = −1|x)\\n= arg max\\ny∈{−1,1}\\nP(f(x) = y|x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第一行到第二行显然成立，第二行到第三行是利用了arg max 函数的定义。arg max\\ny∈{−1,1}\\nP(f(x) = y|x) 表示使\\n得函数P(f(x) = y|x) 取得最大值的y 的值，展开刚好是第二行的式子。\\n这里解释一下贝叶斯错误率的概念。这来源于“西瓜书”P148 的式(7.6) 表示的贝叶斯最优分类器，\\n可以发现式(8.8) 的最终结果是式(7.6) 的二分类特殊形式。\\n到此为止，本节证明了指数损失函数是分类任务原本0/1 损失函数的一致的替代损失函数参见“西瓜书”P131 图\\n而指数损失函数有更好的数学性质，例如它是连续可微函数，因此接下来的式(8.9) 至式(8.19) 基于指数\\n损失函数推导AdaBoost 的理论细节。\\n8.2.6\\n式(8.9) 的推导\\nℓexp (αtht|Dt) = Ex∼Dt\\n\\x02\\ne−f(x)αtht(x)\\x03\\n= Ex∼Dt\\n\\x02\\ne−αtI (f(x) = ht(x)) + eαtI (f(x) ̸= ht(x))\\n\\x03\\n= e−αtPx∼Dt (f(x) = ht(x)) + eαtPx∼Dt (f(x) ̸= ht(x))\\n= e−αt (1 −ϵt) + eαtϵt\\n乍一看本式有些问题, 为什么要最小化ℓexp (αtht | Dt) ?\\n“西瓜书”图8.3 中的第3 行的表达式\\nht = L (D, Dt) 不是代表着应该最小化ℓexp (ht | Dt) 么?\\n或者从整体来看, 第t 轮迭代也应该最小化\\nℓexp (Ht | D) = ℓexp (Ht−1 + αtht | D), 这样最终T 轮迭代结束后得到的式(8.4) 就可以最小化ℓexp(H |\\nD) 了。实际上, 理解了AdaBoost 之后就会发现, ℓexp (αtht | Dt) 与ℓexp (Ht | D) 是等价的, 详见后面的\\n“AdaBoost 的个人推导”。另外, ht = L (D, Dt) 也是推导的结论之一, 即式(8.18), 而不是无缘无故靠直觉\\n用L (D, Dt) 得到ht 。\\n暂时不管以上疑问, 权且按作者思路推导一下：\\n第1 个等号与式(8.5) 的区别仅在于到底针对αtht(x) 还是H(x), 代入即可;\\n第2 个等号是考虑到ht(x) 和f(x) 均只能取−1 和+1 两个值, 其中I(·) 为指示函数;\\n第3 个等号对中括号的两项分别求Ex∼Dt[·], 而eαt 和e−αt 与x 无关, 可以作为常数项拿到Ex∼Dt[.]\\n外面, 而Ex∼Dt [I (f(x) = ht(x))] 表示在数据集D 上、样本权值分布为Dt 时f(x) 和ht(x) 相等次数的\\n期望, 即Px∼Dt (f(x) = ht(x)), 也就是正确率, 即(1 −ϵt); 同理, Ex∼Dt [I (f(x) ̸= ht(x))] 表示在数据集\\nD 上、样本权值分布为Dt 时f(x) 和ht(x) 不相等次数的期望, 即Px∼Dt (f(x) ̸= ht(x)), 也就是错误率\\nϵt;\\n第4 个等号即为将Px∼Dt (f(x) = ht(x)) 替换为(1 −ϵt) 、将Px∼Dt (f(x) ̸= ht(x)) 替换为ϵt 的结\\n果。\\n注意本节符号略有混乱, 如前所述式(8.4) 的H(x) 是连续实值函数, 但在“西瓜书”图8.3 最后一行\\n的输出H(x) 明显只能取−1 和+1 两个值(与式(8.2) 相同), 本节除了“西瓜书”图8.3 最后一行的输出\\n之外, H(x) 均以式(8.4) 的连续实值函数为准。\\n8.2.7\\n式(8.10) 的解释\\n指数损失函数对αt 求偏导，为了得到使得损失函数取最小值时αt 的值。\\n8.2.8\\n式(8.11) 的推导\\n令公式(8.10) 等于0 移项即得到的该式。此时αt 的取值使得该基分类器经αt 加权后的损失函数最\\n小。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图8-4 原始论文对式8.12 的相关推导\\n8.2.9\\n式(8.12) 的解释\\n本式的推导和原始论文[1] 的推导略有差异，虽然并不影响后面式(8.18) 以及式(8.19) 的推导结果。\\nAdaBoost 第t 轮迭代应该求解如下优化问题从而得到αt 和ht(x) :\\n(αt, ht(x)) = arg min\\nα,h\\nℓexp (Ht−1 + αh | D)\\n对于该问题, 先对于固定的任意α > 0, 求解ht(x); 得到ht(x) 后再求αt◦。\\n在原始论文的第346 页，对式(8.12) 的推导如图8-4所示。可以发现原文献中保留了参数c (即α )。\\n当然, 对于任意α > 0, 并不影响推导结果。\\n如果暂且不管以上的差异，我们按照作者的思路推导的话，将Ht(x) = Ht−1(x) + ht(x) 带入公式\\n(8.5) 即可，因为理想的ht 可以纠正Ht−1 的全部错误，所以这里指定ht 其权重系数αt 为1。如果权重\\n系数αt 是个常数的话，对后续结果也没有影响。\\n8.2.10\\n式(8.13) 的推导\\n由ex 的二阶泰勒展开为1 + x + x2\\n2 + o(x2) 得:\\nℓexp (Ht−1 + ht|D) = Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)e−f(x)ht(x)\\x03\\n≃Ex∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)ht(x) + f 2(x)h2\\nt(x)\\n2\\n\\x13\\x15\\n因为f(x) 与ht(x) 取值都为1 或-1，所以f 2(x) = h2\\nt(x) = 1，所以得:\\nℓexp (Ht−1 + ht|D) = Ex∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)ht(x) + 1\\n2\\n\\x13\\x15\\n实际上，此处保留一阶泰勒展开项即可，后面提到的Gradient Boosting 理论框架就是只使用了一阶\\n泰勒展开；当然二阶项为常数，也并不影响推导结果，原文献[1] 中也保留了二阶项。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 93, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.11\\n式(8.14) 的推导\\nht(x) = arg min\\nh\\nℓexp (Ht−1 + h|D)\\n= arg min\\nh\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)h(x) + 1\\n2\\n\\x13\\x15\\n= arg max\\nh\\nEx∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n= arg max\\nh\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\nEx∼D [e−f(x)Ht−1(x)]f(x)h(x)\\n\\x15\\n理想的ht(x) 是使得Ht(x) 的指数损失函数取得最小值时的ht(x)，该式将此转化成某个期望的最大值。\\n第2 个等号就是将式(8.13) 代入；第3 个等号是因为\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)h(x) + 1\\n2\\n\\x13\\x15\\n=Ex∼D\\n\\x143\\n2e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)h(x)\\n\\x15\\n=Ex∼D\\n\\x143\\n2e−f(x)Ht−1(x)\\n\\x15\\n−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n本式自变量为h(x), 而Ex∼D\\n\\x02 3\\n2e−f(x)Ht−1(x)\\x03\\n与h(x) 无关, 也就是一个常数; 只需最大化第二项−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f\\n即可, 将负号去掉, 原最小化问题变为最大化问题;\\n第4 个等号仍是因为Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)\\x03\\n是与自变量h(x) 无关的正常数(因为指数函数与原问题等\\n价，例如arg maxx (1 −x2) 与arg maxx 2 (1 −x2) 的结果均为x = 0);\\n8.2.12\\n式(8.16) 的推导\\n首先解释下符号Ex∼D 的含义，注意在本章中有两个符号D 和D，其中D 表示数据集，而D 表示\\n数据集D 的样本分布，可以理解为在数据集D 上进行一次随机采样，样本x 被抽到的概率是D(x)，那\\n么符号Ex∼D 表示的是在概率分布D 上的期望，可以简单地理解为对数据及D 以概率D 加权之后的期\\n望，因此有：\\nE(g(x)) =\\n|D|\\nX\\ni=1\\nf(xi)g(xi)\\n故可得\\nEx∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)H(xi)\\n由式(8.15) 可知\\nDt (xi) = D (xi)\\ne−f(xi)Ht−1(xi)\\nEx∼D [e−f(x)Ht−1(x)]\\n所以式(8.16) 可以表示为\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\nEx∼D [e−f(x)Ht−1(x)]f(x)h(x)\\n\\x15\\n=\\n|D|\\nX\\ni=1\\nD (xi)\\ne−f(xi)Ht−1(xi)\\nEx∼D [e−f(x)Ht−1(x)] f(xi)h(xi)\\n=\\n|D|\\nX\\ni=1\\nDt (xi) f (xi) h (xi)\\n=Ex∼Dt[f(x)h(x)]\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.13\\n式(8.17) 的推导\\n当f(x) = h(x) 时，I(f(x) ̸= h(x)) = 0，f(x)h(x) = 1，1 −2I(f(x) ̸= h(x)) = 1；\\n当f(x) ̸= h(x) 时，I(f(x) ̸= h(x)) = 1，f(x)h(x) = −1，1 −2I(f(x) ̸= h(x)) = −1。\\n综上，左右两式相等。\\n8.2.14\\n式(8.18) 的推导\\n本式基于式(8.17) 的恒等关系，由式(8.16) 推导而来。\\nEx∼Dt[f(x)h(x)] = Ex∼Dt[1 −2I(f(x) ̸= h(x))]\\n= Ex∼Dt[1] −2Ex∼Dt[I(f(x) ̸= h(x))]\\n= 1 −2Ex∼Dt[I(f(x) ̸= h(x))]\\n类似于式(8.14) 的第3 个和第4 个等号，由式(8.16) 的结果开始推导：\\nht(x) = arg max\\nh\\nEx∼Dt[f(x)h(x)]\\n= arg max\\nh\\n(1 −2Ex∼Dt[I(f(x) ̸= h(x))])\\n= arg max\\nh\\n(−2Ex∼Dt[I(f(x) ̸= h(x))])\\n= arg min Ex∼Dt[I(f(x) ̸= h(x))]\\n此式表示理想的ht(x) 在分布Dt 下最小化分类误差, 因此有“西瓜书”图8.3 第3 行ht(x) = L (D, Dt),\\n即分类器ht(x) 可以基于分布Dt 从数据集D 中训练而得, 而我们在训练分类器时, 一般来说最小化的损\\n失函数就是分类误差。\\n8.2.15\\n式(8.19) 的推导\\nDt+1(x) =\\nD(x)e−f(x)Ht(x)\\nEx∼D [e−f(x)Ht(x)]\\n= D(x)e−f(x)Ht−1(x)e−f(x)αtht(x)\\nEx∼D [e−f(x)Ht(x)]\\n= Dt(x) · e−f(x)αtht(x) Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)\\x03\\nEx∼D [e−f(x)Ht(x)]\\n第1 个等号是将式(8.15) 中的t 换为t + 1 (同时t −1 换为t);\\n第2 个等号是将Ht(x) = Ht−1(x) + αtht(x) 代入分子即可;\\n第3 个等号是乘以\\nEx∼D[e−f(x)Ht−1(x)]\\nEx∼D[e−f(x)Ht−1(x)] 后, 凑出式(8.15) 的Dt(x) 表达式, 以符号Dt(x) 替换即得。到\\n此之后, 得到Dt+1(x) 与Dt(x) 的关系, 但为了确保Dt+1(x) 是一个分布, 需要对得到的Dt+1(x) 进行规\\n范化, 即“西瓜书”图8.3 第7 行的Zt 。式(8.19) 第3 行最后一个分式将在规范化过程被吸收。\\nboosting 算法是根据调整后的样本再去训练下一个基分类器，这就是“重赋权法”的样本分布的调整\\n公式。\\n8.2.16\\nAdaBoost 的个人推导\\n西瓜书中对AdaBoost 的推导和原论文[1] 上有些地方有差异，综合原论文和一些参考资料，这里给\\n出一版更易于理解的推导，亦可参见我们的视频教程。\\nAdaBoost 的目标是学得T 个ht(x) 和相应的T 个αt, 得到式(8.4) 的H(x), 使式(8.5) 指数损失函\\n数ℓexp(H | D) 最小, 这就是求解所谓的“加性模型”。特别强调一下, 分类器ht(x) 如何得到及其相应的\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n权重αt 等于多少都是需要求解的(ht(x) = L (D, Dt), 即基于分布Dt 从数据集D 中经过最小化训练误差\\n训练出分类器ht, 也就是式(8.18), αt 参见式(8.18)。\\n“通常这是一个复杂的优化问题（同时学得T 个ht(x) 和相应的T 个αt 很困难)。前向分步算法求解这\\n一优化问题的想法是：因为学习的是加法模型, 如果能够从前向后, 每一步只学习一个基函数ht(x) 及其系\\n数αt, 逐步逼近最小化指数损失函数ℓexp(H | D), 那么就可以简化优化的复杂度。”摘自李航《统计学习方法》[2] 第14\\n因此, AdaBoost 每轮迭代只需要得到一个基分类器和其投票权重, 设第t 轮迭代需得到基分类器\\nht(x), 对应的投票权重为αt, 则集成分类器Ht(x) = Ht−1(x) + αtht(x), 其中H0(x) = 0 。为表达式简\\n洁, 常常将ht(x) 简写为ht, Ht(x) 简写为Ht 。则第t 轮实际为如下优化问题（本节式(8.4) 到式(8.8)\\n已经证明了指数损失函数是分类任务原本0/1 损失函数的一致替代损失函数):\\n(αt, ht) = arg min\\nα,h\\nℓexp (Ht−1 + αh | D)\\n表示每轮得到的基分类器ht(x) 和对应的权重αt 是最小化集成分类器Ht = Ht−1 + αtht 在数据集D 上、\\n样本权值分布为D (即初始化样本权值分布, 也就是D1 ) 时的指数损失函数ℓexp (Ht−1 + αh | D) 的结果。\\n这就是前向分步算法求解加性模型的思路。根据式(8.5) 将指数损失函数表达式代入, 则\\nℓexp (Ht−1 + αh | D) = Ex∼D\\n\\x02\\ne−f(x)(Ht−1(x)+αh(x))\\x03\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)(Ht−1(xi)+αh(xi))\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−f(xi)αh(xi)\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00e−αI (f (xi) = h (xi)) + eαI (f (xi) ̸= h (xi))\\n\\x01\\n上式推导中, 由于f (xi) 和h (xi) 均只能取−1, +1 两个值, 因此当f (xi) = h (xi) 时, f (xi) h (xi) = 1,\\n当f (xi) ̸= h (xi) 时, f (xi) h (xi) = −1 。另外, f (xi) 和h (xi) 要么相等, 要么不相等, 二者只能有一个\\n为真, 因此以下等式恒成立:\\nI (f (xi) = h (xi)) + I (f (xi) ̸= h (xi)) = 1\\n所以\\ne−αI (f (xi) = h (xi)) + eαI (f (xi) ̸= h (xi))\\n=e−αI (f (xi) = h (xi)) + e−αI (f (xi) ̸= h (xi)) −e−αI (f (xi) ̸= h (xi)) + eαI (f (xi) ̸= h (xi))\\n=e−α (I (f (xi) = h (xi)) + I (f (xi) ̸= h (xi))) +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n=e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n将此结果代入ℓexp (Ht−1 + αh | D), 得注: 以下表达式后面求解权重αt 时仍会使用\\nℓexp (Ht−1 + αh | D) =\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n\\x01\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−α +\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n= e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα −e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n外面; 第一项e−α P|D|\\ni=1 D′\\nt (xi) 与h(x) 无关, 因此对于任意α > 0, 使ℓexp (Ht−1 + αh | D) 最小的h(x)\\n只需要使第二项最小即可, 即\\nht = arg min\\nh\\n\\x00eα −e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n对于任意α > 0, 有eα −e−α > 0, 所以上式中与h(x) 无关的正系数可以省略:\\nht = arg min\\nh\\n|D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n此即式(8.18) 另一种表达形式。注意, 为了确保D′\\nt(x) 是一个分布, 需要对其进行规范化, 即Dt(x) = D′\\nt(x)\\nZt ,\\n然而规范化因子Zt = P|D|\\ni=1 D′\\nt (xi) 为常数, 并不影响最小化的求解。正是基于此结论, AdaBoost 通过\\nht = L (D, Dt ) 得到第t 轮的基分类器。“西瓜书”图8.3 的第3 行\\nDt+1 (xi) = D (xi) e−f(xi)Ht(xi)\\n= D (xi) e−f(xi)(Ht−1(xi)+αtht(xi))\\n= D (xi) e−f(xi)Ht−1(xi)e−f(xi)αtht(xi)\\n= Dt (xi) e−f(xi)αtht(xi)\\n此即类似式(8.19) 的分布权重更新公式。\\n现在只差权重αt 表达式待求。对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得\\n∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′\\nt (xi) + (eα −e−α) P|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\n\\x11\\n∂α\\n= −e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα + e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\nP|D|\\ni=1 D′\\nt (xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1\\nDt (xi) I (f (xi) ̸= h (xi)) = Ex∼Dt [I (f (xi) ̸= h (xi))]\\n= ϵt\\n对上述等式化简, 得\\ne−α\\neα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1\\n2 ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n即式(8.11)。从该式可以发现, 当ϵt = 1 时, αt →∞, 此时集成分类器将由基分类器ht 决定, 而这很可能\\n是由于过拟合产生的结果, 例如不前枝决策树, 如果一直分下去, 一般情况下总能得到在训练集上分类误差\\n很小甚至为0 的分类器, 但这并没有什么意义。所以一般在AdaBoost 中使用弱分类器, 如决策树桩(即单\\n层决策树)。\\n另外, 由以上指数损失函数ℓexp (Ht−1 + αh | D) 的推导可以发现\\nℓexp (Ht−1 + αh | D) =\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−f(xi)αh(xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi) e−f(xi)αh(xi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图8-5 Adaboost 原始文献推论2\\n这与指数损失函数ℓexp (αtht | Dt) 的表达式基本一致:\\nℓexp (αtht | Dt) = Ex∼Dt\\n\\x02\\ne−f(x)αtht(x)\\x03\\n=\\n|D|\\nX\\ni=1\\nDt (xi) e−f(xi)αtht(xt)\\n而D′\\nt(x) 的规范化过程并不影响对ℓexp (Ht−1 + αh | D) 求最小化操作, 因此最小化式(8.9) 等价于最小化\\nℓexp (Ht−1 + αh | D), 这就是式(8.9) 的来历，故并无问题。\\n到此为止, 就逐一完成了“西瓜书”图8.3 中第3 行的ht 的训练(并计算训练误差)、第6 行的权重\\nαt 计算公式以及第7 行的分布Dt 更新公式来历的理论推导。\\n8.2.17\\n进一步理解权重更新公式\\nAdaboost 原始文献[1] 第12 页(pdf 显示第348 页) 有如下推论，如图8-5所示:\\n即Px∼Dt (ht−1(x) ̸= f(x)) = 0.5 。用通俗的话来说就是, ht−1 在数据集D 上、分布为Dt 时的分类\\n误差为0.5, 即相当于随机猜测(最糟糕的二分类器是分类误差为0.5, 当二分类器分类误差为1 时相当于\\n分类误差为0 , 因为将预测结果反过来用就是了)。而ht 由式(8.18) 得到\\nht = arg min\\nh\\nEx∼Dt[I(f(x) ̸= h(x))] = arg min\\nh\\nPx∼Dt(h(x) ̸= f(x))\\n即ht 是在数据集D 上、分布为Dt 时分类误差最小的分类器, 因此在数据集D 上、分布为Dt 时, ht 是\\n最好的分类器, 而ht−1 是最差的分类器, 故二者差别最大。“西瓜书”第8.1 节的图8.2 形象的说明了“集\\n成个体应‘好而不同’”, 此时可以说ht−1 和ht 非常“不同”。证明如下:\\n对于ht−1 来说, 分类误差ϵt−1 为\\nϵt−1 = Px∼Dt−1 (ht−1(x) ̸= f(x)) = Ex∼Dt−1 [I (ht−1(x) ̸= f(x))]\\n=\\n|D|\\nX\\ni=1\\nDt−1 (xi) I (ht−1(x) ̸= f(x))\\n=\\nP|D|\\ni=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\nP|D|\\ni=1 Dt−1 (xi) I (ht−1(x) = f(x)) + P|D|\\ni=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\n在第t 轮, 根据分布更新公式(8.19) 或“西瓜书”图8.3 第7 行(规范化因子Zt−1 为常量):\\nDt = Dt−1\\nZt−1\\ne−f(x)αt−1ht−1(x)\\n其中根据式(8.11), 第t −1 轮的权重\\nαt−1 = 1\\n2 ln 1 −ϵt−1\\nϵt−1\\n= ln\\ns\\n1 −ϵt−1\\nϵt−1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n代入Dt 的表达式, 则\\nDt =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nDt−1\\nZt−1 ·\\nq\\nϵt−1\\n1−ϵt−1\\n, if ht−1(x) = f(x)\\nDt−1\\nZt−1 ·\\nq\\n1−ϵt−1\\nϵt−1\\n, if ht−1(x) ̸= f(x)\\n那么ht−1 在数据集D 上、分布为Dt 时的分类误差Px∼Dt (ht−1(x) ̸= f(x) ) 为(注意, 下式第二行的分\\n母等于1, 因为I (ht−1(x) = f(x)) + I (ht−1(x) ̸= f(x)) = 1 )\\nPx∼Dt (ht−1(x) ̸= f(x)) = Ex∼Dt [I (ht−1(x) ̸= f(x))]\\n=\\nP|D|\\ni=1 Dt (xi) I (ht−1 (xi) ̸= f (xi))\\nP|D|\\ni=1 Dt (xi) I (ht−1 (xi) = f (xi)) + P|D|\\ni=1 Dt (xi) I (ht−1 (xi) ̸= f (xi))\\n=\\nP|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1\\nϵt−1 I (ht−1 (xi) ̸= f (xi))\\nP|D|\\nDt−1(xi)\\nZt−1\\ni=1\\n·\\nq\\nϵt−1\\n1−ϵt−1 I (ht−1 (xi) = f (xi)) + P|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1\\nϵt−1 I (ht−1 (xi) ̸= f (xi))\\n=\\nq\\n1−ϵt−1\\nϵt−1\\n· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\nq\\nϵt−1\\n1−ϵt−1 · P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) = f (xi)) +\\nq\\n1−ϵt−1\\nϵt−1\\n· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\n=\\nq\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\nq\\nϵt−1\\n1−ϵt−1 · (1 −ϵt−1) +\\nq\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\n= 1\\n2\\n8.2.18\\n能够接受带权样本的基学习算法\\n在Adaboost 算法的推导过程中，我们发现能够接受并利用带权样本的算法才能很好的嵌入到Ad-\\naboost 的框架中作为基学习器。因此这里举一些能够接受带权样本的基学习算法的例子，分别是SVM 和\\n基于随机梯度下降(SGD) 的对率回归：\\n其实原理很简单: 对于SVM 来说, 针对“西瓜书”P130 页的优化目标式(6.29) 来说, 第二项为损失\\n项, 此时每个样本的损失ℓ0/1\\n\\x00yi\\n\\x00w⊤xi + b\\n\\x01\\n−1\\n\\x01\\n直接相加, 即样本权值分布为D (xi) =\\n1\\nm, 其中m 为数\\n据集D 样本个数; 若样本权值更新为Dt (xi), 则此时损失求和项应该变为\\nm\\nX\\ni=1\\nmDt (xi) · ℓ0/1\\n\\x00yi\\n\\x00w⊤xi + b\\n\\x01\\n−1\\n\\x01\\n若将D (xi) =\\n1\\nm 替换Dt (xi), 则就是每个样本的损失ℓ0/1\\n\\x00yi\\n\\x00w⊤xi + b\\n\\x01\\n−1\\n\\x01\\n直接相加。如此更改后, 最\\n后推导结果影响的是式(6.39), 将由C = αi + µi 变为\\nC · mDt (xi) = αi + µi\\n进而由αi, µi ≥0 导出0 ≤αi ≤C · mDt (xi) 。\\n对于基于随机梯度下降(SGD) 的对率回归, 每次随机选择一个样本进行梯度下降, 总体上的期望损失\\n即为式(3.27), 此时每个样本被选到的概率相同, 相当于D (xi) =\\n1\\nm 。若样本权值更新为Dt (xi), 则类似\\n于SVM, 针对式(3.27) 只需要给第i 项乘以mDt (xi) 即可, 相当于每次随机梯度下降选择样本时以概率\\nDt (xi) 选择样本xi 即可。\\n注意, 这里总的损失中出现了样本个数m 。这是因为在定义损失时末求均值, 若对式(6.29) 的第二\\n项和式(3.27) 乘以\\n1\\nm 则可以将m 抵消掉。然而常数项在最小化式(3.27) 实际上并不影响什么, 对于式\\n(6.29) 来说只要选择平衡参数C 时选为原来的m 倍即可。\\n当然, 正如“西瓜书”P177 第三段中所说, “对无法接受带权样本的基学习算法, 则可通过“重采样\\n法’ 来处理, 即在每一轮学习中, 根据样本分布对训练集重新进行采样, 再用重采样而得的样本集对基学习\\n器进行训练”。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.3\\nBagging 与随机森林\\n8.3.1\\n式(8.20) 的解释\\nI (ht(x) = y) 表示对T 个基学习器，每一个都判断结果是否与y 一致，y 的取值一般是−1 和1，如\\n果基学习器结果与y 一致，则I (ht(x) = y) = 1，如果样本不在训练集内，则I (x /∈Dt) = 1，综合起来\\n看就是，对包外的数据，用“投票法”选择包外估计的结果，即1 或-1。\\n8.3.2\\n式(8.21) 的推导\\n由式(8.20) 知，Hoob(x) 是对包外的估计，该式表示估计错误的个数除以总的个数，得到泛化误差的\\n包外估计。注意在本式直接除以D | (训练集D 样本个数), 也就是说此处假设T 个基分类器的各自的包\\n外样本的并集一定为训练集D 。实际上, 这个事实成立的概率也是比较大的, 可以计算一下: 样本属于包\\n内的概率为0.632, 那么T 次独立的随机采样均属于包内的概率为0.632T , 当T = 5 时, 0.632T ≈0.1, 当\\nT = 10 时, 0.632T ≈0.01, 这么来看的话T 个基分类器的各自的包外样本的并集为训练集D 的概率的确\\n实比较大。\\n8.3.3\\n随机森林的解释\\n在8.3.2 节开篇第一句话就解释了随机森林的概念：随机森林是Bagging 的一个扩展变体，是以决策\\n树为基学习器构建Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。\\n完整版随机森林当然更复杂，这时只须知道两个重点：(1) 以决策树为基学习器；(2) 在基学习器训练\\n过程中，选择划分属性时只使用当前结点属性集合的一个子集。\\n8.4\\n结合策略\\n8.4.1\\n式(8.22) 的解释\\nH(x) = 1\\nT\\nT\\nX\\ni=1\\nhi(x)\\n对基分类器的结果进行简单的平均。\\n8.4.2\\n式(8.23) 的解释\\nH(x) =\\nT\\nX\\ni=1\\nwihi(x)\\n对基分类器的结果进行加权平均。\\n8.4.3\\n硬投票和软投票的解释\\n“西瓜书”中第183 页提到了硬投票(hard voting) 和软投票(soft voting)，本页左侧注释也提到多数投\\n票法的英文术语使用不太一致，有文献称为majority voting。本人看到有些文献中，硬投票使用majority\\nvoting（多数投票），软投票使用probability voting（概率投票），所以还是具体问题具体分析比较稳妥。\\n8.4.4\\n式(8.24) 的解释\\nH(x) =\\n(\\ncj,\\nif PT\\ni=1 hj\\ni(x) > 0.5 PN\\nk=1\\nPT\\ni=1 hk\\ni (x)\\nreject,\\notherwise.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n当某一个类别j 的基分类器的结果之和，大于所有结果之和的1\\n2，则选择该类别j 为最终结果。\\n8.4.5\\n式(8.25) 的解释\\nH(x) = carg max\\nj\\n∑T\\ni=1 hj\\ni (x)\\n相比于其他类别，该类别j 的基分类器的结果之和最大，则选择类别j 为最终结果。\\n8.4.6\\n式(8.26) 的解释\\nH(x) = carg max\\nj\\n∑T\\ni=1 wihj\\ni (x)\\n相比于其他类别，该类别j 的基分类器的结果之和最大，则选择类别j 为最终结果，与式(8.25) 不同的\\n是，该式在基分类器前面乘上一个权重系数，该系数大于等于0，且T 个权重之和为1。\\n8.4.7\\n元学习器(meta-learner) 的解释\\n书中第183 页最后一行提到了元学习器(meta-learner)，简单解释一下，因为理解meta 的含义有时\\n对于理解论文中的核心思想很有帮助。\\n元(meta)，非常抽象，例如此处的含义，即次级学习器，或者说基于学习器结果的学习器；另外还有\\n元语言，就是描述计算机语言的语言，还有元数学，研究数学的数学等等；\\n另外，论文中经常出现的还有meta-strategy，即元策略或元方法，比如说你的研究问题是多分类问题，\\n那么你提出了一种方法，例如对输入特征进行变换（或对输出类别做某种变换），然后再基于普通的多分\\n类方法进行预测，这时你的方法可以看成是一种通用的框架，它虽然针对多分类问题开发，但它需要某个\\n具体多分类方法配合才能实现，那么这样的方法是一种更高层级的方法，可以称为是一种meta-strategy。\\n8.4.8\\nStacking 算法的解释\\n该算法其实非常简单，对于数据集，试想你现在有了个基分类器预测结果，也就是说数据集中的每个\\n样本均有个预测结果，那么怎么结合这个预测结果呢？\\n本节名为“结合策略”，告诉你各种结合方法，但其实最简单的方法就是基于这个预测结果再进行一\\n次学习，即针对每个样本，将这个预测结果作为输入特征，类别仍为原来的类别，既然无法抉择如何将这\\n些结果进行结合，那么就“学习”一下吧。\\n“西瓜书”图8.9 伪代码第9 行中将第个样本进行变换，特征为个基学习器的输出，类别标记仍为原\\n来的，将所有训练集中的样本进行转换得到新的数据集后，再基于进行一次学习即可，也就是Stacking 算\\n法。\\n至于说“西瓜书”图8.9 中伪代码第1 行到第3 行使用的数据集与第5 行到第10 行使用的数据集之\\n间的关系，在“西瓜书”图8.9 下方的一段话有详细的讨论，不再赘述。\\n8.5\\n多样性\\n8.5.1\\n式(8.27) 的解释\\nA (hi|x) = (hi(x) −H(x))2\\n该式表示个体学习器结果与预测结果的差值的平方，即为个体学习器的“分歧”。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 101, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.5.2\\n式(8.28) 的解释\\n¯A(h|x) =\\nT\\nX\\ni=1\\nwiA (hi|x)\\n=\\nT\\nX\\ni=1\\nwi (hi(x) −H(x))2\\n该式表示对各个个体学习器的“分歧”加权平均的结果，即集成的“分歧”。\\n8.5.3\\n式(8.29) 的解释\\nE (hi|x) = (f(x) −hi(x))2\\n该式表示个体学习器与真实值之间差值的平方，即个体学习器的平方误差。\\n8.5.4\\n式(8.30) 的解释\\nE(H|x) = (f(x) −H(x))2\\n该式表示集成与真实值之间差值的平方，即集成的平方误差。\\n8.5.5\\n式(8.31) 的推导\\n由(8.28) 知\\n¯A(h|x) =\\nT\\nX\\ni=1\\nwi (hi(x) −H(x))2\\n=\\nT\\nX\\ni=1\\nwi(hi(x)2 −2hi(x)H(x) + H(x)2)\\n=\\nT\\nX\\ni=1\\nwihi(x)2 −H(x)2\\n又因为\\nT\\nX\\ni=1\\nwiE (hi|x) −E(H|x)\\n=\\nT\\nX\\ni=1\\nwi (f(x) −hi(x))2 −(f(x) −H(x))2\\n=\\nT\\nX\\ni=1\\nwihi(x)2 −H(x)2\\n所以\\n¯A(h|x) =\\nT\\nX\\ni=1\\nwiE (hi|x) −E(H|x)\\n8.5.6\\n式(8.32) 的解释\\nT\\nX\\ni=1\\nwi\\nZ\\nA (hi|x) p(x)dx =\\nT\\nX\\ni=1\\nwi\\nZ\\nE (hi|x) p(x)dx −\\nZ\\nE(H|x)p(x)dx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 102, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nR\\nA (hi|x) p(x)dx 表示个体学习器在全样本上的“分歧”，PT\\ni=1 wi\\nR\\nA (hi|x) p(x)dx 表示集成在全样本上\\n的“分歧”。式(8.31) 的意义在于, 对于示例x 有¯A(h | x) = ¯E(h | x) −E(H | x) 成立, 即个体学习器分\\n歧的加权均值等于个体学习器误差的加权均值减去集成H(x) 的误差。\\n将这个结论应用于全样本上, 即为式(8.32)。\\n例如Ai =\\nR\\nA (hi | x) p(x)dx, 这是将x 作为连续变量来处理的, 所以这里是概率密度p(x) 和积分\\n号; 若按离散变量来处理, 则变为Ai = P\\nx∈D A (hi | x) px; 其实高等数学中讲过, 积分就是连续求和。\\n8.5.7\\n式(8.33) 的解释\\nEi =\\nZ\\nE (hi|x) p(x)dx\\n表示个体学习器在全样本上的泛化误差。\\n8.5.8\\n式(8.34) 的解释\\nAi =\\nZ\\nA (hi|x) p(x)dx\\n表示个体学习器在全样本上的分歧。\\n8.5.9\\n式(8.35) 的解释\\nE =\\nZ\\nE(H|x)p(x)dx\\n表示集成在全样本上的泛化误差。\\n8.5.10\\n式(8.36) 的解释\\nE = ¯E −¯A\\n¯E 表示个体学习器泛化误差的加权均值，¯A 表示个体学习器分歧项的加权均值，该式称为“误差-分歧分\\n解”。\\n8.5.11\\n式(8.40) 的解释\\n当p1 = p2 时, κ = 0; 当p1 = 1 时, κ = 1; 一般来说p1 ⩾p2, 即κ ⩾0, 但偶尔也有p1 < p2 的情况,\\n此时κ < 0 。有关p1, p2 的意义参见式(8.41) 和式(8.42) 的解释。\\n8.5.12\\n式(8.41) 的解释\\n当p1 = p2 时, κ = 0; 当p1 = 1 时, κ = 1; 一般来说p1 ⩾p2, 即κ ⩾0, 但偶尔也有p1 < p2 的情况,\\n此时κ < 0 。有关p1, p2 的意义参见式(8.41) 和式(8.42) 的解释。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.5.13\\n式(8.42) 的解释\\n将式(8.42) 拆分为如下形式，将会很容易理解其含义：\\np2 = a + b\\nm\\n· a + c\\nm\\n+ c + d\\nm\\n· b + d\\nm\\n其中a+b\\nm\\n为分类器hi 将样本预测为+1 的概率,\\na+c\\nm\\n为分类器hj 将样本预测为+1 的概率, 二者相乘\\na+b\\nm · a+c\\nm 可理解为分类器hi 与hj 将样本预测为+1 的概率; c+d\\nm 为分类器hi 将样本预测为−1 的概率,\\nb+d\\nm 为分类器hj 将样本预测为−1 的概率, 二者相乘c+d\\nm · b+d\\nm 可理解为分类器hi 与hj 将样本预测为−1\\n的概率。\\n注意a+b\\nm · a+c\\nm 与\\na\\nm 的不同, c+d\\nm · b+d\\nm 与\\nd\\nm 的不同:\\na + b\\nm\\n· a + c\\nm\\n= p (hi = +1) p (hj = +1) , a\\nm = p (hi = +1, hj = +1)\\nc + d\\nm\\n· b + d\\nm\\n= p (hi = −1) p (hj = −1) , d\\nm = p (hi = −1, hj = −1)\\n即a+b\\nm · a+c\\nm\\n和c+d\\nm · b+d\\nm 是分别考虑分类器hi 与hj 时的概率( hi 与hj 独立), 而\\na\\nm 和\\nd\\nm 是同时考虑\\nhi 与hj 时的概率(联合概率)。\\n8.5.14\\n多样性增强的解释\\n在8.5.3 节介绍了四种多样性增强的方法, 通俗易懂, 几乎不需要什么注解, 仅强调几个概念:\\n(1) 数据样本扰动中提到了“不稳定基学习器”(例如决策树、神经网络等) 和“稳定基学习器”(例\\n如线性学习器、支持向量机、朴素贝叶斯、k 近邻学习器等), 对稳定基学习器进行集成时数据样本扰动技\\n巧效果有限。这也就可以解释为什么随机森林和GBDT 等以决策树为基分学习器的集成方法很成功吧,\\nGradient Boosting 和Bagging 都是以数据样本扰动来增强多样性的; 而且, 掌握这个经验后在实际工程应\\n用中就可以排除一些候选基分类器, 但论文中的确经常见到以支持向量机为基分类器Bagging 实现, 这可\\n能是由于LIBSVM 简单易用的原因吧。\\n(2)“西瓜书”图8.11 随机子空间算法, 针对每个基分类器ht 在训练时使用了原数据集的部分输入属\\n性（末必是初始属性, 详见第189 页左上注释), 因此在最终集成时“西瓜书”图8.11 最后一行也要使用\\n相同的部分属性。\\n(3) 输出表示扰动中提到了“翻转法”(Flipping Output), 看起来是一个并没有道理的技巧, 为什么要\\n将训练样本的标记改变呢? 若认为原训练样本标记是完全可靠的, 这不是人为地加入噪声么? 但西瓜书作\\n者2017 年提出的深度森林[3] 模型中也用到了该技巧, 正如本小节名为“多样性增强”, 虽然从局部来看\\n引入了标记噪声, 但从模型集成的角度来说却是有益的。\\n8.6\\nGradient Boosting/GBDT/XGBoost 联系与区别\\n在集成学习中，梯度提升(Gradient Boosting, GB)、梯度提升树(GB Decision Tree, GBDT) 很常见，\\n尤其是近几年非常流行的XGBoost 很是耀眼，此处单独介绍对比这些概念。\\n8.6.1\\n梯度下降法\\n本部分内容参考了孙文瑜教授的最优化方法[4] 设目标函数f(x) 在xk 附近连续可微, 且∇f (xk) =\\n∇f(x)\\n∇x\\n\\x0c\\x0c\\x0c\\nx=xk ̸= 0 。将f(x) 在xk 处进行一阶Taylor 展开\\nf(x) ≈f (xk) + ∇f (xk)⊤(x −xk)\\n记x −xk = ∆x, 则上式可写为\\nf (xk + ∆x) ≈f (xk) + ∇f (xk)⊤∆x\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n显然, 若∇f (xk)⊤∆x < 0 则有f (xk + ∆x) < f (xk), 即相比于f (xk), 自变量增量∆x 会使f(x)\\n函数值下降; 若要使f(x) = f (xk + ∆x) 下降最快, 只要选择∆x 使∇f (xk)⊤∆x 最小即可, 而此时\\n∇f (xk)⊤∆x < 0, 因此使绝对值| f (xk)⊤∆x 最大即可。将∆x 分成两部分: ∆x = αkdk, 其中dk 为待\\n求单位向量, αk > 0 为待解常量; dk 表示往哪个方向改变x 函数值下降最快, 而αk 表示沿这个方向的步\\n长。因此, 求解∆x 的问题变为\\n(αk, dk) = arg min\\nα,d\\n∇f (xk)⊤αd\\n将以上优化问题分为两步求解, 即\\ndk = arg min\\nd\\n∇f (xk)⊤d\\ns.t. ∥d∥2 = 1\\nαk = arg min\\nα\\n∇f (xk)⊤dkα\\n以上求解αk 的优化问题明显有问题, 因为对于∇f (xk)⊤dk < 0 来说, 显然αk = +∞时取的最小值, 求\\n解αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)\\n对于凸函数来说, 以上两步可以得到最优解; 但对于非凸函数来说, 联合求解得到dk 和αk, 与先求dk 然\\n后基于此再求αk 的结果应该有时是不同的。由Cauchy-Schwartz 不等式\\n\\x0c\\x0c\\x0c∇f (xk)⊤dk\\n\\x0c\\x0c\\x0c ≤∥∇f (xk)∥2 ∥dk∥2\\n可知, 当且仅当dk = −\\n∇f(xk)\\n∥∇f(xk)∥2 时, ∇f (xk)⊤dk 最小, −∇f (xk)⊤dk 最大。对于αk, 若f (xk + αdk) 对\\nα 的导数存在, 则可简单求解如下单变量方程即可:\\n∂f (xk + αdk)\\n∂α\\n= 0\\n例1: 试求f(x) = x2 在xk = 2 处的梯度方向dk 和步长αk 。解: 对f(x) 在xk = 2 处进行一阶Taylor\\n展开:\\nf(x) = f (xk) + f ′ (xk) (x −xk)\\n= x2\\nk + 2xk (x −xk)\\n= x2\\nk + 2xkαd\\n由于此时自变量为一维, 因此只有两个方向可选, 要么正方向, 要么负方向。此时f ′ (xk) = 4, 因此dk =\\n−f ′(xk)\\n|f ′(xk)| = −1 。接下来求αk, 将xk 和dk 代入:\\nf (xk + αdk) = f(2 −α) = (2 −α)2\\n进而有\\n∂f (xk + αdk)\\n∂α\\n= −2(2 −α)\\n令导数等于0 , 得αk = 2 。此时\\n∆x = αkdk = −2\\n则xk + ∆x = 0, 函数值f (xk + ∆x) = 0 。例2: 试求f(x) = ∥x∥2\\n2 = x⊤x 在xk = [x1\\nk, x2\\nk]\\n⊤= [3, 4]⊤处\\n的梯度方向dk 和步长αk 。解: 对f(x) 在xk = [x1\\nk, x2\\nk]\\n⊤= [3, 4]⊤处进行一阶Taylor 展开:\\nf(x) = f (xk) + ∇f (xk)⊤(x −xk)\\n= ∥x∥2\\n2 + 2x⊤\\nk (x −xk)\\n= ∥x∥2\\n2 + 2x⊤\\nk αd\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时∇f (xk) = [6, 8]⊤, 因此dk = −\\n∇f(xk)\\n∥∇f(xk)∥2 = [−0.6, −0.8]⊤。接下来求αk, 将xk 和dk 代入:\\nf (xk + αdk) = (3 −0.6α)2 + (4 −0.8α)2\\n= α2 −10α + 25\\n= (α −5)2\\n因此可得αk = 5 (或对α 求导, 再令导数等于0 )。此时\\n∆x = αkdk = [−3, −4]⊤\\n则xk + ∆x = [0, 0]⊤, 函数值f (xk + ∆x) = 0 。通过以上分析, 只想强调两点: (1) 梯度下降法求解下降\\n最快的方向dk 时应该求解如下优化问题:\\ndk = arg min\\nd\\n∇f (xk)⊤d s.t. ∥d∥2 = C\\n其中C 为常量, 即不必严格限定∥dk∥2 = 1, 只要固定向量长度, 与αk 搭配即可。(2) 梯度下降法求解步\\n长αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)\\n实际应用中, 很多时候不会去求最优的αk, 而是靠经验设置一个步长。\\n8.6.2\\n从梯度下降的角度解释AdaBoost\\nAdaBoost 第t 轮迭代时最小化式(8.5) 的指数损失函数\\nℓexp (Ht | D) = Ex∼D\\n\\x02\\ne−f(x)Ht(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)Ht(x)\\n对ℓexp (Ht | D) 每一项在Ht−1 处泰勒展开\\nℓexp (Ht | D) ≈\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x) −f(x)e−f(x)Ht−1(x) (Ht(x) −Ht−1(x))\\n\\x01\\n=\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n\\x01\\n= Ex∼D\\n\\x02\\ne−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n\\x03\\n其中Ht = Ht−1 +αtht 。注意: αt, ht 是第t 轮待解的变量。另外补充一下, 在上式展开中的变量为Ht(x),\\n在Ht−1 处一阶导数为\\n∂e−f(x)Ht(x)\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\n= −f(x)e−f(x)Ht−1(x)\\n如果看不习惯上述泰勒展开过程, 可令变量z = Ht(x) 和函数g(z) = e−f(x)z, 对g(z) 在z0 = Ht−1(x) 处\\n泰勒展开, 得\\ng(z) ≈g (z0) + g′ (z0) (z −z0)\\n= g (z0) −f(x)e−f(x)z0 (z −z0)\\n= e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x) (Ht(x) −Ht−1(x))\\n= e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n注意此处ht(x) ∈{−1, +1}, 类似于梯度下降法中的约束∥dk∥2 = 1 。类似于梯度下降法求解下降最快的\\n方向dk, 此处先求ht (先不管αt ):\\nht = arg min\\nh\\nX\\nx∈D\\nD(x)\\n\\x00−e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\ns.t. h(x) ∈{−1, +1}\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n将负号去掉, 最小化变为最大化问题\\nht = arg max\\nh\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\n= arg max\\nh\\nEx∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\ns.t. h(x) ∈{−1, +1}\\n这就是式(8.14) 的第3 个等号的结果, 因此其余推导参见8.2.16节即可。由于这里的h(x) 约束较强, 因此\\n不能直接取负梯度方向, 书中经过推导得到了ht(x) 的表达式, 即式(8.18)。实际上, 可以将此结果理解为\\n满足约束条件的最快下降方向。求得ht(x) 之后再求αt (8.2.16节“AdaBoost 的个人推导”注解中已经写\\n过一遍, 此处仅粘贴至此, 具体参见8.2.16节注解，尤其是ℓexp (Ht−1 + αht | D) 表达式的由来):\\nαk = arg min\\nα\\nℓexp (Ht−1 + αht | D)\\n对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得\\n∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′\\nt (xi) + (eα −e−α) P|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\n\\x11\\n∂α\\n= −e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα + e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\nP|D|\\ni=1 D′\\nt (xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1\\nDt (xi) I (f (xi) ̸= h (xi)) = Ex∼Dt [I (f (xi) ̸= h (xi))]\\n= ϵt\\n对上述等式化简, 得\\ne−α\\neα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1\\n2 ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n即式(8.11)。通过以上推导可以发现: AdaBoost 每一轮的迭代就是基于梯度下降法求解损失函数为指数\\n损失函数的二分类问题约束条件ht(x) ∈{−1, +1} 。\\n8.6.3\\n梯度提升(Gradient Boosting)\\n将AdaBoost 的问题一般化, 即不限定损失函数为指数损失函数, 也不局限于二分类问题, 则可以将式\\n(8.5) 写为更一般化的形式\\nℓ(Ht | D) = Ex∼D [err (Ht(x), f(x))]\\n= Ex∼D [err (Ht−1(x) + αtht(x), f(x))]\\n问题时, f(x) ∈R, 损失函数可使用平方损失err (Ht(x), f(x)) = (Ht(x) −f(x))2 。针对该一般化的损失\\n函数和一般的学习问题, 要通过T 轮迭代得到学习器\\nH(x) =\\nT\\nX\\nt=1\\nαtht(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n类似于AdaBoost, 第t 轮得到αt, ht(x), 可先对损失函数在Ht−1(x) 处进行泰勒展开:\\nℓ(Ht | D) ≈Ex∼D\\n\"\\nerr (Ht−1(x), f(x)) + ∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\n(Ht(x) −Ht−1(x))\\n#\\n= Ex∼D\\n\"\\nerr (Ht−1(x), f(x)) + ∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\nαtht(x)\\n#\\n= Ex∼D [err (Ht−1(x), f(x))] + Ex∼D\\n\"\\n∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\nαtht(x)\\n#\\n注意, 在上式展开中的变量为Ht(x), 且有Ht(x) = Ht−1(x)+αtht(x) (类似于梯度下降法中x = xk + αkdk)\\n。上式中括号内第1 项为常量ℓ(Ht−1 | D), 最小化ℓ(Ht | D) 只须最小化第2 项即可。先不考虑权重αt,\\n求解如下优化问题可得ht(x) :\\nht(x) = arg min\\nh\\nEx∼D\\n\"\\n∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\nh(x)\\n#\\ns.t. constraints for h(x)\\n解得ht(x) 之后, 再求解如下优化问题可得权重αt :\\nαt = arg min\\nα\\nEx∼D [err (Ht−1(x) + αht(x), f(x))]\\n以上就是梯度提升(Gradient Boosting) 的理论框架, 即每轮通过梯度(Gradient) 下降的方式将T 个弱学\\n习器提升(Boosting) 为强学习器。可以看出AdaBoost 是其特殊形式。\\nGradient Boosting 算法的官方版本参见[5] 第5-6 页(第1193-1194 页)，其中算法部分见算法1\\nAlgorithm 1 Gradient_Boost(A, p, r)\\n1: F0(x) = arg minρ\\nPN\\ni=1 L (yi, ρ)\\n2: for m = 1 doM\\n3:\\n˜yi = −\\nh\\n∂L(yi,F (xj))\\n∂F (xi)\\ni\\nF (x)=Fm−1(x) , i = 1, N\\n4:\\nam = arg mina,β\\nPN\\ni=1 [˜yi −βh (xi; a)]2\\n5:\\nρm = arg minρ\\nPN\\ni=1 L (yi, Fm−1 (xi) + ρh (xi; am))\\n6:\\nFm(x) = Fm−1(x) + ρmh (x; am)\\n感觉该伪代码针对的还是在任意损失函数L (yi, F (xi)) 下的回归问题。Algorithm 1 中第3 步和第4\\n步意思是用βh (xi, a) 拟合F(x) = Fm−1(x) 处负梯度, 但第4 步表示只求参数am, 第5 步单独求解参数\\nρm, 这里的疑问是为什么第4 步要用最小二乘法（即3.2 节的线性回归）去拟合负梯度（又称伪残差）?\\n简单理解如下: 第4 步要解的h (xi, a) 相当于梯度下降法中的待解的下降方向d, 在梯度下降法中也\\n已提到不必严格限制∥d∥2 = 1, 长度可以由步长α 调节(例如前面梯度下降方解释中的例1 , 若直接取\\ndk = −f ′ (xk) = −4, 则可得αk = 0.5, 仍有∆x = αkdk = −2), 因此第4 步直接用h (xi, a) 拟合负梯度,\\n与梯度下降中约束∥d∥2 = 1 的区别在于末对负梯度除以其模值进行归一化而已。\\n那为什么不是直接令h (xi, a) 等于负梯度呢? 因为这里实际是求假设函数h, 将数据集中所有的xi\\n经假设函数h 映射到对应的伪残差(负梯度) ˜yi, 所以只能做线性回归了。\\n李航《统计学习方法》[2] 第8.4.3 节中的算法8.4 并末显式体现参数ρm, 这应该是第2 步的(c) 步完\\n成的, 因为(b) 步只是拟合一棵回归树(相当于Algorithm 1 第4 步解得h (xi, a) ), 而(c) 步才确定每个\\n叶结点的取值(相当于Algorithm 1 第5 步解得ρm, 只是每个叶结点均对应一个ρm); 而且回归问题中基\\n函数为实值函数，可以将参数ρm 吸收到基函数中。\\n8.6.4\\n梯度提升树(GBDT)\\n本部分无实质GBDT 内容，仅为梳理GBDT 的概念，具体可参考给出的资源链接。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于GBDT，一般资料是按Gradient Boosting+CART 处理回归问题讲解的，如林轩田《机器学习\\n技法》课程第11 讲。但是，分类问题也可以用回归来处理，例如3.3 节的对数几率回归，只需将平方损\\n失换为对率损失（参见式(3.27) 和式(6.33)，二者关系可参见第3 章注解中有关式(3.27) 的推导）即可。\\n细节可以搜索林轩田老师的《机器学习基石》和《机器学习技法》两门课程以及配套的视频。\\n8.6.5\\nXGBoost\\n本部分无实质XGBoost 内容，仅为梳理XGBoost 的概念，具体可参考给出的资源链接。\\n首先，XGBoost 是eXtreme Gradient Boosting 的简称。其次，XGBoost 与GBDT 的关系，可大致类\\n比为LIBSVM 与SVM（或SMO 算法）的关系。LIBSVM 是SVM 算法的一种高效实现软件包，XGBoost\\n是GBDT 的一种高效实现；在实现层面，LIBSVM 对SMO 算法进行了许多改进，XGBoost 也对GBDT\\n进行了许多改进；另外，LIBSVM 扩展了许多SVM 变体，XGBoost 也不再仅仅是标准的GBDT，也扩\\n展了一些其它功能。最后，XGBoost 是由陈天奇开发的；XGBoost 论文可以参考[6]，XGBoost 工具包、\\n文档和源码等均可以在Github 上搜索到。\\n参考文献\\n[1] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical view\\nof boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):337–407,\\n2000.\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n[3] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. In IJCAI,\\npages 3553–3559, 2017.\\n[4] 朱德通孙文瑜, 徐成贤. 最优化方法. 最优化方法, 2010.\\n[5] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics,\\npages 1189–1232, 2001.\\n[6] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the\\n22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794,\\n2016.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第9 章\\n聚类\\n到目前为止，前面章节介绍的方法都是针对监督学习(supervised learning) 的，本章介绍的聚类(clus-\\ntering) 和下一章介绍的降维属于无监督学习(unsupervised learning)。\\n9.1\\n聚类任务\\n单词“cluster”既是动词也是名词，作为名词时翻译为“簇”，即聚类得到的子集；一般谈到“聚类”\\n这个概念时对应其动名词形式“clustering”。\\n9.2\\n性能度量\\n本节给出了聚类性能度量的三种外部指标和两种内部指标，其中式(9.5) ~ 式(9.7) 是基于式(9.1) ~\\n式(9.4) 导出的三种外部指标，而式(9.12) 和式(9.13) 是基于式(9.8) ~ 式(9.11) 导出的两种内部指标。\\n读本节内容需要心里清楚的一点：本节给出的指标仅是该领域的前辈们定义的指标，在个人研究过程中可\\n以根据需要自己定义，说不定就会被业内同行广泛使用。\\n9.2.1\\n式(9.5) 的解释\\n给定两个集合A 和B，则Jaccard 系数定义为如下公式\\nJC = |A T B|\\n|A S B| =\\n|A T B|\\n|A| + |B| −|A T B|\\nJaccard 系数可以用来描述两个集合的相似程度。推论：假设全集U 共有n 个元素，且A ⊆U，B ⊆U，\\n则每一个元素的位置共有四种情况：\\n1. 元素同时在集合A 和B 中，这样的元素个数记为M11\\n2. 元素出现在集合A 中，但没有出现在集合B 中，这样的元素个数记为M10\\n3. 元素没有出现在集合A 中，但出现在集合B 中，这样的元素个数记为M01\\n4. 元素既没有出现在集合A 中，也没有出现在集合B 中，这样的元素个数记为M00\\n根据Jaccard 系数的定义，此时的Jaccard 系数为如下公式\\nJC =\\nM11\\nM11 + M10 + M01\\n由于聚类属于无监督学习，事先并不知道聚类后样本所属类别的类别标记所代表的意义，即便参考模型的\\n类别标记意义是已知的，我们也无法知道聚类后的类别标记与参考模型的类别标记是如何对应的，况且聚\\n类后的类别总数与参考模型的类别总数还可能不一样，因此只用单个样本无法衡量聚类性能的好坏。\\n由于外部指标的基本思想就是以参考模型的类别划分为参照，因此如果某一个样本对中的两个样本在\\n聚类结果中同属于一个类，在参考模型中也同属于一个类，或者这两个样本在聚类结果中不同属于一个类，\\n在参考模型中也不同属于一个类，那么对于这两个样本来说这是一个好的聚类结果。\\n总的来说所有样本对中的两个样本共存在四种情况：\\n1. 样本对中的两个样本在聚类结果中属于同一个类，在参考模型中也属于同一个类\\n2. 样本对中的两个样本在聚类结果中属于同一个类，在参考模型中不属于同一个类\\n3. 样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中属于同一个类\\n4. 样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中也不属于同一个类\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n综上所述，即所有样本对存在着书中式(9.1) ~ 式(9.4) 的四种情况，现在假设集合A 中存放着两个样本\\n都同属于聚类结果的同一个类的样本对，即A = SS S SD，集合B 中存放着两个样本都同属于参考模型\\n的同一个类的样本对，即B = SS S DS，那么根据Jaccard 系数的定义有：\\nJC = |A T B|\\n|A S B| =\\n|SS|\\n|SS S SD S DS| =\\na\\na + b + c\\n也可直接将书中式(9.1) ~ 式(9.4) 的四种情况类比推论，即M11 = a，M10 = b，M01 = c，所以\\nJC =\\nM11\\nM11 + M10 + M01\\n=\\na\\na + b + c\\n9.2.2\\n式(9.6) 的解释\\n其中\\na\\na+b 和\\na\\na+c 为Wallace 提出的两个非对称指标，a 代表两个样本在聚类结果和参考模型中均属于\\n同一类的样本对的个数，a + b 代表两个样本在聚类结果中属于同一类的样本对的个数，a + c 代表两个样\\n本在参考模型中属于同一类的样本对的个数，这两个非对称指标均可理解为样本对中的两个样本在聚类结\\n果和参考模型中均属于同一类的概率。由于指标的非对称性，这两个概率值往往不一样，因此Fowlkes 和\\nMallows 提出利用几何平均数将这两个非对称指标转化为一个对称指标，即Fowlkes and Mallows Index,\\nFMI。\\n9.2.3\\n式(9.7) 的解释\\nRand Index 定义如下：\\nRI =\\na + d\\na + b + c + d =\\na + d\\nm(m −1)/2 = 2(a + d)\\nm(m −1)\\n由第一个等号可知RI 肯定不大于1。之所以a + b + c + d = m(m −1)/2, 这是因为式(9.1) ~ 式(9.4) 遍\\n历了所有(xi, xj) 组合对(i ̸= j) : 其中i = 1 时j 可以取2 到m 共m −1 个值, i = 2 时j 可以取3\\n到m 共m −2 个值, · · · · · · , i = m −1 时j 仅可以取m 共1 个值, 因此(xi, xj) 组合对的个数为从1 到\\nm −1 求和, 根据等差数列求和公式即得m(m −1)/2 。\\n这个指标可以理解为两个样本都属于聚类结果和参考模型中的同一类的样本对的个数与两个样本都\\n分别不属于聚类结果和参考模型中的同一类的样本对的个数的总和在所有样本对中出现的频率，可以简单\\n理解为聚类结果与参考模型的一致性。\\n9.2.4\\n式(9.8) 的解释\\n簇内距离的定义式：求和号左边是(xi, xj) 组合个数的倒数，求和号右边是这些组合的距离和，所以\\n两者相乘定义为平均距离。\\n9.2.5\\n式(9.12) 的解释\\n式中, k 表示聚类结果中簇的个数。该式的DBI 值越小越好, 因为我们希望“物以类聚”, 即同一簇的\\n样本尽可能彼此相似, avg (Ci) 和avg (Cj) 越小越好; 我们希望不同簇的样本尽可能不同, 即dcen (Ci, Cj)\\n越大越好。勘误: 第25 次印刷将分母dcen\\n\\x00µi, µj\\n\\x01\\n改为dcen (Ci, Cj)\\n9.3\\n距离计算\\n距离计算在各种算法中都很常见，本节介绍的距离计算方式和“西瓜书”10.6 节介绍的马氏距离基本\\n囊括了一般的距离计算方法。另外可能还会碰到“西瓜书”10.5 节的测地线距离。\\n本节有很多概念和名词很常见，比如本节开篇介绍的距离度量的四个基本性质、闵可夫斯基距离、欧\\n氏距离、曼哈顿距离、切比雪夫距离、数值属性、离散属性、有序属性、无序属性、非度量距离等，注意\\n对应的中文和英文。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n9.3.1\\n式(9.21) 的解释\\n该式符号较为抽象, 下面计算“西瓜书”第76 页表4.1 西瓜数据集2.0 属性根蒂上“蜷缩”和“稍蜷”\\n两个离散值之间的距离。\\n此时, u 为“根蒂”, a 为属性根蒂上取值为“蜷缩”, b 为属性根蒂上取值为“稍蜷”, 根据边注, 此\\n时样本类别已知(好瓜/坏瓜), 因此k = 2 。\\n从“西瓜书”表4.1 中可知, 根蒂为蜷缩的样本共有8 个(编号1 ∼5、编号12、编号16 ∼17), 即\\nmu,a = 8, 根蒂为稍蜷的样本共有7 个(编号6 ∼9 和编号13 ∼15), 即mu,b = 7; 设i = 1 对应好瓜,\\ni = 2 对应坏瓜, 好瓜中根蒂为蜷缩的样本共有5 个（编号1 ∼5 ), 即mu,a,1 = 5, 好瓜中根蒂为稍蜷的样\\n本共有3 个(编号6 8), 即mu,b,1 = 3, 坏瓜中根蒂为蜷缩的样本共有3 个(编号12 和编号16 ∼17 ), 即\\nmu,a,2 = 3, 坏瓜中根蒂为稍蜷的样本共有4 个（编号9 和编号13 ∼15), 即mu,b,2 = 4, 因此VDM 距离为\\nVDMp(a, b) =\\n\\x0c\\x0c\\x0c\\x0c\\nmu,a,1\\nmu,a\\n−mu,b,1\\nmu,b\\n\\x0c\\x0c\\x0c\\x0c\\np\\n+\\n\\x0c\\x0c\\x0c\\x0c\\nmu,a,2\\nmu,a\\n−mu,b,2\\nmu,b\\n\\x0c\\x0c\\x0c\\x0c\\np\\n=\\n\\x0c\\x0c\\x0c\\x0c\\n5\\n8 −3\\n7\\n\\x0c\\x0c\\x0c\\x0c\\np\\n+\\n\\x0c\\x0c\\x0c\\x0c\\n3\\n8 −4\\n7\\n\\x0c\\x0c\\x0c\\x0c\\np\\n9.4\\n原型聚类\\n本节介绍了三个原型聚类算法, 其中k 均值算法最为经典, 几乎成为聚类的代名词, 在Matlab、scikit-\\nlearn 等主流的科学计算包中均有kmeans 函数供调用。学习向量量化也是无监督聚类的一种方式, 在向量\\n检索的引擎，比如facebook faiss 中发挥重要的应用。\\n前两个聚类算法比较易懂, 下面主要推导第三个聚类算法：高斯混合聚类。\\n9.4.1\\n式(9.28) 的解释\\n该式就是多元高斯分布概率密度函数的定义式:\\np(x) =\\n1\\n(2π)\\nn\\n2 |Σ|\\n1\\n2 e−1\\n2 (x−µ)⊤Σ−1(x−µ)\\n对应到我们常见的一元高斯分布概率密度函数的定义式:\\np(x) =\\n1\\n√\\n2πσ e−(x−µ)2\\n2σ2\\n其中\\n√\\n2π = (2π)\\n1\\n2 对应(2π)\\nn\\n2 , σ 对应|Σ|\\n1\\n2 , 指数项中分母中的方差σ2 对应协方差矩阵Σ, (x−µ)2\\nσ2\\n对应(x−\\nµ)⊤Σ−1(x −µ)。\\n概率密度函数p(x) 是x 的函数。其中对于某个特定的x 来说, 函数值p(x) 就是一个数, 若x 的\\n维度为2 , 则可以将函数p(x) 的图像可视化, 是三维空间的一个曲面。类似于一元高斯分布p(x) 与横\\n轴p(x) = 0 之间的面积等于1 (即\\nR\\np(x)dx = 1 )，p(x) 曲面与平面p(x) = 0 之间的体积等于1 （即\\nR\\np(x)dx = 1 。\\n注意, “西瓜书”中后面将p(x) 记为p(x | µ, Σ) 。\\n9.4.2\\n式(9.29) 的解释\\n对于该式表达的高斯混合分布概率密度函数pM(x), 与式(9.28) 中的p(x) 不同的是, 它由k 个不同\\n的多元高斯分布加权而来。具体来说, p(x) 仅由参数µ, Σ 确定, 而pM(x) 由k 个“混合系数”αi 以及k\\n组参数µi, Σi 确定。\\n在“西瓜书”中该式下方(P207 最后一段) 中介绍了样本的生成过程, 实际也反应了“混合系数”αi 的\\n含义, 即αi 为选择第i 个混合成分的概率, 或者反过来说, αi 为样本属于第i 个混合成分的概率。重新描述\\n一下样本生成过程, 根据先验分布α1, α2, . . . , αk 选择其中一个高斯混合成分(即第i 个高斯混合成分被选\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n到的概率为αi ), 假设选到了第i 个高斯混合成分, 其参数为µi, Σi; 然后根据概率密度函数p (x | µi, Σi)\\n(即将式(9.28) 中的µ, Σ 替换为µi, Σi) 进行采样生成样本x 。两个步骤的区别在于第1 步选择高斯混\\n合成分时是从k 个之中选其一(相当于概率密度函数是离散的), 而第2 步生成样本时是从x 定义域中根\\n据p (x | µi, Σi ) 选择其中一个样本, 样本x 被选中的概率即为p (x | µi, Σi) 。即第1 步对应于离散型随\\n机变量, 第2 步对应于连续型随机变量。\\n9.4.3\\n式(9.30) 的解释\\n若由上述样本生成方式得到训练集D = {x1, x2, . . . , xm}, 现在的问题是对于给定样本xj, 它是由哪\\n个高斯混合成分生成的呢? 该问题即求后验概率pM (zj | xj), 其中zj ∈{1, 2, . . . , k} 。下面对式(9.30) 进\\n行推导。\\n对于任意样本, 在不考虑样本本身之前(即先验), 若瞎猜一下它由第i 个高斯混合成分生成的概率\\nP (zj = i), 那么肯定按先验概率α1, α2, . . . , αk 进行猜测, 即P (zj = i) = αi 。若考虑样本本身带来的信\\n息(即后验), 此时再猜一下它由第i 个高斯混合成分生成的概率pM (zj = i | xj), 根据贝叶斯公式, 后验概\\n率pM (zj = i | xj) 可写为\\npM (zj = i | xj) = P (zj = i) · pM (xj | zj = i)\\npM (xj)\\n分子第1 项P (zj = i) = αi; 第2 项即第i 个高斯混合成分生成样本xj 的概率p (xj | µi, Σi), 根据式\\n(9.28) 将x, µ, Σ 替换为xj, µi, Σi 即得; 分母pM (xj) 即为将xj 代入式(9.29) 即得。\\n注意, 西瓜书中后面将pM (zj = i | xj) 记为γji, 其中1 ≤j ≤m, 1 ≤i ≤k。\\n9.4.4\\n式(9.31) 的解释\\n若将所有γji 组成一个矩阵Γ, 其中γji 为第j 行第例的元素, 矩阵Γ 大小为m × k, 即\\nΓ =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nγ11\\nγ12\\n· · ·\\nγ1k\\nγ21\\nγ22\\n· · ·\\nγ2k\\n...\\n...\\n...\\n...\\nγm1\\nγm2\\n· · ·\\nγmk\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nm×k\\n其中m 为训练集样本个数, k 为高斯混合模型包含的混合模型个数。可以看出, 式(9.31) 就是找出矩阵Γ\\n第j 行的所有k 个元素中最大的那个元素的位置。\\n9.4.5\\n式(9.32) 的解释\\n对于训练集D = {x1, x2, . . . , xm}, 现在要把m 个样本划分为k 个簇, 即认为训练集D 的样本是根\\n据k 个不同的多元高斯分布加权而得的高斯混合模型生成的。\\n现在的问题是, k 个不同的多元高斯分布的参数{(µi, Σi) | 1 ⩽i ⩽k} 及它们各自的权重α1, α2, . . . , αk\\n不知道, m 个样本归到底属于哪个簇也不知道, 该怎么办呢?\\n其实这跟k 均值算法类似, 开始时既不知道k 个簇的均值向量, 也不知道m 个样本归到底属于哪个\\n簇, 最后我们采用了贪心策略, 通过迭代优化来近似求解式(9.24)。\\n本节的高斯混合聚类求解方法与k 均值算法, 只是具体问题具体解法不同, 从整体上来说, 它们都应用\\n了7.6 节的期望最大化算法(EM 算法)。\\n具体来说, 现假设已知式(9.30) 的后验概率, 此时即可通过式(9.31) 知道m 个样本归到底属于哪个簇,\\n再来求解参数{(αi, µi, Σi) | 1 ⩽i ⩽k}, 怎么求解呢? 对于每个样本xj 来说, 它出现的概率是pM (xj), 既\\n然现在训练集D 中确实出现了xj, 我们当然希望待求解的参数{(αi, µi, Σi) | 1 ⩽i ⩽k} 能够使这种可能\\n性pM (xj) 最大; 又因为我们假设m 个样本是独立的, 因此它们恰好一起出现的概率就是Qm\\nj=1 pM (xj),\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n即所谓的似然函数; 一般来说, 连乘容易造成下溢( m 个大于0 小于1 的数相乘, 当m 较大时, 乘积会非\\n常非常小, 以致于计算机无法表达这么小的数, 产生下溢), 所以常用对数似然替代, 即式(9.32)。\\n9.4.6\\n式(9.33) 的推导\\n根据公式(9.28) 可知：\\np (xj|µi, Σi) =\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2 (xj −µi)T Σ−1\\ni\\n(xj −µi)\\n\\x13\\n又根据公式(9.32)，由\\n∂LL(D)\\n∂µi\\n=\\n∂LL(D)\\n∂p (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂µi\\n= 0\\n其中：\\n∂LL(D)\\n∂p (xj|µi, \\uffffi) =\\n∂Pm\\nj=1 ln\\n\\x10Pk\\nl=1 αl · p (xj|µl, Σl)\\n\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\n∂ln\\n\\x10Pk\\nl=1 αl · p (xj|µl, Σl)\\n\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\nαi\\nPk\\nl=1 αl · p (xj|µl, Σl)\\n∂p (xj|µi, Σi)\\n∂µi\\n=\\n∂\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x10\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x11\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 ·\\n∂exp\\n\\x10\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x11\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 · exp\\n\\x12\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x13\\n· −1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 · exp\\n\\x12\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x13\\n· Σ−1\\ni\\n(xj −µi)\\n= p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n其中，由矩阵求导的法则∂aT Xa\\n∂a\\n= 2Xa 可得：\\n−1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n= −1\\n2 · 2Σ−1\\ni\\n(µi −xj)\\n= Σ−1\\ni\\n(xj −µi)\\n因此有：\\n∂LL(D)\\n∂µi\\n=\\nm\\nX\\nj=1\\nαi\\nPk\\nl=1 αl · p (xj|µl, \\uffffl)\\n· p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi) = 0\\n9.4.7\\n式(9.34) 的推导\\n由式(9.30)\\nγji = pM (zj = i|Xj) =\\nαi · p (Xj|µi, Σi)\\nPk\\nl=1 αl · p (Xj|µl, Σl)\\n带入式(9.33)\\nm\\nX\\nj=1\\nγji (Xj −µi) = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n移项, 得\\nm\\nX\\nj=1\\nγjixj =\\nm\\nX\\nj=1\\nγjiµi = µi ·\\nm\\nX\\nj=1\\nγji\\n第二个等号是因为µi 对于求和变量j 来说是常量, 因此可以提到求和号外面; 因此\\nµi =\\nPm\\nj=1 γjixj\\nPm\\nj=1 γji\\n9.4.8\\n式(9.35) 的推导\\n根据公式(9.28) 可知：\\np(xj|µi, Σi) =\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\n又根据公式(9.32)，由\\n∂LL(D)\\n∂Σi\\n= 0\\n可得\\n∂LL(D)\\n∂Σi\\n=\\n∂\\n∂Σi\\n\" m\\nX\\nj=1\\nln\\n \\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\n!#\\n=\\nm\\nX\\nj=1\\n∂\\n∂Σi\\n\"\\nln\\n \\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\n!#\\n=\\nm\\nX\\nj=1\\nαi ·\\n∂\\n∂Σi\\n(p(xj|µi, Σi))\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n其中\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) =\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f9\\n\\uf8fb\\n=\\n∂\\n∂Σi\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3exp\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8ed\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\n= p(xj|µi, Σi) ·\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8ed\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\n= p(xj|µi, Σi) ·\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0ln\\n1\\n(2π)\\nn\\n2 −\\n1\\n2 ln |Σi| −1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\uf8f9\\n\\uf8fb\\n= p(xj|µi, Σi) ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2\\n∂(ln |Σi|)\\n∂Σi\\n−\\n1\\n2\\n∂\\n\\x02\\n(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x03\\n∂Σi\\n\\uf8f9\\n\\uf8fb\\n由矩阵微分公式\\n∂|X|\\n∂X = |X| · (X−1)T ,\\n∂aT X−1B\\n∂X\\n= −X−T abT X−T 可得\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) = p(xj|µi, Σi) ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n将此式代回\\n∂LL(D)\\n∂Σi\\n中可得\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n又由公式(9.30) 可知\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= γji，所以上式可进一步化简为\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nγji ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n令上式等于0 可得\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nγji ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb= 0\\n移项推导有：\\nm\\nX\\nj=1\\nγji ·\\n\\x02\\n−I + (xj −µi)(xj −µi)T Σ−1\\ni\\n\\x03\\n= 0\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T Σ−1\\ni\\n=\\nm\\nX\\nj=1\\nγjiI\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T =\\nm\\nX\\nj=1\\nγjiΣi\\nΣ−1\\ni\\n·\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T =\\nm\\nX\\nj=1\\nγji\\nΣi =\\nPm\\nj=1 γji(xj −µi)(xj −µi)T\\nPm\\nj=1 γji\\n此即为公式(9.35)。\\n9.4.9\\n式(9.36) 的解释\\n该式即LL(D) 添加了等式约束Pk\\ni=1 αi = 1 的拉格朗日形式。\\n9.4.10\\n式(9.37) 的推导\\n重写式(9.32) 如下:\\nLL(D) =\\nm\\nX\\nj=1\\nln\\n k\\nX\\nl=1\\nαl · p (xj | µl, Σl)\\n!\\n这里将第2 个求和号的求和变量由式(9.32) 的i 改为了l, 这是为了避免对αi 求导时与变量i 相混淆。将\\n式(9.36) 中的两项分别对αi 求导, 得\\n∂LL(D)\\n∂αi\\n=\\n∂Pm\\nj=1 ln\\n\\x10Pk\\nl=1 αl · p (xj | µl, Σl)\\n\\x11\\n∂αi\\n=\\nm\\nX\\nj=1\\n1\\nPk\\nl=1 αl · p (xj | µl, Σl)\\n· ∂Pk\\nl=1 αl · p (xj | µl, Σl)\\n∂αi\\n=\\nm\\nX\\nj=1\\n1\\nPk\\nl=1 αl · p (xj | µl, Σl)\\n· p (xj | µi, Σi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n∂\\n\\x10Pk\\nl=1 αl −1\\n\\x11\\n∂αi\\n= ∂(α1 + α2 + . . . + αi + . . . + αk −1)\\n∂αi\\n= 1\\n综合两项求导结果, 并令导数等于零即得式(9.37)。\\n9.4.11\\n式(9.38) 的推导\\n注意, 在西瓜书第14 次印刷中式(9.38) 上方的一行话进行了勘误: “两边同乘以αi, 对所有混合成分\\n求和可知λ = −m ”, 将原来的“样本”修改为“混合成分”。\\n对公式(9.37) 两边同时乘以αi 可得\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n+ λαi = 0\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λαi\\n两边对所有混合成分求和可得\\nk\\nX\\ni=1\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λ\\nk\\nX\\ni=1\\nαi\\nm\\nX\\nj=1\\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λ\\nk\\nX\\ni=1\\nαi\\n因为\\nk\\nX\\ni=1\\nαi · p(xj|µi, \\uffffi)\\nPk\\nl=1 αl · p(xj|µl, \\uffffl)\\n=\\nPk\\ni=1 αi · p(xj|µi, \\uffffi)\\nPk\\nl=1 αl · p(xj|µl, \\uffffl)\\n= 1\\n且Pk\\ni=1 αi = 1，所以有m = −λ，因此\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λαi = mαi\\n因此\\nαi =\\n1\\nm\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n又由公式(9.30) 可知\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= γji，所以上式可进一步化简为\\nαi =\\n1\\nm\\nm\\nX\\nj=1\\nγji\\n此即为公式(9.38)。\\n9.4.12\\n图9.6 的解释\\n第1 行初始化参数, 本页接下来的例子是按如下策略初始化的: 混合系数αi = 1\\nk; 任选训练集中的k\\n个样本分别初始化k 个均值向量µi(1 ⩽i ⩽k); 使用对角元素为0.1 的对角阵初始化k 个协方差矩阵\\nΣi(1 ⩽i ⩽k) 。\\n第3~5 行根据式(9.30) 计算共m × k 个γji 。\\n第6~10 行分别根据式(9.34)、式(9.35)、式(9.38) 使用刚刚计算得到的γji 更新均值向量、协方差\\n矩阵、混合系数; 注意第8 行计算协方差矩阵时使用的是第7 行计算得到的均值向量, 这并没错, 因为协方\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n差矩阵Σ′\\ni 与均值向量µ′\\ni 是对应的, 而非µi; 第7 行的µ′\\ni 在第8 行使用之后会在下一轮迭代中第4 行计\\n算γji 再次使用。\\n整体来说, 第2 ~12 行就是一个EM 算法的具体使用例子, 学习完7.6 节EM 算法可能根本无法理解\\n其思想。此例中有两组变量, 分别是γji 和(αi, µi, Σi), 它们之间相互影响, 但都是末知的, 因此EM 算法\\n就有了用武之地: 初始化其中一组变量(αi, µi, Σi), 然后计算γji; 再根据γji 根据最大似然推导出的公式\\n更新(αi, µi, Σi), 反复迭代, 直到满足停止条件。\\n9.5\\n密度聚类\\n本节介绍的DBSCAN 算法并不难懂，只是本节在最后举例时并没有说清楚密度聚类算法与前面原型\\n聚类算法的区别，当然这也可能是作者有意为之，因为在“西瓜书”本章习题9.7 题就提到了“凸聚类”\\n的概念。具体来说，前面介绍的聚类算法只能产生“凸聚类”，而本节介绍的DBSCAN 则能产生“非凸聚\\n类”，其本质原因，个人感觉在于聚类时使用的距离度量，均值算法使用欧氏距离，而DBSCAN 使用类似\\n于测地线距离（只是类似，并不相同，测地线距离参见“西瓜书”10.5 节），因此可以产生如下聚类结果\\n（中间为典型的非凸聚类）。\\n注意，虽然左图为“凸聚类”（四个簇都有一个凸包），但均值算法却无法产生此结果，因为最大的簇\\n太大了，其外围样本与另三个小簇的中心之间的距离更近，因此中间最大的簇肯定会被均值算法划分到不\\n同的簇之中，这显然不是我们希望的结果。\\n密度聚类算法可以产生任意形状的簇，不需要事先指定聚类个数k，并且对噪声鲁棒。\\n9.5.1\\n密度直达、密度可达与密度相连\\nxj 由xi 密度直达, 该概念最易理解, 但要特别注意: 密度直达除了要求xj 位于xi 的ϵ−领域的条件\\n之外, 还额外要求xi 是核心对象; ϵ-领域满足对称性, 但xj 不一定为核心对象, 因此密度直达关系通常不\\n满足对称性。\\nxj 由xi 密度可达，该概念基于密度直达，因此样本序列p1, p2, . . . , pn 中除了pn = xj 之外, 其余样\\n本均为核心对象(当然包括p1 = xi ), 所以同理, 一般不满足对称性。\\n以上两个概念中, 若xj 为核心对象, 已知xj 由xi 密度直达/可达, 则xi 由xj 密度直达/ 可达, 即满\\n足对称性(也就是说, 核心对象之间的密度直达/可达满足对称性)。\\nxi 与xj 密度相连, 不要求xi 与xj 为核心对象, 所以满足对称性。\\n9.5.2\\n图9.9 的解释\\n在第1 ∼7 行中, 算法先根据给定的邻域参数(ϵ, MinPts) 找出所有核心对象, 并存于集合Ω之中; 第\\n4 行的if 判断语句即在判别xj 是否为核心对象。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在第10 ∼24 行中, 以任一核心对象为出发点(由第12 行实现), 找出其密度可达的样本生成聚类簇\\n(由第14 ∼21 行实现), 直到所有核心对象被访问过为止（由第10 行和第23 行配合实现)。具体来说:\\n其中第14 ∼21 行while 循环中的if 判断语句（第16 行）在第一次循环时一定为真（因为Q 在第\\n12 行初始化为某核心对象), 此时会往队列Q 中加入q 密度直达的样本(已知q 为核心对象, q 的ϵ-领域\\n中的样本即为q 密度直达的), 队列遵循先进先出规则, 接下来的循环将依次判别q 的ϵ-领域中的样本是\\n否为核心对象(第16 行), 若为核心对象, 则将密度直达的样本( ϵ-领域中的样本) 加入Q。根据密度可达\\n的概念, while 循环中的if 判断语句（第16 行）找出的核心对象之间一定是相互密度可达的, 非核心对象\\n一定是密度相连的。\\n第14 ∼21 行while 循环每跳出一次, 即生成一个聚类簇。每次生成聚类\\x00之前, 会记录当前末访问过\\n样本集合(第11 行Γold = Γ ), 然后当前要生成的聚类簇每决定录取一个样本后会将该样本从厂去除(第\\n13 行和第19 行), 因此第14~21 行while 循环每跳出一次后, Γold 与Γ 差别即为聚类簇的样本成员(第\\n22 行), 并将该聚类簇中的核心对象从第1 ∼7 行生成的核心对象集合Ω中去除。\\n符号“\\\\”为集合求差, 例如集合A = {a, b, c, d, e, f}, B = {a, d, f, g, h}, 则A\\\\B 为A\\\\B = {b, c, e},\\n即将A, B 所有相同元素从A 中去除。\\n9.6\\n层次聚类\\n本节主要介绍了层次聚类的代表算法AGNES。\\n式(9.41) (9.43) 介绍了三种距离计算方式, 这与“西瓜书”9.3 节中介绍的距离不同之处在于, 此三种\\n距离计算面向集合之间, 而9.3 节的距离则面向两点之间。正如“西瓜书”第215 页左上边注所示, 集合间\\n的距离计算常采用豪斯多夫距离(Hausdorff distance)。\\n算法AGNES 很简单, 就是不断重复执行合并距离最近的两个聚类簇。“西瓜书”图9.11 为具体实现\\n方法, 核心就是在合并两个聚类簇后更新距离矩阵（第11 ∼23 行), 之所以看起来复杂, 是因为该实现只\\n更新原先距离矩阵中发生变化的行和列, 因此需要为此做一些调整。\\n在第1 ∼9 行, 算法先对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化。注意, 距离矩阵\\n中, 第i 行为聚类簇Ci 到各聚类簇的距离, 第i 列为各聚类簇到聚类簇Ci 的距离, 由第7 行可知, 距离矩\\n阵为对称矩阵, 即使用的集合间的距离计算方法满足对称性。\\n第18 ∼21 行更新距离矩阵M 的第i∗行与第i∗列, 因为此时的聚类簇Ci∗已经合并了Cj∗, 因此与\\n其余聚类簇之间的距离都发生了变化, 需要更新。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第10 章\\n降维与度量学习\\n10.1\\n预备知识\\n本章内容需要较多的线性代数和矩阵分析的基础，因此将相关的预备知识整体整理如下：\\n10.1.1\\n符号约定\\n向量元素之间分号“;”表示列元素分隔符, 如α = (a1; a2; . . . ; ai; . . . ; am) 表示m × 1 的列向量; 而逗\\n号“,”表示行元素分隔符, 如α = (a1, a2, . . . , ai, . . . , am) 表示1 × m 的行向量。\\n10.1.2\\n矩阵与单位阵、向量的乘法\\n(1) 矩阵左乘对角阵相当于矩阵每行乘以对应对角阵的对角线元素, 如:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1x11\\nλ1x12\\nλ1x13\\nλ2x21\\nλ2x22\\nλ2x23\\nλ3x31\\nλ3x32\\nλ3x33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n(2) 矩阵右乘对角阵相当于矩阵每列乘以对应对角阵的对角线元素, 如:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1x11\\nλ2x12\\nλ3x13\\nλ1x21\\nλ2x22\\nλ3x23\\nλ1x31\\nλ2x32\\nλ3x33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n(3) 矩阵左乘行向量相当于矩阵每行乘以对应行向量的元素之和, 如:\\nh\\nλ1\\nλ2\\nλ3\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= λ1\\nh\\nx11\\nx12\\nx13\\ni\\n+ λ2\\nh\\nx21\\nx22\\nx23\\ni\\n+ λ3\\nh\\nx31\\nx32\\nx33\\ni\\n=\\n\\x10\\nλ1x11 + λ2x21 + λ3x31, λ1x12 + λ2x22 + λ3x32, λ1x13 + λ2x23 + λ3x33\\n\\x11\\n(4) 矩阵右乘列向量相当于矩阵每列乘以对应列向量的元素之和, 如:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\nx31\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+ λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx12\\nx22\\nx32\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+ λ3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx13\\nx23\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n3\\nX\\ni=1\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8edλi\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx1i\\nx2i\\nx3i\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n= (λ1x11 + λ2x12 + λ3x13; λ1x21 + λ2x22 + λ3x23; λ1x31 + λ2x32 + λ3x33)\\n综上, 左乘是对矩阵的行操作, 而右乘则是对矩阵的列操作, 第(2) 个和第(4) 个结论后面推导过程中\\n灵活应用较多。\\n10.2\\n矩阵的F 范数与迹\\n(1) 对于矩阵A ∈Rm×n, 其Frobenius 范数(简称F 范数) ∥A∥F 定义为\\n∥A∥F =\\n m\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2\\n! 1\\n2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中aij 为矩阵A 第i 行第j 列的元素, 即\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n...\\n...\\n...\\n...\\n...\\n...\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n...\\n...\\n...\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n(2) 若A = (α1, α2, . . . , αj, . . . , αn), 其中αj = (a1j; a2j; . . . ; aij; . . . ; amj) 为其列向量, A ∈Rm×n, αj ∈\\nRm×1, 则∥A∥2\\nF = Pn\\nj=1 ∥αj∥2\\n2;\\n同理, 若A = (β1; β2; . . . ; βi; . . . ; βm), 其中βi = (ai1, ai2, . . . , aij, . . . , ain) 为其行向量, A ∈Rm×n, βi ∈\\nR1×n, 则∥A∥2\\nF = Pm\\ni=1 ∥βi∥2\\n2 。\\n证明: 该结论是显而易见的, 因为∥αj∥2\\n2 = Pm\\ni=1 |aij|2, 而∥A∥F = Pm\\ni=1\\nPn\\nj=1 |aij|2 。\\n(3) 若λj\\n\\x00A⊤A\\n\\x01\\n表示n 阶方阵A⊤A 的第j 个特征值, tr\\n\\x00A⊤A\\n\\x01\\n是A⊤A 的迹（对角线元素之和);\\nλi\\n\\x10\\nAA⊤\\x11\\n表示m 阶方阵AA⊤的第i 个特征值, tr\\n\\x10\\nAA⊤\\x11\\n是AA⊤的迹, 则\\n∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n=\\nn\\nX\\nj=1\\nλj\\n\\x00A⊤A\\n\\x01\\n= tr\\n\\x00AA⊤\\x01\\n=\\nm\\nX\\ni=1\\nλi\\n\\x00AA⊤\\x01\\n证明: (a) 先证∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n:\\n令B = A⊤A ∈Rn×n, bij 表示B 第i 行第j 列元素, tr(B) = Pn\\nj=1 bjȷ\\nB = A⊤A =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2\\n· · ·\\nam2\\n...\\n...\\n...\\n...\\n...\\n...\\na1j\\na2j\\n· · ·\\naij\\n· · ·\\namj\\n...\\n...\\n...\\n...\\n...\\n...\\na1n\\na2n\\n· · ·\\nain\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n...\\n...\\n...\\n...\\n...\\n...\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n...\\n...\\n...\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n由矩阵运算规则, bjj 等于A⊤的第j 行与A 的第j 列的内积, 因此\\ntr(B) =\\nn\\nX\\nj=1\\nbjj =\\nn\\nX\\nj=1\\n m\\nX\\ni=1\\n|aij|2\\n!\\n=\\nm\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2 = ∥A∥2\\nF\\n以上第三个等号交换了求和号次序（类似于交换积分号次序), 显然这不影响求和结果。\\n(b) 同理, 可证∥A∥2\\nF = tr\\n\\x00AA⊤\\x01\\n:\\nC = AA⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n...\\n...\\n...\\n...\\n...\\n...\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n...\\n...\\n...\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2\\n· · ·\\nam2\\n...\\n...\\n...\\n...\\n...\\n...\\na1j\\na2j\\n· · ·\\naij\\n· · ·\\namj\\n...\\n...\\n...\\n...\\n...\\n...\\na1n\\na2n\\n· · ·\\nain\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由矩阵运算规则, cii 等于A 的第i 行与A⊤的第i 列的内积（红色元素), 因此\\ntr(C) =\\nm\\nX\\ni=1\\ncii =\\nm\\nX\\ni=1\\n n\\nX\\nj=1\\n|aij|2\\n!\\n=\\nm\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2 = ∥A∥2\\nF\\n有关方阵的特征值之和等于对角线元素之和, 可以参见线性代数教材, 如同济大学主编的《线性代数\\n(第五版)》第五章第2 节“方阵的特征值与特征向量”(第117 页):\\n设n 阶矩阵A = (aij) 的特征值为λ1, λ2, · · · , λn, 不难证明\\n(i) λ1 + λ2 + · · · + λn = a11 + a22 + · · · + ann;\\n(ii) λ1λ2 · · · λn = |A|.\\n10.3\\nk 近邻学习\\n10.3.1\\n式(10.1) 的解释\\nP(err) = 1 −\\nX\\nc∈Y\\nP(c|x)P(c|z)\\n首先, P(c | x) 表示样本x 为类别c 的后验概率, P(c | z) 表示样本z 为类别c 的后验概率; 其次,\\nP(c | x)P(c | z) 表示样本x 和样本z 同时为类别c 的概率;\\n再次, P\\nc∈Y P(c | x)P(c | z) 表示样本x 和样本z 类别相同的概率; 这一点可以进一步解释，设\\nY = {c1, c2, · · · , cN}, 则该求和式子变为:\\nP (c1 | x) P (c1 | z) + P (c2 | x) P (c2 | z) + · · · + P (cN | x) P (cN | z)\\n即样本x 和样本z 同时为c1 的概率, 加上同时为c2 的概率, · · · · · · , 加上同时为cN 的概率, 即样本\\nx 和样本z 类别相同的概率;\\n最后, P(err) 表示样本x 和样本z 类别不相同的概率, 即1 减去二者类别相同的概率。\\n10.3.2\\n式(10.2) 的推导\\n式(10.2) 推导关键在于理解第二行的“约等(≃) ”关系和第三行的“小于等于(⩽) ”关系。\\n第二行的“约等(≃) ”关系的依据在于该式前面一段话: “假设样本独立同分布, 且对任意x 和任\\n意小正数δ, 在x 附近δ 距离范围内总能找到一个训练样本”, 这意味着对于任意测试样本在训练集中\\n都可以找出一个与其非常像(任意小正数δ ) 的近邻, 这里还有一个假设书中末提及: P(c | x) 必须是\\n连续函数(对于连续函数f(x) 和任意小正数δ, f(x) ≃f(x + δ)), 即对于两个非常像的样本z 与x 有\\nP(c | x) ≃P(c | z), 即\\nX\\nc∈Y\\nP(c | x)P(c | z) ≃\\nX\\nc∈Y\\nP 2(c | x)\\n第三行的“小于等于(⩽) ”关系更简单: 由于c∗∈Y, 所以P 2 (c∗| x) ⩽P\\nc∈Y P 2(c | x), 也就是“小\\n于等于(⩽) ”左边只是右边的一部分, 所以肯定是小于等于的关系;\\n第四行就是数学公式a2 −b2 = (a + b)(a −b)；\\n第五行是由于1 + P (c∗| x) ⩽2, 这是由于概率值P (c∗| x) ⩽1\\n经过以上推导, 本节最后给出一个惊人的结论: 最近邻分类器虽简单, 但它的泛化错误率不超过贝叶斯\\n最优分类器的错误率的两倍!\\n然而这是一个没啥实际用途的结论, 因为这个结论必须满足两个假设条件, 且不说P(c | x) 是连续函\\n数（第一个假设）是否满足, 单就“对任意x 和任意小正数δ, 在x 附近δ 距离范围内总能找到一个训练\\n样本”(第二个假设) 是不可能满足的, 这也就有了10.2 节开头一段的讨论, 抛开“任意小正数δ ”不谈, 具\\n体到δ = 0.001 都是不现实的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.4\\n低维嵌入\\n10.4.1\\n图10.2 的解释\\n只要注意一点就行：在图(a) 三维空间中，红色线是弯曲的，但去掉高度这一维（竖着的坐标轴）后，\\n红色线变成直线，而直线更容易学习。\\n10.4.2\\n式(10.3) 的推导\\n已知Z = {z1, z2, . . . , zi, . . . , zm} ∈Rd′×m, 其中zi = (zi1; zi2; . . . ; zid′) ∈Rd′×1; 降维后的内积矩阵\\nB = Z⊤Z ∈Rm×m, 其中第i 行第j 列元素bij, 特别的\\nbii = z⊤\\ni zi = ∥zi∥2 , bjj = z⊤\\nj zj = ∥zj∥2 , bij = z⊤\\ni zj\\nMDS 算法的目标是∥zi −zj∥= distij = ∥xi −xj∥, 即保持样本的欧氏距离在d′ 维空间和原始d 维空间\\n相同(d′ ⩽d) 。\\ndist2\\nij = ∥zi −zj∥2 = (zi1 −zj1)2 + (zi2 −zj2)2 + . . . + (zid′ −zjd′)2\\n=\\n\\x00z2\\ni1 −2zi1zj1 + z2\\nj1\\n\\x01\\n+\\n\\x00z2\\ni2 −2zi2zj2 + z2\\nj2\\n\\x01\\n+ . . . +\\n\\x00z2\\nid′ −2zid′zjd′ + z2\\njd′\\n\\x01\\n=\\n\\x00z2\\ni1 + z2\\ni2 + . . . + z2\\nid′\\n\\x01\\n+\\n\\x00z2\\nj1 + z2\\nj2 + . . . + z2\\njd′\\n\\x01\\n−2 (zi1zj1 + zi2zj2 + . . . + zid′zjd′)\\n= ∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n= bii + bjj −2bij\\n本章矩阵运算非常多, 刚刚是从矩阵元素层面的推导; 实际可发现上式运算结果基本与标量运算规则\\n相同, 因此后面会尽可能不再从元素层面推导。具体来说:\\ndist2\\nij = ∥zi −zj∥2 = (zi −zj)⊤(zi −zj)\\n= z⊤\\ni zi −z⊤\\ni zj −z⊤\\nj zi + z⊤\\nj zj\\n= z⊤\\ni zi + z⊤\\nj zj −2z⊤\\ni zj\\n= ∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n= bii + bjj −2bij\\n上式第三个等号化简是由于内积z⊤\\ni zj 和z⊤\\nj zi 均为标量, 因此转置等于本身。\\n10.4.3\\n式(10.4) 的推导\\n首先解释两个条件:\\n(1) 令降维后的样本Z 被中心化, 即Pm\\ni=1 zi = 0 注意Z ∈Rd′×m, d′ 是样本维度(属性个数), m 是样\\n本个数, 易知Z 的每一行有m 个元素(每行表示样本集的一维属性), Z 的每一列有d′ 个元素(每列表示\\n一个样本)。\\n式Pm\\ni=1 zi = 0 中的zi 明显表示的是第i 列, m 列相加得到一个零向量0d′×1, 意思是样本集合中所\\n有样本的每一维属性之和均等于0 , 因此被中心化的意思是将样本集合Z 的每一行（属性）减去该行的均\\n值。\\n(2) 显然, 矩阵B 的行与列之各均为零, 即Pm\\ni=1 bij = Pm\\nj=1 bij = 0 。\\n注意bij = z⊤\\ni zj (也可以写为bij = z⊤\\nj zi, 其实就是对应元素相乘, 再求和)\\nm\\nX\\ni=1\\nbij =\\nm\\nX\\ni=1\\nz⊤\\nj zi = z⊤\\nj\\nm\\nX\\ni=1\\nzi = z⊤\\nj · 0d′×1 = 0\\nm\\nX\\nj=1\\nbij =\\nm\\nX\\nj=1\\nz⊤\\ni zj = z⊤\\ni\\nm\\nX\\nj=1\\nzj = z⊤\\ni · 0d′×1 = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n接下来我们推导式(10.4), 将式(10.3) 的dist2\\nij 表达式代入:\\nm\\nX\\ni=1\\ndist2\\nij =\\nm\\nX\\ni=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\ni=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nz⊤\\ni zj\\n根据定义:\\nm\\nX\\ni=1\\n∥zi∥2 =\\nm\\nX\\ni=1\\nz⊤\\ni zi =\\nm\\nX\\ni=1\\nbii = tr(B)\\nm\\nX\\ni=1\\n∥zj∥2 = ∥zj∥2\\nm\\nX\\ni=1\\n1 = m ∥zj∥2 = mz⊤\\nj zj = mbjj\\n根据前面结果:\\nm\\nX\\ni=1\\nz⊤\\ni zj =\\n m\\nX\\ni=1\\nz⊤\\ni\\n!\\nzj = 01×d′ · zj = 0\\n代入上式即得:\\nm\\nX\\ni=1\\ndist2\\nij =\\nm\\nX\\ni=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nz⊤\\ni zj\\n= tr(B) + mbjj\\n10.4.4\\n式(10.5) 的推导\\n与式(10.4) 类似:\\nm\\nX\\nj=1\\ndist2\\nij =\\nm\\nX\\nj=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\nj=1\\n∥zi∥2 +\\nm\\nX\\nj=1\\n∥zj∥2 −2\\nm\\nX\\nj=1\\nz⊤\\ni zj\\n= mbii + tr(B)\\n10.4.5\\n式(10.6) 的推导\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\ndist2\\nij =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nz⊤\\ni zj\\n其中各子项的推导如下：\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zi∥2 = m\\nm\\nX\\ni=1\\n∥zi∥2 = m tr(B)\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zj∥2 = m\\nm\\nX\\nj=1\\n∥zj∥2 = m tr(B)\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nz⊤\\ni zj = 0\\n最后一个式子是来自于书中的假设，假设降维后的样本Z 被中心化。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.4.6\\n式(10.10) 的推导\\n由式(10.3) 可得\\nbij = −1\\n2(dist2\\nij −bii −bjj)\\n由式(10.6) 和(10.9) 可得\\ntr(B) =\\n1\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\ndist2\\nij\\n= m\\n2 dist2\\n·\\n由式(10.4) 和(10.8) 可得\\nbjj = 1\\nm\\nm\\nX\\ni=1\\ndist2\\nij −1\\nmtr(B)\\n= dist2\\n·j −1\\n2dist2\\n·\\n由式(10.5) 和式(10.7) 可得\\nbii = 1\\nm\\nm\\nX\\nj=1\\ndist2\\nij −1\\nmtr(B)\\n= dist2\\ni· −1\\n2dist2\\n·\\n综合可得\\nbij = −1\\n2(dist2\\nij −bii −bjj)\\n= −1\\n2(dist2\\nij −dist2\\ni· + 1\\n2dist2\\n·· −dist2\\n·j + 1\\n2dist2\\n··)\\n= −1\\n2(dist2\\nij −dist2\\ni· −dist2\\n·j + dist2\\n··)\\n在式(10.10) 后紧跟着一句话: “由此即可通过降维前后保持不变的距离矩阵D 求取内积矩阵B”,\\n我们来解释一下这句话。\\n首先解释式(10.10) 等号右侧的变量含义:\\ndistij = ∥zi −zj∥表示降维后zi 与zj 的欧氏距离, 注\\n意这同时也应该是原始空间xi 与xj 的距离, 因为降维的目标（也是约束条件）是“任意两个样本在d′ 维\\n空间中的欧氏距离等于原始空间中的距离”, 也就是说dist2\\nij 是降维前后的距离矩阵D 的元素distij 的平\\n方; 其次, 式(10.10) 等号左侧bij 是降维后内积矩阵B 的元素, 即B 的元素bij 可以由距离矩阵D 来表达\\n求取。\\n10.4.7\\n式(10.11) 的解释\\n由题设知，d∗为V 的非零特征值，因此B = VΛV⊤可以写成B = V∗Λ∗V⊤\\n∗，其中Λ∗∈Rd×d 为d\\n个非零特征值构成的特征值对角矩阵，而V∗∈Rm×d 为Λ∗∈Rd×d 对应的特征值向量矩阵，因此有\\nB =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n故而Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m\\n10.4.8\\n图10.3 关于MDS 算法的解释\\n首先要清楚此处降维算法要完成的任务: 获得d 维空间的样本集合X ∈Rd×m 在d′ 维空间的表示\\nZ ∈Rd′×m, 并且保证距离矩阵D ∈Rm×m 相同, 其中d′ < d, m 为样本个数, 距离矩阵即样本之间的欧氏\\n距离。那么怎么由X ∈Rd×m 得到Z ∈Rd′×m 呢?\\n经过推导发现(式(10.3) 式(10.10)), 在保证距离矩阵D ∈Rm×m 相同的前提下, d′ 维空间的样本集\\n合Z ∈Rd′×m 的内积矩阵B = Z⊤Z ∈Rm×m 可以由距离矩阵D ∈Rm×m 得到(参见式(10.10)), 此时只\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n要对B 进行矩阵分解即可得到Z; 具体来说, 对B 进行特征值分解可得B = VΛV⊤, 其中V ∈Rm×m 为\\n特征值向量矩阵, \\uffff∈Rm×m 为特征值构成的对角矩阵, 接下来分类讨论:\\n(1) 当d > m 时, 即样本属性比样本个数还要多此时, 样本集合X ∈Rd×m 的d 维属性一定是线性相\\n关的(即有品几余), 因为矩阵X 的秩不会大于m (此处假设矩阵X 的秩恰好等于m ), 因此Λ ∈Rm×m\\n主对角线有m 个非零值, 进而B =\\n\\x10\\nVΛ1/2\\x11 \\x10\\nΛ1/2V⊤\\x11\\n, 得到的Z = Λ1/2V⊤∈Rd′×m 实际将d 维属性降\\n成了d′ = m 维属性。\\n(2) 当d < m 时, 即样本个数比样本属性多这是现实中最常见的一种情况。此时Λ ∈Rm×m 至多\\n有d 个非零值（此处假设恰有d 个非零值), 因此B = VΛV⊤可以写成B = V∗Λ∗V⊤\\n∗, 其中Λ∗∈Rd×d\\n为d 个非零值特征值构成的特征值对角矩阵, V∗∈Rm×d 为Λ∗∈Rd×d 相应的特征值向量矩阵, 进而\\nB =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n, 求得Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m, 此时属性没有冗杂, 因此按降维的规则（降维后距\\n离矩阵不变）并不能实现有效降维。\\n由以上分析可以看出, 降维后的维度d′ 实际为B 特征值分解后非零特征值的个数。\\n10.5\\n主成分分析\\n注意，作者在数次印刷中对本节符号进行修订，详见勘误修订，直接搜索页码即可，此处仅按个人推\\n导需求定义符号，可能与不同印次书中符号不一致。\\n10.5.1\\n式(10.14) 的推导\\n预备知识:\\n在一个坐标系中, 任意向量等于其在各个坐标轴的坐标值乘以相应坐标轴单位向量之和。例如, 在\\n二维直角坐标系中, x 轴和y 轴的单位向量分别为v1 = (1; 0) 和v2 = (0; 1), 向量r = (2; 3) 可以\\n表示为r = 2v1 + 3v2; 其实v1 = (1; 0) 和v2 = (0; 1) 只是二维平面的一组标准正交基, 但二维平面\\n实际有无数标准正交基, 如v′\\n1 =\\n\\x10\\n1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n和v′\\n2 =\\n\\x10\\n−1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n, 此时向量r =\\n5\\n√\\n2v′\\n1 +\\n1\\n√\\n2v′\\n2, 其中\\n5\\n√\\n2 = (v′\\n1)⊤r,\\n1\\n√\\n2 = (v′\\n2)⊤r, 即新坐标系里的坐标。\\n下面开始推导:\\n对于d 维空间Rd×1 来说, 传统的坐标系为{v1, v2, . . . , vk, . . . , vd}, 其中vk 为除第k 个元素为1\\n其余元素均0 的d 维列向量; 此时对于样本点xi = (xi1; xi2; . . . ; xid) ∈Rd×1 来说亦可表示为xi =\\nxi1v1 + xi2v2 + . . . + xidvd 。\\n现假定投影变换后得到的新坐标系为{w1, w2, . . . , wk, . . . , wd} （即一组新的标准正交基), 则xi 在\\n新坐标系中的坐标为\\n\\x00w⊤\\n1 xi; w⊤\\n2 xi; . . . ; w⊤\\nd xi\\n\\x01\\n。若丢弃新坐标系中的部分坐标, 即将维度降低到d′ < d\\n(不失一般性, 假设丢掉的是后d −d′ 维坐标), 并令\\nW = (w1, w2, . . . , wd′) ∈Rd×d′\\n则xi 在低维坐标系中的投影为\\nzi = (zi1; zi2; . . . ; zid′) =\\n\\x00w⊤\\n1 xi; w⊤\\n2 xi; . . . ; w⊤\\nd′xi\\n\\x01\\n= W⊤xi\\n若基于zi 来重构xi, 则会得到ˆxi = Pd′\\nj=1 zijwj = Wzi (“西瓜书”P230 第11 行)。\\n有了以上符号基础, 接下来将式(10.14) 化简成式(10.15) 目标函数形式(可逐一核对各项维数以验证\\n推导是否有误):\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\r\\nd′\\nX\\nj=1\\nzijwj −xi\\n\\r\\r\\r\\r\\r\\n2\\n2\\n(1)\\n=\\nm\\nX\\ni=1\\n∥Wzi −xi∥2\\n2\\n(2)\\n=\\nm\\nX\\ni=1\\n\\r\\rWW⊤xi −xi\\n\\r\\r2\\n2\\n(3)\\n=\\nm\\nX\\ni=1\\n\\x00WW⊤xi −xi\\n\\x01⊤\\x00WW⊤xi −xi\\n\\x01\\n(4)\\n=\\nm\\nX\\ni=1\\n\\x00x⊤\\ni WW⊤WW⊤xi −2x⊤\\ni WW⊤xi + x⊤\\ni xi\\n\\x01\\n(5)\\n=\\nm\\nX\\ni=1\\n\\x00x⊤\\ni WW⊤xi −2x⊤\\ni WW⊤xi + x⊤\\ni xi\\n\\x01\\n(6)\\n=\\nm\\nX\\ni=1\\n\\x00−x⊤\\ni W⊤xi + x⊤\\ni xi\\n\\x01\\n(1)\\n=\\nm\\nX\\ni=1\\n\\x10\\n−\\n\\x00W⊤xi\\n\\x01⊤\\x00W⊤xi\\n\\x01\\n+ x⊤\\ni xi\\n\\x11\\n(8)\\n=\\nm\\nX\\ni=1\\n\\x10\\n−\\n\\r\\rW⊤xi\\n\\r\\r2\\n2 + x⊤\\ni xi\\n\\x11\\n(9)\\n∝−\\nm\\nX\\ni=1\\n\\r\\rW⊤xi\\n\\r\\r2\\n2\\n上式从第三个等号到第四个等号: 由于\\n\\x00WW⊤\\x01⊤=\\n\\x00W⊤\\x01⊤(W)⊤= WW⊤, 因此\\n\\x00WW⊤xi\\n\\x01⊤= x⊤\\ni\\n\\x00WW⊤\\x01⊤= x⊤\\ni WW⊤\\n代入即得第四个等号; 从第四个等号到第五个等号: 由于w⊤\\ni wj = 0, (i ̸= j), ∥wi∥= 1, 因此W⊤W =\\nI ∈Rd′×d′, 代入即得第五个等号。由于最终目标是寻找W 使目标函数(10.14) 最小, 而x⊤\\ni xi 与W 无关,\\n因此在优化时可以去掉。令X = (x1, x2, . . . , xm) ∈Rd×m, 即每列为一个样本, 则式(10.14) 可继续化简为\\n(参见10.2节中“矩阵的F 范数与迹”)\\n−\\nm\\nX\\ni=1\\n\\r\\rW⊤xi\\n\\r\\r2\\n2 = −\\n\\r\\rW⊤X\\n\\r\\r2\\nF\\n= −tr\\n\\x10\\x00W⊤X\\n\\x01 \\x00W⊤X\\n\\x01⊤\\x11\\n= −tr\\n\\x00W⊤XX⊤W\\n\\x01\\n这里W⊤xi = zi, 这里仅为得到式(10.15) 的形式才最终保留W 和xi 的; 若令Z = (z1, z2, . . . , zm) ∈\\nRd′×m 为低维坐标系中的样本集合, 则Z = W⊤X, 即zi 为矩阵Z 的第i 列; 而Pm\\ni=1\\n\\r\\rW⊤xi\\n\\r\\r2\\n2 =\\nPm\\ni=1 ∥zi∥2\\n2 表示Z 所有列向量2 范数的平方, 也就是Z 所有元素的平方和, 即为∥Z∥2\\nF, 此即第一个等号的\\n由来; 而根据10.0 中“矩阵的F 范数与迹”中第(3) 个结论, 即对于矩阵Z 有∥Z∥2\\nF = tr\\n\\x00Z⊤Z\\n\\x01\\n= tr\\n\\x00ZZ⊤\\x01\\n,\\n其中tr(·) 表示求矩阵的迹, 即对角线元素之和, 此即第二个等号的由来; 第三个等号将转置化简即得。\\n到此即得式(10.15) 的目标函数, 约束条件W⊤W = I 已在推导中说明。\\n式(10.15) 的目标函数式(10.14) 结果略有差异, 接下来推导Pm\\ni=1 xix⊤\\ni = XX⊤以弥补这个差异（这\\n个结论可以记下来)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n先化简Pm\\ni=1 xix⊤\\ni , 首先:\\nxix⊤\\ni =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nxi1\\nxi2\\n...\\nxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nh\\nxi1\\nxi2\\n· · ·\\nxid\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nxi1xi1\\nxi1xi2\\n· · ·\\nxi1xid\\nxi2xi1\\nxi2xi2\\n· · ·\\nxi2xid\\n...\\n...\\n...\\n...\\nxidxi1\\nxidxi2\\n· · ·\\nxidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n整体代入求和号Pm\\ni=1 xix⊤\\ni , 得\\nm\\nX\\ni=1\\nxix⊤\\ni =\\nm\\nX\\ni=1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nxi1xi1\\nxi1xi2\\n· · ·\\nxi1xid\\nxi2xi1\\nxi2xi2\\n· · ·\\nxi2xid\\n...\\n...\\n...\\n...\\nxidxi1\\nxidxi2\\n· · ·\\nxidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n...\\n...\\n...\\n...\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n再化简XX⊤∈Rd×d:\\nXX⊤=\\nh\\nx1\\nx2\\n· · ·\\nxd\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx⊤\\n1\\nx⊤\\n2\\n...\\nx⊤\\nd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n将列向量xi = (xi1; xi2; . . . ; xid) ∈Rd×1 代入:\\nXX⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\n· · ·\\nxm1\\nx12\\nx22\\n· · ·\\nxm2\\n...\\n...\\n...\\n...\\nx1d\\nx2d\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n•\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\n· · ·\\nx1d\\nx21\\nx22\\n· · ·\\nx2d\\n...\\n...\\n...\\n...\\nxm1\\nxm2\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nm×d\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n...\\n...\\n...\\n...\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n综合Pm\\ni=1 xix⊤\\ni 和XX⊤的化简结果, 即Pm\\ni=1 xix⊤\\ni = XX⊤(协方差矩阵)。根据刚刚推导得到的结\\n论, 式(10.14) 最后的结果即可化为式(10.15) 的目标函数:\\ntr\\n \\nW⊤\\n m\\nX\\ni=1\\nxix⊤\\ni\\n!\\nW\\n!\\n= tr\\n\\x00W⊤XX⊤W\\n\\x01\\n式(10.15) 描述的优化问题的求解详见式(10.17) 最后的解释。\\n10.5.2\\n式(10.16) 的解释\\n先说什么是方差：\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于包含n 个样本的一组数据X = {x1, x2, . . . , xn} 来说, 均值M 为\\nM = x1 + x2 + . . . + xn\\nn\\n=\\nn\\nX\\ni=1\\nxi\\n则方差σ2\\nX 公式为\\nσ2 = (x1 −M)2 + (x2 −M)2 + . . . + (xn −M)2\\nn\\n= 1\\nn\\nn\\nX\\ni=1\\n(xi −M)2\\n方差衡量了该组数据偏离均值的程度; 样本越分散, 其方差越大。\\n再说什么是协方差:\\n若还有包含n 个样本的另一组数据X′ = {x′\\n1, x′\\n2, . . . , x′\\nn}, 均值为M ′, 则下式\\nσ2\\nXX′ = (x1 −M) (x′\\n1 −M ′) + (x2 −M) (x′\\n2 −M ′) + . . . + (xn −M) (x′\\nn −M ′)\\nn\\n= 1\\nn\\nn\\nX\\ni=1\\n(xi −M) (x′\\ni −M ′)\\n称为两组数据的协方差。σ2\\nXX′ 能说明第一组数据x1, x2, . . . , xn 和第二组数据x′\\n1, x′\\n2, . . . , x′\\nn 的变\\n化情况。具体来说, 如果两组数据总是同时大于或小于自己的均值, 则(xi −M) (x′\\ni −M ′) > 0, 此时\\nσ2\\nXX′ > 0; 如果两组数据总是一个大于(或小于) 自己的均值而别一个小于(或大于) 自己的均值, 则\\n(xi −M) (x′\\ni −M ′) < 0, 此时σ2\\nXX′ < 0; 如果两组数据与自己的均值的大小关系无规律, 则(xi −M) (x′\\ni −M ′)\\n的正负号随机变化, 其平均数σ2\\nXX, 则会趋近于0 。引用百度百科协方差词条原话: “从直观上来看, 协方\\n差表示的是两个变量总体误差的期望。如果两个变量的变化趋势一致, 也就是说如果其中一个大于自身的\\n期望值时另外一个也大于自身的期望值, 那么两个变量之间的协方差就是正值; 如果两个变量的变化趋势\\n相反, 即其中一个变量大于自身的期望值时另外一个却小于自身的期望值, 那么两个变量之间的协方差就\\n是负值。如果两个变量是统计独立的, 那么二者之间的协方差就是0 , 但是, 反过来并不成立。协方差为0\\n的两个随机变量称为是不相关的。”\\n最后说什么是协方差矩阵：\\n结合本书中的符号:\\nX = (x1, x2, . . . , xm) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\n· · ·\\nxm1\\nx12\\nx22\\n· · ·\\nxm2\\n...\\n...\\n...\\n...\\nx1d\\nx2d\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n矩阵X 每一行表示一维特征, 每一列表示该数据集的一个样本; 而本节开始已假定数据样本进行了中心化,\\n即Pm\\ni=1 xi = 0 ∈Rd×1 (中心化过程可通过X\\n\\x00I −1\\nm11⊤) 实现, 其中I ∈Rm×m 为单位阵, 1 ∈Rm×1 为\\n全1 列向量, 参见习题10.3), 即上式矩阵的每一行平均值等于零(其实就是分别对所有xi 的每一维坐标\\n进行中心化, 而不是分别对单个样本xi 中心化）对于包含d 个特征的特征空间（或称d 维特征空间）来\\n说, 每一维特征可以看成是一个随机变量, 而X 中包含m 个样本, 也就是说每个随机变量有m 个数据, 根\\n据前面XX⊤的矩阵表达形式:\\n1\\nmXX⊤= 1\\nm\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n...\\n...\\n...\\n...\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n根据前面的结果知道\\n1\\nmXX⊤的第i 行第j 列的元素表示X 中第i 行和X⊤第j 列（即X 中第j 行）\\n的方差(i = j) 或协方差(i ̸= j) 。注意: 协方差矩阵对角线元素为各行的方差。\\n接下来正式解释式(10.16): 对于X = (x1, x2, . . . , xm) ∈Rd×m, 将其投影为Z = (z1, z2, . . . , zm) ∈\\nRd′×m, 最大可分性出发, 我们希望在新空间的每一维坐标轴上样本都尽可能分散(即每维特征尽可能分\\n散, 也就是Z 各行方差最大; 参见图10.4 所示, 原空间只有两维坐标, 现考虑降至一维, 希望在新坐标系\\n下样本尽可能分散, 图中画出了一种映射后的坐标系, 显然橘红色坐标方向样本更分散, 方差更大), 即寻\\n找W ∈Rd×d′ 使协方差矩阵\\n1\\nmZZ⊤对角线元素之和(矩阵的迹）最大（即使Z 各行方差之和最大), 由\\n于Z = W⊤X, 而常系数\\n1\\nm 在最大化时并不发生影响, 求矩阵对角线元素之和即为矩阵的迹, 综上即得式\\n(10.16)。\\n另外, 中心化后X 的各行均值为零, 变换后Z = W⊤X 的各行均值仍为零, 这是因为Z 的第i 行\\n(1 ⩽i ⩽d′) 为\\n\\x08\\nw⊤\\ni x1, w⊤\\ni x2, . . . , w⊤\\ni xm\\n\\t\\n, 该行之和w⊤\\ni\\nPm\\nj=1 xj = w⊤\\ni 0 = 0 。\\n最后, 有关方差的公式, 有人认为应该除以样本数量m, 有人认为应该除以样本数量减1 即m −1 。\\n简单来说, 根据总体样本集求方差就除以总体样本数量, 而根据抽样样本集求方差就除以抽样样本集数量\\n减1; 总体样本集是真正想调查的对象集合, 而抽样样本集是从总体样本集中被选出来的部分样本组成的\\n集合, 用来估计总体样本集的方差; 一般来说, 总体样本集是不可得的, 我们拿到的都是抽样样本集。严格\\n上来说，样本方差应该除以n −1 才会得到总体样本的无偏估计, 若除以n 则得到的是有偏估计。\\n式(10.16) 描述的优化问题的求解详见式(10.17) 最后的解释。\\n10.5.3\\n式(10.17) 的推导\\n由式（10.15）可知，主成分分析的优化目标为\\nmin\\nW\\n−tr (WTXXTW)\\ns.t.\\nWTW = I\\n其中，X = (x1, x2, . . . , xm) ∈Rd×m, W = (w1, w2, . . . , wd′) ∈Rd×d′，I ∈Rd′×d′ 为单位矩阵。对于带矩\\n阵约束的优化问题，根据[1] 中讲述的方法可得此优化目标的拉格朗日函数为\\nL(W, Θ) = −tr (WTXXTW) + ⟨Θ, WTW −I⟩\\n= −tr (WTXXTW) + tr\\n\\x00ΘT(WTW −I)\\n\\x01\\n其中，Θ ∈Rd′×d′ 为拉格朗日乘子矩阵，其维度恒等于约束条件的维度，且其中的每个元素均为未知的\\n拉格朗日乘子，⟨Θ, WTW−I⟩= tr\\n\\x00ΘT(WTW −I)\\n\\x01\\n为矩阵的内积[2]。若此时仅考虑约束wT\\ni wi = 1(i =\\n1, 2, ..., d′)，则拉格朗日乘子矩阵Θ 此时为对角矩阵，令新的拉格朗日乘子矩阵为Λ = diag(λ1, λ2, ..., λd′) ∈\\nRd′×d′，则新的拉格朗日函数为\\nL(W, Λ) = −tr (WTXXTW) + tr\\n\\x00ΛT(WTW −I)\\n\\x01\\n对拉格朗日函数关于W 求导可得\\n∂L(W, Λ)\\n∂W\\n=\\n∂\\n∂W\\n\\x02\\n−tr (WTXXTW) + tr\\n\\x00ΛT(WTW −I)\\n\\x01\\x03\\n= −\\n∂\\n∂W tr (WTXXTW) +\\n∂\\n∂W tr\\n\\x00ΛT(WTW −I)\\n\\x01\\n由矩阵微分公式\\n∂\\n∂X tr (XTBX) = BX + BTX,\\n∂\\n∂X tr\\n\\x00BXTX\\n\\x01\\n= XBT + XB 可得\\n∂L(W, Λ)\\n∂W\\n= −2XXTW + WΛ + WΛT\\n= −2XXTW + W(Λ + ΛT)\\n= −2XXTW + 2WΛ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n令\\n∂L(W, Λ)\\n∂W\\n= 0 可得\\n−2XXTW + 2WΛ = 0\\nXXTW = WΛ\\n将W 和Λ 展开可得\\nXXTwi = λiwi,\\ni = 1, 2, ..., d′\\n显然，此式为矩阵特征值和特征向量的定义式，其中λi, wi 分别表示矩阵XXT 的特征值和单位特征\\n向量。由于以上是仅考虑约束wT\\ni wi = 1 所求得的结果，而wi 还需满足约束wT\\ni wj = 0(i ̸= j)。观察\\nXXT 的定义可知，XXT 是一个实对称矩阵，实对称矩阵的不同特征值所对应的特征向量之间相互正交，\\n同一特征值的不同特征向量可以通过施密特正交化使其变得正交，所以通过上式求得的wi 可以同时满足\\n约束wT\\ni wi = 1, wT\\ni wj = 0(i ̸= j)。根据拉格朗日乘子法的原理可知，此时求得的结果仅是最优解的必要\\n条件，而且XXT 有d 个相互正交的单位特征向量，所以还需要从这d 个特征向量里找出d′ 个能使得目\\n标函数达到最优值的特征向量作为最优解。将XXTwi = λiwi 代入目标函数可得\\nmin\\nW −tr (WTXXTW) = max\\nW\\ntr (WTXXTW)\\n= max\\nW\\nd′\\nX\\ni=1\\nwT\\ni XXTwi\\n= max\\nW\\nd′\\nX\\ni=1\\nwT\\ni · λiwi\\n= max\\nW\\nd′\\nX\\ni=1\\nλiwT\\ni wi\\n= max\\nW\\nd′\\nX\\ni=1\\nλi\\n显然，此时只需要令λ1, λ2, ..., λd′ 和w1, w2, . . . , wd′ 分别为矩阵XXT 的前d′ 个最大的特征值和单\\n位特征向量就能使得目标函数达到最优值。\\n10.5.4\\n根据式(10.17) 求解式(10.16)\\n注意式(10.16) 中W ∈Rd×d′, 只有d′ 列, 而式(10.17) 可以得到d 列, 如何根据式(10.17) 求解式\\n(10.16) 呢? 对XX⊤W = WΛ 两边同乘W⊤, 得\\nW⊤XX⊤W = W⊤WΛ = Λ\\n注意使用了约束条件W⊤W = I; 上式左边与式(10.16) 的优化目标对应矩阵相同, 而右边Λ ∈Rd′×d′\\n是由XXX⊤的d′ 个特征值组成的对角阵, 两边同时取矩阵的迹, 得\\ntr\\n\\x00W⊤XX⊤W\\n\\x01\\n= tr(Λ) =\\nd′\\nX\\ni=1\\nλi\\nd 个特征值, 因此当然是取出最大的前d′ 个特征值, 而W 即特征值对应的标准化特征向量组成的矩阵。\\n特别注意, 图10.5 只是得到了投影矩阵W, 而降维后的样本为Z = W⊤X。\\n10.6\\n核化线性降维\\n注意, 本节符号在第14 次印刷中进行了修订, 另外有一点需要注意的是，在上一节中用zi 表示xi 降\\n维后的像, 而本节用zi 表示xi 在高维特征空间中的像。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n本节推导实际上有一个前提, 以式(10.19) 为例（式(10.21) 仅将zi 换为ϕ (xi) 而已), 那就是zi 已\\n经中心化(计算方差要用样本减去均值, 式(10.19) 是均值为零时特殊形式, 详见式(10.16) 的解释), 但\\nzi = ϕ (xi) 是xi 高维特征空间中的像, 即使xi 已进行中心化, 但zi 却不一定是中心化的, 此时本节推导\\n均不再成立。推广工作详见KPCA[3] 的附录A。\\n10.6.1\\n式(10.19) 的解释\\n首先, 类似于式(10.14) 的推导后半部分内容可知Pm\\ni=1 ziz⊤\\ni = ZZ⊤, 其中Z 的每一列为一个样本, 设\\n高维空间的维度为h, 则Z ∈Rh×m, 其中m 为数据集样本数量。\\n其次, 式(10.19) 中的W 为从高维空间降至低维(维度为d ) 后的正交基, 在第14 次印刷中加入表述\\nW = (w1, w2, . . . , wd), 其中W ∈Rh×d, 降维过程为X = W⊤Z 。\\n最后, 式(10.19) 类似于式(10.17), 是为了求解降维投影矩阵W = (w1, w2, . . . , wd) 。但问题在于\\nZZ⊤∈Rh×h, 当维度h 很大时(注意本节为核化线性降维, 第六章核方法中高斯核会把样本映射至无穷\\n维), 此时根本无法求解Z⊤的特征值和特征向量。因此才有了后面的式(10.20)。\\n第14 次印刷及之后印次, 式(10.19) 为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01\\nwj = λjwj, 而在之前的印次中表达有误, 实际应\\n该为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01\\nW = WΛ, 类似于式(10.17)。而这两种表达本质相同, λjwj 为WΛ 的第j 列, 仅此而\\n已。\\n10.6.2\\n式(10.20) 的解释\\n本节为核化线性降维, 而式(10.19) 是在维度为h 的高维空间运算, 式(10.20) 变形（咋一看似乎有点\\n无厘头) 的目的是为了避免直接在高维空间运算, 即想办法能够使用式(6.22) 的核技巧, 也就是后面的式\\n(10.24)。\\n第14 次印刷及之后印次该式没问题, 之前的式(10.20) 应该是:\\nW =\\n m\\nX\\ni=1\\nziz⊤\\ni\\n!\\nWΛ−1 =\\nm\\nX\\ni=1\\n\\x00zi\\n\\x00z⊤\\ni WΛ−1\\x01\\x01\\n=\\nm\\nX\\ni=1\\n(ziαi)\\n其中αi = z⊤\\ni WΛ−1 ∈R1×d, z⊤\\ni ∈R1×h, W ∈Rh×d, Λ ∈Rd×d 为对角阵。这个结果看似等号右侧也包含\\nW, 但将此式代入式(10.19) 后经化简可避免在高维空间的运算, 而将目标转化为求低维空间的αi ∈R1×d,\\n详见式(10.24) 的推导。\\n10.6.3\\n式(10.21) 的解释\\n该式即为将式(10.19) 中的zi 换为ϕ (xi) 的结果。\\n10.6.4\\n式(10.22) 的解释\\n该式即为将式(10.20) 中的zi 换为ϕ (xi) 的结果。\\n10.6.5\\n式(10.24) 的推导\\n已知zi = ϕ(xi)，类比X = {x1, x2, ..., xm} 可以构造Z = {z1, z2, ..., zm}，所以公式(10.21) 可变换\\n为\\n m\\nX\\ni=1\\nϕ(xi)ϕ(xi)T\\n!\\nwj =\\n m\\nX\\ni=1\\nzizT\\ni\\n!\\nwj = ZZTwj = λjwj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n又由公式(10.22) 可知\\nwj =\\nm\\nX\\ni=1\\nϕ (xi) αj\\ni =\\nm\\nX\\ni=1\\nziαj\\ni = Zαj\\n其中，αj = (αj\\n1; αj\\n2; ...; αj\\nm) ∈Rm×1。所以公式(10.21) 可以进一步变换为\\nZZTZαj = λjZαj\\nZZTZαj = Zλjαj\\n由于此时的目标是要求出wj，也就等价于要求出满足上式的αj，显然，此时满足ZTZαj = λjαj 的αj\\n一定满足上式，所以问题转化为了求解满足下式的αj：\\nZTZαj = λjαj\\n令ZTZ = K，那么上式可化为\\nKαj = λjαj\\n此式即为公式(10.24)，其中矩阵K 的第i 行第j 列的元素(K)ij = zT\\ni zj = ϕ(xi)Tϕ(xj) = κ (xi, xj)\\n10.6.6\\n式(10.25) 的解释\\n式(10.25) 仅需将第14 次印刷中式(10.22) 的wj 表达式转置后代入即可。\\n该式的意义在于, 求解新样本x ∈Rd×1 映射至高维空间ϕ(x) ∈Rh×1 后再降至低维空高维空间Rh×1\\n的运算。但是由于此处没有类似第6 章支持向量的概念, 可以发现式(10.25) 计算时需要对所有样本求和,\\n因此它的计算开销比较大。\\n注意, 此处书中符号使用略有混乱, 因为在式(10.19) 中zi 表示xi 在高维特征空间中的像, 而此处又\\n用zj 表示新样本x 映射为ϕ(x) 后再降维至Rd′×1 空间时的第j 维坐标。\\n10.7\\n流形学习\\n不要被“流形学习”的名字所欺骗, 本节开篇就明确说了, 它是一类借鉴了拓扑流形概念的降维方法而\\n已, 因此称为“流形学习”。10.2 节MDS 算法的降维准则是要求原始空间中样本之间的距离在低维空间中\\n得以保持, 10.3 节PCA 算法的降维准则是要求低维子空间对样本具有最大可分性, 因为它们都是基于线\\n性变换来进行降维的方法（参见式(10.13)，故称为线性降维方法。\\n10.7.1\\n等度量映射(Isomap) 的解释\\n如图“西瓜书”10.8 所示, Isomap 算法与MDS 算法的区别仅在于距离矩阵D ∈Rm×m 的计算方法\\n不同。在MDS 算法中, 距离矩阵D ∈Rm×m 即为普通的样本之间欧氏距离; 而本节的Isomap 算法中, 距\\n离矩阵D ∈Rm×m 由“西瓜书”图10.8 的Step1 ~Step5 生成, 即遵循流形假设。当然, 对新样本降维时\\n也有不同, 这在“西瓜书”图10.8 下的一段话中已阐明。\\n另外解释一下测地线距离, 欧氏距离即两点之间的直线距离, 而测地线距离是实际中可以到达的路径,\\n如“西瓜书”图10.7(a) 中黑线(欧氏距离) 和红线(测地线距离)。\\n10.7.2\\n式(10.28) 的推导\\nwij =\\nP\\nk∈Qi\\nC−1\\njk\\nP\\nl,s∈Qi\\nC−1\\nls\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由书中上下文可知，式(10.28) 是如下优化问题的解。\\nmin\\nw1,w2,...,wm\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\rxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\r\\r\\r\\r\\n2\\n2\\ns.t.\\nX\\nj∈Qi\\nwij = 1\\n若令xi ∈Rd×1, Qi = {q1\\ni , q2\\ni , ..., qn\\ni }，则上述优化问题的目标函数可以进行如下恒等变形\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\rxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\r\\r\\r\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\r\\nX\\nj∈Qi\\nwijxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\r\\r\\r\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\r\\nX\\nj∈Qi\\nwij(xi −xj)\\n\\r\\r\\r\\r\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n∥Xiwi∥2\\n2\\n=\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi\\n其中wi = (wiq1\\ni , wiq2\\ni , ..., wiqn\\ni ) ∈Rn×1，Xi =\\n\\x00xi −xq1\\ni , xi −xq2\\ni , ..., xi −xqn\\ni\\n\\x01\\n∈Rd×n。同理，约束条件\\n也可以进行如下恒等变形\\nX\\nj∈Qi\\nwij = wi\\nTI = 1\\n其中I = (1, 1, ..., 1) ∈Rn×1 为n 行1 列的元素值全为1 的向量。因此，上述优化问题可以重写为\\nmin\\nw1,w2,...,wm\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi\\ns.t. wi\\nTI = 1\\n显然，此问题为带约束的优化问题，因此可以考虑使用拉格朗日乘子法来进行求解。由拉格朗日乘子法可\\n得此优化问题的拉格朗日函数为\\nL(w1, w2, . . . , wm, λ) =\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi + λ\\n\\x00wi\\nTI −1\\n\\x01\\n对拉格朗日函数关于wi 求偏导并令其等于0 可得\\n∂L(w1, w2, . . . , wm, λ)\\n∂wi\\n=\\n∂\\n\\x02Pm\\ni=1 wiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n=\\n∂\\n\\x02\\nwiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n又由矩阵微分公式\\n∂xT Bx\\n∂x\\n=\\n\\x00B + BT\\x01\\nx,\\n∂xT a\\n∂x\\n= a 可得\\n∂\\n\\x02\\nwiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 2XT\\ni Xiwi + λI = 0\\nXT\\ni Xiwi = −1\\n2λI\\n若XT\\ni Xi 可逆，则\\nwi = −1\\n2λ(XT\\ni Xi)−1I\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n又因为wiTI = ITwi = 1，则上式两边同时左乘IT 可得\\nITwi = −1\\n2λIT(XT\\ni Xi)−1I = 1\\n−1\\n2λ =\\n1\\nIT(XT\\ni Xi)−1I\\n将其代回wi = −1\\n2λ(XT\\ni Xi)−1I 即可解得\\nwi =\\n(XT\\ni Xi)−1I\\nIT(XT\\ni Xi)−1I\\n若令矩阵(XT\\ni Xi)−1 第j 行第k 列的元素为C−1\\njk ，则\\nwij = wiqj\\ni =\\nP\\nk∈Qi\\nC−1\\njk\\nP\\nl,s∈Qi\\nC−1\\nls\\n此即为公式(10.28)。显然，若XT\\ni Xi 可逆，此优化问题即为凸优化问题，且此时用拉格朗日乘子法求得的\\nwi 为全局最优解。\\n10.7.3\\n式(10.31) 的推导\\n以下推导需要使用预备知识中的10.2节: 矩阵的F 范数与迹。\\n观察式(10.29), 求和号内实际是一个列向量的2 范数平方, 令vi = zi −P\\nj∈Qi wijzj, vi 的维度与zi\\n相同, vi ∈Rd′×1, 则式(10.29) 可重写为\\nmin\\nz1,z2,...,zm\\nm\\nX\\ni=1\\n∥vi∥2\\n2\\ns.t. vi = zi −\\nX\\nj∈Qi\\nwijzj, i = 1, 2, . . . , m\\n令Z = (z1, z2, . . . , zi, . . . , zm) ∈Rd′×m, Ii = (0; 0; . . . ; 1; . . . ; 0) ∈Rm×1, 即Ii 为m × 1 的列向量, 除\\n第i 个元素等于1 之外其余元素均为零, 则\\nzi = ZIi\\n令(W)ij = wij （P237 页第1 行), 即W = (w1, w2, . . . , wi, . . . , wm)⊤∈Rm×m, 也就是说W 的第i 行的\\n转置（没错, 就是第i 行) 对应第i 个样数wi （这里符号之所以别扭是因为wij 已用来表示列向量wi 的\\n第j 个元素, 但为了与习惯保持一致即wij 表示W 的第i 行第j 列元素, 只能忍忍, 此处暂时别扭着），即\\nW = (w1, w2, . . . , wi, . . . , wm)⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nw11\\nw21\\n· · ·\\nwi1\\n· · ·\\nwm1\\nw12\\nw22\\n· · ·\\nwi2\\n· · ·\\nwm2\\n...\\n...\\n...\\n...\\n...\\n...\\nw1j\\nw2j\\n· · ·\\nwij\\n· · ·\\nwmj\\n...\\n...\\n...\\n...\\n...\\n...\\nw1m\\nw2m\\n· · ·\\nwim\\n· · ·\\nwmm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n对于wi ∈Rm×1 来说, 只有xi 的K 个近邻样本对应的下标对应的wij ̸= 0, j ∈Qi, 且它们的和等于\\n1 , 则\\nX\\nj∈Qi\\nwijzj = Zwi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n因此\\nvi = zi −\\nX\\nj∈Qi\\nwijzj = ZIi −Zwi = Z (Ii −wi)\\n令V = (v1, v2, . . . , vi, . . . , vm) ∈Rd′×m, I = (I1, I2, . . . , Ii, . . . , Im) ∈Rm×m, 则\\nV = Z\\n\\x00I −W⊤\\x01\\n= Z\\n\\x00I⊤−W⊤\\x01\\n= Z(I −W)⊤\\n根据前面的预备知识, 并将上式V 和式(10.30) 代入, 得式(10.31) 目标函数:\\nm\\nX\\ni=1\\n∥vi∥2\\n2 = ∥V∥2\\nF\\n= tr\\n\\x00VV⊤\\x01\\n= tr\\n\\x10\\x00Z(I −W)⊤\\x01 \\x00Z(I −W)⊤\\x01⊤\\x11\\n= tr\\n\\x00Z(I −W)⊤(I −W)Z⊤\\x01\\n= tr\\n\\x00ZMZZ⊤\\x01\\n接下来求解式(10.31)。\\n参考式(10.17) 的推导, 应用拉格朗日乘子法, 先写出拉格朗日函数\\nL(Z, Λ) = tr\\n\\x00ZMZ⊤\\x01\\n+\\n\\x00ZZ⊤−I\\n\\x01\\nΛ\\n令P = Z⊤(否则有点别扭), 则拉格朗日函数变为\\nL(P, Λ) = tr\\n\\x00P⊤MP\\n\\x01\\n+\\n\\x00P⊤P −I\\n\\x01\\n\\uffff\\n求导并令导数等于0 :\\n∂L(P, Λ)\\n∂P\\n= ∂tr\\n\\x00P⊤MP\\n\\x01\\n∂P\\n+ ∂\\n\\x00P⊤P −I\\n\\x01\\n∂P\\nΛ\\n= 2MP −2PΛ = 0\\n特征值对角阵; 然后两边再同时左乘P⊤并取矩阵的迹, 注意P⊤P = I ∈Rd′×d′, 得tr\\n\\x00P⊤MP\\n\\x01\\n=\\ntr\\n\\x00P⊤PΛ\\n\\x01\\n= tr(Λ) 因此, P = Z⊤是由M ∈Rm×m 最小的d′ 个特征值对应的特征向量组成的矩阵。\\n10.8\\n度量学习\\n回忆10.5.1 节的Isomap 算法相比与10.2 节的MDS 算法的区别在于距离矩阵的计算方法不同，\\nIsomap 算法在计算样本间距离时使用的（近似）测地线距离，而MDS 算法使用的是欧氏距离，也就是说\\n二者的距离度量不同。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.8.1\\n式(10.34) 的解释\\n为了推导方便, 令u = (u1; u2; . . . ; ud) = xi −xj ∈Rd×1, 其中uk = xik −xjk, 则式(10.34) 重写为\\nu⊤Mu = ∥u∥2\\nM, 其中M ∈Rd×d, 具体到元素级别的表达:\\nu⊤Mu =\\nh\\nu1\\nu2\\n. . .\\nud\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nm11\\nm12\\n. . .\\nm1d\\nm21\\nm22\\n. . .\\nm2d\\n...\\n...\\n...\\n...\\nmd1\\nmd2\\n. . .\\nmdd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nu1\\nu2\\n...\\nud\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\nh\\nu1\\nu2\\n. . .\\nud\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nu1m11 + u2m12 + . . . + udm1d\\nu1m21 + u2m22 + . . . + udm2d\\n...\\nu1md1 + u2md2 + . . . + udmdd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= u1u1m11 + u1u2m12 + . . . + u1udm1d\\n+ u2u1m21 + u2u2m22 + . . . + u2udm2d\\n. . .\\n+ udu1md1 + udu2md2 + . . . + ududmdd\\n注意, 对应到本式符号, 式(10.33) 的结果即为上面最后一个等式的对角线部分, 即\\nu1u1m11 + u2u2m22 + . . . + ududmdd\\n而式(10.32) 的结果则要更进一步, 去除对角线部分中的权重mii(1 ⩽i ⩽d) 部分, 即\\nu1u1 + u2u2 + . . . + udud\\n对比以上三个结果, 即式(10.32) 的平方欧氏距离, 式(10.33) 的加权平方欧氏距离, 式(10.34) 的马氏距\\n离, 可以细细体会度量矩阵究竟带来了什么。\\n因此, 所谓“度量学习”, 即将系统中的平方欧氏距离换为式(10.34) 的马氏距离, 通过优化某个目标函\\n数, 得到最恰当的度量矩阵M （新的距离度量计算方法）的过程。书中在式(10.34) (10.38) 介绍的NCA\\n即为一个具体的例子, 可以从中品味“度量学习”的本质。\\n对于度量矩阵M 要求半正定, 文中提到必有正交基P 使得M 能写为M = PP⊤, 此时马氏距离\\nu⊤Mu = u⊤PP⊤u =\\n\\r\\rP⊤u\\n\\r\\r2\\n2 。\\n10.8.2\\n式(10.35) 的解释\\n这就是一种定义而已，没什么别的意思。传统近邻分类器使用多数投票法，有投票权的样本为xi 最近\\n的K 个近邻, 即KNN; 但也可以将投票范围扩大到整个样本集, 但每个样本的投票权重不一样，距离xi 越\\n近的样本投票权重越大，例如可取为第5 章式(5.19) 当βi = 1 时的高斯径向基函数exp\\n\\x10\\n−∥xi −xj∥2\\x11\\n。从式中可以看出, 若xj 与xi 重合, 则投票权重为1 , 距离越大该值越小。式(10.35) 的分母是对所有投\\n票值规一化至[0, 1] 范围, 使之为概率。\\n可能会有疑问：式(10.35) 分母求和变量l 是否应该包含xi 的下标即l = i ? 其实无所谓, 进一步说\\n其实是否进行规一化也无所谓, 熟悉KNN 的话就知道, 在预测时是比较各类投票数的相对大小, 各类样本\\n对xi 的投票权重的分母在式(10.35) 中相同, 因此不影响相对大小。\\n注意啊, 这里有计算投票权重时用到了距离度量, 所以可以进一步将其换为马氏距离, 通过优化某个目\\n标(如式(10.38)）得到最优的度量矩阵M。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.8.3\\n式(10.36) 的解释\\n先简单解释留一法(LOO), KNN 是选出样本xi 的在样本集中最近的K 个近邻, 而现在将范围扩大,\\n使用样本集中的所有样本进行投票, 每个样本的投票权重为式(10.35), 将各类样本的投票权重分别求和,\\n注意xi 自己的类别肯定与自己相同（现在是训练阶段, 还没到对末见样本的预测阶段, 训练集样本的类别\\n信息均已知), 但自己不能为自己投票吧, 所以要将自己除外, 即留一法。\\n假设训练集共有N 个类别, Ωn 表示第n 类样本的下标集合(1 ⩽n ⩽N), 对于样本xi 来说, 可以分\\n别计算N 个概率:\\npxi\\nn =\\nX\\nj∈Ωn\\npij, 1 ⩽n ⩽N\\n注意, 若样本xi 的类别为n∗, 则在根据上式计算pxi\\nn∗时要将xi 的下标去除, 即刚刚解释的留一法(自己\\n不能为自己投票)。pxi\\nn∗即为训练集将样本xi 预测为第n∗类的概率, 若pxi\\nn∗在所有的pxi\\nn (1 ⩽n ⩽N) 中\\n最大, 则预测正确, 反之预测错误。\\n其中pxi\\nn∗即为式(10.36)。\\n10.8.4\\n式(10.37) 的解释\\n换为刚才式(10.36) 的符号, 式(10.37) 即为Pm\\ni=1 pxi\\nn∗, 也就是所有训练样本被训练集预测正确的概率\\n之和。我们当然希望这个概率和最大, 但若采用平方欧氏距离时, 对于某个训练集来说这个概率和是固定\\n的; 但若采用了马氏距离, 这个概率和与度量矩阵M 有关。\\n10.8.5\\n式(10.38) 的解释\\n刚才式(10.37) 中提到希望寻找一个度量矩阵M 使训练样本被训练集预测正确的概率之和最大, 即\\nmaxM\\nPm\\ni=1 pxi\\nn∗, 但优化问题习惯是最小化, 所以改为minM −Pm\\ni=1 pxi\\nn∗即可, 而式(10.38) 目标函数中的\\n常数1 并不影响优化结果, 有没有无所谓的。\\n式(10.38) 中有关将M = PP⊤代入的形式参见前面式(10.34) 的解释最后一段。\\n10.8.6\\n式(10.39) 的解释\\n式(10.39) 是本节第二个“度量学习”的具体例子。优化目标函数是要求必连约束集合M 中的样本\\n对之间的距离之和尽可能的小，而约束条件则是要求勿连约束集合C 中的样本对之间的距离之和大于1 。\\n这里的“1”应该类似于第6 章SVM 中间隔大于“1”, 纯属约定, 没有推导。\\n参考文献\\n[1] Michael Grant. Lagrangian optimization with matrix constrains, 2015.\\n[2] Wikipedia contributors. Frobenius inner product, 2020.\\n[3] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Kernel principal component analy-\\nsis. In Artificial Neural Networks—ICANN’97: 7th International Conference Lausanne, Switzerland,\\nOctober 8–10, 1997 Proceeedings, pages 583–588. Springer, 2005.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第11 章\\n特征选择与稀疏学习\\n11.1\\n子集搜索与评价\\n开篇给出了“特征选择”的概念, 并谈到特征选择与第10 章的降维有相似的动机。特征选择与降维的\\n区别在于特征选择是从所有特征中简单地选出相关特征, 选择出来的特征就是原来的特征; 降维则对原来\\n的特征进行了映射变换, 降维后的特征均不再是原来的特征。\\n本节涉及“子集评价”的式(14.1) 和式(14.2) 与第4 章的式(4.2) 和式(4.1) 相同, 这是因为“决策\\n树算法在构建树的同时也可看作进行了特征选择”(参见“11.7 阅读材料”)。接下来在11.2 节、11.3 节、\\n11.4 节分别介绍的三类特征选择方法: 过滤式(filter)、包裹式(wrapper) 和嵌入式(embedding)。\\n11.1.1\\n式(11.1) 的解释\\n此为信息熵的定义式，其中pk, k = 1, 2, . . . |Y| 表示D 中第i 类样本所占的比例。可以看出，样本越\\n纯，即pk →0 或pk →1 时，Ent(D) 越小，其最小值为0（约定0 log2 0 = 0）。\\n11.1.2\\n式(11.2) 的解释\\nEnt(D) = −\\n|Y|\\nX\\ni=1\\npk log2 pk\\n此为信息熵的定义式，其中pk, k = 1, 2, . . . |Y| 表示D 中第i 类样本所占的比例。可以看出，样本越纯，\\n即pk →0 或pk →1 时，Ent(D) 越小，其最小值为0。此时必有pi = 1, p\\\\i = 0, i = 1, 2, . . . , |Y|。\\n11.2\\n过滤式选择\\n“过滤式方法先对数据集进行特征选择, 然后再训练学习器, 特征选择过程与后续学习器无关。这相当\\n于先用特征选择过程对初始特征进行’ 过滤’, 再用过滤后的特征来训练模型。”, 这是本节开篇第一段原话,\\n之所以重写于此, 是因为这段话里包含了“过滤”的概念, 该概念并非仅针对特征选择, 那些所有先对数据\\n集进行某些预处理, 然后基于预处理结果再训练学习器的方法(预处理过程独立于训练学习器过程) 均可\\n以称之为“过滤式算法”。特别地, 本节介绍的Relief 方法只是过滤式特征选择方法的其中一种而已。\\n从式(11.3) 可以看出, Relief 方法本质上基于“空间上相近的样本具有相近的类别标记”假设。Relief\\n基于样本与同类和异类的最近邻之间的距离来计算相关统计量δj, 越是满足前提假设, 原则上样本与同类\\n最近邻之间的距离diff\\n\\x00xj\\ni, xj\\ni,nh\\n\\x012 会越小, 样本与异类最近邻之间的距离diff\\n\\x00xj\\ni, xj\\ni, nm\\n\\x012 会越大，因此相\\n关统计量δj 越大，对应属性的分类能力就越强。\\n对于能处理多分类问题的扩展变量Relief-F, 由于有多个异类, 因此对所有异类最近邻进行加权平均,\\n各异类的权重为其在数据集中所占的比例。\\n11.2.1\\n包裹式选择\\n“与过滤式特征选择不考虑后续学习器不同, 包裹式特征选择直接把最终将要使用的学习器的性能作\\n为特征子集的评价准则。换言之, 包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定\\n做’ 的特征子集。”, 这是本节开篇第一段原话, 之所以重写于此, 是因为这段话里包含了“包裹”的概念,\\n该概念并非仅针对特征选择, 那些所有基于学习器的性能作为评价准则对数据集进行预处理的方法(预处\\n理过程依赖训练所得学习器的测试性能）均可以称之为“包裹式算法”。特别地, 本节介绍的LVW 方法只\\n是包裹式特征选择方法的其中一种而已。\\n图11.1 中, 第1 行E = ∞表示初始化学习器误差为无穷大, 以至于第1 轮迭代第9 行的条件就一定\\n为真; 第2 行d = |A| 中的|A| 表示特征集A 的包含的特征个数; 第9 行E′ < E 表示学习器L 在特征子\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n集A′ 上的误差比当前特征子集A 上的误差更小, (E′ = E) ∨(d′ < d) 表示学习器L 在特征子集A′ 上的\\n误差与当前特征子集A 上的误差相当但A′ 中包含的特征数更小; 表示“逻辑与”，V 表示“逻辑或”。注\\n意到, 第5 行至第17 行的while 循环中t 并非一直增加, 当第9 行条件满足时t 会被清零。\\n最后, 本节LVW 算法基于拉斯维加斯方法框架, 可以仔细琢磨体会拉斯维加斯方法和蒙特卡罗方法\\n的区别。一个通俗的解释如下：\\n蒙特卡罗算法: 采样越多, 越近似最优解;\\n拉斯维加斯算法: 采样越多, 越有机会找到最优解;\\n举个例子, 假如筐里有100 个苹果, 让我每次闭眼拿1 个, 挑出最大的。于是我随机拿1 个，再随机拿\\n1 个跟它比，留下大的，再随机拿1 个...... 我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多,\\n挑出的苹果就越大, 但我除非拿100 次, 否则无法肯定挑出了最大的。这个挑苹果的算法, 就属于蒙特卡罗\\n算法一一尽量找好的, 但不保证是最好的。而拉斯维加斯算法, 则是另一种情况。假如有一把锁, 给我100\\n把钥匙, 只有1 把是对的。于是我每次随机拿1 把钥匙去试, 打不开就再换1 把。我试的次数越多, 打开\\n(最优解) 的机会就越大, 但在打开之前, 那些错的钥匙都是没有用的。这个试钥匙的算法, 就是拉斯维加斯\\n的一一尽量找最好的, 但不保证能找到。\\n11.3\\n嵌入式选择与L1 正则化\\n“嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即\\n在学习器训练过程中自动地进行了特征选择。”，具体可以对比本节式(11.7) 的例子与前两节方法的本质\\n区别，细细体会本节第一段的这句有关“嵌入式”的概念描述。\\n11.3.1\\n式(11.5) 的解释\\n该式为线性回归的优化目标式，yi 表示样本i 的真实值，而w⊤xi 表示其预测值，这里使用预测值和\\n真实值差的平方衡量预测值偏离真实值的大小。\\n11.3.2\\n式(11.6) 的解释\\n该式为加入了L2 正规化项的优化目标，也叫“岭回归”，λ 用来调节误差项和正规化项的相对重要性，\\n引入正规化项的目的是为了防止w 的分量过大而导致过拟合的风险。\\n11.3.3\\n式(11.7) 的解释\\n该式将11.6 中的L2 正规化项替换成了L1 正规化项，也叫LASSO 回归。关于L2 和L1 两个正规化\\n项的区别，“西瓜书”图11.2 给出了很形象的解释。具体来说，结合L1 范数优化的模型参数分量取值尽\\n量稀疏，即非零分量个数尽量小，因此更容易取得稀疏解。\\n11.3.4\\n式(11.8) 的解释\\n从本式开始至本节结束, 都在介绍近端梯度下降求解L1 正则化问题。若将本式对应到式(11.7), 则本\\n式中f(w) = Pm\\ni=1\\n\\x00yi −w⊤xi\\n\\x012, 注意变量为w （若感觉不习惯就将其用x 替换好了)。最终推导结果仅\\n含f(w) 的一阶导数∇f(w) = −Pm\\ni=1 2\\n\\x00yi −w⊤xi\\n\\x01\\nxi 。\\n11.3.5\\n式(11.9) 的解释\\n该式即为L-Lipschitz(利普希茨) 条件的定义。简单来说, 该条件约束函数的变化不能太快。将式(11.9)\\n变形则更为直观(注: 式中应该是2 范数, 而非2 范数平方):\\n∥∇f (x′) −∇f(x)∥2\\n∥x′ −x∥2\\n⩽L,\\n(∀x, x′)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n进一步地, 若x′ →x, 即\\nlim\\nx′→x\\n∥∇f (x′) −∇f(x)∥2\\n∥x′ −x∥2\\n这明显是在求解函数∇f(x) 的导数绝对值（模值。因此, 式(11.9) 即要求f(x) 的二阶导数不大于L, 其\\n中L 称为Lipschitz 常数。\\n“Lipschitz 连续”可以形象得理解为: 以陆地为例, Lipschitz 连续就是说这块地上没有特别陡的坡; 其\\n中最陡的地方有多陡呢? 这就是所谓的Lipschitz 常数。\\n11.3.6\\n式(11.10) 的推导\\n首先注意优化目标式和11.7 LASSO 回归的联系和区别，该式中的x 对应到式11.7 的w，即我们优\\n化的目标。再解释下什么是L−Lipschitz 条件，根据维基百科的定义：它是一个比通常连续更强的光滑性\\n条件。直觉上，利普希茨连续函数限制了函数改变的速度，符合利普希茨条件的函数的斜率，必小于一个\\n称为利普希茨常数的实数（该常数依函数而定）。注意这里存在一个笔误，在wiki 百科的定义中，式11.9\\n应该写成\\n|∇f (x′) −∇f(x)| ⩽L |x′ −x|\\n(∀x, x′)\\n移项得\\n|∇f (x′) −∇f(x)|\\n|x′ −x|\\n⩽L\\n(∀x, x′)\\n由于上式对所有的x, x′ 都成立，由导数的定义，上式可以看成是f(x) 的二阶导数恒不大于L。即\\n∇2f(x) ⩽L\\n得到这个结论之后，我们来推导式11.10。由泰勒公式，xk 附近的f(x) 通过二阶泰勒展开式可近似为\\nˆf(x) ≃f (xk) + ⟨∇f (xk) , x −xk⟩+ ∇2f(xk)\\n2\\n∥x −xk∥2\\n⩽f (xk) + ⟨∇f (xk) , x −xk⟩+ L\\n2 ∥x −xk∥2\\n= f (xk) + ∇f (xk)⊤(x −xk) + L\\n2 (x −xk)⊤(x −xk)\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk)⊤(x −xk) + 2\\nL∇f (xk)⊤(x −xk)\\n\\x13\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk)⊤(x −xk) + 2\\nL∇f (xk)⊤(x −xk) + 1\\nL2 ∇f(xk)⊤∇f(xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk) + 1\\nL∇f (xk)\\n\\x13⊤\\x12\\n(x −xk) + 1\\nL∇f (xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)\\n= L\\n2\\n\\r\\r\\r\\rx −\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\r\\r\\r\\r\\n2\\n2\\n+ const\\n其中const = f(xk) −\\n1\\n2L∇f (xk)⊤∇f (xk)\\n11.3.7\\n式(11.11) 的解释\\n这个很容易理解，因为2 范数的最小值为0，当xk+1 = xk −1\\nL∇f (xk) 时，ˆf(xk+1) ⩽ˆf(xk) 恒成立，\\n同理ˆf(xk+2) ⩽ˆf(xk+1), · · · ，因此反复迭代能够使ˆf(x) 的值不断下降。\\n11.3.8\\n式(11.12) 的解释\\n注意ˆf(x) 在式(11.11) 处取得最小值, 因此, 以下不等式肯定成立:\\nˆf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf (xk)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在式(11.10) 推导中有f(x) ⩽ˆf(x) 恒成立, 因此, 以下不等式肯定成立:\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n在式(11.10) 推导中还知道f (xk) = ˆf (xk), 因此\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf (xk) = f (xk)\\n也就是说通过迭代xk+1 = xk −1\\nL∇f (xk) 可以使f(x) 的函数值逐步下降。\\n同理, 对于函数g(x) = f(x) + λ∥x∥1, 可以通过最小化ˆg(x) = ˆf(x) + λ∥x∥1 逐步求解。式(11.12) 就\\n是在最小化ˆg(x) = ˆf(x) + λ∥x∥1◦。\\n以上优化方法被称为Majorization-Minimization。可以搜索相关资料做详细了解。\\n11.3.9\\n式(11.13) 的解释\\n这里将式11.12 的优化步骤拆分成了两步，首先令z = xk −1\\nL∇f (xk) 以计算z，然后再求解式11.13，\\n得到的结果是一致的。\\n11.3.10\\n式(11.14) 的推导\\n令优化函数\\ng(x) = L\\n2 ∥x −z∥2\\n2 + λ∥x∥1\\n= L\\n2\\nd\\nX\\ni=1\\n\\r\\rxi −zi\\r\\r2\\n2 + λ\\nd\\nX\\ni=1\\n\\r\\rxi\\r\\r\\n1\\n=\\nd\\nX\\ni=1\\n\\x12L\\n2\\n\\x00xi −zi\\x012 + λ\\n\\x0c\\x0cxi\\x0c\\x0c\\n\\x13\\n这个式子表明优化g(x) 可以被拆解成优化x 的各个分量的形式，对分量xi，其优化函数\\ng\\n\\x00xi\\x01\\n= L\\n2\\n\\x00xi −zi\\x012 + λ\\n\\x0c\\x0cxi\\x0c\\x0c\\n求导得\\ndg (xi)\\ndxi\\n= L\\n\\x00xi −zi\\x01\\n+ λsgn\\n\\x00xi\\x01\\n其中\\nsign\\n\\x00xi\\x01\\n=\\n(\\n1,\\nxi > 0\\n−1,\\nxi < 0\\n称为符号函数[1]，对于xi = 0 的特殊情况，由于|xi| 在xi = 0 点出不光滑，所以其不可导，需单独\\n讨论。令\\ndg(xi)\\ndxi\\n= 0 有\\nxi = zi −λ\\nL sign\\n\\x00xi\\x01\\n此式的解即为优化目标g(xi) 的极值点，因为等式两端均含有未知变量xi，故分情况讨论。\\n1. 当zi > λ\\nL 时：a. 假设xi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL > 0 与假设矛盾；b. 假设\\nxi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL > 0 和假设相符和，下面来检验xi = zi −λ\\nL 是否是使\\n函数g(xi) 的取得最小值。当xi > 0 时，\\ndg (xi)\\ndxi\\n= L\\n\\x00xi −zi\\x01\\n+ λ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在定义域内连续可导，则g(xi) 的二阶导数\\nd2g (xi)\\ndxi2\\n= L\\n由于L 是Lipschitz 常数恒大于0，因为xi = zi −λ\\nL 是函数g(xi) 的最小值。\\n2. 当zi < −λ\\nL 时：a. 假设xi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL < 0 与假设矛盾；b. 假设\\nxi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL < 0 与假设相符，由上述二阶导数恒大于0 可知，\\nxi = zi + λ\\nL 是g(xi) 的最小值。\\n3. 当−λ\\nL ⩽zi ⩽λ\\nL 时：a. 假设xi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL ⩽0 与假设矛盾；b. 假设\\nxi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL ⩾0 与假设矛盾。\\n4. 最后讨论xi = 0 的情况，此时g(xi) = L\\n2 (zi)\\n2\\n• 当|zi| > λ\\nL 时，由上述推导可知g(xi) 的最小值在xi = zi −λ\\nL 处取得，因为\\ng(xi)|xi=0 −g(xi)|xi=zi−λ\\nL = L\\n2\\n\\x00zi\\x012 −\\n\\x12\\nλzi −λ2\\n2L\\n\\x13\\n= L\\n2\\n\\x12\\nzi −λ\\nL\\n\\x132\\n> 0\\n因此当|zi| > λ\\nL 时，xi = 0 不会是函数g(xi) 的最小值。\\n• 当−λ\\nL ⩽zi ⩽λ\\nL 时，对于任何∆x ̸= 0 有\\ng(∆x) = L\\n2\\n\\x00∆x −zi\\x012 + λ|∆x|\\n= L\\n2\\n\\x12\\n(∆x)2 −2∆x · zi + 2λ\\nL |∆x|\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2\\n\\x12\\n(∆x)2 −2∆x · zi + 2λ\\nL ∆x\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2 (∆x)2 + L\\n2\\n\\x00zi\\x012\\n> g(xi)|xi=0\\n因此xi = 0 是g(xi) 的最小值点。\\n综上所述，11.14 成立。\\n该式称为软阈值(Soft Thresholding) 函数，很常见，建议掌握。另外，常见的变形是式(11.13) 中的\\n或时的形式，其解直接代入式(11.14) 即可。与软阈值函数相对的是硬阈值函数，是将式(11.13) 中的1\\n范数替换为0 范数的优化问题的闭式解。\\n11.4\\n稀疏表示与字典学习\\n稀疏表示与字典学习实际上是信号处理领域的概念。本节内容核心就是K-SVD 算法。\\n11.4.1\\n式(11.15) 的解释\\n这个式子表达的意思很容易理解，即希望样本xi 的稀疏表示αi 通过字典B 重构后和样本xi 的原始\\n表示尽量相似，如果满足这个条件，那么稀疏表示αi 是比较好的。后面的1 范数项是为了使表示更加稀\\n疏。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n11.4.2\\n式(11.16) 的解释\\n为了优化11.15，我们采用变量交替优化的方式(有点类似EM 算法)，首先固定变量B，则11.15 求\\n解的是m 个样本相加的最小值，因为公式里没有样本之间的交互(即文中所述αu\\ni αv\\ni (u ̸= v) 这样的形式)，\\n因此可以对每个变量做分别的优化求出αi，求解方法见式(11.13)，式(11.14)。\\n11.4.3\\n式(11.17) 的推导\\n这是优化11.15 的第二步，固定住αi, i = 1, 2, . . . , m，此时式11.15 的第二项为一个常数，优化11.15\\n即优化minB\\nPm\\ni=1 ∥xi −Bαi∥2\\n2。其写成矩阵相乘的形式为minB ∥X −BA∥2\\n2，将2 范数扩展到F 范数即\\n得优化目标为minB ∥X −BA∥2\\nF。\\n11.4.4\\n式(11.18) 的推导\\n这个公式难点在于推导BA = Pk\\nj=1 bjαj。大致的思路是bjαj 会生成和矩阵BA 同样维度的矩阵，\\n这个矩阵对应位置的元素是BA 中对应位置元素的一个分量，这样的分量矩阵一共有k 个，把所有分量\\n矩阵加起来就得到了最终结果。推导过程如下：\\nBA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\n1\\nb1\\n2\\n·\\n·\\n·\\nb1\\nk\\nb2\\n1\\nb2\\n2\\n·\\n·\\n·\\nb2\\nk\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nbd\\n1\\nbd\\n2\\n·\\n·\\n·\\nbd\\nk\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×k\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nα1\\n1\\nα1\\n2\\n·\\n·\\n·\\nα1\\nm\\nα2\\n1\\nα2\\n2\\n·\\n·\\n·\\nα2\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nαk\\n1\\nαk\\n2\\n·\\n·\\n·\\nαk\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nk×m\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPk\\nj=1 b1\\njαj\\n1\\nPk\\nj=1 b1\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b1\\njαj\\nm\\nPk\\nj=1 b2\\njαj\\n1\\nPk\\nj=1 b2\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\n1\\nPk\\nj=1 bd\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\nbjαj =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\nj\\nb2\\nj\\n·\\n·\\n·\\nbd\\nj\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n·\\nh\\nαj\\n1\\nαj\\n2\\n·\\n·\\n·\\nαj\\nm\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\njαj\\n1\\nb1\\njαj\\n2\\n·\\n·\\n·\\nb1\\njαj\\nm\\nb2\\njαj\\n1\\nb2\\njαj\\n2\\n·\\n·\\n·\\nb2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nbd\\njαj\\n1\\nbd\\njαj\\n2\\n·\\n·\\n·\\nbd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n求和可得：\\nk\\nX\\nj=1\\nbjαj =\\nk\\nX\\nj=1\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\nj\\nb2\\nj\\n·\\n·\\n·\\nbd\\nj\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n·\\nh\\nαj\\n1\\nαj\\n2\\n·\\n·\\n·\\nαj\\nm\\ni\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPk\\nj=1 b1\\njαj\\n1\\nPk\\nj=1 b1\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b1\\njαj\\nm\\nPk\\nj=1 b2\\njαj\\n1\\nPk\\nj=1 b2\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\n1\\nPk\\nj=1 bd\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n得证。\\n将矩阵B 分解成矩阵列bj, j = 1, 2, . . . , k 带来一个好处，即和11.16 的原理相同，矩阵列与列之间\\n无关，因此可以分别优化各个列，即将min\\nB ∥. . . B . . . ∥2\\nF 转化成了min\\nbi ∥. . . bi . . . ∥2\\nF，得到第三行的等式\\n之后，再利用文中介绍的K-SVD 算法求解即可。\\n11.5\\nK-SVD 算法\\n本节前半部分铺垫概念，后半部分核心就是K-SVD。作为字典学习的最经典的算法，K-SVD[2] 自\\n2006 年发表以来已逾万次引用。理解K-SVD 的基础是SVD，即奇异值分解，参见“西瓜书”附录A.3。\\n对于任意实矩阵A ∈Rm×n, 都可分解为A = UΣV⊤, 其中U ∈Rm×m, V ∈Rn×n, 分别为m 阶和\\nn 阶正交矩阵(复数域时称为酉矩阵), 即U⊤U = I, V⊤V = I (逆矩阵等于自身的转置), Σ ∈Rm×n, 且除\\n(Σ)ii = σi 之外其它位置的元素均为零, σi 称为奇异值, 可以证明, 矩阵A 的秩等于非零奇异值的个数。\\n正如西瓜书附录A.3 所述, K-SVD 分解中主要使用SVD 解决低秩矩阵近似问题。之所以称为K-SVD,\\n原文献中专门有说明:\\nWe shall call this algorithm ”K-SVD” to parallel the name K-means.\\nWhile K-means applies K\\ncomputations of means to update the codebook, K-SVD obtains the updated dictionary by K SVD com-\\nputations, each determining one column.\\n具体来说, 就是原文献中的字典共有K 个原子(列), 因此需要迭代K 次; 这类似于K 均值算法欲将\\n数据聚成K 个簇, 需要计算K 次均值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图11-6 K-SVD 算法在论文中的描述\\nK-SVD 算法伪代码详见图11-6, 符号与本节符号有差异。具体来说, 原文献中字典矩阵用D 表示(书\\n中用B ), 稀疏系数用xi 表示(书中用αi ), 数据集用Y 表示(书中用X)。\\n在初始化字典矩阵D 以后, K-SVD 算法迭代过程分两步: 第1 步Sparse Coding Stage 就是普通的已\\n知字典矩阵D 的稀疏表示问题, 可以使用很多现成的算法完成此步, 不再详述; K-SVD 的核心创新点在第\\n2 步Codebook Update Stage, 在该步骤中分K 次分别更新字典矩阵D 中每一列, 更新第k 列dk 时其它\\n各列都是固定的, 如原文献式(21) 所示:\\n∥Y −DX∥2\\nF =\\n\\r\\r\\r\\r\\rY −\\nK\\nX\\nj=1\\ndjxj\\nT\\n\\r\\r\\r\\r\\r\\n2\\nF\\n=\\n\\r\\r\\r\\r\\r\\n \\nY −\\nX\\nj̸=k\\ndjxj\\nT\\n!\\n−dkxk\\nT\\n\\r\\r\\r\\r\\r\\n2\\nF\\n=\\n\\r\\rEk −dkxk\\nT\\n\\r\\r2\\nF .\\n注意, 矩阵dkxk\\nT 的秩为1 (其中, xk\\nT 表示稀疏系数矩阵X 的第k 行, 以区别于其第k 列xk ), 对比\\n西瓜书附录A.3 中的式(A.34), 这就是一个低秩矩阵近似问题, 即对于给定矩阵Ek, 求其最优1 秩近似矩\\n阵dkxk\\nT ; 此时可对Ek 进行SVD 分解, 类似于西瓜书附录式(A.35), 仅保留最大的1 个奇异值; 具体来说\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nEk = U∆V⊤, 仅保留∆中最大的奇异值∆(1, 1), 则dkxk\\nT = U1∆(1, 1)V⊤\\n1 , 其中U1, V1 分别为U, V 的第\\n1 列, 此时dk = U1, xk\\nT = ∆(1, 1)V⊤\\n1 。但这样更新会破坏第1 步中得到的稀疏系数的稀疏性!\\n为了保证第1 步中得到的稀疏系数的稀疏性, K-SVD 并不直接对Ek 进行SVD 分解, 而是根据xk\\nT 仅\\n取出与xk\\nT 非零元素对应的部分列, 例如行向量xk\\nT 只有第1、3、5、8、9 个元素非零, 则仅取出Ek 的第\\n1、3、5、8、9 列组成矩阵进行SVD 分解ER\\nk = U∆V⊤, 则\\n˜dk = U1,\\n˜xk\\nT = ∆(1, 1)V⊤\\n1\\n即得到更新后的˜dk 和˜xk\\nT (注意, 此时的行向量˜xk\\nT 长度仅为原xk\\nT 非零元素个数, 需要按原xk\\nT 对其\\n余位置填0)。如此迭代K 次即得更新后的字典矩阵˜D, 以供下一轮Sparse Coding 使用。K −SVD 原文\\n献中特意提到, 在K 次迭代中要使用最新的稀疏系数˜xk\\nT , 但并没有说是否要用最新的˜dk (推测应该也要\\n用最新的˜dk ):\\nIn the K-SVD algorithm, we sweep through the columns and use always the most updated coeﬀicients\\nas they emerge from preceding SVD steps. Parallel versions of this algorithm can also be considered, where\\nall updates of the previous dictionary are done based on the same X. Experiments show that while this\\nversion also converges, it yields an inferior solution and typically requires more than four times the number\\nof iterations.\\n11.6\\n压缩感知\\n虽然压缩感知与稀疏表示关系密切, 但它是彻彻底底的信号处理领域的概念。“西瓜书”在本章有几个\\n专业术语翻译与信号处理领域人士的习惯翻译略不一样：比如第258 页的Restricted Isometry Property\\n(RIP)“西瓜书”翻译为“限定等距性”, 信号处理领域一般翻译为“有限等距性质”; 第259 页的Basis\\nPursuit De-Noising、第261 页的Basis Pursuit 和Matching Pursuit 中的“Pursuit”“西瓜书”翻译为\\n“寻踪”, 信号处理领域一般翻译为“追踪”, 即基追踪降噪、基追踪、匹配追踪。\\n11.6.1\\n式(11.21) 的解释\\n将式(11.21) 进行变形\\n(1 −δk) ⩽∥Aks∥2\\n2\\n∥s∥2\\n2\\n⩽(1 + δk)\\n注意不等式中间, 若s 为输入信号, 则分母∥s∥2\\n2 为输入信号的能量, 分子∥Aks∥2\\n2 为对应的观测信号的能\\n量, 即RIP 要求观测信号与输入信号的能量之比在一定的范围之内; 例如当δk 等于0 时, 观测信号与输入\\n信号的能量相等, 即实现了等距变换，相关文献可以参考[3]; RIP 放松了对矩阵A 的约束(而且A 并非\\n方阵), 因此称为“有限”等距性质。\\n11.6.2\\n式(11.25) 的解释\\n该式即为核范数定义: 矩阵的核范数（迹范数）为矩阵的奇异值之和。\\n有关“凸包”的概念, 引用百度百科里的两句原话: 在二维欧几里得空间中, 凸包可想象为一条刚好包\\n著所有点的橡皮圈; 用不严谨的话来讲, 给定二维平面上的点集, 凸包就是将最外层的点连接起来构成的凸\\n多边形, 它能包含点集中所有的点。\\n个人理解, 将rank(X) 的“凸包”是X 的核范数∥X∥∗这件事简单理解为∥X∥∗是rank(X) 的上限即\\n可, 即∥X∥∗恒大于rank(X), 类似于式(11.10) 中的式子恒大于f(x) 。\\n参考文献\\n[1] Wikipedia contributors. Sign function, 2020.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 147, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n[2] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete\\ndictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311–4322, 2006.\\n[3] 杨孝春. 欧氏空间中的等距变换与等距映射. 四川工业学院学报, 1999.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第12 章\\n计算学习理论\\n正如本章开篇所述，计算学习理论研究目的是分析学习任务的困难本质，为学习算法提供理论保证，\\n并根据分析结果指导算法设计。例如，“西瓜书”定理12.1、定理12.3、定理12.6 所表达意思的共同点是，\\n泛化误差与经验误差之差的绝对值以很大概率(1 −δ) 很小，且这个差的绝对值随着训练样本个数(m) 的\\n增加而减小，随着模型复杂度（定理12.1 为假设空间包含的假设个数|H|，定理12.3 中为假设空间的VC\\n维，定理12.6 中为(经验)Rademacher 复杂度）的减小而减小。因此，若想要得到一个泛化误差很小的模\\n型，足够的训练样本是前提，最小化经验误差是实现途径，另外还要选择性能相同的模型中模型复杂度最\\n低的那一个；“最小化经验误差”即常说的经验风险最小化，“选择模型复杂度最低的那一个”即结构风险\\n最小化，可以参见“西瓜书”6.4 节最后一段的描述，尤其是式(6.42) 所表达的含义。\\n12.1\\n基础知识\\n统计学中有总体集合和样本集合之分, 比如要统计国内本科生对机器学习的掌握情况, 此时全国所有\\n的本科生就是总体集合, 但总体集合往往太大而不具有实际可操作性, 一般都是取总体集合的一部分, 比如\\n从双一流A 类、双一流B 类、一流学科建设高校、普通高校中各找一部分学生(即样本集合) 进行调研,\\n以此来了解国内本科生对机器学习的掌握情况。在机器学习中, 样本空间(参见1.2 节) 对应总体集合, 而\\n我们手头上的样例集D 对应样本集合, 样例集D 是从样本空间中采样而得, 分布D 可理解为当从样本空\\n间采样获得样例集D 时每个样本被采到的概率, 我们用D(t) 表示样本空间第t 个样本被采到的概率。\\n12.1.1\\n式(12.1) 的解释\\n该式为泛化误差的定义式，所谓泛化误差，是指当样本x 从真实的样本分布D 中采样后其预测值\\nh(x) 不等于真实值y 的概率。在现实世界中，我们很难获得样本分布D，我们拿到的数据集可以看做是\\n从样本分布D 中独立同分布采样得到的。在西瓜书中，我们拿到的数据集，称为样例集D[也叫观测集、\\n样本集，注意与花体D 的区别]。\\n12.1.2\\n式(12.2) 的解释\\n该式为经验误差的定义式，所谓经验误差，是指观测集D 中的样本xi, i = 1, 2, · · · , m 的预测值h(xi)\\n和真实值yi 的期望误差。\\n12.1.3\\n式(12.3) 的解释\\n假设我们有两个模型h1 和h2，将它们同时作用于样本x 上，那么他们的”不合“度定义为这两个模\\n型预测值不相同的概率。\\n12.1.4\\n式(12.4) 的解释\\nJensen 不等式：这个式子可以做很直观的理解，比如说在二维空间上，凸函数可以想象成开口向上的\\n抛物线，假如我们有两个点x1, x2，那么f(E(x)) 表示的是两个点的均值的纵坐标，而E(f(x)) 表示的是\\n两个点纵坐标的均值，因为两个点的均值落在抛物线的凹处，所以均值的纵坐标会小一些。\\n12.1.5\\n式(12.5) 的解释\\n随机变量的观测值是随机的, 进一步地, 随机过程的每个时刻都是一个随机变量。\\n式中,\\n1\\nm\\nPm\\ni=1 xi 表示m 个独立随机变量各自的某次观测值的平均,\\n1\\nm\\nPm\\ni=1 E (xi) 表示m 个独立随\\n机变量各自的期望的平均。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(12.5) 表示事件\\n1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi) ⩾ϵ 出现的概率不大于(i.e., ⩽) e−2mϵ2; 式(12.6) 的事\\n件\\n\\x0c\\x0c 1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi)\\n\\x0c\\x0c ⩾ϵ 等价于以下事件:\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩾ϵ\\n∨\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩽−ϵ\\n其中, ∨表示逻辑或(以上其实就是将绝对值表达式拆成两部分而已)。这两个子事件并无交集, 因此\\n总概率等于两个子事件概率之和; 而\\n1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi) ⩽−ϵ 与式(12.5) 表达的事情对称, 因此\\n概率相同。\\nHoeffding 不等式表达的意思是\\n1\\nm\\nPm\\ni=1 xi 和\\n1\\nm\\nPm\\ni=1 E (xi) 两个值应该比较接近, 二者之差大于ϵ 的\\n概率很小(不大于2e−2mϵ2 )。\\n如果对Hoeffding 不等式的证明感兴趣，可以参考Hoeffding 在1963 年发表的论文[1]，这篇文章也\\n被引用了逾万次。\\n12.1.6\\n式(12.7) 的解释\\nMcDiarmid 不等式：首先解释下前提条件：\\nsup\\nx1,...,xm,x′\\ni\\n|f (x1, . . . , xm) −f (x1, . . . , xi−1, x′\\ni, xi+1, . . . , xm)| ⩽ci\\n表示当函数f 某个输入xi 变到x′\\ni 的时候，其变化的上确sup 仍满足不大于ci。所谓上确界sup 可以理\\n解成变化的极限最大值，可能取到也可能无穷逼近。当满足这个条件时，McDiarmid 不等式指出：函数值\\nf(x1, . . . , xm) 和其期望值E (f(x1, . . . , xm)) 也相近，从概率的角度描述是：它们之间差值不小于ϵ 这样\\n的事件出现的概率不大于exp\\n\\x10\\n−2ϵ2\\n∑\\ni c2\\ni\\n\\x11\\n，可以看出当每次变量改动带来函数值改动的上限越小，函数值和\\n其期望越相近。\\n12.2\\nPAC 学习\\n本节内容几乎都是概念, 建议多读几遍，仔细琢磨一下。\\n概率近似正确(Probably Approximately Correct, PAC) 学习, 可以读为[pæk] 学习。\\n本节第2 段讨论的目标概念, 可简单理解为真实的映射函数;\\n本节第3 段讨论的假设空间, 可简单理解为学习算法不同参数时的存在, 例如线性分类超平面f(x) =\\nw⊤x + b, 每一组(w, b) 取值就是一个假设;\\n本节第4 段讨论的可分的(separable) 和不可分的(non-separable), 例如西瓜书第100 页的图5.4, 若\\n假设空间是线性分类器, 则(a)(b)(c) 是可分的, 而(d) 是不可分的; 当然, 若假设空间为椭圆分类器(分类\\n边界为椭圆), 则(d) 也是可分的;\\n本节第5 段提到的“等效的假设”指的是第7 页图1.3 中的A 和B 两条曲线都可以完美拟合有限的\\n样本点, 故称之为“等效”的假设; 另外本段最后还给出了概率近似正确的含义, 即“以较大概率学得误差\\n满足预设上限的模型”。\\n定义12.1 PAC 辨识的式(12.9) 表示输出假设h 的泛化误差E(h) ⩽ϵ 的概率不小于1 −δ; 即“学习\\n算法L 能以较大概率(至少1 −δ ) 学得目标概念c 的近似(误差最多为ϵ )”。\\n定义12.2 PAC 可学习的核心在于, 需要的样本数目m 是1/ϵ, 1/δ, size(x), size(c) 的多项式函数。\\n定义12.3 PAC 学习算法的核心在于, 完成PAC 学习所需要的时间是1/ϵ, 1/δ, size(x), size(c) 的多项\\n式函数。\\n定义12.4 样本复杂度指完成PAC 学习过程需要的最少的样本数量, 而在实际中当然也希望用最少的\\n样本数量完成学习过程。\\n在定义12.4 之后, 抛出来三个问题:\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n• 研究某任务在什么样的条件下可学得较好的模型？（定义12.2）\\n• 某算法在什么样的条件下可进行有效的学习?（定义12.3）\\n• 需多少训练样例才能获得较好的模型？（定义12.4）\\n有限假设空间指H 中包含的假设个数是有限的, 反之则为无限假设空间; 无限假设空间更为常见, 例\\n如能够将图5.4(a)(b)(c) 中的正例和反例样本分开的线性超平面个数是无限多的。\\n12.2.1\\n式(12.9) 的解释\\nPAC 辨识的定义：E(h) 表示算法L 在用观测集D 训练后输出的假设函数h，它的泛化误差(见公\\n式12.1)。这个概率定义指出，如果h 的泛化误差不大于ϵ 的概率不小于1 −δ，那么我们称学习算法L\\n能从假设空间H 中PAC 辨识概念类C。\\n12.3\\n有限假设空间\\n本节内容分两部分, 第1 部分“可分情形”时, 可以达到经验误差bE(h) = 0, 做的事情是以1 −δ 概率\\n学得目标概念的ϵ 近似, 即式(12.12); 第2 部分“不可分情形”时, 无法达到经验误差bE(h) = 0, 做的事\\n情是以1 −δ 概率学得minh∈H E(h) 的ϵ 近似, 即式(12.20)。无论哪种情形, 对于h ∈H, 可以得到该假\\n设的泛化误差E(h) 与经验误差bE(h) 的关系, 即“当样例数目m 较大时, h 的经验误差是泛化误差很好\\n的近似”, 即式(12.18); 实际研究中经常需要推导类似的泛化误差上下界。\\n从式12.10 到式12.14 的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c 的有效近\\n似。只要训练集D 的规模能使学习算法L 以概率1 −δ 找到目标假设的ϵ 近似即可。下面就是用数学公\\n式进行抽象。\\n12.3.1\\n式(12.10) 的解释\\nP(h(x) = y) = 1 −P(h(x) ̸= y) 因为它们是对立事件，P(h(x) ̸= y) = E(h) 是泛化误差的定义(见\\n12.1)，由于我们假定了泛化误差E(h) > ϵ，因此有1 −E(h) < 1 −ϵ。\\n12.3.2\\n式(12.11) 的解释\\n先解释什么是h 与D“表现一致”，12.2 节开头阐述了这样的概念，如果h 能将D 中所有样本按与真实\\n标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h (x1) = y1)∧. . .∧(h (xm) = ym) 为True。\\n因为每个事件是独立的，所以上式可以写成P ((h (x1) = y1) ∧. . . ∧(h (xm) = ym)) = Qm\\ni=1 P (h (xi) = yi)。\\n根据对立事件的定义有：Qm\\ni=1 P (h (xi) = yi) = Qm\\ni=1 (1 −P (h (xi) ̸= yi))，又根据公式(12.10)，有\\nm\\nY\\ni=1\\n(1 −P (h (xi) ̸= yi)) <\\nm\\nY\\ni=1\\n(1 −ϵ) = (1 −ϵ)m\\n12.3.3\\n式(12.12) 的推导\\n首先解释为什么”我们事先并不知道学习算法L 会输出H 中的哪个假设“，因为一些学习算法对\\n用一个观察集D 的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知\\n机学习到的假设h 参数的值。泛化误差大于ϵ 且经验误差为0 的假设(即在训练集上表现完美的假设)\\n出现的概率可以表示为P(h ∈H : E(h) > ϵ ∧bE(h) = 0)，根据式12.11，每一个这样的假设h 都满足\\nP(E(h) > ϵ ∧bE(h) = 0) < (1 −ϵ)m，假设一共有|H| 这么多个这样的假设h，因为每个假设h 满足\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nE(h) > ϵ 且bE(h) = 0 是互斥的，因此总的概率P(h ∈H : E(h) > ϵ ∧bE(h) = 0) 就是这些互斥事件之和，\\n即\\nP\\n\\x10\\nh ∈H : E(h) > ϵ ∧bE(h) = 0\\n\\x11\\n=\\n|H|\\nX\\ni\\nP\\n\\x10\\nE(hi) > ϵ ∧bE(hi) = 0\\n\\x11\\n< |H|(1 −ϵ)m\\n小于号依据公式(12.11)。\\n第二个小于号实际上是要证明|H|(1 −ϵ)m < |H|e−mϵ，即证明(1 −ϵ)m < e−mϵ，其中ϵ ∈(0, 1]，m\\n是正整数，推导如下：\\n当ϵ = 1 时，显然成立，当ϵ ∈(0, 1) 时，因为左式和右式的值域均大于0，所以可以左右两边同时取\\n对数，又因为对数函数是单调递增函数，所以即证明m ln(1 −ϵ) < −mϵ，即证明ln(1 −ϵ) < −ϵ，这个式\\n子很容易证明：令f(ϵ) = ln(1 −ϵ) + ϵ，其中ϵ ∈(0, 1)，f ′(ϵ) = 1 −\\n1\\n1−ϵ = 0 ⇒ϵ = 0 取极大值0，因此\\nln(1 −ϵ) < −ϵ 也即|H|(1 −ϵ)m < |H|e−mϵ 成立。\\n12.3.4\\n式(12.13) 的解释\\n回到我们要回答的问题：到底需要多少样例才能学得目标概念c 的有效近似。只要训练集D 的规模\\n能使学习算法L 以概率1 −δ 找到目标假设的ϵ 近似即可。根据式12.12，学习算法L 生成的假设大于目\\n标假设的ϵ 近似的概率为P\\n\\x10\\nh ∈H : E(h) > ϵ ∧bE(h) = 0\\n\\x11\\n< |H|e−mϵ，因此学习算法L 生成的假设落在\\n目标假设的ϵ 近似的概率为1 −P\\n\\x10\\nh ∈H : E(h) > ϵ ∧bE(h) = 0\\n\\x11\\n≥1 −|H|e−mϵ，这个概率我们希望至少\\n是1 −δ，因此1 −δ ⩽1 −|H|e−mϵ ⇒|H|e−mϵ ⩽δ\\n12.3.5\\n式(12.14) 的推导\\n|H|e−mϵ ⩽δ\\ne−mϵ ⩽\\nδ\\n|H|\\n−mϵ ⩽ln δ −ln |H|\\nm ⩾1\\nϵ\\n\\x12\\nln |H| + ln 1\\nδ\\n\\x13\\n这个式子告诉我们，在假设空间H 是PAC 可学习的情况下，输出假设h 的泛化误差ϵ 随样本数目\\nm 增大而收敛到0，收敛速率为O( 1\\nm)。这也是我们在机器学习中的一个共识，即可供模型训练的观测集\\n样本数量越多，机器学习模型的泛化性能越好。\\n12.3.6\\n引理12.1 的解释\\n根据式(12.2), bE(h) =\\n1\\nm\\nPm\\ni=1 I (h (xi) ̸= yi), 而指示函数I(·) 取值非0 即1 , 也就是说0 ≤\\nI (h (xi) ̸= yi) ≤1; 对于式(12.1) 的E(h) 实际上表示I (h (xi) ̸= yi) 为1 的期望E (I (h (xi) ̸= yi)) (泛化\\n误差表示样本空间中任取一个样本, 其预测类别不等于真实类别的概率), 当假设h 确定时, 泛化误差固定\\n不变, 因此可记为E(h) =\\n1\\nm\\nPm\\ni=1 E (I (h (xi) ̸= yi)) 。\\n此时, 将bE(h) 和E(h) 代入式(12.15) 到式(12.17), 对比式(12.5) 和式(12.6) 的Hoeffding 不等式可\\n知, 式(12.15) 对应式(12.5), 式(12.16) 与式(12.15) 对称, 式(12.17) 对应式(12.6)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.3.7\\n式(12.18) 的推导\\n令δ = 2e−2mϵ2，则ϵ =\\nq\\nln(2/δ)\\n2m ，由式(12.17)\\nP(|E(h) −bE(h)| ⩾ϵ) ⩽2 exp\\n\\x00−2mϵ2\\x01\\nP(|E(h) −bE(h)| ⩾ϵ) ⩽δ\\nP(|E(h) −bE(h)| ⩽ϵ) ⩾1 −δ\\nP(−ϵ ⩽E(h) −bE(h) ⩽ϵ) ⩾1 −δ\\nP( bE(h) −ϵ ⩽E(h) ⩽bE(h) + ϵ) ⩾1 −δ\\n带入ϵ =\\nq\\nln(2/δ)\\n2m\\n得证。\\n这个式子进一步阐明了当观测集样本数量足够大的时候，h 的经验误差是其泛化误差很好的近似。\\n12.3.8\\n式(12.19) 的推导\\n令h1, h2, . . . , h|H| 表示假设空间H 中的假设，有\\nP(∃h ∈H : |E(h) −bE(h)| > ϵ)\\n=P\\n\\x10\\x10\\x0c\\x0c\\x0cEh1 −bEh1\\n\\x0c\\x0c\\x0c > ϵ\\n\\x11\\n∨. . . ∨\\n\\x10\\n|Eh|H| −bEh|H||>ϵ\\n\\x11\\x11\\n⩽\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ)\\n这一步是很好理解的，存在一个假设h 使得|E(h) −bE(h)| > ϵ 的概率可以表示为对假设空间内\\n所有的假设hi, i ∈1, . . . , |H|，使得\\n\\x0c\\x0c\\x0cEhi −bEhi\\n\\x0c\\x0c\\x0c > ϵ 这个事件成立的” 或” 事件。因为P(A ∨B) =\\nP(A) + P(B) −P(A ∧B)，而P(A ∧B) ⩾0，所以最后一行的不等式成立。\\n由式12.17：\\nP(|E(h) −bE(h)| ⩾ϵ) ⩽2 exp\\n\\x00−2mϵ2\\x01\\n⇒\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ) ⩽2|H| exp\\n\\x00−2mϵ2\\x01\\n因此：\\nP(∃h ∈H : |E(h) −bE(h)| > ϵ) ⩽\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ)\\n⩽2|H| exp\\n\\x00−2mϵ2\\x01\\n其对立事件：\\nP(∀h ∈H : |E(h) −bE(h)| ⩽ϵ) = 1 −P(∃h ∈H : |E(h) −bE(h)| > ϵ)\\n⩾1 −2|H| exp\\n\\x00−2mϵ2\\x01\\n令δ = 2|H|e−2mϵ2，则ϵ =\\nq\\nln |H|+ln(2/δ)\\n2m\\n，带入上式中即可得到\\nP\\n \\n∀h ∈H : |E(h) −bE(h)| ⩽\\nr\\nln |H| + ln(2/δ)\\n2m\\n!\\n⩾1 −δ\\n其中∀h ∈H 这个前置条件可以省略。\\n12.3.9\\n式(12.20) 的解释\\n这个式子是”不可知PAC 可学习“的定义式，不可知是指当目标概念c 不在算法L 所能生成的假设\\n空间H 里。可学习是指如果H 中泛化误差最小的假设是arg minh∈H E(h)，且这个假设的泛化误差满足\\n其与目标概念的泛化误差的差值不大于ϵ 的概率不小于1 −δ。我们称这样的假设空间H 是不可知PAC\\n可学习的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.4\\nVC 维\\n不同于12.3 节的有限假设空间，从本节开始，本章剩余内容均针对无限假设空间。\\n12.4.1\\n式(12.21) 的解释\\n这个是增长函数的定义式。增长函数ΠH(m) 表示假设空间H 对m 个样本所能赋予标签的最大可能\\n的结果数。比如对于两个样本的二分类问题，一共有4 中可能的标签组合[[0, 0], [0, 1], [1, 0], [1, 1]]，如果假\\n设空间H1 能赋予这两个样本两种标签组合[[0, 0], [1, 1]]，则ΠH1(2) = 2。显然，H 对样本所能赋予标签\\n的可能结果数越多，H 的表示能力就越强。增长函数可以用来反映假设空间H 的复杂度。\\n12.4.2\\n式(12.22) 的解释\\n值得指出的是，这个式子的前提假设有误，应当写成对假设空间H，m ∈N，0 < ϵ < 1，存在h ∈H\\n详细证明参见原论文On the uniform convergence of relative frequencies of events to their probabilities\\n[2]，在该论文中，定理的形式如下：\\nTheorem 2 The probability that the relative frequency of at least one event in class S differs from its\\nprobability in an experiment of size l by more then ε, for l ≧2/ε2, satisfies the inequality\\nP\\n\\x00π(l) > ε\\n\\x01\\n≦4mS(2l)e−ε2l/8.\\n注意定理描述中使用的是“at least one event in class S”, 因此应该是class S 中“存在”one event\\n而不是class S 中的“任意”event。\\n另外, 该定理为基于增长函数对无限假设空间的泛化误差分析, 与上一节有限假设空间的定理12.1。在\\n证明定理12.1 的式(12.19) 过程中, 实际证明的结论是\\nP(∃h ∈H : |E(h) −bE(h)| > ϵ) ⩽2|H|e−2mϵ2\\n根据该结论可得式(12.19) 的原型（式(12.19) 就是将ϵ 用δ 表示):\\nP(∀h ∈H : |E(h) −bE(h)| ⩽ϵ) ⩽1 −2|H|e−2mϵ2\\n这是因为事件∃h ∈H : |E(h) −bE(h)| > ϵ 与事件∀h ∈H : |E(h) −bE(h)| ⩽ϵ 为对立事件。\\n注意到当使用|E(h) −bE(h)| > ϵ 表达时对应于“存在”, 当使用|E(h) −bE(h)| ⩽ϵ 表达时则对应于\\n“任意”。\\n综上所述, 式(12.22) 使用|E(h) −bE(h)| > ϵ, 所以这里应该对应于“存在”。\\n12.4.3\\n式(12.23) 的解释\\n这是VC 维的定义式：VC 维的定义是能被H 打散的最大示例集的大小。“西瓜书”中例12.1 和例\\n12.2 给出了形象的例子。\\n式(12.23) 中的{m : ΠH(m) = 2m} 表示一个集合, 集合的元素是能使ΠH(m) = 2m 成立的所有m;\\n最外层的max 表示取集合的最大值。注意, 这里仅讨论二分类问题。注意，VC 维的定义式上的底数2 表\\n示这个问题是2 分类的问题。如果是n 分类的问题，那么定义式中底数需要变为n。\\nVC 维的概念还是很容易理解的, 有个常见的思维误区西瓜书也指出来了, 即“这并不意味着所有大小\\n为d 的示例集都能被假设空间H 打散”, 也就是说只要“存在大小为d 的示例集能被假设空间H 打散”\\n即可, 这里的区别与前面“定理12.2 的解释”中提到的“任意”与“存在”的关系一样。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.4.4\\n引理12.2 的解释\\n首先解释下数学归纳法的起始条件” 当m = 1, d = 0 或d = 1 时，定理成立”，当m = 1, d = 0 时，\\n由VC 维的定义(式12.23) VC(H) = max {m : ΠH(m) = 2m} = 0 可知ΠH(1) < 2，否则d 可以取到1，\\n又因为ΠH(m) 为整数，所以ΠH(1) ∈[0, 1]，式12.24 右边为P0\\ni=0\\n \\n1\\ni\\n!\\n= 1，因此不等式成立。当\\nm = 1, d = 1 时，因为一个样本最多只能有两个类别，所以ΠH(1) = 2，不等式右边为P1\\ni=0\\n \\n1\\ni\\n!\\n= 2，\\n因此不等式成立。\\n再介绍归纳过程，这里采样的归纳方法是假设式(12.24) 对(m −1, d −1) 和(m −1, d) 成立，推导\\n出其对(m, d) 也成立。证明过程中引入观测集D = {x1, x2, . . . , xm} 和观测集D′ = {x1, x2, . . . , xm−1}，\\n其中D 比D′ 多一个样本xm，它们对应的假设空间可以表示为：\\nH|D = {(h (x1) , h (x2) , . . . , h (xm)) |h ∈H}\\nH|D′ = {(h (x1) , h (x2) , . . . , h (xm−1)) |h ∈H}\\n如果假设h ∈H 对xm 的分类结果为+1，或为−1，那么任何出现在H|D′ 中的串都会在H|D 中出\\n现一次或者两次。这里举个例子就很容易理解了，假设m = 3：\\nH|D = {(+, −, −), (+, +, −), (+, +, +), (−, +, −), (−, −, +)}\\nH|D′ = {(+, +), (+, −), (−, +), (−, −)}\\n其中串(+, +) 在H|D 中出现了两次(+, +, +), (+, +, −)，H|D′ 中得其他串(+, −), (−, +), (−, −) 均\\n只在H|D 中出现了一次。这里的原因是每个样本是二分类的，所以多出的样本xm 要么取+，要么取−，\\n要么都取到(至少两个假设h 对xm 做出了不一致的判断)。记号HD′|D 表示在H|D 中出现了两次的H|D′\\n组成的集合，比如在上例中HD′|D = {(+, +)}，有\\n\\x0c\\x0cH|D\\n\\x0c\\x0c =\\n\\x0c\\x0cH|D′\\n\\x0c\\x0c +\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c\\n由于H|D′ 表示限制在样本集D′ 上的假设空间H 的表达能力(即所有假设对样本集D′ 所能赋予的\\n标记种类数)，样本集D′ 的数目为m −1，根据增长函数的定义，假设空间H 对包含m −1 个样本的集合\\n所能赋予的最大标记种类数为ΠH(m −1)，因此|H|D′| ⩽ΠH(m −1)。又根据数学归纳法的前提假设，有：\\n\\x0c\\x0cH|D′\\n\\x0c\\x0c ⩽ΠH(m −1) ⩽\\nd\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n由记号H|D′ 的定义可知，|H|D′| ⩾\\nj\\n|H|D|\\n2\\nk\\n，又由于|H|D′| 和|HD′|D| 均为整数，因此|HD′|D| ⩽\\nj\\n|H|D|\\n2\\nk\\n，由于样本集D 的大小为m，根据增长函数的概念，有\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c ⩽\\nj\\n|H|D|\\n2\\nk\\n⩽ΠH(m −1)。假设\\nQ 表示能被HD′|D 打散的集合，因为根据HD′|D 的定义，HD 必对元素xm 给定了不一致的判定，因此\\nQ ∪{xm} 必能被H|D 打散，由前提假设H 的VC 维为d，因此HD′|D 的VC 维最大为d −1，综上有\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c ⩽ΠH(m −1) ⩽\\nd−1\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n因此：\\n\\x0c\\x0cH|D\\n\\x0c\\x0c =\\n\\x0c\\x0cH|D′\\n\\x0c\\x0c +\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c\\n⩽\\nd\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n+\\nd+1\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n=\\nd\\nX\\ni=0\\n  \\nm −1\\ni\\n!\\n+\\n \\nm −1\\ni −1\\n!!\\n=\\nd\\nX\\ni=0\\n \\nm\\ni\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n注：最后一步依据组合公式，推导如下：\\n \\nm −1\\ni\\n!\\n+\\n \\nm −1\\ni −1\\n!\\n=\\n(m −1)!\\n(m −1 −i)!i! +\\n(m −1)!\\n(m −1 −i + 1)!(i −1)!\\n=\\n(m −1)!(m −i)\\n(m −i)(m −1 −i)!i! +\\n(m −1)!i\\n(m −i)!(i −1)!i\\n= (m −1)!(m −i) + (m −1)!i\\n(m −i)!i!\\n= (m −1)!(m −i + i)\\n(m −i)!i!\\n= (m −1)!m\\n(m −i)!i!\\n=\\nm!\\n(m −i)!i! =\\n \\nm\\ni\\n!\\n12.4.5\\n式(12.28) 的解释\\nΠH(m) ⩽\\nd\\nX\\ni=0\\n \\nm\\ni\\n!\\n⩽\\nd\\nX\\ni=0\\n \\nm\\ni\\n! \\x10m\\nd\\n\\x11d−i\\n=\\n\\x10m\\nd\\n\\x11d\\nd\\nX\\ni=0\\n \\nm\\ni\\n! \\x12 d\\nm\\n\\x13i\\n⩽\\n\\x10m\\nd\\n\\x11d\\nm\\nX\\ni=0\\n \\nm\\ni\\n! \\x12 d\\nm\\n\\x13i\\n=\\n\\x10m\\nd\\n\\x11d\\x12\\n1 + d\\nm\\n\\x13m\\n<\\n\\x10e · m\\nd\\n\\x11d\\n第一步到第二步和第三步到第四步均因为m ⩾d，第四步到第五步是由于二项式定理[3]：(x + y)n =\\nPn\\nk=0\\n \\nn\\nk\\n!\\nxn−kyk，其中令k = i, n = m, x = 1, y =\\nd\\nm 得\\n\\x00 m\\nd\\n\\x01d Pm\\ni=0\\n \\nm\\ni\\n!\\n\\x00 d\\nm\\n\\x01i =\\n\\x00 m\\nd\\n\\x01d (1 + d\\nm)m，\\n最后一步的不等式即需证明\\n\\x001 + d\\nm\\n\\x01m ⩽ed，因为\\n\\x001 + d\\nm\\n\\x01m =\\n\\x001 + d\\nm\\n\\x01 m\\nd d，根据自然对数底数e 的定义\\n[4]，\\n\\x001 + d\\nm\\n\\x01 m\\nd d < ed，注意原文中用的是⩽，但是由于e = lim d\\nm →0\\n\\x001 + d\\nm\\n\\x01 m\\nd 的定义是一个极限，所以\\n应该是用<。\\n12.4.6\\n式(12.29) 的解释\\n这里应该是作者的笔误，根据式12.22，E(h)−bE(h) 应当被绝对值符号包裹。将式12.28 带入式12.22\\n得\\nP\\n\\x10\\n|E(h) −bE(h)| > ϵ\\n\\x11\\n⩽4\\n\\x122em\\nd\\n\\x13d\\nexp\\n\\x12\\n−mϵ2\\n8\\n\\x13\\n令4\\n\\x00 2em\\nd\\n\\x01d exp\\n\\x10\\n−mϵ2\\n8\\n\\x11\\n= δ 可解得\\nδ =\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ\\nm\\n带入式12.22，则定理得证。这个式子是用VC 维表示泛化界，可以看出，泛化误差界只与样本数量m 有\\n关，收敛速率为\\nq\\nln m\\nm\\n(书上简化为\\n1\\n√m)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.4.7\\n式(12.30) 的解释\\n这个是经验风险最小化的定义式。即从假设空间中找出能使经验风险最小的假设。\\n12.4.8\\n定理12.4 的解释\\n首先回忆PAC 可学习的概念，见定义12.2，而可知/不可知PAC 可学习之间的区别仅仅在于概念类\\nc 是否包含于假设空间H 中。令\\nδ′ = δ\\n2\\nr\\n(ln 2/δ′)\\n2m\\n= ϵ\\n2\\n结合这两个标记的转换，由推论12.1 可知：\\nbE(g) −ϵ\\n2 ⩽E(g) ⩽bE(g) + ϵ\\n2\\n至少以1 −δ/2 的概率成立。写成概率的形式即：\\nP\\n\\x10\\n|E(g) −bE(g)| ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ/2\\n即P\\n\\x10\\x10\\nE(g) −bE(g) ⩽ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ/2，因此P\\n\\x10\\nE(g) −bE(g) ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ/2 且\\nP\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n⩾1 −δ/2 成立。再令\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ′\\nm\\n= ϵ\\n2\\n由式12.29 可知\\nP\\n\\x10\\x0c\\x0c\\x0cE(h) −bE(h)\\n\\x0c\\x0c\\x0c ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ\\n2\\n同理，P\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\n⩾1−δ/2 且P\\n\\x10\\nE(h) −bE(h) ⩾−ϵ\\n2\\n\\x11\\n⩾1−δ/2 成立。由P\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n⩾\\n1−δ/2 和P\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\n⩾1−δ/2 均成立可知则事件E(g)−bE(g) ⩾−ϵ\\n2 和事件E(h)−bE(h) ⩽ϵ\\n2\\n同时成立的概率为：\\nP\\n\\x10\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n=P\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n+ P\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\n−P\\n\\x10\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∨\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ/2 + 1 −δ/2 −1\\n=1 −δ\\n即\\nP\\n\\x10\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ\\n因此\\nP\\n\\x10\\nbE(g) −E(g) + E(h) −bE(h) ⩽ϵ\\n2 + ϵ\\n2\\n\\x11\\n= P\\n\\x10\\nE(h) −E(g) ⩽bE(h) −bE(g) + ϵ\\n\\x11\\n⩾1 −δ\\n再由h 和g 的定义，h 表示假设空间中经验误差最小的假设，g 表示泛化误差最小的假设，将这两个假设\\n共用作用于样本集D，则一定有bE(h) ⩽bE(g)，因此上式可以简化为：\\nP (E(h) −E(g) ⩽ϵ) ⩾1 −δ\\n根据式12.32 和式12.34，可以求出m 为关于(1/ϵ, 1/δ, size(x), size(c)) 的多项式，因此根据定理12.2，定\\n理12.5，得到结论任何VC 维有限的假设空间H 都是(不可知)PAC 可学习的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.5\\nRademacher 复杂度\\n上一节中介绍的基于VC 维的泛化误差界是分布无关、数据独立的，本节将要介绍的Rademacher 复\\n杂度则在一定程度上考虑了数据分布。\\n12.5.1\\n式(12.36) 的解释\\n这里解释从第一步到第二步的推导，因为前提假设是2 分类问题，yk ∈{−1, +1}，因此I (h(xi) ̸= yi) ≡\\n1−yih(xi)\\n2\\n。这是因为假如yi = +1, h(xi) = +1 或yi = −1, h(xi) = −1，有I (h(xi) ̸= yi) = 0 = 1−yih(xi)\\n2\\n；\\n反之，假如yi = −1, h(xi) = +1 或yi = +1, h(xi) = −1，有I (h(xi) ̸= yi) = 1 = 1−yih(xi)\\n2\\n。\\n12.5.2\\n式(12.37) 的解释\\n由公式12.36 可知，经验误差bE(h) 和\\n1\\nm\\nPm\\ni=1 yih (xi) 呈反比的关系，因此假设空间中能使经验误差\\n最小的假设h 即是使\\n1\\nm\\nPm\\ni=1 yih (xi) 最大的h。\\n12.5.3\\n式(12.38) 的解释\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi)\\n[解析]：上确界sup 这个概念前面已经解释过，见式(12.7) 的解析。相比于式(12.37), 样例真实标记\\nyi 换为了Rademacher 随机变量σi, arg maxh∈H 换为了上确界suph∈H◦该式表示, 对于样例集D =\\n{x1, x2, . . . , xm}, 假设空间H 中的假设对其预测结果{h (x1) , h (x2) , . . . , h (xm)} 与随机变量集合σ =\\n{σ1, σ2, . . . , σm} 的契合程度。接下来解释一下该式的含义。1\\nm\\nPm\\ni=1 σih (xi) 中的σ = {σ1, σ2, . . . , σm} 表\\n示单次随机生成的结果（生成后就固定不动), 而{h (x1) , h (x2) , . . . , h (xm)} 表示某个假设h ∈H 的预测\\n结果, 至于\\n1\\nm\\nPm\\ni=1 σih (xi) 的取值则取决于本次随机生成的σ 和假设h 的预测结果的契合程度。\\n进一步地, suph∈H\\n1\\nm\\nPm\\ni=1 σih (xi) 中的σ = {σ1, σ2, . . . , σm} 仍表示单次随机生成的结果(生成后就\\n固定不动), 但此时需求解的是假设空间H 中所有假设与σ 最契合的那个h 。\\n例如, σ = {−1, +1, −1, +1} （即m = 4, 这里σ 仅为本次随机生成结果而已, 下次生成结果可能是另\\n一组结果), 假设空间H = {h1, h2, h3}, 其中\\n{h1 (x1) , h1 (x2) , h1 (x3) , h1 (x4)} = {−1, −1, −1, −1}\\n{h2 (x1) , h2 (x2) , h2 (x3) , h2 (x4)} = {−1, +1, −1, −1}\\n{h3 (x1) , h3 (x2) , h3 (x3) , h3 (x4)} = {+1, +1, +1, +1}\\n易知\\n1\\nm\\nPm\\ni=1 σih1 (xi) = 0, 1\\nm\\nPm\\ni=1 σih2 (xi) = 2\\n4, 1\\nm\\nPm\\ni=1 σih3 (xi) = 0, 因此\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi) = 2\\n4\\n12.5.4\\n式(12.39) 的解释\\nEσ\\n\"\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi)\\n#\\n[解析]：这个式子可以用来衡量假设空间H 的表达能力，对变量σ 求期望可以理解为当变量σ 包含所有\\n可能的结果时，假设空间H 中最契合的假设h 和变量的平均契合程度。因为前提假设是2 分类的问题，\\n因此σi 一共有2m 种，这些不同的σi 构成了数据集D = {(x1, y1), (x2, y2), . . . , (xm, ym)} 的”对分“(12.4\\n节)，如果一个假设空间的表达能力越强，那么就越有可能对于每一种σi，假设空间中都存在一个h 使得\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nh(xi) 和σi 非常接近甚至相同，对所有可能的σi 取期望即可衡量假设空间的整体表达能力，这就是这个\\n式子的含义。\\n12.5.5\\n式(12.40) 的解释\\n对比式12.39，这里使用函数空间F 代替了假设空间H，函数f 代替了假设h，很容易理解，因为假设\\nh 即可以看做是作用在数据xi 上的一个映射，通过这个映射可以得到标签yi。注意前提假设实值函数空间\\nF : Z →R，即映射f 将样本zi 映射到了实数空间，这个时候所有的σi 将是一个标量即σi ∈{+1, −1}。\\n12.5.6\\n式(12.41) 的解释\\n这里所要求的是F 关于分布D 的Rademacher 复杂度，因此从D 中采出不同的样本Z，计算这些\\n样本对应的Rademacher 复杂度的期望。\\n12.5.7\\n定理12.5 的解释\\n首先令记号\\nbEZ(f) = 1\\nm\\nm\\nX\\ni=1\\nf (zi)\\nΦ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11\\n即bEZ(f) 表示函数f 作为假设下的经验误差，Φ(Z) 表示泛化误差和经验误差的差的上确界。再令\\nZ′ 为只与Z 有一个示例(样本) 不同的训练集，不妨设zm ∈Z 和z′\\nm ∈Z′ 为不同的示例，那么有\\nΦ (Z′) −Φ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −bEZ′(f)\\n\\x11\\n−sup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11\\n⩽sup\\nf∈F\\n\\x10\\nbEZ(f) −bEZ′(f)\\n\\x11\\n= sup\\nf∈F\\nPm\\ni=1 f(zi) −Pm\\ni=1 f(z′\\ni)\\nm\\n= sup\\nf∈F\\nf (zm) −f (z′\\nm)\\nm\\n⩽1\\nm\\n第一个不等式是因为上确界的差不大于差的上确界[5]，第四行的等号由于Z′ 与Z 只有zm 不相同，\\n最后一行的不等式是因为前提假设F : Z →[0, 1]，即f(zm), f(z′\\nm) ∈[0, 1]。同理\\nΦ(Z) −Φ (Z′) = sup\\nf∈F\\nf (z′\\nm) −f (zm)\\nm\\n⩽1\\nm\\n综上二式有：\\n|Φ(Z) −Φ (Z′)| ⩽1\\nm\\n将Φ 看做函数f(注意这里的f 不是Φ 定义里的f)，那么可以套用McDiarmid 不等式的结论式12.7\\nP (Φ(Z) −EZ[Φ(Z)] ⩾ϵ) ⩽exp\\n\\x12 −2ϵ2\\nP\\ni c2\\ni\\n\\x13\\n令exp\\n\\x10\\n−2ϵ2\\n∑\\ni c2\\ni\\n\\x11\\n= δ 可以求得ϵ =\\nq\\nln(1/δ)\\n2m ，所以\\nP\\n \\nΦ(Z) −EZ[Φ(Z)] ⩾\\nr\\nln(1/δ)\\n2m\\n!\\n⩽δ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由逆事件的概率定义得\\nP\\n \\nΦ(Z) −EZ[Φ(Z)] ⩽\\nr\\nln(1/δ)\\n2m\\n!\\n⩾1 −δ\\n即书中式12.44 的结论。下面来估计EZ[Φ(Z)] 的上界：\\nEZ[Φ(Z)] = EZ\\n\\x14\\nsup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11\\x15\\n= EZ\\n\\x14\\nsup\\nf∈F\\nEZ′\\nh\\nbEZ′(f) −bEZ(f)\\ni\\x15\\n⩽EZ,Z′\\n\\x14\\nsup\\nf∈F\\n\\x10\\nbEZ′(f) −bEZ(f)\\n\\x11\\x15\\n= EZ,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\n(f (z′\\ni) −f (zi))\\n#\\n= Eσ,Z,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσi (f (z′\\ni) −f (zi))\\n#\\n⩽Eσ,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (z′\\ni)\\n#\\n+ Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\n−σif (zi)\\n#\\n= 2Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (zi)\\n#\\n= 2Rm(F)\\n第二行等式是外面套了一个对服从分布D 的示例集Z′ 求期望，因为EZ′∼D[ bEZ′(f)] = E(f)，而采样\\n出来的Z′ 和Z 相互独立，因此有EZ′∼D[ bEZ(f)] = bEZ(f)。\\n第三行不等式基于上确界函数sup 是个凸函数，将supf∈F 看做是凸函数f，将bEZ′(f)−bEZ(f) 看做变\\n量x 根据Jesen 不等式(式12.4)，有EZ\\nh\\nsupf∈F EZ′\\nh\\nbEZ′(f) −bEZ(f)\\nii\\n⩽EZ,Z′\\nh\\nsupf∈F\\n\\x10\\nbEZ′(f) −bEZ(f)\\n\\x11i\\n，\\n其中EZ,Z′[·] 是EZ[EZ′[·]] 的简写形式。\\n第五行引入对Rademacher 随机变量的期望，由于函数值空间是标量，因为σi 也是标量，即σi ∈\\n{−1, +1}，且σi 总以相同概率可以取到这两个值，因此可以引入Eσ 而不影响最终结果。\\n第六行利用了上确界的和不小于和的上确界[5]，因为第一项中只含有变量z′，所以可以将EZ 去掉，\\n因为第二项中只含有变量z，所以可以将EZ′ 去掉。\\n第七行利用σ 是对称的，所以−σ 的分布和σ 完全一致，所以可以将第二项中的负号去除，又因为\\nZ 和Z′ 均是从D 中i.i.d. 采样得到的数据，因此可以将第一项中的z′\\ni 替换成z，将Z′ 替换成Z。\\n最后根据定义式12.41 可得EZ[Φ(Z)] = 2Rm(F)，式(12.42) 得证。\\n12.6\\n定理12.6 的解释\\n针对二分类问题, 定理12.5 给出了“泛化误差”和“经验误差”的关系, 即:\\n• 式(12.47) 基于Rademacher 复杂度Rm(H) 给出了泛化误差E(h) 的上界;\\n• 式(12.48) 基于经验Rademacher 复杂度bRD(H) 给出了泛化误差E(h) 的上界。\\n可能大家都会有疑问：定理12.6 的设定其实也适用于定理12.5, 即值域为二值的{−1, +1} 也属于值\\n域为连续值的[0, 1] 的一种特殊情况, 这一点从接下来的式(12.49) 的转换可以看出。那么, 为什么还要针\\n对二分类问题专门给出定理12.6 呢?\\n根据(经验)Rademacher 复杂度的定义可以知道, Rm(H) 和bRD(H) 均大于零(参见前面有关式(12.39)\\n的解释, 书中式(12.39) 下面的一行也提到该式取值范围是[0, 1]); 因此, 相比于定理12.5 来说, 定理12.6 的\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n上界更紧, 因为二者的界只有中间一项关于(经验)Rademacher 复杂度的部分不同, 在定理12.5 中是两倍\\n的(经验)Rademacher 复杂度, 而在定理12.6 中是一倍的(经验)Rademacher 复杂度, 而(经验)Rademacher\\n复杂度大于零。\\n因此, 为二分类问题量身定制的定理12.6 相比于通用的定理12.5 来说, 二者的区别在于定理12.6 考\\n虑了二分类的特殊情况, 得到了比定理12.5 更紧的泛化误差界, 仅此而已。\\n下面做一些证明：\\n(1) 首先通过式(12.49) 将值域为{−1, +1} 的假设空间H 转化为值域为[0, 1] 的函数空间FH ;\\n(2) 接下来是该证明最核心部分, 即证明式(12.50) 的结论bRZ (FH) = 1\\n2 bRD(H) : 第1 行等号就是定\\n义12.8; 第2 行等号就是根据式(12.49) 将fh (xi, yi) 换为I (h (xi) ̸= yi); 第3 行等号类似于式(12.36) 的\\n第2 个等号; 第4 行等号说明如下:\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσi\\n1 −yih (xi)\\n2\\n= sup\\nh∈H\\n1\\n2m\\nm\\nX\\ni=1\\nσi + sup\\nh∈H\\n1\\n2m\\nm\\nX\\ni=1\\n−yiσih (xi)\\n2\\n其中suph∈H\\n1\\n2m\\nPm\\ni=1 σi 与h 无关, 所以suph∈H\\n1\\n2m\\nPm\\ni=1 σi =\\n1\\n2m\\nPm\\ni=1 σi, 即第4 行等号; 第5 行等号是\\n由于Eσ\\n\\x02 1\\nm\\nPm\\ni=1 σi\\n\\x03\\n= 0, 例如当m = 2 时, 所有可能得σ 包括(−1, −1), (−1, +1), (+1, −1) 和(+1, +1),\\n求期望后显然结果等于0 ; 第6 行等号正如边注所说, “−yiσi 与σi 分布相同”(原因跟定理12.5 中证明\\nEZ[Φ(Z)] ⩽2Rm(F) 相同, 即求期望时要针对所有可能的σ 参见“西瓜书”第282 页第8 行); 第7 行等\\n号再次使用了定义12.8。\\n(3) 关于式(12.51), 根据式(12.50) 的结论, 可证明如下:\\nRm (FH) = EZ\\nh\\nbRZ (FH)\\ni\\n= ED\\n\\x141\\n2\\nbRD(H)\\n\\x15\\n= 1\\n2ED\\nh\\nbRD(H)\\ni\\n= 1\\n2Rm(H)\\n其中第2 个等号由Z 变为D 只是符号根据具体情况的适时变化而已。\\n(4) 最后, 将式(12.49) 定义的fh 替换定理12.5 中的函数f, 则\\nE[f(z)] = E[I(h(x) ̸= y)] = E(h)\\n1\\nm\\nm\\nX\\ni=1\\nf (zi) = 1\\nm\\nm\\nX\\ni=1\\nI (h (xi) ̸= yi) = bE(h)\\n将式(12.51) 代入式(12.42), 即用1\\n2Rm(H) 替换式(12.42) 的Rm(F), 式(12.47) 得证;\\n将式(12.50) 代入式(12.43), 即用1\\n2 bRD(H) 替换式(12.43) 的bRZ(F), 式(12.48) 得证。\\n这里有个疑问在于，定理12.5 的前提是“实值函数空间F : Z →[0, 1] ”, 而式(12.49) 得到的函数\\nfh(z) 的值域实际为{0, 1}, 仍是离散的而非实值的; 当然, 定理12.5 的证明也只需要其函数值在[0, 1] 范\\n围内即可, 并不需要其连续。\\n12.6.1\\n式(12.52) 的证明\\n比较繁琐，同书上所示，参见Foundations of Machine Learning[6]\\n12.6.2\\n式(12.53) 的推导\\n根据式12.28 有ΠH(m) ⩽\\n\\x00 e·m\\nd\\n\\x01d，根据式12.52 有Rm(H) ⩽\\nq\\n2 ln ΠH(m)\\nm\\n，因此ΠH(m) ⩽\\nq\\n2d ln em\\nd\\nm\\n，\\n再根据式12.47 E(h) ⩽bE(h) + Rm(H) +\\nq\\nln(1/δ)\\n2m\\n即证。\\n12.7\\n稳定性\\n上上节中介绍的基于VC 维的泛化误差界是分布无关、数据独立的，上一节介绍的Rademacher 复杂\\n度则在一定程度上考虑了数据分布，但二者得到的结果均与具体学习算法无关；本节将要介绍的稳定性分\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n析可以获得与算法有关的分析结果。算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之\\n发生较大的变化。\\n12.7.1\\n泛化/经验/留一损失的解释\\n根据式(12.54) 上方关于损失函数的描述：“刻画了假设的预测标记与真实标记之间的差别”，这里针\\n对的是二分类，预测标记和真实标记均只能取和两个值，它们之间的“差别”又能是什么呢？\\n因此，当“差别”取为时，式(12.54) 的泛化损失就是式(12.1) 的泛化误差，式(12.55) 的经验损失\\n就是式(12.2) 的经验误差，如果类似于式(12.1) 和式(12.2) 继续定义留一误差，那么式(12.56) 就对应\\n于留一误差。\\n12.7.2\\n式(12.57) 的解释\\n根据三角不等式[7]，有|a + b| ≤|a| + |b|，将a = ℓ(LD, z) −ℓ(LDi)，b = ℓ(LDi,z) −ℓ\\n\\x00LD\\\\i,z\\n\\x01\\n带入\\n即可得出第一个不等式，根据D\\\\i 表示移除D 中第i 个样本，Di 表示替换D 中第i 个样本，那么a, b\\n的变动均为一个样本，根据式12.57，a ⩽β, b ⩽β，因此a + b ⩽2β。\\n12.7.3\\n定理12.8 的解释\\n西瓜书在该定理下方已明确给出该定理的意义, 即“定理12.8 给出了基于稳定性分析推导出的学习算\\n法L 学得假设的泛化误差界”, 式(12.58) 和式(12.59) 分别基于经验损失和留一损失给出了泛化损失的\\n上界。接下来讨论两个相关问题:\\n(1) 定理12.8 的条件包括损失函数有界, 即0 ⩽ℓ(LD, z) ⩽M; 如本节第1 条注解“泛化/经验/留一\\n损失的解释”中所述, 若“差别”取为I (LD(x), y), 则泛化损失对应于泛化误差, 此时上限M = 1 。\\n(2) 在前面泛化误差上界的推导中（例如定理12.1、定理12.3、定理12.6、定理12.7), 上界中与样本数\\nm 有关的项收玫率均为O(1/√m), 但在该定理中却是O(β√m); 一般来讲, 随着样本数m 的增加, 经验误\\n差/损失应该收玫于泛化误差/损失, 因此这里假设β = 1/m (书中式(12.59) 下方第3 行写为β = O(1/m)\\n), 而在第2 条注解“定义12.10 的解释”中已经提到β 的取值的确会随着样本数m 的增多会变小, 虽然\\n书中并没有严格去讨论β 随m 增多的变化规律, 但至少直觉上是对的。\\n12.7.4\\n式(12.60) 的推导\\n将β =\\n1\\nm 带入至式(12.58) 即得证。\\n12.7.5\\n经验损失最小化\\n顾名思义, “经验损失最小化”指通过最小化经验损失来求得假设函数。\\n这里, “对于损失函数ℓ, 若学习算法L 所输出的假设满足经验损失最小化, 则称算法L 满足经验风险\\n最小化原则, 简称算法是ERM 的”。在”西瓜书”第278 页, 若学习算法L 输出的假设h 满足式(12.30),\\n则也称L 为满足经验风险最小化原则的算法。而很明显, 式(12.30) 是在最小化经验误差。\\n那么最小化经验误差和最小化经验损失有什么区别么?\\n在” 西瓜书“第286 页左下角边注中提到, “最小化经验误差和最小化经验损失有时并不相同, 这是由\\n于存在某些病态的损失函数ℓ使得最小化经验损失并不是最小化经验误差”。\\n对于“误差”、“损失”、“风险”等概念的辨析，参见“西瓜书”第2 章2.1 节的注解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.7.6\\n定理(12.9) 的证明的解释\\n首先明确几个概念，ERM 表示算法L 满足经验风险最小化(Empirical Risk Minimization)。由于L\\n满足经验误差最小化，则可令g 表示假设空间中具有最小泛化损失的假设，即\\nℓ(g, D) = min\\nh∈H ℓ(h, D)\\n再令\\nϵ′ = ϵ\\n2\\nδ\\n2 = 2 exp\\n\\x10\\n−2m (ϵ′)2\\x11\\n将ϵ′ = ϵ\\n2 带入到δ\\n2 = 2 exp\\n\\x10\\n−2m (ϵ′)2\\x11\\n可以解得m =\\n2\\nϵ2 ln 4\\nδ，由Hoeffding 不等式12.6，\\nP\\n \\x0c\\x0c\\x0c\\x0c\\x0c\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi)\\n\\x0c\\x0c\\x0c\\x0c\\x0c ⩾ϵ\\n!\\n⩽2 exp\\n\\x00−2mϵ2\\x01\\n其中\\n1\\nm\\nPm\\ni=1 E (xi) = ℓ(g, D)，1\\nm\\nPm\\ni=1 xi = bℓ(g, D)，带入可得\\nP(|ℓ(g, D) −bℓ(g, D)| ⩾ϵ\\n2) ⩽δ\\n2\\n根据逆事件的概率可得\\nP(|ℓ(g, D) −bℓ(g, D)| ⩽ϵ\\n2) ⩾1 −δ\\n2\\n即文中|ℓ(g, D) −bℓ(g, D)| ⩽ϵ\\n2 至少以1 −δ/2 的概率成立。\\n由\\n2\\nm + (4 + M)\\nq\\nln(2/δ)\\n2m\\n= ϵ\\n2 可以求解出\\n√m =\\n(4 + M)\\nq\\nln(2/δ)\\n2\\n+\\nq\\n(4 + M)2 ln(2/δ)\\n2\\n−4 × ϵ\\n2 × (−2)\\n2 × ϵ\\n2\\n即m = O\\n\\x00 1\\nϵ2 ln 1\\nδ\\n\\x01\\n。\\n由P(|ℓ(g, D) −bℓ(g, D)| ⩽ϵ\\n2) ⩾1 −δ\\n2 可以按照同公式12.31 中介绍的相同的方法推导出\\nP(ℓ(L, D) −ℓ(g, D) ⩽ϵ) ⩾1 −δ\\n又因为m 为与(1/ϵ, 1/δ, size(x), size(c)) 相关的多项式的值，因此根据定理12.2，定理12.5，得到结\\n论H 是(不可知)PAC 可学习的。\\n参考文献\\n[1] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the\\nAmerican statistical association, 58(301):13–30, 1963.\\n[2] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of\\nevents to their probabilities. In Measures of complexity, pages 11–30. Springer, 2015.\\n[3] Wikipedia contributors. Binomial theorem, 2020.\\n[4] Wikipedia contributors. E, 2020.\\n[5] robjohn. Supremum of the difference of two functions, 2013.\\n[6] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. 2018.\\n[7] Wikipedia contributors. Triangle inequality, 2020.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第13 章\\n半监督学习\\n13.1\\n未标记样本\\n“西瓜书”两张插图可谓本节亮点: 图13.1 直观地说明了使用末标记样本后带来的好处; 图13.2 对比\\n了主动学习、(纯) 半监督学习和直推学习, 尤其是巧妙地将主动学习的概念融入进来。\\n直推学习是综合运用手头上已有的少量有标记样本和大量末标记样本, 对这些大量末标记样本预测其\\n标记; 而(纯) 半监督学习是综合运用手头上已有的少量有标记样本和大量末标记样本, 对新的末标记样本\\n预测其标记。\\n对于直推学习, 当然可以仅利用有标记样本训练一个学习器, 再对末标记样本进行预测, 此即传统的监\\n督学习; 对于(纯) 半监督学习, 当然也可以舍弃大量末标记样本, 仅利用有标记样本训练一个学习器, 再对\\n新的末标记样本进行预测。但图13.1 直观地说明了使用末标记样本后带来的好处, 然而利用了末标记样本\\n后是否真的会如图13.1 所示带来预期的好处呢? 此即13.7 节阅读材料中提到的安全半监督学习。\\n接下来在13.2 节、13.3 节、13.4 节、13.5 节介绍的四种半监督学习方法, 都可以应用于直推学习, 但\\n若要应用于(纯) 半监督学习, 则要有额外的考虑, 尤其是13.4 节介绍的图半监督学习, 因为该节最后一段\\n也明确提到“构图过程仅能考虑训练样本集, 难以判知新样本在图中的位置, 因此, 在接收到新样本时, 或\\n是将其加入原数据集对图进行重构并重新进行标记传播, 或是需引入额外的预测机制”。\\n13.2\\n生成式方法\\n本节与9.4.3 节的高斯混合聚类密切相关, 有关9.4.3 节的公式推导参见附录, 建议将高斯混合聚类的\\n内容理解之后再学习本节算法。\\n13.2.1\\n式(13.1) 的解释\\n高斯混合分布的定义式。该式即为9.4.3 节的式(9.29)，式(9.29) 中的k 个混合成分对应于此处的N\\n个可能的类别。\\n13.2.2\\n式(13.2) 的推导\\n首先, 该式的变量Θ ∈{1, 2, . . . , N} 即为式(9.30) 中的zj ∈{1, 2, . . . , k} 。\\n从公式第1 行到第2 行是对概率进行边缘化(marginalization)；通过引入Θ 并对其求和PN\\ni=1 以抵\\n消引入的影响。从公式第2 行到第3 行推导如下\\np(y = j, Θ = i|x) = p(y = j, Θ = i, x)\\np(x)\\n= p(y = j, Θ = i, x)\\np(Θ = i, x)\\n· p(Θ = i, x)\\np(x)\\n= p(y = j|Θ = i, x) · p(Θ = i|x)\\np(y = j | x) 表示x 的类别y 为第j 个类别标记的后验概率（注意条件是已知x);\\np(y = j, Θ = i | x) 表示x 的类别y 为第j 个类别标记且由第i 个高斯混合成分生成的后验概率（注\\n意条件是已知x );\\np(y = j | Θ = i, x) 表示第i 个高斯混合成分生成的x 其类别y 为第j 个类别标记的概率（注意条件\\n是已知Θ 和x, 这里修改了西瓜书式(13.3) 下方对p(y = j | Θ = i, x) 的表述);\\np(Θ = i | x) 表示x 由第i 个高斯混合成分生成的后验概率（注意条件是已知x) 。\\n“西瓜书”第296 页第2 行提到“假设样本由高斯混合模型生成, 且每个类别对应一个高斯混合成分”\\n, 也就是说, 如果已知x 是由哪个高斯混合成分生成的, 也就知道了其类别。而p(y = j | Θ = i, x) 表示已\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n知Θ 和x 的条件概率（已知Θ 就足够, 不需x 的信息), 因此\\np(y = j | Θ = i, x) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1,\\ni = j\\n0,\\ni ̸= j\\n13.2.3\\n式(13.3) 的推导\\n根据式(13.1)\\np(x) =\\nN\\nX\\ni=1\\nαi · p (x|µi, Σi)\\n因此\\np(Θ = i|x) = p(Θ = i, x)\\nP(x)\\n=\\nαi · p (x|µi, Σi)\\nPN\\ni=1 αi · p (x|µi, Σi)\\n13.2.4\\n式(13.4) 的推导\\n第二项很好解释，当不知道类别信息的时候，样本xj 的概率可以用式13.1 表示，所有无类别信息的\\n样本Du 的似然是所有样本的乘积，因为ln 函数是单调的，所以也可以将ln 函数作用于这个乘积消除因\\n为连乘产生的数值计算问题。第一项引入了样本的标签信息，由\\np(y = j|Θ = i, x) =\\n(\\n1,\\ni = j\\n0,\\ni ̸= j\\n可知，这项限定了样本xj 只可能来自于yj 所对应的高斯分布。\\n13.2.5\\n式(13.5) 的解释\\n参见式(13.3)，这项可以理解成样本xj 属于类别标签i(或者说由第i 个高斯分布生成) 的后验概率。\\n其中αi, µiΣi 可以通过有标记样本预先计算出来。即：\\nαi =\\nli\\n|Dl|, where |Dl| = PN\\ni=1 li\\nµi = 1\\nli\\nP\\n(xj,yj)∈Dl∧yj=i xj\\nΣi = 1\\nli\\nP\\n(xj,yj)∈Dl∧yj=i (xj −µi) (xj −µi)⊤\\n其中li 表示第i 类样本的有标记样本数目, |Dl| 为有标记样本集样本总数, ∧为“逻辑与”。\\n13.2.6\\n式(13.6) 的解释\\n这项可以由\\n∂LL(Dl ∪Du)\\n∂µi\\n= 0\\n而得，将式13.4 的两项分别记为：\\nLL(Dl) =\\nX\\n(xj,yj∈Dl)\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs) · p(yi|Θ = s, xj)\\n!\\n=\\nX\\n(xj,yj∈Dl)\\nln\\n\\x10\\nαyj · p(xj|µyj, Σyj)\\n\\x11\\nLL(Du) =\\nX\\nxj∈Du\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs)\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n首先，LL(Dl) 对µi 求偏导，LL(Dl) 求和号中只有yj = i 的项能留下来，即\\n∂LL (Dl)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi)\\nLL(Du) 对µi 求导，参考9.33 的推导：\\n∂LL (Du)\\n∂µi\\n=\\nX\\nxj∈Du\\nαi\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n· p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n=\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi)\\n综上，\\n∂LL (Dl ∪Du)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi) +\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi)\\n= Σ−1\\ni\\n\\uf8eb\\n\\uf8ed\\nX\\n(xj,yj)∈Dl∧yj=i\\n(xj −µi) +\\nX\\nxj∈Du\\nγji · (xj −µi)\\n\\uf8f6\\n\\uf8f8\\n= Σ−1\\ni\\n\\uf8eb\\n\\uf8ed\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj +\\nX\\nxj∈Du\\nγji · xj −\\nX\\n(xj,yj)∈Dl∧yj=i\\nµi −\\nX\\nxj∈Du\\nγji · µi\\n\\uf8f6\\n\\uf8f8\\n令∂LL(Dl∪Du)\\n∂µi\\n= 0，两边同时左乘Σi 并移项：\\nX\\nxj∈Du\\nγji · µi +\\nX\\n(xj,yj)∈Dl∧yj=i\\nµi =\\nX\\nxj∈Du\\nγji · xj +\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj\\n上式中，µi 可以作为常量提到求和号外面，而P\\n(xj,yj)∈Dl∧yj=i 1 = li，即第i 类样本的有标记样本数\\n目，因此\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji +\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\n\\uf8f6\\n\\uf8f8µi =\\nX\\nxj∈Du\\nγji · xj +\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj\\n即得式(13.6)。\\n13.2.7\\n式(13.7) 的解释\\n首先LL(Dl) 对Σi 求偏导，类似于式(13.6)\\n∂LL (Dl)\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · p (xj|µi, Σi) ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n然后LL(Du) 对Σi 求偏导，类似于式(9.35)\\n∂LL (Du)\\n∂Σi\\n=\\nX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n综合可得：\\n∂LL (Dl ∪Du)\\n∂Σi\\n=\\nX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n+\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n+\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n\\uf8f6\\n\\uf8f8· 1\\n2Σ−1\\ni\\n令∂LL(Dl∪Du)\\n∂Σi\\n= 0，两边同时右乘2Σi 并移项：\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤\\n=\\nX\\nxj∈Du\\nγji · I +\\nX\\n(xj,yj)∈Dl∧yj=i\\nI\\n=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji + li\\n\\uf8f6\\n\\uf8f8I\\n两边同时左乘以Σi：\\nX\\nxj∈Du\\nγji · (xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj)∈Dl∧yj=i\\n(xj −µi) (xj −µi)⊤=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji + li\\n\\uf8f6\\n\\uf8f8Σi\\n即得式(13.7)。\\n13.2.8\\n式(13.8) 的解释\\n类似于式(9.36)，写出LL(Dl ∪Du) 的拉格朗日形式\\nL (Dl ∪Du, λ) = LL (Dl ∪Du) + λ\\n N\\nX\\ns=1\\nαs −1\\n!\\n= LL (Dl) + LL (Du) + λ\\n N\\nX\\ns=1\\nαs −1\\n!\\n类似于式(9.37)，对αi 求偏导。对于LL(Du)，求导结果与式(9.37) 的推导过程一样\\n∂LL (Du)\\n∂αi\\n=\\nX\\nxj∈Du\\n1\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n· p (xj|µi, Σi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于LL(Dl)，类似于式(13.6) 和式(13.7) 的推导过程\\n∂LL (Dl)\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi · p (xj|µi, Σi) · ∂(αi · p (xj|µi, Σi))\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi · p (xj|µi, Σi) · p (xj|µi, Σi)\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi\\n= 1\\nαi\\n·\\nX\\n(xj,yj)∈Dl∧yj=i\\n1 = li\\nαi\\n上式推导过程中，重点注意变量是αi ，p(xj|µi, Σi) 是常量；最后一行αi 相对于求和变量为常量，因\\n此作为公因子提到求和号外面；li 为第i 类样本的有标记样本数目。\\n综合两项结果：\\n∂L (Dl ∪Du, λ)\\n∂αi\\n= li\\nαi\\n+\\nX\\nxj∈Du\\np (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ\\n令\\n∂LL(Dl ∪Du)\\n∂αi\\n= 0 并且两边同乘以αi，得\\nαi · li\\nαi\\n+\\nX\\nxj∈Du\\nαi · p (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ · αi = 0\\n结合式(9.30) 发现，求和号内即为后验概率γji, 即\\nli +\\nX\\nxi∈Du\\nγji + λαi = 0\\n对所有混合成分求和，得\\nN\\nX\\ni=1\\nli +\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji +\\nN\\nX\\ni=1\\nλαi = 0\\n这里PN\\ni=1 αi = 1 ，因此PN\\ni=1 λαi = λ PN\\ni=1 αi = λ，根据9.30 中γji 表达式可知\\nN\\nX\\ni=1\\nγji =\\nN\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\nΣN\\ns=1αs · p(xj|µs, Σs) =\\nPN\\ni=1 αi · p(xj|µi, Σi)\\nPN\\ns=1 αs · p(xj|µs, Σs)\\n= 1\\n再结合加法满足交换律，所以\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji =\\nX\\nxi∈Du\\nN\\nX\\ni=1\\nγji =\\nX\\nxi∈Du\\n1 = u\\n以上分析过程中，P\\nxj∈Du 形式与Pu\\nj=1 等价，其中u 为未标记样本集的样本个数；PN\\ni=1 li = l 其中\\nl 为有标记样本集的样本个数；将这些结果代入\\nN\\nX\\ni=1\\nli +\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji +\\nN\\nX\\ni=1\\nλαi = 0\\n解出l + u + λ = 0 且l + u = m 其中m 为样本总个数，移项即得λ = −m，最后带入整理解得\\nli +\\nX\\nxj∈Du\\nγji −λαi = 0\\n即li + P\\nxj∈Du γji −mαi = 0, 整理即得式13.8。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 168, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n13.3\\n半监督SVM\\n从本节名称“半监督SVM”即可知道与第6 章的SVM 内容联系紧密。建议理解了SVM 之后再学\\n习本节算法，会发现实际很简单；否则会感觉无从下手，难以理解。\\n由本节开篇的两段介绍可知，S3VM 是SVM 在半监督学习上的推广，是此类算法的总称而非某个具\\n体的算法，其最著名的代表是TSVM。\\n13.3.1\\n图13.3 的解释\\n注意对比S3VM 划分超平面穿过的区域与SVM 划分超平面穿过的区域的差别，明显S3VM 划分超\\n平面周围样本较少，也就是“数据低密度区域”，即“低密度分隔”。\\n13.3.2\\n式(13.9) 的解释\\n这个公式和式(6.35) 基本一致，除了引入了无标记样本的松弛变量ξi, i = l + 1, · · · m 和对应的权重\\n系数Cu 和无标记样本的标记指派ˆyi。因此，欲理解本节内容应该先理解SVM，否则会感觉无从下手，难\\n以理解。\\n13.3.3\\n图13.4 的解释\\n解释一下第6 行:\\n(1) ˆyiˆyj < 0 意味着末标记样本xi, xj 在此次迭代中被指派的标记ˆyi, ˆyj 相反(正例+1 和反例−1 各\\n1 个);\\n(2) ξi > 0 意味着末标记样本xi 在此次迭代中为支持向量: (a) 在间隔带内但仍与自己标记同侧\\n(0 < ξi < 1), (b) 在间隔带内但与自己标记异侧(1 < ξi < 2), (c) 不在间隔带且与自己标记异侧(ξi > 2);\\n三种情况分别如下图(a)(b)(c) 所示:\\n(3) ξi + ξj > 2 分两种情况: (I) (ξi > 1) ∧(ξj > 1), 表示都位于自己指派标记异侧, 交换它们的标记后,\\n二者就都位于自己新指派标记同侧了, 如下图所示(1 < ξi, ξj < 2) :\\n可以发现, 当1 < ξi, ξj < 2 时, 交换之后虽然松弛变量仍然大于0 , 但至少ξi + ξj 比交换之前变小了;\\n若进一步的, 当ξi, ξj > 2 时, 则交换之后ξi + ξj 将变为0 , 如下图所示:\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 169, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n可以发现, 交换之后两个样本均被分类正确, 因此松弛变量均等于0 ; 至于ξi, ξj 其中之一位于1 ∼2\\n之间, 另一个大于2 , 情况类似, 不单列出分析。\\n(II) (0 < ξi < 1) ∧(ξj > 2 −ξi), 表示有一个与自己标记同侧, 有一个与自己标记异侧, 此时可分两种\\n情况: (II.1) 1 < ξj < 2, 表示样本与自己标记异侧, 但仍在间隔带内:\\n可以发现, 此时两个样本位置超平面同一侧, 交换标记之后似乎没发生什么变化, 但是仔细观察会发现\\n交换之后ξi + ξj 比交换之前变小了; (II.2) ξj > 2, 表示样本在间隔带外:\\n可以发现, 交换之后其中之一被正确分类, ξi + ξj 比交换之前也变小了。综上所述, 当ξi + ξj > 2 时,\\n交换指派标记ˆyi, ˆyj 可以使ξi + ξj 下降, 也就是说分类结果会得到改善。再解释一下第11 行: 逐步增长\\nCu, 但不超过Cl, 末标记样本的权重小于有标记样本。\\n13.3.4\\n式(13.10) 的解释\\n将该式变形为C+\\nu\\nC−\\nu = u−\\nu+ , 即样本个数多的权重小, 样本个数少的权重大, 总体上保持二者的作用相同。\\n13.4\\n图半监督学习\\n本节共讲了两种方法，其中式(13.11) ~ 式(13.17) 讲述了一个针对二分类问题的标记传播方法，式\\n(13.18) ~ 式(13.21) 讲述了一个针对多分类问题的标记传播方法，两种方法的原理均为两种方法的原理均\\n为“相似的样本应具有相似的标记”，只是面向的问题不同，而且具体实现的方法也不同。\\n13.4.1\\n式(13.12) 的推导\\n注意, 该方法针对二分类问题的标记传播方法。我们希望能量函数E(f) 越小越好, 注意到式(13.11)\\n的0 < (W)ij ⩽1, 且样本xi 和样本xj 越相似(即∥xi −xj∥2 越小) 则(W)ij 越大, 因此要求式(13.12)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n中的(f (xi) −f (xj))2 相应地越小越好(即“相似的样本应具有相似的标记”), 如此才能达到能量函数\\nE(f) 越小的目的。首先对式(13.12) 的第1 行式子进行展开整理:\\nE(f) = 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ij (f (xi) −f (xj))2\\n= 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ij\\n\\x00f 2 (xi) −2f (xi) f (xj) + f 2 (xj)\\n\\x01\\n= 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xi) + 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n然后证明Pm\\ni=1\\nPm\\nj=1(W)ijf 2 (xi) = Pm\\ni=1\\nPm\\nj=1(W)ijf 2 (xj), 并变形:\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) =\\nm\\nX\\nj=1\\nm\\nX\\ni=1\\n(W)jif 2 (xi) =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xi)\\n=\\nm\\nX\\ni=1\\nf 2 (xi)\\nm\\nX\\nj=1\\n(W)ij\\n其中, 第1 个等号是把变量i, j 分别用j, i 替代(统一替换公式中的符号并不影响公式本身); 第2 个等\\n号是由于W 是对称矩阵(即(W)ij = W)ji ), 并交换了求和号次序(类似于多重积分中交换积分号次序),\\n到此完成了该步骤的证明; 第3 个等号是由于f 2 (xi) 与求和变量j 无关, 因此拿到了该求和号外面(与\\n求和变量无关的项相对于该求和变量相当于常数), 该步骤的变形主要是为了得到di 。令di = Pm\\nj=1(W)ij\\n( 既是W 第i 行元素之和, 实际亦是第i 列元素之和, 因为由于W 是对称矩阵, 即(W)ij = W)ji, 因此\\ndi = Pm\\nj=1(W)ji, 即第例元素之和), 则\\nE(f) =\\nm\\nX\\ni=1\\ndif 2 (xi) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n即式(13.12) 的第3 行, 其中第一项Pm\\ni=1 dif 2 (xi) 可以写为如下矩阵形式:\\n= f TDf\\n第二项Pm\\ni=1\\nPm\\nj=1(W)ijf (xi) f (xj) 也可以写为如下矩阵形式:\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n=\\nh\\nf (x1)\\nf (x2)\\n· · ·\\nf (xm)\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(W)11\\n(W)12\\n· · ·\\n(W)1m\\n(W)21\\n(W)22\\n· · ·\\n(W)2m\\n...\\n...\\n...\\n...\\n(W)m1\\n(W)m2\\n· · ·\\n(W)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nf (x1)\\nf (x2)\\n...\\nf (xm)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= f TW f\\n所以E(f) = f TD −f TW f = f T(D −W )f, 即式(13.12)。\\n13.4.2\\n式(13.13) 的推导\\n本式就是将式(13.12) 用分块矩阵形式表达而已, 拆分为标记样本和末标记样本两部分。\\n另外解释一下该式之前一段话中第一句的含义:“具有最小能量的函数f 在有标记样本上满足f (xi) =\\nyi(i = 1, 2, . . . , l), 在末标记样本上满足∆f = 0 ”, 前半句是很容易理解的, 有标记样本上满足f (xi) =\\nyi(i = 1, 2, . . . , l), 这时末标记样本的f (xi) 是待求变量且应该使E(f) 最小, 因此应将式(13.12) 对末标\\n记样本的f (xi) 求导并令导数等于0 即可, 此即表达式∆f = 0, 此处可以查看该算法的原始文献。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n13.4.3\\n式(13.14) 的推导\\n将式(13.13) 根据矩阵运算规则进行变形, 这里第一项西瓜书中的符号有歧义，应该表示成\\nh\\nf T\\nl\\nf T\\nu\\ni\\n即一个R1×(l+u) 的行向量。根据矩阵乘法的定义，有：\\nE(f) =\\nh\\nf T\\nl\\nf T\\nu\\ni \"\\nDll −W ll\\n−W lu\\n−W ul\\nDuu −W uu\\n# \"\\nf l\\nf u\\n#\\n=\\nh\\nf T\\nl (Dll −W ll) −f T\\nuW ul\\n−f T\\nl W lu + f T\\nu (Duu −W uu)\\ni \"\\nf l\\nf u\\n#\\n=\\n\\x00f T\\nl (Dll −W ll) −f T\\nuW ul\\n\\x01\\nf l +\\n\\x00−f T\\nl W lu + f T\\nu (Duu −W uu)\\n\\x01\\nf u\\n= f T\\nl (Dll −W ll) f l −f T\\nuW ulf l −f T\\nl W luf u + f T\\nu (Duu −W uu) f u\\n= f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T\\nu (Duu −W uu) f u\\n其中最后一步，f T\\nl W luf u =\\n\\x00f T\\nl W luf u\\n\\x01T = f T\\nu W ulf l，因为这个式子的结果是一个标量。\\n13.4.4\\n式(13.15) 的推导\\n首先，基于式(13.14) 对f u 求导：\\n∂E(f)\\n∂f u\\n= ∂f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T\\nu (Duu −W uu) f u\\n∂f u\\n= −2W ulf l + 2 (Duu −W uu) f u\\n令结果等于0 即得13.15。\\n注意式中各项的含义:\\nf u 即函数f 在末标记样本上的预测结果;\\nDuu, W uu, W ul 均可以由式(13.11) 得到;\\nf l 即函数f 在有标记样本上的预测结果(即已知标记, 详见“西瓜书”P301 倒数第3 行);\\n也就是说可以根据式(13.15) 根据Dl 上的标记信息(即f l ) 求得末标记样本的标记(即f u ), 式\\n(13.17) 仅是式(13.15) 的进一步变形化简, 不再细述。\\n仔细回顾该方法, 实际就是根据“相似的样本应具有相似的标记”的原则, 构建了目标函数式(13.12),\\n求解式(13.12) 得到了使用标记样本信息表示的末标记样本的预测标记。\\n13.4.5\\n式(13.16) 的解释\\n根据矩阵乘法的定义计算可得该式，其中需要注意的是，对角矩阵D 的拟等于其各个对角元素的倒\\n数。\\n13.4.6\\n式(13.17) 的推导\\n第一项到第二项是根据矩阵乘法逆的定义：(AB)−1 = B−1A−1，在这个式子中\\nPuu = D−1\\nuuWuu\\nPul = D−1\\nuuWul\\n均可以根据Wij 计算得到，因此可以通过标记fl 计算未标记数据的标签fu。\\n13.4.7\\n式(13.18) 的解释\\n其中Y 的第i 行表示第i 个样本的类别; 具体来说, 对于前l 个有标记样本来说, 若第i 个样本的类\\n别为j(1 ≤j ≤|Y|), 则Y 的第行第j 列即为1 , 第行其余元素为0 ; 对于后u 个末标记样本来说, Y 统\\n一为零。注意|Y| 表示集合Y 的势, 即包含元素(类别) 的个数。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n13.4.8\\n式(13.20) 的解释\\nF∗= lim\\nt→∞F(t) = (1 −α)(I −αS)−1Y\\n[解析]：由式(13.19)\\nF(t + 1) = αSF(t) + (1 −α)Y\\n当t 取不同的值时，有：\\nt = 0 : F(1) = αSF(0) + (1 −α)Y\\n= αSY + (1 −α)Y\\nt = 1 : F(2) = αSF(1) + (1 −α)Y = αS(αSY + (1 −α)Y) + (1 −α)Y\\n= (αS)2Y + (1 −α)\\n \\n1\\nX\\ni=0\\n(αS)i\\n!\\nY\\nt = 2 : F(3) = αSF(2) + (1 −α)Y\\n= αS\\n \\n(αS)2Y + (1 −α)\\n \\n1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n!\\n+ (1 −α)Y\\n= (αS)3Y + (1 −α)\\n \\n2\\nX\\ni=0\\n(αS)i\\n!\\nY\\n可以观察到规律\\nF(t) = (αS)tY + (1 −α)\\n t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n则\\nF∗= lim\\nt→∞F(t) = lim\\nt→∞(αS)tY + lim\\nt→∞(1 −α)\\n t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n其中第一项由于S = D−1\\n2 WD−1\\n2 的特征值介于[-1, 1] 之间[1]，而α ∈(0, 1)，所以limt→∞(αS)t = 0，第\\n二项由等比数列公式\\nlim\\nt→∞\\nt−1\\nX\\ni=0\\n(αS)i = I −limt→∞(αS)t\\nI −αS\\n=\\nI\\nI −αS = (I −αS)−1\\n综合可得式(13.20)。\\n13.4.9\\n公式(13.21)\\n这里主要是推导式(13.21) 的最优解即为式(13.20)。将式(13.21) 的目标函数进行变形:\\n第1 部分:\\n先将范数平方拆开为四项:\\n\\r\\r\\r\\r\\r\\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n\\r\\r\\r\\r\\r\\n2\\n=\\n \\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n!  \\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n!⊤\\n= 1\\ndi\\nFiF⊤\\ni + 1\\ndj\\nFjF⊤\\nj −\\n1\\np\\ndidj\\nFiF⊤\\nj −\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n其中Fi ∈R1×|Y| 表示矩阵F 的第i 行, 即第i 个示例xi 的标记向量。将第1 项中的Pm\\ni,j=1 写为两\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n个和求号Pm\\ni=1\\nPm\\ni=1 的形式, 并将上面拆分的四项中的前两项代入, 得\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\ndi\\nFiF⊤\\ni =\\nm\\nX\\ni=1\\n1\\ndi\\nFiF⊤\\ni\\nm\\nX\\nj=1\\n(W)ij =\\nm\\nX\\ni=1\\n1\\ndi\\nFiF⊤\\ni · di =\\nm\\nX\\ni=1\\nFiF⊤\\ni\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\ndj\\nFjF⊤\\nj =\\nm\\nX\\nj=1\\n1\\ndj\\nFjF⊤\\nj\\nm\\nX\\ni=1\\n(W)ij =\\nm\\nX\\nj=1\\n1\\ndj\\nFjF⊤\\nj · dj =\\nm\\nX\\nj=1\\nFjF⊤\\nj\\n以上化简过程中, 两个求和号可以交换求和次序; 又因为W 为对称阵, 因此对行求和与对列求和效果\\n一样, 即di = Pm\\nj=1(W)ij = Pm\\nj=1(W)ji （已在式(13.12) 推导时说明）。显然,\\nm\\nX\\ni=1\\nFiF⊤\\ni =\\nm\\nX\\nj=1\\nFjF⊤\\nj =\\nm\\nX\\ni=1\\n∥Fi∥2 = ∥F∥2\\nF = tr\\n\\x00FF⊤\\x01\\n以上推导过程中, 第1 个等号显然成立, 因为二者仅是求和变量名称不同; 第2 个等号即将FiF⊤\\ni 写\\n为∥Fi∥2 形式; 从第2 个等号的结果可以看出这明显是在求矩阵F 各元素平方之和, 也就是矩阵F 的\\nFrobenius 范数（简称F 范数）的平方, 即第3 个等号; 根据矩阵F 范数与矩阵的迹的关系有第4 个等号\\n(详见本章预备知识: 矩阵的F 范数与迹)。接下来, 将上面拆分的四项中的第三项代入，得\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj = tr\\n\\x00S⊤FF⊤\\x01\\n= tr\\n\\x00SFF⊤\\x01\\n具体来说, 以上化简过程为:\\nS =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)11\\n(S)12\\n· · ·\\n(S)1m\\n(S)21\\n(S)22\\n· · ·\\n(S)2m\\n...\\n...\\n...\\n...\\n(S)m1\\n(S)m2\\n· · ·\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= D−1\\n2 WD−1\\n2\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n√d1\\n1\\n√d2\\n...\\n1\\n√dm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(W)11\\n(W)12\\n· · ·\\n(W)1m\\n(W)21\\n(W)22\\n· · ·\\n(W)2m\\n...\\n...\\n...\\n...\\n(W)m1\\n(W)m2\\n· · ·\\n(W)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n√d1\\n1\\n√d2\\n...\\n1\\n√dm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n由以上推导可以看出(S)ij =\\n1\\n√\\ndidj (W)ij, 即第1 个等号; 而\\nFF⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1\\nF2\\n...\\nFm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nh\\nF⊤\\n1\\nF⊤\\n2\\n· · ·\\nF⊤\\nm\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n1\\nF1F⊤\\n2\\n. . .\\nF1F⊤\\nm\\nF2F⊤\\n1\\nF2F⊤\\n2\\n· · ·\\nF2F⊤\\nm\\n...\\n...\\n...\\n...\\nFmF⊤\\n1\\nFmF⊤\\n2\\n· · ·\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n若令A = S ◦FF⊤, 其中\\uffff表示Hadmard 积, 即矩阵S 与矩阵FF⊤元素对应相乘（参见百度百科哈\\n达玛积), 因此\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(A)ij\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n可以验证, 上式的矩阵A = S ◦FF⊤元素之和Pm\\ni,j=1(A)ij 等于tr\\n\\x00S⊤FF⊤\\x01\\n, 这是因为\\ntr\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)11\\n(S)12\\n· · ·\\n(S)1m\\n(S)21\\n(S)22\\n· · ·\\n(S)2m\\n...\\n...\\n...\\n...\\n(S)m1\\n(S)m2\\n· · ·\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n1\\nF1F⊤\\n2\\n· · ·\\nF1F⊤\\nm\\nF2F⊤\\n1\\nF2F⊤\\n2\\n· · ·\\nF2F⊤\\nm\\n...\\n...\\n...\\n...\\nFmF⊤\\n1\\nFmF⊤\\n2\\n· · ·\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)11\\n(S)21\\n...\\n(S)m1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n1\\nF2F⊤\\n1\\n...\\nFmF⊤\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)12\\n(S)22\\n...\\n(S)m2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n2\\nF2F⊤\\n2\\n...\\nFmF⊤\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ . . . +\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)1m\\n(S)2m\\n...\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\nm\\nF2F⊤\\nm\\n...\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\nm\\nX\\ni=1\\n(S)i1FiF⊤\\n1 +\\nm\\nX\\ni=1\\n(S)i2FiF⊤\\n2 + . . . +\\nX\\ni=1\\n(S)imFiF⊤\\nm\\n=\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj\\n即第2 个等号; 易知矩阵S 是对称阵\\n\\x00S⊤= S\\n\\x01\\n, 即得第3 个等号。又由于内积FiF⊤\\nj 是一个数(即大\\n小为1 × 1 的矩阵), 因此其转置等于本身,\\nFiF⊤\\nj =\\n\\x00FiF⊤\\nj\\n\\x01⊤=\\n\\x00F⊤\\nj\\n\\x01⊤(Fi)⊤= FjF⊤\\ni\\n因此\\n1\\np\\ndidj\\nFiF⊤\\nj =\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n进而上面拆分的四项中的第三项和第四项相等:\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n综上所述(以上拆分的四项中前两项相等、后两项相等, 正好抵消系数1\\n2 ):\\n1\\n2\\n\\uf8eb\\n\\uf8ed\\nm\\nX\\ni,j=1\\n(W)ij\\n\\r\\r\\r\\r\\r\\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n\\r\\r\\r\\r\\r\\n2\\uf8f6\\n\\uf8f8= tr\\n\\x00FF⊤\\x01\\n−tr\\n\\x10\\nSFF⊤\\x11\\n第2 部分:\\n西瓜书中式(13.21) 的第2 部分与原文献[2] 中式(4) 的第2 部分不同:\\nQ(F) = 1\\n2\\nn\\nX\\ni,j=1\\nWij\\n\\r\\r\\r\\r\\r\\nFi\\n√Dii\\n−\\nFj\\np\\nDjj\\n\\r\\r\\r\\r\\r\\n2\\n+ µ\\nn\\nX\\ni=1\\n∥Fi −Yi∥2 ,\\n原文献中第2 部分包含了所有样本(求和变量上限为n ), 而西瓜书只包含有标记样本, 并且第304 页\\n第二段提到“式(13.21) 右边第二项是迫使学得结果在有标记样本上的预测与真实标记尽可能相同”; 若\\n按原文献式(4) 在第二项中将末标记样本也包含进来, 由于对于末标记样本Yi = 0, 因此直观上理解是迫\\n使末标记样本学习结果尽可能接近0 , 这显然是不对的; 有关这一点作者在第24 次印刷勘误中进行了补\\n充: “考虑到有标记样本通常很少而末标记样本很多, 为缓解过拟合, 可在式(13.21) 中引入针对末标记样\\n本的L2 范数项µ Pl+u\\ni=l+1 ∥Fi∥2, 式(13.21) 加上此项之后就与原文献的式(4) 完全相同了。将第二项写为\\nF 范数形式:\\nm\\nX\\ni=1\\n∥Fi −Yi∥2 = ∥F −Y∥2\\nF\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n综上, 式(13.21) 目标函数Q(F) = tr\\n\\x00FF⊤\\x01\\n−tr\\n\\x00SFF⊤\\x01\\n+ µ∥F −Y∥2\\nF, 求导:\\n∂Q(F)\\n∂F\\n= ∂tr\\n\\x00FF⊤\\x01\\n∂F\\n−∂tr\\n\\x00SFF⊤\\x01\\n∂F\\n+ µ∂∥F −Y∥2\\nF\\n∂F\\n= 2F −2SF + 2µ(F −Y)\\n令µ = 1−α\\nα , 并令∂Q(F)\\n∂F\\n= 2F −2SF + 2 1−α\\nα (F −Y) = 0, 移项化简即可得式(13.20), 即式(13.20) 是正则\\n化框架式(13.21) 的解。\\n13.5\\n基于分歧的方法\\n“西瓜书”的伟大之处在于巧妙地融入了很多机器学习的研究分支, 而非仅简单介绍经典的机器学习\\n算法。比如本节处于半监督学习章节范围内, 巧妙地将机器学习的研究热点之一多视图学习[3](multi-view\\nlearning) 融入进来, 类似地还有本章第一节将主动学习融入进来, 在第10 章第一节将k 近邻算法融入进\\n来, 在最后一节巧妙地将度量学习(metric learning) 融入进来等等。\\n协同训练是多视图学习代表性算法之一, 本章叙述简单易懂。\\n13.5.1\\n图13.6 的解释\\n第2 行表示从样本集Du 中去除缓冲池样本Ds;\\n第4 行, 当j = 1 时\\n\\nxj\\ni, x3−j\\ni\\n即为⟨x1\\ni , x2\\ni ⟩, 当j = 2 时\\n\\nxj\\ni, x3−j\\ni\\n即为⟨x2\\ni , x1\\ni ⟩, 往后的3 −j 与此\\n相同; 注意本页左上角的注释: ⟨x1\\ni , x2\\ni ⟩与⟨x2\\ni , x1\\ni ⟩表示的是同一个样本, 因此第1 个视图的有标记标训练\\n集为D1\\nl = {(x1\\n1, y1) , . . . , (x1\\nl , yl)}, 第2 个视图的有标记标训练集为D2\\nl = {(x2\\n1, y1) , . . . , (x2\\nl , yl)};\\n第9 行到第11 行是根据第j 个视图的对缓冲池末标记样本预测结果置信度赋予伪标记, 准备交给第\\n3 −j 个视图使用。\\n13.6\\n半监督聚类\\n13.6.1\\n图13.7 的解释\\n注意算法第4 行到第21 行是依次对每个样本进行处理, 其中第8 行到第21 行是尝试将样本xi 到底\\n应该划入哪个族, 具体来说是按样本xi 到各均值向量的距离从小到大依次尝试, 若最小的不违背M 和C\\n中的约束, 则将样本xi 划入该簇并置is_merge=true, 此时第8 行的while 循环条件为假不再继续循环,\\n若从小到大依次尝试各簇后均违背M 和C 中的约束则第16 行的if 条件为真, 算法报错结束; 依次对每\\n个样本进行处理后第22 行到第24 行更新均值向量, 重新开始新一轮迭代, 直到均值向量均末更新。\\n13.6.2\\n图13.9 的解释\\n算法第6 行到第10 行即在聚类簇迭代更新过程中不改变种子样本的簇隶属关系；第11 行到第15 行\\n即对非种子样本进行普通的k-means 聚类过程；第16 行到第18 行更新均值向量，反复迭代，直到均值\\n向量均未更新。\\n参考文献\\n[1] Wikipedia contributors. Laplacian matrix, 2020.\\n[2] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf. Learning\\nwith local and global consistency. Advances in neural information processing systems, 16, 2003.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n[3] Chang Xu, Dacheng Tao, and Chao Xu.\\nA survey on multi-view learning.\\narXiv preprint\\narXiv:1304.5634, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 177, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第14 章\\n概率图模型\\n本章介绍概率图模型，前三节分别介绍了有向图模型之隐马尔可夫模型以及无向图模型之马尔可夫随\\n机场和条件随机场；接下来两节分别介绍精确推断和近似推断；最后一节简单介绍了话题模型的典型代表\\n隐狄利克雷分配模型(LDA)。\\n14.1\\n隐马尔可夫模型\\n本节前三段内容实际上是本章的概述，从第四段才开始介绍“隐马尔可夫模型”。马尔可夫的大名相信\\n很多人听说过，比如马尔可夫链；虽然隐马尔可夫模型与马尔可夫链并非同一人提出，但其中关键字“马\\n尔可夫”蕴含的概念是相同的，即系统下一时刻的状态仅由当前状态决定。\\n14.1.1\\n生成式模型和判别式模型\\n一般来说, 机器学习的任务是根据输入特征x 预测输出变量y; 生成式模型最终求得联合概率P(x, y),\\n而判别式模型最终求得条件概率P(y | x) 。\\n统计机器学习算法都是基于样本独立同分布(independent and identically distributed, 简称i.i.d. .)\\n的假设, 也就是说, 假设样本空间中全体样本服从一个末知的“分布”D, 我们获得的每个样本都是独立地\\n从这个分布上采样获得的。\\n对于一个样本(x, y), 联合概率P(x, y) 表示从样本空间中采样得到该样本的概率; 因为P(x, y) 表示\\n“生成”样本本身的概率, 故称之为“生成式模型”。而条件概率P(y | x) 则表示已知x 的条件下输出为y\\n的概率, 即根据x “判别”y, 因此称为“判别式模型”。\\n常见的对率回归、支持向量机等都属于判别式模型, 而朴素贝叶斯则属于生成式模型。\\n14.1.2\\n式(14.1) 的推导\\n由概率公式P(AB) = P(A | B) · P(B) 可得:\\nP (x1, y1, . . . , xn, yn) = P (x1, . . . , xn | y1, . . . , yn) · P (y1, . . . , yn)\\n其中, 进一步可将P (y1, . . . , yn) 做如下变换:\\nP (y1, . . . , yn) = P (yn | y1, . . . , yn−1) · P (y1, . . . , yn−1)\\n= P (yn | y1, . . . , yn−1) · P (yn−1 | y1, . . . , yn−2) · P (y1, . . . , yn−2)\\n= . . . . . .\\n= P (yn | y1, . . . , yn−1) · P (yn−1 | y1, . . . , yn−2) · . . . · P (y2 | y1) · P (y1)\\n由于状态y1, . . . , yn 构成马尔可夫链, 即yt 仅由yt−1 决定; 基于这种依赖关系, 有\\nP (yn | y1, . . . , yn−1) = P (yn | yn−1)\\nP (yn−1 | y1, . . . , yn−2) = P (yn−1 | yn−2)\\nP (yn−2 | y1, . . . , yn−3) = P (yn−2 | yn−3)\\n因此P (y1, . . . , yn) 可化简为\\nP (y1, . . . , yn) = P (yn | yn−1) · P (yn−1 | yn−2) · . . . · P (y2 | y1) · P (y1)\\n= P (y1)\\nn\\nY\\ni=2\\nP (yi | yi−1)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n而根据“西瓜书”图14.1 表示的变量间的依赖关系: 在任一时刻, 观测变量的取值仅依赖于状态变量,\\n即xt 由yt 确定, 与其它状态变量及观测变量的取值无关。因此\\nP (x1, . . . , xn | y1, . . . , yn) = P (x1 | y1, . . . , yn) · . . . · P (xn | y1, . . . , yn)\\n= P (x1 | y1) · . . . · P (xn | yn)\\n=\\nn\\nY\\ni=1\\nP (xi | yi)\\n综上所述, 可得\\nP (x1, y1, . . . , xn, yn) = P (x1, . . . , xn | y1, . . . , yn) · P (y1, . . . , yn)\\n=\\n n\\nY\\ni=1\\nP (xi | yi)\\n!\\n·\\n \\nP (y1)\\nn\\nY\\ni=2\\nP (yi | yi−1)\\n!\\n= P (y1) P (x1 | y1)\\nn\\nY\\ni=2\\nP (yi | yi−1) P (xi | yi)\\n14.1.3\\n隐马尔可夫模型的三组参数\\n状态转移概率和输出观测概率都容易理解, 简单解释一下初始状态概率。特别注意, 初始状态概率中\\nπi = P (y1 | si) , 1 ⩽i ⩽N, 这里只有y1, 因为y2 及以后的其它状态是由状态转移概率和y1 确定的, 具体\\n参见课本第321 页“给定隐马尔可夫模型λ, 它按如下过程产生观测序列{x1, x2, . . . , xn} ” 的四个步骤。\\n14.2\\n马尔可夫随机场\\n本节介绍无向图模型的著名代表之一：马尔可夫随机场。本节的部分概念（例如势函数、极大团等）\\n比较抽象，我亦无好办法，只能建议多读几遍，从心里接受这些概念就好。另外，从因果关系角度来讲，\\n首先是因为满足全局、局部或成对马尔可夫性的无向图模型称为马尔可夫随机场，所以马尔可夫随机场才\\n具有全局、局部或成对马尔可夫性。\\n14.2.1\\n式(14.2) 和式(14.3) 的解释\\n注意式(14.2) 之前的介绍是“则联合概率P(x) 定义为”, 而在式(14.3) 之前也有类似的描述。因\\n此, 可以将式(14.2) 和式(14.3) 理解为一种定义, 记住并接受这个定义就好了。实际上, 该定义是根据\\nHammersley-Clifford 定理而得, 可以具体了解一下该定理, 这里不再赘述。\\n值得一提的是, 在接下来讨论“条件独立性”时, 即式(14.4) 式(14.7) 的推导过程直接使用了该定义。\\n注意：在有了式(14.3) 的定义后, 式(14.2) 已作废, 不再使用。\\n14.2.2\\n式(14.4) 到式(14.7) 的推导\\n首先, 式(14.4) 直接使用了式(14.3) 有关联合概率的定义。\\n对于式(14.5), 第一行两个等号变形就是概率论中的知识; 第二行的变形直接使用了式(14.3) 有关联\\n合概率的定义; 第三行中, 由于ψAC (x′\\nA, xC) 与变量x′\\nB 无关, 因此可以拿到求和号P\\nx′\\nB 外面, 即\\nX\\nx′\\nA\\nX\\nx′\\nB\\nψAC (x′\\nA, xC) ψBC (x′\\nB, xC) =\\nX\\nx′\\nA\\nψAC (x′\\nA, xC)\\nX\\nx′\\nB\\nψBC (x′\\nB, xC)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n举个例子, 假设x = {x1, x2, x3} , y = {y1, y2, y3}, 则\\n3\\nX\\ni=1\\n3\\nX\\nj=1\\nxiyj = x1y1 + x1y2 + x1y3 + x2y1 + x2y2 + x2y3 + x3y1 + x3y2 + x3y3\\n= x1 × (y1 + y2 + y3) + x2 × (y1 + y2 + y3) + x3 × (y1 + y2 + y3)\\n= (x1 + x2 + x3) × (y1 + y2 + y3) =\\n \\n3\\nX\\ni=1\\nxi\\n!  \\n3\\nX\\nj=1\\nyj\\n!\\n同理可得式(14.6)。类似于式(14.6), 还可以得到P (xB | xC) =\\nψBC(xB,xC)\\n∑\\nx′\\nB ψBC(x′\\nB,xC)\\n最后, 综合可得式(14.7) 成立, 即马尔可夫随机场“条件独立性”得证。\\n14.2.3\\n马尔可夫毯(Markov blanket)\\n本节共提到三个性质, 分别是全局马尔可夫性、局部马尔可夫性和成对马尔可夫性, 三者本质上是一\\n样的, 只是适用场景略有差异。\\n在“西瓜书”第325 页左上角边注提到“马尔可夫\\x00”的概念, 专门提一下这个概念主要是其名字与\\n马尔可夫链、隐马尔可夫模型、马尔可夫随机场等很像; 但实际上, 马尔可夫\\x00是一个局部的概念, 而马尔\\n可夫链、隐马尔可夫模型、马尔可夫随机场则是整体模型级别的概念。\\n对于某变量, 当它的马尔可夫\\x00 (即其所有邻接变量, 包含父变量、子变量、子变量的其他父变量等组\\n成的集合）确定时, 则该变量条件独立于其它变量, 即局部马尔可夫性。\\n14.2.4\\n势函数(potential function)\\n势函数贯穿本节，但却一直以抽象函数符号形式出现，直到本节最后才简单介绍势函数的具体形式，\\n个人感觉这为理解本节内容增加不少难度。具体来说，若已知势函数，例如以“西瓜书”图14.4 为例的和\\n取值，则可以根据式(14.3) 基于最大团势函数定义的联合概率公式解得各种可能变量值指派的联合概率，\\n进而完成一些预测工作；若势函数未知，在假定势函数的形式之后，应该就需要根据数据去学习势函数的\\n参数。\\n14.2.5\\n式(14.8) 的解释\\n此为势函数的定义式，即将势函数写作指数函数的形式。指数函数满足非负性，且便于求导，因此在\\n机器学习中具有广泛应用，例如西瓜书式(8.5) 和式(13.11)。\\n14.2.6\\n式(14.9) 的解释\\n此为定义在变量xQ 上的函数HQ (·) 的定义式，第二项考虑单节点，第一项考虑每一对节点之间的关\\n系。\\n14.3\\n条件随机场\\n条件随机场是给定一组输入随机变量x 条件下, 另一组输出随机变量y 构成的马尔可夫随机场, 即本\\n页边注中所说“条件随机场可看作给定观测值的马尔可夫随机场”, 条件随机场的“条件”应该就来源于\\n此吧, 因为需要求解的概率为条件联合概率P(y | x), 因此它是一种判别式模型, 参见“西瓜书”图14.6。\\n14.3.1\\n式(14.10) 的解释\\nP\\n\\x00yv|x, yV \\\\{v}\\n\\x01\\n= P\\n\\x00yv|x, yn(v)\\n\\x01\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n[解析]：根据局部马尔科夫性，给定某变量的邻接变量，则该变量独立与其他变量，即该变量只与其邻接\\n变量有关，所以式(14.10) 中给定变量v 以外的所有变量与仅给定变量v 的邻接变量是等价的。\\n特别注意, 本式下方写到“则(y, x) 构成一个条件随机场”; 也就是说, 因为(y, x) 满足式(14.10), 所\\n以(y, x) 构成一个条件随机场, 类似马尔可夫随机场与马尔可夫性的因果关系。\\n14.3.2\\n式(14.11) 的解释\\n注意本式前面的话：“条件概率被定义为”。至于式中使用的转移特征函数和状态特征函数，一般这两\\n个函数取值为1 或0，当满足特征条件时取值为1，否则为0。\\n14.3.3\\n学习与推断\\n本节前4 段内容（标题“14.4.1 变量消去”之前）至关重要，可以看作是14.4 节和14.5 节的引言，为\\n后面这两节内容做铺垫，因此一定要反复研读几遍，因为这几段内容告诉你接下来两节要解决什么问题，\\n心中装着问题再去看书会事半功倍，否则即使推明白了公式也不知道为什么要去推这些公式。本节介绍两\\n种精确推断方法，下一节则介绍两种近似推断方法。\\n14.3.4\\n式(14.14) 的推导\\n该式本身的含义很容易理解, 即为了求P (x5) 对联合分布中其他无关变量（即x1, x2, x3, x4 ）进行积\\n分(或求和) 的过程, 也就是“边际化”(marginalization)。\\n关键在于为什么从第1 个等号可以得到第2 个等号, 边注中提到“基于有向图模型所描述的条件独立\\n性”, 此即第7 章式(7.26)。这里的变换类似于式(7.27) 的推导过程, 不再赘述。\\n总之，在消去变量的过程中，在消去每一个变量时需要保证其依赖的变量已经消去，因此消去顺序应\\n该是有向概率图中的一条以目标节点为终点的拓扑序列。\\n14.3.5\\n式(14.15) 和式(14.16) 的推导\\n这里定义新符号mij (xj), 请一定理解并记住其含义。依次推导如下:\\nm12 (x2) =\\nX\\nx1\\nP (x1) P (x2 | x1) =\\nX\\nx1\\nP (x2, x1) = P (x2)\\nm23 (x3) =\\nX\\nx2\\nP (x3 | x2) m12 (x2) =\\nX\\nx2\\nP (x3, x2) = P (x3)\\nm43 (x3) =\\nX\\nx4\\nP (x4 | x3) m23 (x3) =\\nX\\nx4\\nP (x4, x3) = P (x3) (这里与书中不一样\\n!\\nm35 (x5) =\\nX\\nx3\\nP (x5 | x3) m43 (x3) =\\nX\\nx3\\nP (x5, x3) = P (x5)\\n注意: 这里的过程与“西瓜书”中不太一样, 但本质一样, 因为m43 (x3) = P\\nx4 P (x4 | x3) = 1 。\\n14.3.6\\n式(14.17) 的解释\\n忽略图14.7(a) 中的箭头，然后把无向图中的每条边的两个端点作为一个团将其分解为四个团因子的\\n乘积。Z 为规范化因子确保所有可能性的概率之和为1。本式就是基于极大团定义的联合概率分布，参见\\n式(14.3)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.3.7\\n式(14.18) 的推导\\n原理同式14.15, 区别在于把条件概率替换为势函数。由于势函数的定义是抽象的, 无法类似于P\\nx4 P (x4 | x3) =\\n1 去处理P\\nx4 ψ (x3, x4) 。\\n但根据边际化运算规则，可以知道：\\nm12 (x2) = P\\nx1 ψ12 (x1, x2) 只含x2 不含x1;\\nm23 (x3) = P\\nx2 ψ23 (x2, x3) m12 (x2) 只含x3 不含x2;\\nm43 (x3) = P\\nx4 ψ34 (x3, x4) m23 (x3) 只含x3 不含x4;\\nm35 (x5) = P\\nx3 ψ35 (x3, x5) m43 (x3) 只含x5 不含x3, 即最后得到P (x5) 。\\n14.3.8\\n式(14.19) 的解释\\n首先解释符号含义, k ∈n(i)\\\\j 表示k 属于除去j 之外的xi 的邻接结点, 例如n(1)\\\\2 为空集(因为\\nx1 只有邻接结点2 ), n(2)\\\\3 = {1} (因为x2 有邻接结点1 和3 )，n(4) n3 为空集(因为x4 只有邻接结\\n点3 ), n(3)\\\\5 = {2, 4} （因为x3 有邻接结点2,4 和5 )。\\n接下来, 仍然以图14.7 计算P (x5) 为例:\\nm12 (x2) =\\nX\\nx1\\nψ12 (x1, x2)\\nY\\nk∈n(1)\\\\2\\nmk1 (x1) =\\nX\\nx1\\nψ12 (x1, x2)\\nm23 (x3) =\\nX\\nx2\\nψ23 (x2, x3)\\nY\\nk∈n(2)\\\\3\\nmk2 (x2) =\\nX\\nx1\\nψ12 (x1, x2) m12 (x2)\\nm43 (x3) =\\nX\\nx4\\nψ34 (x3, x4)\\nY\\nk∈n(4)\\\\3\\nmk4 (x4) =\\nX\\nx4\\nψ34 (x3, x4)\\nm35 (x5) =\\nX\\nx3\\nψ35 (x3, x5)\\nY\\nk∈n(3)\\\\5\\nmk3 (x3) =\\nX\\nx3\\nψ35 (x3, x5) m23 (x3) m43 (x3)\\n该式表示从节点i 传递到节点j 的过程，求和号表示要考虑节点i 的所有可能取值。连乘号解释见式\\n14.20。应当注意这里连乘号的下标不包括节点j，节点i 只需要把自己知道的关于j 以外的消息告诉节点\\nj 即可。\\n14.3.9\\n式(14.20) 的解释\\n应当注意这里是正比于而不是等于，因为涉及到概率的规范化。可以这么解释，每个变量可以看作一\\n个有一些邻居的房子，每个邻居根据其自己的见闻告诉你一些事情(消息)，任何一条消息的可信度应当与\\n所有邻居都有相关性，此处这种相关性用乘积来表达。\\n14.3.10\\n式(14.22) 的推导\\n假设x 有M 种不同的取值，xi 的采样数量为mi(连续取值可以采用微积分的方法分割为离散的取\\n值)，则\\nˆf = 1\\nN\\nM\\nX\\nj=1\\nf (xj) · mj\\n=\\nM\\nX\\nj=1\\nf (xj) · mj\\nN\\n≈\\nM\\nX\\nj=1\\nf (xj) · p(xj)\\n≈\\nZ\\nf(x)p(x)dx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.3.11\\n图14.8 的解释\\n图(a) 表示信念传播算法的第1 步, 即指定一个根结点, 从所有叶结点开始向根结点传递消息, 直到根\\n结点收到所有邻接结点的消息; 图(b) 表示信念传播算法的第2 步, 即从根结点开始向叶结点传递消息, 直\\n到所有叶结点均收到消息。\\n本图并不难理解, 接下来思考如下两个问题:\\n【思考1】如何编程实现本图信念传播的过程? 这其中涉及到很多问题, 例如从叶结点x4 向根结点传\\n递消息时, 当传递到x3 时如何判断应该向x2 传递还是向x5 传递? 当然, 你可能感觉x5 是叶结点, 所以\\n肯定是向x2 传递, 那是因为这个无向图模型很简单, 如果x5 和x3 之间还有很多个结点呢? 复计算问题”\\n, 但如果图模型很复杂而我本身只需要计算少量边际分布, 是否还应该使用信念传播呢? 其实计算边际分\\n布类似于第10.1 节提到的“懒惰学习”, 只有在计算边际分布时才需要计算某些“消息”。这可能要根据\\n实际情况在变量消去和信念传播两种方法之间取舍。\\n【思考2】14.4.2 节开头就说到“信念传播...... 较好地解决了求解多个边际分布时的重复计算问题”,\\n但如果图模型很复杂而我本身只需要计算少量边际分布, 是否还应该使用信念传播呢? 其实计算边际分布\\n类似于第10.1 节提到的“懒惰学习”, 只有在计算边际分布时才需要计算某些“消息”。这可能要根据实\\n际情况在变量消去和信念传播两种方法之间取舍。\\n14.4\\n近似推断\\n本节介绍两种近似推断方法：MCMC 采样和变分推断。提到推断，一般是为了求解某个概率分布（参\\n见上一节的例子），但需要特别说明的是，本节将要介绍的MCMC 采样并不是为了求解某个概率分布，而\\n是在已知某个概率分布的前提下去构造服从该分布的独立同分布的样本集合，理解这一点对于读懂14.5.1\\n节的内容非常关键，即14.5.1 节中的p(x) 是已知的；变分推断是概率图模型常用的推断方法，要尽可能\\n理解并掌握其中的细节。\\n14.4.1\\n式(14.21) 到式(14.25) 的解释\\n这五个公式都是概率论课程中的基本公式, 很容易理解; 从14.5.1 节开始到式(14.25), 实际都在为\\nMCMC 采样做铺垫, 即为什么要做MCMC 采样? 以下分三点说明:\\n(1) 若已知概率密度函数p(x), 则可通过式(14.21) 计算函数f(x) 在该概率密度函数p(x) 下的期望;\\n这个过程也可以先根据p(x) 抽取一组样本再通过式(14.22) 近似完成。\\n(2) 为什么要通过式(14.22) 近似完成呢? 这是因为“若x 不是单变量而是一个高维多元变量x, 且服\\n从一个非常复杂的分布, 则对式(14.24) 求积分通常很困难”。\\n(3) “然而, 若概率密度函数p(x) 很复杂, 则构造服从p 分布的独立同分布样本也很困难”, 这时可以\\n使用MCMC 采样技术完成采样过程。\\n式(14.23) 就是在区间A 中的概率计算公式, 而式(14.24) 与式(14.21) 的区别也就在于式(14.24) 限\\n定了积分变量x 的区间(可能写成定积分形式可能更容易理解)。\\n14.4.2\\n式(14.26) 的解释\\n假设变量x 所在的空间有n 个状态(s1, s2, .., sn), 定义在该空间上的一个转移矩阵T ∈Rn×n 满足一\\n定的条件则该马尔可夫过程存在一个稳态分布π, 使得\\nπT = π\\n其中, π 是一个是一个n 维向量，代表s1, s2, .., sn 对应的概率. 反过来, 如果我们希望采样得到符合某个\\n分布π 的一系列变量x1, x2, .., xt, 应当采用哪一个转移矩阵T ∈Rn×n 呢？\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n事实上，转移矩阵只需要满足马尔可夫细致平稳条件\\nπiTij = πjTji\\n即式(14.26)，这里采用的符号与西瓜书略有区别以便于理解. 证明如下\\nπTj· =\\nX\\ni\\nπiTij =\\nX\\ni\\nπjTji = πj\\n假设采样得到的序列为x1, x2, .., xt−1, xt，则可以使用MH 算法来使得xt−1(假设为状态si) 转移到xt(假\\n设为状态sj) 的概率满足式。\\n本式为某个时刻马尔可夫链平稳的条件, 注意式中的p (xt) 和p (xt−1) 已知, 但状态转移概率T (xt−1 | xt)\\n和T (xt | xt−1) 末知。如何构建马尔可夫链转移概率至关重要, 不同的构造方法将产生不同的MCMC 算\\n法（可以认为MCMC 算法是一个大的框架或一种思想, 即“MCMC 方法先设法构造一条马尔可夫链, 使\\n其收玫至平稳分布恰为待估计参数的后验分布, 然后通过这条马尔可夫链来产生符合后验分布的样本, 并\\n基于这些样本来进行估计”, 具体如何构建马尔可夫链有多种实现途径, 接下来介绍的MH 算法就是其中\\n一种)。\\n14.4.3\\n式(14.27) 的解释\\n若将本式xt−1 和x∗分别对应式(14.27) 的xt 和xt−1, 则本式与式(14.27) 区别仅在于状态转移概率\\nT (x∗| xt−1) 由先验概率Q (x∗| xt−1) 和被接受的概率A (x∗| xt−1) 的乘积表示。\\n14.4.4\\n式(14.28) 的推导\\n注意, 本式中的概率分布p(x) 和先验转移概率Q 均为已知, 因此可计算出接受概率。将本式代入式\\n(14.27) 可以验证本式是正确的。具体来说, 式(14.27) 等号左边将变为:\\np\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\nA\\n\\x00x∗| xt−1\\x01\\n=p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\nmin\\n\\x12\\n1, p (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n= min\\n\\x12\\np\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\n, p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01 p (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n= min\\n\\x00p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\n, p (x∗) Q\\n\\x00xt−1 | x∗\\x01\\x01\\n将A (xt−1 | x∗) 代入右边(符号式xt−1 和x∗调换位置), 同理可得如上结果, 即本式的接受概率形式\\n可保证式(14.27) 成立。\\n验证完毕之后可以再做一个简单的推导。其实若想要式(14.27) 成立, 简单令:\\nA\\n\\x00x∗| xt−1\\x01\\n= C · p (x∗) Q\\n\\x00xt−1 | x∗\\x01\\n(则等号右则的A (xt−1 | x∗) = C · p (xt−1) Q (x∗| xt−1) )\\n即可, 其中C 为大于零的常数, 且不能使A (x∗| xt−1) 和A (xt−1 | x∗) 大于1 （因为它们是概率)。注\\n意待解A (x∗| xt−1) 为接受概率, 在保证式(14.27) 成立的基础上, 其值应该尽可能大一些(但概率值不会\\n超过1 ), 否则在图14.9 描述的MH 算法中采样出的候选样本将会有大部分会被拒绝。所以, 常数C 尽可\\n能大一些, 那么C 最大可以为多少呢?\\n对于A (x∗| xt−1) = C·p (x∗) Q (xt−1 | x∗), 易知C 最大可以取值\\n1\\np(x∗)Q(xt−1|x∗), 再大则会使A (x∗| xt−1)\\n大于1 ; 对于A (xt−1 | x∗) = C · p (xt−1) Q (x∗| xt−1), 易知C 最大可以取值\\n1\\np(xt−1)Q(x∗|xt−1); 常数C 的取\\n值需要同时满足两个约束, 因此\\nC = min\\n\\x12\\n1\\n·p (x∗) Q (xt−1 | x∗),\\n1\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n将这个常数C 的表达式代入A (x∗| xt−1) = C · p (x∗) Q (xt−1 | x∗) 即得式(14.28)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.5\\n吉布斯采样与MH 算法\\n这里解释一下为什么说吉布斯采样是MH 算法的特例。\\n吉布斯采样算法如下(“西瓜书”第334 页):\\n(1) 随机或以某个次序选取某变量xi;\\n(2) 根据x 中除xi 外的变量的现有取值, 计算条件概率p (xi | x¯i), 其中x¯i = {x1, x2, . . . , xi−1, xi+1, . . . , xN};\\n(3) 根据p (xi | x¯i) 对变量xi 采样, 用采样值代替原值.\\n对应到式(14.27) 和式(14.28) 表示的MH 采样, 候选样本x∗与t −1 时刻样本xt−1 的区别仅在于第\\ni 个变量的取值不同, 即x∗\\n¯i 与xt−1\\n¯i\\n相同。先给几个概率等式:\\n(1) Q (x∗| xt−1) = p\\n\\x00x∗\\ni | xt−1\\n¯i\\n\\x01\\n(2) Q (xt−1 | x∗) = p\\n\\x00xt−1\\ni\\n| x∗\\n¯i\\n\\x01\\n(3) p (x∗) = p\\n\\x00x∗\\ni , x∗\\n¯i\\n\\x01\\n= p\\n\\x00x∗\\ni | x∗\\n¯i\\n\\x01\\np\\n\\x00x∗\\n¯i\\n\\x01\\n(4) p (xt−1) = p\\n\\x00xt−1\\ni\\n, xt−1\\n¯i\\n\\x01\\n= p\\n\\x00xt−1\\ni\\n| xt−1\\n¯i\\n\\x01\\np\\n\\x00xt−1\\n¯i\\n\\x01\\n其中等式(1) 是由于吉布斯采样中“根据p (xi | xi) 对变量xi 采样”(参见以上第(3) 步), 即用户给\\n定的先验概率为p (xi | x¯i), 同理得等式(2); 等式(3) 就是将联合概率p (x∗) 换了种形式, 然写成了条件概\\n率和先验概率乘积, 同理得等式(4)。\\n对于式(14.28) 来说(注意: x∗\\n¯i = xt−1\\n¯i\\n)\\np (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1) =\\np (x∗\\ni | x∗\\ni ) p (x∗\\ni ) p\\n\\x00xt−1\\ni\\n| x∗\\n¯i\\n\\x01\\np\\n\\x00xt−1\\ni\\n| xt−1\\n¯i\\n\\x01\\np\\n\\x00xt−1\\n¯i\\n\\x01\\np\\n\\x00x∗\\ni | xt−1\\n¯i\\n\\x01 = 1\\n即在吉布斯采样中接受概率恒等于1 , 也就是说吉布斯采样是接受概率为1 的MH 采样。\\n该推导参考了Bishop 的PRML 第544 页:\\nWe can obtain the Gibbs sampling procedure as a particular instance of the Metropolis-Hastings\\nalgorithm as follows. Consider a Metropolis-Hastings sampling step involving the variable zk in which\\nthe remaining variables z\\\\k remain fixed, and for which the transition probability from z to z⋆is given by\\nqk (z⋆| z) = p\\n\\x00z⋆\\nk | z\\\\k\\n\\x01\\n.\\nWe note that z⋆\\n\\\\k = z\\\\k because these components are unchanged by the sampling step. Also, p(z) =\\np\\n\\x00zk | z\\\\k\\n\\x01\\np\\n\\x00z\\\\k\\n\\x01\\n. Thus the factor that determines the acceptance probability in the Metropolis-Hastings\\n(11.44) is given by\\nA (z⋆, z) = p (z⋆) qk (z | z⋆)\\np(z)qk (z⋆| z) =\\np\\n\\x00z⋆\\nk | z⋆\\n↓k\\n\\x01\\np\\n\\x10\\nz⋆\\n\\\\k\\n\\x11\\np\\n\\x10\\nzk | z⋆\\n\\\\k\\n\\x11\\np\\n\\x00zk | z\\\\k\\n\\x01\\np\\n\\x00z\\\\k\\n\\x01\\np\\n\\x00z⋆\\nk | z\\\\k\\n\\x01 = 1\\nwhere we have used z⋆\\n\\\\k = z\\\\k. Thus the Metropolis-Hastings steps are always accepted.\\n14.4.6\\n式(14.29) 的解释\\n连乘号是因为N 个变量的生成过程相互独立。求和号是因为每个变量的生成过程需要考虑中间隐变\\n量的所有可能性，类似于边际分布的计算方式。\\n14.4.7\\n式(14.30) 的解释\\n对式(14.29) 取对数。本式就是求对数后, 原来的连乘变为了连加, 即性质ln(ab) = ln a + ln b 。\\n接下来提到“图14.10 所对应的推断和学习任务主要是由观察到的变量x 来估计隐变量Z 和分布参\\n数变量Θ, 即求解p(z | x, Θ) 和Θ ”, 这里可以对应式(3.26) 来这样不严谨理解: Θ 对应式(3.26) 的w, b,\\n而z 对应式(3.26) 的y 。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.8\\n式(14.31) 的解释\\n对应7.6 节EM 算法中的M 步，参见第163 页的式(7.36) 和式(7.37)。\\n14.4.9\\n式(14.32) 到式(14.34) 的推导\\n从式(14.31) 到式(14.32) 之间的跳跃比较大, 接下来为了方便忽略分布参数变量Θ 。这里的主要问\\n题是后验概率p(z | x) 难于获得, 进而使用一个已知简单分布q(z) 去近似需要推导的复杂分布p(z | x), 这\\n就是变分推断的核心思想。\\n根据概率论公式p(x, z) = p(z | x)p(x), 得:\\np(x) = p(x, z)\\np(z | x)\\n分子分母同时除以q(z), 得:\\np(x) = p(x, z)/q(z)\\np(z | x)/q(z)\\n等号两边同时取自然对数, 得:\\nln p(x) = ln p(x, z)/q(z)\\np(z | x)/q(z) = ln p(x, z)\\nq(z)\\n−ln p(z | x)\\nq(z)\\n等号两边同时乘以q(z) 并积分, 得:\\nZ\\nq(z) ln p(x)dz =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz\\n对于等号左边的积分, 由于p(x) 与变量z 无关, 因此可以当作常数拿到积分号外面:\\nZ\\nq(z) ln p(x)dz = ln p(x)\\nZ\\nq(z)dz = ln p(x)\\n其中q(z) 为一个概率分布, 所以积分等于1 。至此, 前面式子变为:\\nln p(x) =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz\\n此即式(14.32), 等号右边第1 项即式(14.33) 称为Evidence Lower Bound (ELBO), 等号右边第2 项\\n即式(14.34) 为KL 散度（参见附录C.3）。我们的目标是用分布q(z) 去近似后验概率p(z | x), 而KL 散度\\n用于度量两个概率分布之间的差异, 其中KL 散度越小表示两个分布差异越小, 因此可以最小化式(14.34):\\nmin\\nq(z) KL(q(z)∥p(z | x))\\n但这并没有什么意义, 因为p(z | x) 末知。注意, 式(14.32) 恒等于常数ln p(x), 因此最小化式(14.34)\\n等价于最大化式(14.33) 的ELBO。在本节接下来的推导中, 就是通过最大化式(14.33) 来求解p(z | x) 的\\n近似q(z) 。\\n14.4.10\\n式(14.35) 的解释\\n在“西瓜书”14.5.2 节开篇提到, “变分推断通过使用已知简单分布来逼近需推断的复杂分布”, 这里\\n我们使用q(z) 去近似后验分布p(z | x) 。而本式进一步假设复杂的多变量z 可拆解为一系列相互独立的\\n多变量zi, 进而有q(z) = QM\\ni=1 qi (zi), 以便于后面简化求解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.11\\n式(14.36) 的推导\\n将式(14.35) 代入式(14.33), 得:\\nL(q) =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz =\\nZ\\nq(z){ln p(x, z) −ln q(z)}dz\\n=\\nZ\\nM\\nY\\ni=1\\nqi (zi)\\n(\\nln p(x, z) −ln\\nM\\nY\\ni=1\\nqi (zi)\\n)\\ndz\\n=\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz −\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln\\nM\\nY\\ni=1\\nqi (zi) dz ≜L1(q) −L2(q)\\n接下来推导中大量使用交换积分号次序, 记积分项为Q(x, z), 则上式可变形为:\\nL(q) =\\nZ\\nQ(x, z)dz =\\nZ\\n· · ·\\nZ\\nQ(x, z)dz1 dz2 · · · dzM\\n根据积分相关知识, 在满足某种条件下, 积分号的次序可以任意交换。\\n对于第1 项L1(q), 交换积分号次序, 得:\\nL1(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz =\\nZ\\nqj\\n(Z\\nln p(x, z)\\nM\\nY\\ni̸=j\\n(qi (zi) dzi)\\n)\\ndzj\\n令ln ˜p (x, zj) =\\nR\\nln p(x, z) QM\\ni̸=j (qi (zi) dzi) （这里与式(14.37) 略有不同, 具体参见接下来一条的解\\n释), 代入, 得:\\nL1(q) =\\nZ\\nqj ln ˜p (x, zj) dzj\\n对于第2 项L2(q) :\\nL2(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln\\nM\\nY\\ni=1\\nqi (zi) dz =\\nZ\\nM\\nY\\ni=1\\nqi (zi)\\nM\\nX\\ni=1\\nln qi (zi) dz\\n=\\nM\\nX\\ni=1\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln qi (zi) dz =\\nM\\nX\\ni1=1\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qi1 (zi1) dz\\n解释一下第2 行的第2 个等号后的结果, 这是因为课本在这里符号表示并不严谨, 求和变量和连乘\\n变量不能同时使用i, 这里求和变量和连乘变量分布使用i1 和i2 表示。对于求和号内的积分项，考虑当\\ni1 = j 时:\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qj (zj) dz =\\nZ\\nqj (zj)\\nY\\ni2̸=j\\nqi2 (zi2) ln qj (zj) dz\\n=\\nZ\\nqj (zj) ln qj (zj)\\n(Z Y\\ni2̸=j\\nqi2 (zi2)\\nY\\ni2̸=j\\ndzi2\\n)\\ndzj\\n注意到\\nR Q\\ni2̸=j qi2 (zi2) Q\\ni2̸=j dzi2 = 1, 为了直观说明这个结论, 假设这里只有q1 (z1), q2 (z2) 和\\nq3 (z3), 即:\\nZZZ\\nq1 (z1) q2 (z2) q3 (z3) dz1 dz2 dz3 =\\nZ\\nq1 (z1)\\nZ\\nq2 (z2)\\nZ\\nq3 (z3) dz3 dz2 dz1\\n对于概率分布, 我们有\\nR\\nq1 (z1) dz1 =\\nR\\nq2 (z2) dz2 =\\nR\\nq3 (z3) dz3 = 1, 代入即得。因此:\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qj (zj) dz =\\nZ\\nqj (zj) ln qj (zj) dzj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n进而第2 项可化简为:\\nL2(q) =\\nM\\nX\\ni1=1\\nZ\\nqi1 (zi1) ln qi1 (zi1) dzi1\\n=\\nZ\\nqj (zj) ln qj (zj) dzj +\\nM\\nX\\ni1̸=j\\nZ\\nqi1 (zi1) ln qi1 (zi1) dzi1\\n由于这里只关注qj （即固定qi̸=j ）, 因此第2 项进一步表示为第j 项加上一个常数:\\nL2(q) =\\nZ\\nqj (zj) ln qj (zj) dzj + const\\n综上所述, 可得式(14.36) 的形式。\\n14.4.12\\n式(14.37) 到式(14.38) 的解释\\n首先解释式(14.38), 该式等号右侧就是式(14.36) 第2 个等号后面花括号中的内容, 之所以这里写成\\n了期望的形式, 这是将Q\\ni̸=j qi 看作为一个概率分布, 则该式表示函数ln p(x, z) 在概率分布Q\\ni̸=j qi 下的\\n期望, 类似于式(14.21) 和式(14.24)。\\n然后解释式(14.37), 该式就是一个定义, 即令等号右侧的项为ln ˜p (x, zj), 但该式却包含一个常数项\\nconst, 当然这并没有什么问题, 并不影响式(14.36) 本身。具体来说, 将本项反代回式(14.36) 第二个等号\\n右侧第1 项, 即:\\nZ\\nqj\\n(Z\\nln p(x, z)\\nM\\nY\\ni̸=j\\n(qi (zi) dzi)\\n)\\ndzj =\\nZ\\nqjEi̸=j[ln p(x, z)]dzj\\n=\\nZ\\nqj (ln ˜p (x, zj) −const ) dzj\\n=\\nZ\\nqj ln ˜p (x, zj) dzj −\\nZ\\nqj constd zj\\n=\\nZ\\nqj ln ˜p (x, zj) dzj −const\\n注意, 加或减一个常数const 实际等价, 只需const 定义时添个符号即可。将这个const 与式(14.36)\\n第2 个等号后面的const 合并（注意二者表示不同的值), 即式(14.36) 第3 个等号后面的const。\\n14.4.13\\n式(14.39) 的解释\\n对于式(14.36), 可继续变形为:\\nL(q) =\\nZ\\nqj ln ˜p (x, zj) dzj −\\nZ\\nqj ln qj dzj + const\\n=\\nZ\\nqj ln ˜p (x, zj)\\nqj\\ndzj + const\\n= −KL (qj∥˜p (x, zj)) + const\\n注意, 在前面关于“式(14.32) 式(14.34) 的推导”中提到, 我们的目标是用分布q(z) 去近似后验概率\\np(z | x), 而KL 散度则用于度量两个概率分布之间的差异, 其中KL 散度越小表示两个分布差异越小, 因\\n此可以最小化式(14.34), 但这并没有什么意义, 因为p(z | x) 末知。又因为式(14.32) 恒等于常数ln p(x),\\n因此最小化式(14.34) 等价于最大化式(14.33)。刚刚又得到式(14.33) 等于−KL (qj∥˜p (x, zj)) + const, 因\\n此最大化式(14.33) 等价于最小化这里的KL 散度, 因此可知当qj = ˜p (x, zj) 时这个KL 散度最小, 即式\\n(14.33) 最大, 也就是分布q(z) 与后验概率p(z | x) 最相似。\\n而根据式(14.37) 有ln ˜p (x, zj) = Ei̸=j[ln p(x, z)]+const, 再结合qj = ˜p (x, zj), 可知ln qj = Ei̸=j[ln p(x, z)]+\\nconst, 即本式。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.14\\n式(14.40) 的解释\\n对式(14.39) 两边同时取exp(·) 操作, 得\\nq∗\\nj (zj) = exp (Ei̸=j[ln p(x, z)] + const )\\n= exp (Ei̸=j[ln p(x, z)]) · exp( const )\\n两边同时取积分\\nR\\n(·)dzj 操作, 由于q∗\\nj (zj) 为概率分布, 所以\\nR\\nq∗\\nj (zj) dzj = 1, 因此有\\n1 =\\nZ\\nexp (Ei̸=j[ln p(x, z)]) · exp( const )dzj\\n= exp( const )\\nZ\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n这里就是将常数拿到了积分号外面, 因此:\\nexp( const ) =\\n1\\nR\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n代入刚开始的表达式, 可得本式:\\nq∗\\nj (zj) = exp (Ei̸=j[ln p(x, z)]) · exp( const )\\n=\\nexp (Ei̸=j[ln p(x, z)])\\nR\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n实际上, 本式的分母为归一化因子, 以保证q∗\\nj (zj) 为概率分布。\\n14.5\\n话题模型\\n本节介绍话题模型的概念及其典型代表：隐狄利克雷分配模型（LDA）。\\n概括来说，给定一组文档，话题模型可以告诉我们这组文档谈论了哪些话题，以及每篇文档与哪些话\\n题有关。举个例子，社会中出现了一个热点事件，为了大致了解网民的思想动态，于是抓取了一组比较典\\n型的网页（博客、评论等）；每个网页就是一篇文档，我们通过分析这组网页，可以大致了解到网民都从什\\n么角度关注这件事情（每个角度可视为一个主题，其中LDA 模型中主题个数需要人工指定），并大致知道\\n每个网页都涉及哪些角度；这里学得的主题类似于聚类（参见第9 章）中所得的簇（没有标记），每个主\\n题最终由一个词频向量表示（即本节），通过分析该主题下的高频词，就可对其有大致的了解。\\n14.5.1\\n式(14.41) 的解释\\np(W , z, β, θ|α, η) =\\nT\\nY\\nt=1\\np(θt|α)\\nK\\nY\\nk=1\\np(βk|η)(\\nN\\nY\\nn=1\\nP(wt,n|zt,n, βk)P(zt,n|θt))\\n此式表示LDA 模型下根据参数α, η 生成文档W 的概率。其中z, β, θ 是生成过程的中间变量。具体的生\\n成步骤可见概率图14.12，图中的箭头和式14.41 中的条件概率中的因果项目一一对应。这里共有三个连\\n乘符号，表示三个相互独立的概率关系。第一个连乘表示T 个文档每个文档的话题分布都是相互独立的。\\n第二个连乘表示K 个话题每个话题下单词的分布是相互独立的。最后一个连乘号表示每篇文档中的所有\\n单词的生成是相互独立的。\\n14.5.2\\n式(14.42) 的解释\\n本式就是狄利克雷分布的定义式, 参见“西瓜书”附录C1.6。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 189, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.5.3\\n式(14.43) 的解释\\n本式为对数似然, 其中p (wt | α, η) =\\nRRR\\np (wt, z, β, Θ | α, η) dzdβdΘ, 即通过边际化p (wt, z, β, Θ | α, η)\\n而得。\\n由于T 篇文档相互独立, 所以p(W, z, β, Θ | α, η) = QT\\nt=1 p (wt, z, β, Θ | α, η), 求对数似然后连乘变\\n为了连加, 即得本式。参见7.2 极大似然估计。\\n14.5.4\\n式(14.44) 的解释\\n本式就是联合概率、先验概率、条件概率之间的关系, 换种表示方法可能更易理解:\\npα,η(z, β, Θ | W) = pα,η(W, z, β, Θ)\\npα,η(W)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第15 章\\n规则学习\\n规则学习是“符号主义学习”的代表性方法，用来从训练数据中学到一组能对未见示例进行判别的规\\n则，形如“如果A 或B，并且C 的条件下，D 满足”这样的形式。因为这种学习方法更加贴合人类从数\\n据中学到经验的描述，具有非常良好的可解释性，是最早开始研究机器学习的技术之一。\\n15.1\\n剪枝优化\\n15.1.1\\n式(15.2) 和式(15.3) 的解释\\n似然率统计量LRS 定义为：\\nLRS = 2 ·\\n\\uf8eb\\n\\uf8edˆm+ log2\\n\\x10\\nˆm+\\nˆm++ ˆm−\\n\\x11\\n\\x10\\nm+\\nm++m−\\n\\x11 + ˆm−log2\\n\\x10\\nˆm−\\nˆm++ ˆm−\\n\\x11\\n\\x10\\nm−\\nm++m−\\n\\x11\\n\\uf8f6\\n\\uf8f8\\n同时，根据对数函数的定义，我们可以对式(15.3) 进行化简：\\nF−Gain = ˆm+ ×\\n\\x12\\nlog2\\nˆm+\\nˆm+ + ˆm−\\n−log2\\nm+\\nm+ + m−\\n\\x13\\n= ˆm+\\n \\nlog2\\nˆm+\\nˆm++ ˆm−\\nm+\\nm++m−\\n!\\n可以观察到F_Gain 即为式(15.2) 中LRS 求和项中的第一项。这里“西瓜书”中做了详细的解释，FOIL\\n仅考虑正例的信息量，由于关系数据中正例数旺旺远少于反例数，因此通常对正例应该赋予更多的关注。\\n15.2\\n归纳逻辑程序设计\\n15.2.1\\n式(15.6) 的解释\\n定义析合范式的删除操作符为“−”，表示在A 和B 的析合式中删除成分B，得到成分A。\\n15.2.2\\n式(15.7) 的推导\\nC = A ∨B，把A = C1 −{L} 和L = C2 −{¬L} 带入即得。\\n15.2.3\\n式(15.9) 的推导\\n根据式(15.7) C = (C1 −{L}) ∨(C2 −{¬L}) 和析合范式的删除操作，等式两边同时删除析合项\\nC2 −{¬L} 有：\\nC −(C1 −{L}) = C2 −{¬L}\\n再次运用析合范式删除操作符的逆定义，等式两边同时加上析合项{¬L} 有：\\nC2 = (C −(C1 −{L})) ∨{¬L}\\n15.2.4\\n式(15.10) 的解释\\n该式是吸收(absorption) 操作的定义。注意作者在文章中所用的符号定义，用X\\nY 表示X 蕴含Y ，X\\n的子句或是Y 的归结项，或是Y 中某个子句的等价项。所谓吸收，是指替换部分逻辑子句（大写字母），\\n生成一个新的逻辑文字（小写字母）用于定义这些被替换的逻辑子句。在式(15.10) 中，逻辑子句A 被逻\\n辑文字q 替换。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 191, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n15.2.5\\n式(15.11) 的解释\\n该式是辨识(identification) 操作的定义。辨识操作依据已知的逻辑文字，构造新的逻辑子句和文字的\\n关系。在式(15.11) 中，已知p ←A ∧B 和p ←A ∧q，构造的新逻辑文字为q ←B。\\n15.2.6\\n式(15.12) 的解释\\n该式是内构(intra-construction) 操作的定义。内构操作找到关于同一逻辑文字中的共同逻辑子句部\\n分，并且提取其中不同的部分作为新的逻辑文字。在式(15.12) 中，逻辑文字p ←A ∧B 和p ←A ∧C 的\\n共同部分为p ←A ∧q，其中新逻辑文字q ←B\\nq ←C。\\n15.2.7\\n式(15.13) 的解释\\n该式是互构(inter-construction) 操作的定义。互构操作找到不同逻辑文字中的共同逻辑子句部分，并\\n定义新的逻辑文字已描述这个共同的逻辑子句。在式(15.13) 中，逻辑文字p ←A ∧B 和q ←A ∧C 的\\n共同逻辑子句A 提取出来，并用逻辑文字定义为r ←A。逻辑文字p 和q 的定义也用r 做相应的替换得\\n到p ←r ∧B 与q ←r ∧C。\\n15.2.8\\n式(15.16) 的推导\\nθ1 为作者笔误，由15.9\\nC2 = (C −(C1 −{L1})) ∨{L2}\\n因为L2 = (¬L1θ1)θ−1\\n2 ，替换得证。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 192, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第16 章\\n强化学习\\n强化学习作为机器学习的子领域，其本身拥有一套完整的理论体系，以及诸多经典和最新前沿算\\n法，“西瓜书”该章内容仅可作为综述查阅，若想深究建议查阅其他相关书籍（例如《Easy RL：强化\\n学习教程》\\n[1]）进行系统性学习。\\n16.1\\n任务与奖赏\\n本节理解强化学习的定义和相关术语的含义即可。\\n16.2\\nK-摇臂赌博机\\n16.2.1\\n式(16.2) 和式(16.3) 的推导\\nQn(k) = 1\\nn\\nn\\nX\\ni=1\\nvi\\n= 1\\nn\\n n−1\\nX\\ni=1\\nvi + vn\\n!\\n= 1\\nn ((n −1) × Qn−1(k) + vn)\\n= Qn−1(k) + 1\\nn (vn −Qn−1(k))\\n16.2.2\\n式(16.4) 的解释\\nP(k) =\\ne\\nQ(k)\\nτ\\nPK\\ni=1 e\\nQ(i)\\nτ\\n∝e\\nQ(k)\\nτ\\n∝Q(k)\\nτ\\n∝1\\nτ\\n如果τ 很大，所有动作几乎以等概率选择（探索）；如果τ 很小，Q 值大的动作更容易被选中（利用）。\\n16.3\\n有模型学习\\n16.3.1\\n式(16.7) 的解释\\n因为\\nπ(x, a) = P(action = a|state = x)\\n表示在状态x 下选择动作a 的概率，又因为动作事件之间两两互斥且和为动作空间，由全概率展开公式\\nP(A) =\\n∞\\nX\\ni=1\\nP(Bi)P(A | Bi)\\n可得\\nEπ[ 1\\nT r1 + T −1\\nT\\n1\\nT −1\\nT\\nX\\nt=2\\nrt | x0 = x]\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′( 1\\nT Ra\\nx→x′ + T −1\\nT\\nEπ[\\n1\\nT −1\\nT −1\\nX\\nt=1\\nrt | x0 = x′])\\n其中\\nr1 = π(x, a)P a\\nx→x′Ra\\nx→x′\\n最后一个等式用到了递归形式。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 193, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nBellman 等式定义了当前状态与未来状态之间的关系，表示当前状态的价值函数可以通过下个状态的\\n价值函数来计算。\\n16.3.2\\n式(16.8) 的推导\\nV π\\nγ (x) = Eπ[\\n∞\\nX\\nt=0\\nγtrt+1 | x0 = x]\\n= Eπ[r1 +\\n∞\\nX\\nt=1\\nγtrt+1 | x0 = x]\\n= Eπ[r1 + γ\\n∞\\nX\\nt=1\\nγt−1rt+1 | x0 = x]\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′(Ra\\nx→x′ + γEπ[\\n∞\\nX\\nt=0\\nγtrt+1 | x0 = x′])\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′(Ra\\nx→x′ + γV π\\nγ (x′))\\n16.3.3\\n式(16.10) 的推导\\n参见式(16.7) 和式(16.8) 的推导\\n16.3.4\\n式(16.14) 的解释\\n为了获得最优的状态值函数V ，这里取了两层最优，分别是采用最优策略π∗和选取使得状态动作值\\n函数Q 最大的状态maxa∈A。\\n16.3.5\\n式(16.15) 的解释\\n最优Bellman 等式表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的\\n累积奖赏值的期望。\\n16.3.6\\n式(16.16) 的推导\\nV π(x) ⩽Qπ (x, π′(x))\\n=\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n\\x10\\nRπ′(x)\\nx→x′ + γV π (x′)\\n\\x11\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n\\x10\\nRπ′(x)\\nx→x′ + γQπ (x′, π′ (x′))\\n\\x11\\n=\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n\\x10\\nγRπ′(x′)\\nx′→x′′ + γ2V π (x′′)\\n\\x11!\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n\\x10\\nγRπ′(x′)\\nx′→x′′ + γ2Qπ (x′′, π′ (x′′))\\n\\x11!\\n⩽· · ·\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n \\nγRπ′(x′)\\nx′→x′′ +\\nX\\nx′′∈X\\nP π′(x′′)\\nx′′→x′′′\\n\\x10\\nγ2Rπ′(x′′)\\nx′′→x′′′ + · · ·\\n\\x11!!\\n= V π′(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 194, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中，使用了动作改变条件\\nQπ(x, π′(x)) ⩾V π(x)\\n以及状态-动作值函数\\nQπ(x′, π′(x′)) =\\nX\\nx′∈X\\nP π′(x′)\\nx′→x′(Rπ′(x′)\\nx′→x′ + γV π(x′))\\n于是，当前状态的最优值函数为\\nV ∗(x) = V π′(x) ⩾V π(x)\\n16.4\\n免模型学习\\n16.4.1\\n式(16.20) 的解释\\n如果ϵk = 1\\nk，并且其值随k 增大而主角趋于零，则ϵ−贪心是在无限的探索中的极限贪心（Greedy\\nin the Limit with Infinite Exploration，简称GLIE）。\\n16.4.2\\n式(16.23) 的解释\\np(x)\\nq(x) 称为重要性权重（Importance Weight），其用于修正两个分布的差异。\\n16.4.3\\n式(16.31) 的推导\\n对比公式16.29\\nQπ\\nt+1(x, a) = Qπ\\nt (x, a) +\\n1\\nt + 1(rt+1 −Qπ\\nt (x, a))\\n以及由\\n1\\nt + 1 = α\\n可知，若下式成立，则公式16.31 成立\\nrt+1 = Ra\\nx→x′ + γQπ\\nt (x′, a′)\\n而rt+1 表示t + 1 步的奖赏，即状态x 变化到x′ 的奖赏加上前面t 步奖赏总和Qπ\\nt (x′, a′) 的γ 折扣，因\\n此这个式子成立。\\n16.5\\n值函数近似\\n16.5.1\\n式(16.33) 的解释\\n古代汉语中“平方”称为“二乘”，此处的最小二乘误差也就是均方误差。\\n16.5.2\\n式(16.34) 的推导\\n−∂Eθ\\n∂θ = −\\n∂Ex∼π\\nh\\n(V π(x) −Vθ(x))2i\\n∂θ\\n将V π(x) −Vθ(x) 看成一个整体，根据链式法则（chain rule）可知\\n−\\n∂Ex∼π\\nh\\n(V π(x) −Vθ(x))2i\\n∂θ\\n= Ex∼π\\n\\x14\\n2 (V π(x) −Vθ(x)) ∂Vθ(x)\\n∂θ\\n\\x15\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 195, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nVθ(x) 是一个标量，θ 是一个向量，∂Vθ(x)\\n∂θ\\n属于矩阵微积分中的标量对向量求偏导，因此\\n∂Vθ(x)\\n∂θ\\n= ∂θTx\\n∂θ\\n=\\n\"\\n∂θTx\\n∂θ1\\n, ∂θTx\\n∂θ2\\n, · · · , ∂θTx\\n∂θn\\n#T\\n= [x1, x2, · · · , xm]T\\n= x\\n故\\n−∂Eθ\\n∂θ = Ex∼π\\n\\x14\\n2 (V π(x) −Vθ(x)) ∂Vθ(x)\\n∂θ\\n\\x15\\n= Ex∼π [2 (V π(x) −Vθ(x)) x]\\n16.6\\n模仿学习\\n本节公式无复杂推导，在此不做赘述。\\n参考文献\\n[1] 王琦，杨毅远，江季. Easy RL：强化学习教程. 人民邮电出版社, 2022.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "for loader in loaders:\n",
    "    texts.extend(loader.load())\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': './data_base/knowledge_db/prompt_engineering/9. 总结 Summary.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "第九章 总结\n",
      "\n",
      "恭喜您完成了本书第一单元内容的学习！\n",
      "\n",
      "总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：\n",
      "\n",
      "编写清晰具体的指令；\n",
      "\n",
      "如果适当的话，给模型一些思考时间。\n",
      "\n",
      "您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。\n",
      "\n",
      "我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第一部分中，您的收获应该颇丰，希望通过第一部分学习能为您带来愉悦的体验。\n",
      "\n",
      "我们期待您能灵感迸发，尝试创建自己的应用。请大胆尝试，并分享给我们您的想法。您可以从一个微型项目开始，或许它具备一定的实用性，或者仅仅是一项有趣的创新。请利用您在第一个项目中得到的经验，去创造更优秀的下一项目，以此类推。如果您已经有一个宏大的项目设想，那么，请毫不犹豫地去实现它。\n",
      "\n",
      "最后，希望您在完成第一部分的过程中感到满足，感谢您的参与。我们热切期待着您的惊艳作品。接下来，我们将进入第二部分的学习！\n"
     ]
    }
   ],
   "source": [
    "text = texts[1]\n",
    "print(f\"每一个元素的类型：{type(text)}.\", \n",
    "    f\"该文档的描述性数据：{text.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{text.page_content[0:]}\", \n",
    "    sep=\"\\n------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='第五章 推断\\n\\n在这一章中，我们将通过一个故事，引领你了解如何从产品评价和新闻文章中推导出情感和主题。\\n\\n让我们先想象一下，你是一名初创公司的数据分析师，你的任务是从各种产品评论和新闻文章中提取出关键的情感和主题。这些任务包括了标签提取、实体提取、以及理解文本的情感等等。在传统的机器学习流程中，你需要收集标签化的数据集、训练模型、确定如何在云端部署模型并进行推断。尽管这种方式可能会产生不错的效果，但完成这一全流程需要耗费大量的时间和精力。而且，每一个任务，比如情感分析、实体提取等等，都需要训练和部署单独的模型。\\n\\n然而，就在你准备投入繁重工作的时候，你发现了大型语言模型（LLM）。LLM 的一个明显优点是，对于许多这样的任务，你只需要编写一个 Prompt，就可以开始生成结果，大大减轻了你的工作负担。这个发现像是找到了一把神奇的钥匙，让应用程序开发的速度加快了许多。最令你兴奋的是，你可以仅仅使用一个模型和一个 API 来执行许多不同的任务，无需再纠结如何训练和部署许多不同的模型。\\n\\n让我们开始这一章的学习，一起探索如何利用 LLM 加快我们的工作进程，提高我们的工作效率。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='让我们开始这一章的学习，一起探索如何利用 LLM 加快我们的工作进程，提高我们的工作效率。\\n\\n一、情感推断\\n\\n1.1 情感倾向分析\\n\\n让我们以一则电商平台上的台灯评论为例，通过此例，我们将学习如何对评论进行情感二分类（正面/负面）。\\n\\npython lamp_review = \"\"\" 我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\\\\ 我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\\\\ 几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\\\\ 在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！ \"\"\"\\n\\n接下来，我们将尝试编写一个 Prompt ，用以分类这条商品评论的情感。如果我们想让系统解析这条评论的情感倾向，只需编写“以下商品评论的情感倾向是什么？”这样的 Prompt ，再加上一些标准的分隔符和评论文本等。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='然后，我们将这个程序运行一遍。结果表明，这条商品评论的情感倾向是正面的，这似乎非常准确。尽管这款台灯并非完美无缺，但是这位顾客对它似乎相当满意。这个公司看起来非常重视客户体验和产品质量，因此，认定评论的情感倾向为正面似乎是正确的判断。\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 以下用三个反引号分隔的产品评论的情感是什么？\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n情感是积极的。\\n\\n如果你想要给出更简洁的答案，以便更容易进行后期处理，可以在上述 Prompt 基础上添加另一个指令：用一个单词回答：「正面」或「负面」。这样就只会打印出 “正面” 这个单词，这使得输出更加统一，方便后续处理。\\n\\n```python prompt = f\"\"\" 以下用三个反引号分隔的产品评论的情感是什么？\\n\\n用一个单词回答：「正面」或「负面」。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='用一个单词回答：「正面」或「负面」。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n正面\\n\\n1.2 识别情感类型\\n\\n接下来，我们将继续使用之前的台灯评论，但这次我们会试用一个新的 Prompt 。我们希望模型能够识别出评论作者所表达的情感，并且将这些情感整理为一个不超过五项的列表。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 识别以下评论的作者表达的情感。包含不超过五个项目。将答案格式化为以逗号分隔的单词列表。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n满意,感激,赞赏,信任,满足\\n\\n大型语言模型非常擅长从一段文本中提取特定的东西。在上面的例子中，评论所表达的情感有助于了解客户如何看待特定的产品。\\n\\n1.3 识别愤怒'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='1.3 识别愤怒\\n\\n对于许多企业来说，洞察到顾客的愤怒情绪是至关重要的。这就引出了一个分类问题：下述的评论作者是否流露出了愤怒？因为如果有人真的情绪激动，那可能就意味着需要给予额外的关注，因为每一个愤怒的顾客都是一个改进服务的机会，也是一个提升公司口碑的机会。这时，客户支持或者客服团队就应该介入，与客户接触，了解具体情况，然后解决他们的问题。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 以下评论的作者是否表达了愤怒？评论用三个反引号分隔。给出是或否的答案。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n上面这个例子中，客户并没有生气。注意，如果使用常规的监督学习，如果想要建立所有这些分类器，不可能在几分钟内就做到这一点。我们鼓励大家尝试更改一些这样的 Prompt ，也许询问客户是否表达了喜悦，或者询问是否有任何遗漏的部分，并看看是否可以让 Prompt 对这个灯具评论做出不同的推论。\\n\\n二、信息提取\\n\\n2.1 商品信息提取'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='二、信息提取\\n\\n2.1 商品信息提取\\n\\n信息提取是自然语言处理（NLP）的重要组成部分，它帮助我们从文本中抽取特定的、我们关心的信息。我们将深入挖掘客户评论中的丰富信息。在接下来的示例中，我们将要求模型识别两个关键元素：购买的商品和商品的制造商。\\n\\n想象一下，如果你正在尝试分析一个在线电商网站上的众多评论，了解评论中提到的商品是什么、由谁制造，以及相关的积极或消极情绪，将极大地帮助你追踪特定商品或制造商在用户心中的情感趋势。\\n\\n在接下来的示例中，我们会要求模型将回应以一个 JSON 对象的形式呈现，其中的 key 就是商品和品牌。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 从评论文本中识别以下项目： - 评论者购买的物品 - 制造该物品的公司\\n\\n评论文本用三个反引号分隔。将你的响应格式化为以 “物品” 和 “品牌” 为键的 JSON 对象。 如果信息不存在，请使用 “未知” 作为值。 让你的回应尽可能简短。\\n\\n评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='{\\n  \"物品\": \"卧室灯\",\\n  \"品牌\": \"Lumina\"\\n}\\n\\n如上所示，它会说这个物品是一个卧室灯，品牌是 Luminar，你可以轻松地将其加载到 Python 字典中，然后对此输出进行其他处理。\\n\\n2.2 综合情感推断和信息提取\\n\\n在上面小节中，我们采用了三至四个 Prompt 来提取评论中的“情绪倾向”、“是否生气”、“物品类型”和“品牌”等信息。然而，事实上，我们可以设计一个单一的 Prompt ，来同时提取所有这些信息。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 从评论文本中识别以下项目： - 情绪（正面或负面） - 审稿人是否表达了愤怒？（是或否） - 评论者购买的物品 - 制造该物品的公司\\n\\n评论用三个反引号分隔。将你的响应格式化为 JSON 对象，以 “情感倾向”、“是否生气”、“物品类型” 和 “品牌” 作为键。 如果信息不存在，请使用 “未知” 作为值。 让你的回应尽可能简短。 将 “是否生气” 值格式化为布尔值。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"情感倾向\": \"正面\",\\n  \"是否生气\": false,\\n  \"物品类型\": \"卧室灯\",\\n  \"品牌\": \"Lumina\"\\n}\\n\\n这个例子中，我们指导 LLM 将“是否生气”的情况格式化为布尔值，并输出 JSON 格式。你可以尝试对格式化模式进行各种变化，或者使用完全不同的评论来试验，看看 LLM 是否仍然可以准确地提取这些内容。\\n\\n三、主题推断\\n\\n大型语言模型的另一个很酷的应用是推断主题。假设我们有一段长文本，我们如何判断这段文本的主旨是什么？它涉及了哪些主题？让我们通过以下一段虚构的报纸报道来具体了解一下。\\n\\n```python\\n\\n中文\\n\\nstory = \"\"\" 在政府最近进行的一项调查中，要求公共部门的员工对他们所在部门的满意度进行评分。 调查结果显示，NASA 是最受欢迎的部门，满意度为 95％。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='一位 NASA 员工 John Smith 对这一发现发表了评论，他表示： “我对 NASA 排名第一并不感到惊讶。这是一个与了不起的人们和令人难以置信的机会共事的好地方。我为成为这样一个创新组织的一员感到自豪。”\\n\\nNASA 的管理团队也对这一结果表示欢迎，主管 Tom Johnson 表示： “我们很高兴听到我们的员工对 NASA 的工作感到满意。 我们拥有一支才华横溢、忠诚敬业的团队，他们为实现我们的目标不懈努力，看到他们的辛勤工作得到回报是太棒了。”\\n\\n调查还显示，社会保障管理局的满意度最低，只有 45％的员工表示他们对工作满意。 政府承诺解决调查中员工提出的问题，并努力提高所有部门的工作满意度。 \"\"\" ```\\n\\n3.1 推断讨论主题\\n\\n以上是一篇关于政府员工对其工作单位感受的虚构报纸文章。我们可以要求大语言模型确定其中讨论的五个主题，并用一两个词语概括每个主题。输出结果将会以逗号分隔的Python列表形式呈现。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 确定以下给定文本中讨论的五个主题。\\n\\n每个主题用1-2个词概括。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='prompt = f\"\"\" 确定以下给定文本中讨论的五个主题。\\n\\n每个主题用1-2个词概括。\\n\\n请输出一个可解析的Python列表，每个元素是一个字符串，展示了一个主题。\\n\\n给定文本: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n[\\'NASA\\', \\'满意度\\', \\'评论\\', \\'管理团队\\', \\'社会保障管理局\\']\\n\\n3.2 为特定主题制作新闻提醒\\n\\n假设我们有一个新闻网站或类似的平台，这是我们感兴趣的主题：美国航空航天局、当地政府、工程、员工满意度、联邦政府等。我们想要分析一篇新闻文章，理解其包含了哪些主题。可以使用这样的 Prompt：确定以下主题列表中的每个项目是否是以下文本中的主题。以 0 或 1 的形式给出答案列表。\\n\\n```python\\n\\n中文\\n\\nprompt = f\"\"\" 判断主题列表中的每一项是否是给定文本中的一个话题，\\n\\n以列表的形式给出答案，每个元素是一个Json对象，键为对应主题，值为对应的 0 或 1。\\n\\n主题列表：美国航空航天局、当地政府、工程、员工满意度、联邦政府'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='主题列表：美国航空航天局、当地政府、工程、员工满意度、联邦政府\\n\\n给定文本: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n[\\n  {\"美国航空航天局\": 1},\\n  {\"当地政府\": 1},\\n  {\"工程\": 0},\\n  {\"员工满意度\": 1},\\n  {\"联邦政府\": 1}\\n]\\n\\n从输出结果来看，这个 story 与关于“美国航空航天局”、“员工满意度”、“联邦政府”、“当地政府”有关，而与“工程”无关。这种能力在机器学习领域被称为零样本（Zero-Shot）学习。这是因为我们并没有提供任何带标签的训练数据，仅凭 Prompt ，它便能判定哪些主题在新闻文章中被包含。\\n\\n如果我们希望制定一个新闻提醒，我们同样可以运用这种处理新闻的流程。假设我对“美国航空航天局”的工作深感兴趣，那么你就可以构建一个如此的系统：每当出现与\\'美国宇航局\\'相关的新闻，系统就会输出提醒。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='python result_lst = eval(response) topic_dict = {list(i.keys())[0] : list(i.values())[0] for i in result_lst} print(topic_dict) if topic_dict[\\'美国航空航天局\\'] == 1: print(\"提醒: 关于美国航空航天局的新消息\")\\n\\n{\\'美国航空航天局\\': 1, \\'当地政府\\': 1, \\'工程\\': 0, \\'员工满意度\\': 1, \\'联邦政府\\': 1}\\n提醒: 关于美国航空航天局的新消息\\n\\n这就是我们关于推断的全面介绍。在短短几分钟内，我们已经能够建立多个用于文本推理的系统，这是以前需要机器学习专家数天甚至数周时间才能完成的任务。这一变化无疑是令人兴奋的，因为无论你是经验丰富的机器学习开发者，还是刚入门的新手，都能利用输入 Prompt 快速开始复杂的自然语言处理任务。\\n\\n英文版\\n\\n1.1 情感倾向分析'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='python lamp_review = \"\"\" Needed a nice lamp for my bedroom, and this one had \\\\ additional storage and not too high of a price point. \\\\ Got it fast. The string to our lamp broke during the \\\\ transit and the company happily sent over a new one. \\\\ Came within a few days as well. It was easy to put \\\\ together. I had a missing part, so I contacted their \\\\ support and they very quickly got me the missing piece! \\\\ Lumina seems to me to be a great company that cares \\\\ about their customers and'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='company that cares \\\\ about their customers and products!! \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='```python prompt = f\"\"\" What is the sentiment of the following product review, which is delimited with triple backticks?\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nThe sentiment of the product review is positive.\\n\\n```python prompt = f\"\"\" What is the sentiment of the following product review, which is delimited with triple backticks?\\n\\nGive your answer as a single word, either \"positive\" \\\\ or \"negative\".'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='Review text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\npositive\\n\\n1.2识别情感类型\\n\\n```python prompt = f\"\"\" Identify a list of emotions that the writer of the \\\\ following review is expressing. Include no more than \\\\ five items in the list. Format your answer as a list of \\\\ lower-case words separated by commas.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nsatisfied, pleased, grateful, impressed, happy\\n\\n1.3 识别愤怒'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='1.3 识别愤怒\\n\\n```python prompt = f\"\"\" Is the writer of the following review expressing anger?\\\\ The review is delimited with triple backticks. \\\\ Give your answer as either yes or no.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nNo\\n\\n2.1 商品信息提取\\n\\n```python prompt = f\"\"\" Identify the following items from the review text: - Item purchased by reviewer - Company that made the item'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='The review is delimited with triple backticks. \\\\ Format your response as a JSON object with \\\\ \"Item\" and \"Brand\" as the keys. If the information isn\\'t present, use \"unknown\" \\\\ as the value. Make your response as short as possible.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"Item\": \"lamp\",\\n  \"Brand\": \"Lumina\"\\n}\\n\\n2.2 综合情感推断和信息提取'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='2.2 综合情感推断和信息提取\\n\\n```python prompt = f\"\"\" Identify the following items from the review text: - Sentiment (positive or negative) - Is the reviewer expressing anger? (true or false) - Item purchased by reviewer - Company that made the item'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='The review is delimited with triple backticks. \\\\ Format your response as a JSON object with \\\\ \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys. If the information isn\\'t present, use \"unknown\" \\\\ as the value. Make your response as short as possible. Format the Anger value as a boolean.\\n\\nReview text: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n{\\n  \"Sentiment\": \"positive\",\\n  \"Anger\": false,\\n  \"Item\": \"lamp\",\\n  \"Brand\": \"Lumina\"\\n}\\n\\n3.1 推断讨论主题'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='3.1 推断讨论主题\\n\\n```python story = \"\"\" In a recent survey conducted by the government, public sector employees were asked to rate their level of satisfaction with the department they work at. The results revealed that NASA was the most popular department with a satisfaction rating of 95%.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='One NASA employee, John Smith, commented on the findings, stating, \"I\\'m not surprised that NASA came out on top. It\\'s a great place to work with amazing people and incredible opportunities. I\\'m proud to be a part of such an innovative organization.\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='The results were also welcomed by NASA\\'s management team, with Director Tom Johnson stating, \"We are thrilled to hear that our employees are satisfied with their work at NASA. We have a talented and dedicated team who work tirelessly to achieve our goals, and it\\'s fantastic to see that their hard work is paying off.\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='The survey also revealed that the Social Security Administration had the lowest satisfaction rating, with only 45% of employees indicating they were satisfied with their job. The government has pledged to address the concerns raised by employees in the survey and work towards improving job satisfaction across all departments. \"\"\" ```\\n\\n```python prompt = f\"\"\" Determine five topics that are being discussed in the \\\\ following text, which is delimited by triple backticks.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='Make each item one or two words long.\\n\\nFormat your response as a list of items separated by commas. Give me a list which can be read in Python.\\n\\nText sample: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nsurvey, satisfaction rating, NASA, Social Security Administration, job satisfaction\\n\\npython response.split(sep=\\',\\')\\n\\n[\\'survey\\',\\n \\' satisfaction rating\\',\\n \\' NASA\\',\\n \\' Social Security Administration\\',\\n \\' job satisfaction\\']\\n\\n3.2 为特定主题制作新闻提醒'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='3.2 为特定主题制作新闻提醒\\n\\npython topic_list = [ \"nasa\", \"local government\", \"engineering\", \"employee satisfaction\", \"federal government\" ]\\n\\n```python prompt = f\"\"\" Determine whether each item in the following list of \\\\ topics is a topic in the text below, which is delimited with triple backticks.\\n\\nGive your answer as list with 0 or 1 for each topic.\\\\\\n\\nList of topics: {\", \".join(topic_list)}\\n\\nText sample: {story} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n[1, 0, 0, 1, 1]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/5. 推断 Inferring.md'}, page_content='[1, 0, 0, 1, 1]\\n\\npython topic_dict = {topic_list[i] : eval(response)[i] for i in range(len(eval(response)))} print(topic_dict) if topic_dict[\\'nasa\\'] == 1: print(\"ALERT: New NASA story!\")\\n\\n{\\'nasa\\': 1, \\'local government\\': 0, \\'engineering\\': 0, \\'employee satisfaction\\': 1, \\'federal government\\': 1}\\nALERT: New NASA story!'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/9. 总结 Summary.md'}, page_content='第九章 总结\\n\\n恭喜您完成了本书第一单元内容的学习！\\n\\n总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：\\n\\n编写清晰具体的指令；\\n\\n如果适当的话，给模型一些思考时间。\\n\\n您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。\\n\\n我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第一部分中，您的收获应该颇丰，希望通过第一部分学习能为您带来愉悦的体验。\\n\\n我们期待您能灵感迸发，尝试创建自己的应用。请大胆尝试，并分享给我们您的想法。您可以从一个微型项目开始，或许它具备一定的实用性，或者仅仅是一项有趣的创新。请利用您在第一个项目中得到的经验，去创造更优秀的下一项目，以此类推。如果您已经有一个宏大的项目设想，那么，请毫不犹豫地去实现它。\\n\\n最后，希望您在完成第一部分的过程中感到满足，感谢您的参与。我们热切期待着您的惊艳作品。接下来，我们将进入第二部分的学习！'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='第七章 文本扩展\\n\\n文本扩展是大语言模型的一个重要应用方向，它可以输入简短文本，生成更加丰富的长文。这为创作提供了强大支持，但也可能被滥用。因此开发者在使用时，必须谨记社会责任，避免生成有害内容。\\n\\n在本章中,我们将学习基于 OpenAI API 实现一个客户邮件自动生成的示例，用于根据客户反馈优化客服邮件。这里还会介绍“温度”（temperature）这一超参数，它可以控制文本生成的多样性。\\n\\n需要注意，扩展功能只应用来辅助人类创作，而非大规模自动生成内容。开发者应审慎使用，避免产生负面影响。只有以负责任和有益的方式应用语言模型，才能发挥其最大价值。相信践行社会责任的开发者可以利用语言模型的扩展功能，开发出真正造福人类的创新应用。\\n\\n一、定制客户邮件\\n\\n在这个客户邮件自动生成的示例中，我们将根据客户的评价和其中的情感倾向，使用大语言模型针对性地生成回复邮件。\\n\\n具体来说，我们先输入客户的评论文本和对应的情感分析结果(正面或者负面)。然后构造一个 Prompt，要求大语言模型基于这些信息来生成一封定制的回复电子邮件。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='下面先给出一个实例，包括一条客户评价和这个评价表达的情感。这为后续的语言模型生成回复邮件提供了关键输入信息。通过输入客户反馈的具体内容和情感态度，语言模型可以生成针对这个特定客户、考虑其具体情感因素的个性化回复。这种针对个体客户特点的邮件生成方式，将大大提升客户满意度。\\n\\n```python\\n\\n我们可以在推理那章学习到如何对一个评论判断其情感倾向\\n\\nsentiment = \"消极的\"\\n\\n一个产品的评价'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='一个产品的评价\\n\\nreview = f\"\"\" 他们在11月份的季节性销售期间以约49美元的价格出售17件套装，折扣约为一半。\\\\ 但由于某些原因（可能是价格欺诈），到了12月第二周，同样的套装价格全都涨到了70美元到89美元不等。\\\\ 11件套装的价格也上涨了大约10美元左右。\\\\ 虽然外观看起来还可以，但基座上锁定刀片的部分看起来不如几年前的早期版本那么好。\\\\ 不过我打算非常温柔地使用它，例如，\\\\ 我会先在搅拌机中将像豆子、冰、米饭等硬物研磨，然后再制成所需的份量，\\\\ 切换到打蛋器制作更细的面粉，或者在制作冰沙时先使用交叉切割刀片，然后使用平面刀片制作更细/不粘的效果。\\\\ 制作冰沙时，特别提示：\\\\ 将水果和蔬菜切碎并冷冻（如果使用菠菜，则轻轻煮软菠菜，然后冷冻直到使用；\\\\ 如果制作果酱，则使用小到中号的食品处理器），这样可以避免在制作冰沙时添加太多冰块。\\\\ 大约一年后，电机发出奇怪的噪音，我打电话给客服，但保修已经过期了，所以我不得不再买一个。\\\\ 总的来说，这些产品的总体质量已经下降，因此它们依靠品牌认可和消费者忠诚度来维持销售。\\\\ 货物在两天内到达。 \"\"\" ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='在这个例子中，我们已经利用前面章节学到的方法，从客户评价中提取出其表达的情感倾向。这里是一条关于搅拌机的评论。现在我们要基于这条评论中的情感倾向，使用大语言模型自动生成一封回复邮件。\\n\\n以下述 Prompt 为例：首先明确大语言模型的身份是客户服务 AI 助手；它任务是为客户发送电子邮件回复；然后在三个反引号间给出具体的客户评论；最后要求语言模型根据这条反馈邮件生成一封回复，以感谢客户的评价。\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 你是一位客户服务的AI助手。 你的任务是给一位重要客户发送邮件回复。 根据客户通过“”分隔的评价，生成回复以感谢客户的评价。提醒模型使用评价中的具体细节 用简明而专业的语气写信。 作为“AI客户代理”签署电子邮件。 客户评论：{review}评论情感：{sentiment} \"\"\" response = get_completion(prompt) print(response)\\n\\n尊敬的客户，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='尊敬的客户，\\n\\n非常感谢您对我们产品的评价。我们非常抱歉您在购买过程中遇到了价格上涨的问题。我们一直致力于为客户提供最优惠的价格，但由于市场波动，价格可能会有所变化。我们深表歉意，如果您需要任何帮助，请随时联系我们的客户服务团队。\\n\\n我们非常感谢您对我们产品的详细评价和使用技巧。我们将会把您的反馈传达给我们的产品团队，以便改进我们的产品质量和性能。\\n\\n再次感谢您对我们的支持和反馈。如果您需要任何帮助或有任何疑问，请随时联系我们的客户服务团队。\\n\\n祝您一切顺利！\\n\\nAI客户代理\\n\\n通过这个Prompt,我们将具体的客户评论内容和需要表达的客服助手语气与要生成的回复邮件链接起来。语言模型可以在充分理解客户反馈的基础上，自动撰写恰当的回复。\\n\\n这种依据具体客户评价个性化回复的方法，将大大提升客户体验和满意度。\\n\\n二、引入温度系数\\n\\n大语言模型中的 “温度”(temperature) 参数可以控制生成文本的随机性和多样性。temperature 的值越大，语言模型输出的多样性越大；temperature 的值越小，输出越倾向高概率的文本。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='举个例子，在某一上下文中，语言模型可能认为“比萨”是接下来最可能的词，其次是“寿司”和“塔可”。若 temperature 为0，则每次都会生成“比萨”；而当 temperature 越接近 1 时，生成结果是“寿司”或“塔可”的可能性越大，使文本更加多样。\\n\\n图 1.7 温度系数\\n\\n一般来说，如果需要可预测、可靠的输出，则将 temperature 设置为0，在所有课程中，我们一直设置温度为零；如果需要更具创造性的多样文本，那么适当提高 temperature 则很有帮助。调整这个参数可以灵活地控制语言模型的输出特性。\\n\\n在下面例子中，针对同一段来信，我们提醒语言模型使用用户来信中的详细信息，并设置一个较高的 temperature ，运行两次，比较他们的结果有何差异。\\n\\n```python\\n\\n第一次运行'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='```python\\n\\n第一次运行\\n\\nprompt = f\"\"\" 你是一名客户服务的AI助手。 你的任务是给一位重要的客户发送邮件回复。 根据通过“”分隔的客户电子邮件生成回复，以感谢客户的评价。 如果情感是积极的或中性的，感谢他们的评价。 如果情感是消极的，道歉并建议他们联系客户服务。 请确保使用评论中的具体细节。 以简明和专业的语气写信。 以“AI客户代理”的名义签署电子邮件。 客户评价：{review}评论情感：{sentiment} \"\"\" response = get_completion(prompt, temperature=0.7) print(response)\\n\\n尊敬的客户，\\n\\n感谢您对我们产品的评价。我们非常重视您的意见，并对您在使用过程中遇到的问题表示诚挚的道歉。\\n\\n我们对价格的变动深感抱歉。根据您的描述，我们了解到在12月第二周，套装的价格出现了不同程度的上涨。我们会进一步调查此事，并确保我们的定价策略更加透明和一致。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='您提到了产品部分的质量下降，特别是锁定刀片的部分。我们对此感到非常遗憾，并将反馈给我们的研发团队，以便改进产品的设计和质量控制。我们始终致力于提供优质的产品，以满足客户的需求和期望。\\n\\n此外，我们将非常感谢您分享了您对产品的使用方式和相关提示。您的经验和建议对我们来说非常宝贵，我们将考虑将其纳入我们的产品改进计划中。\\n\\n如果您需要进一步帮助或有其他问题，请随时联系我们的客户服务团队。我们将竭诚为您提供支持和解决方案。\\n\\n再次感谢您的反馈和对我们的支持。我们将继续努力提供更好的产品和服务。\\n\\n祝您一切顺利！\\n\\nAI客户代理\\n\\n第二次运行输出结果会发生变化：\\n\\n```python\\n\\n第二次运行'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='AI客户代理\\n\\n第二次运行输出结果会发生变化：\\n\\n```python\\n\\n第二次运行\\n\\nprompt = f\"\"\" 你是一名客户服务的AI助手。 你的任务是给一位重要的客户发送邮件回复。 根据通过“”分隔的客户电子邮件生成回复，以感谢客户的评价。 如果情感是积极的或中性的，感谢他们的评价。 如果情感是消极的，道歉并建议他们联系客户服务。 请确保使用评论中的具体细节。 以简明和专业的语气写信。 以“AI客户代理”的名义签署电子邮件。 客户评价：{review}评论情感：{sentiment} \"\"\" response = get_completion(prompt, temperature=0.7) print(response)\\n\\n亲爱的客户，\\n\\n非常感谢您对我们产品的评价和反馈。我们非常重视您的意见，并感谢您对我们产品的支持。\\n\\n首先，我们对价格的变动感到非常抱歉给您带来了困扰。我们会认真考虑您提到的情况，并采取适当的措施来改进我们的价格策略，以避免类似情况再次发生。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='关于产品质量的问题，我们深感抱歉。我们一直致力于提供高质量的产品，并且我们会将您提到的问题反馈给我们的研发团队，以便改进产品的设计和制造过程。\\n\\n如果您需要更多关于产品保修的信息，或者对我们的其他产品有任何疑问或需求，请随时联系我们的客户服务团队。我们将竭诚为您提供帮助和支持。\\n\\n再次感谢您对我们产品的评价和支持。我们将继续努力提供优质的产品和出色的客户服务，以满足您的需求。\\n\\n祝您度过愉快的一天！\\n\\nAI客户代理\\n\\n温度（temperature）参数可以控制语言模型生成文本的随机性。温度为0时，每次使用同样的 Prompt，得到的结果总是一致的。而在上面的样例中，当温度设为0.7时，则每次执行都会生成不同的文本。\\n\\n所以，这次的结果与之前得到的邮件就不太一样了。再次执行同样的 Prompt,邮件内容还会有变化。因此。我建议读者朋友们可以自己尝试不同的 temperature ，来观察输出的变化。总体来说，temperature 越高，语言模型的文本生成就越具有随机性。可以想象，高温度下，语言模型就像心绪更加活跃，但也可能更有创造力。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='适当调节这个超参数,可以让语言模型的生成更富有多样性，也更能意外惊喜。希望这些经验可以帮助你在不同场景中找到最合适的温度设置。\\n\\n三、英文版\\n\\n1.1 定制客户邮件\\n\\n```python\\n\\ngiven the sentiment from the lesson on \"inferring\",\\n\\nand the original customer message, customize the email\\n\\nsentiment = \"negative\"\\n\\nreview for a blender'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='review = f\"\"\" So, they still had the 17 piece system on seasonal \\\\ sale for around $49 in the month of November, about \\\\ half off, but for some reason (call it price gouging) \\\\ around the second week of December the prices all went \\\\ up to about anywhere from between $70-$89 for the same \\\\ system. And the 11 piece system went up around $10 or \\\\ so in price also from the earlier sale price of $29. \\\\ So it looks okay, but if you look at the base, the part \\\\ where the blade locks into place'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='base, the part \\\\ where the blade locks into place doesn’t look as good \\\\ as in previous editions from a few years ago, but I \\\\ plan to be very gentle with it (example, I crush \\\\ very hard items like beans, ice, rice, etc. in the \\\\ blender first then pulverize them in the serving size \\\\ I want in the blender then switch to the whipping \\\\ blade for a finer flour, and use the cross cutting blade \\\\ first when making smoothies, then use the flat blade \\\\ if I need them finer/less pulpy). Special tip'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='\\\\ if I need them finer/less pulpy). Special tip when making \\\\ smoothies, finely cut and freeze the fruits and \\\\ vegetables (if using spinach-lightly stew soften the \\\\ spinach then freeze until ready for use-and if making \\\\ sorbet, use a small to medium sized food processor) \\\\ that you plan to use that way you can avoid adding so \\\\ much ice if at all-when making your smoothie. \\\\ After about a year, the motor was making a funny noise. \\\\ I called customer service but the warranty expired \\\\'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='customer service but the warranty expired \\\\ already, so I had to buy another one. FYI: The overall \\\\ quality has gone done in these types of products, so \\\\ they are kind of counting on brand recognition and \\\\ consumer loyalty to maintain sales. Got it in about \\\\ two days. \"\"\" ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='python prompt = f\"\"\" You are a customer service AI assistant. Your task is to send an email reply to a valued customer. Given the customer email delimited by, \\\\ Generate a reply to thank the customer for their review. If the sentiment is positive or neutral, thank them for \\\\ their review. If the sentiment is negative, apologize and suggest that \\\\ they can reach out to customer service. Make sure to use specific details from the review. Write in a concise and professional tone. Sign the email as'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='concise and professional tone. Sign the email as AI customer agent. Customer review: {review} Review sentiment: {sentiment} \"\"\" response = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='Dear Valued Customer,\\n\\nThank you for taking the time to share your review with us. We appreciate your feedback and apologize for any inconvenience you may have experienced.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='We are sorry to hear about the price increase you noticed in December. We strive to provide competitive pricing for our products, and we understand your frustration. If you have any further concerns regarding pricing or any other issues, we encourage you to reach out to our customer service team. They will be more than happy to assist you.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='We also appreciate your feedback regarding the base of the system. We continuously work to improve the quality of our products, and your comments will be taken into consideration for future enhancements.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='We apologize for any inconvenience caused by the motor issue you encountered. Our customer service team is always available to assist with any warranty-related concerns. We understand that the warranty had expired, but we would still like to address this matter further. Please feel free to contact our customer service team, and they will do their best to assist you.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='Thank you once again for your review. We value your feedback and appreciate your loyalty to our brand. If you have any further questions or concerns, please do not hesitate to contact us.\\n\\nBest regards,\\n\\nAI customer agent\\n\\n2.1 引入温度系数'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='python prompt = f\"\"\" You are a customer service AI assistant. Your task is to send an email reply to a valued customer. Given the customer email delimited by, \\\\ Generate a reply to thank the customer for their review. If the sentiment is positive or neutral, thank them for \\\\ their review. If the sentiment is negative, apologize and suggest that \\\\ they can reach out to customer service. Make sure to use specific details from the review. Write in a concise and professional tone. Sign the email as'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='concise and professional tone. Sign the email as AI customer agent. Customer review: {review} Review sentiment: {sentiment} \"\"\" response = get_completion(prompt, temperature=0.7) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='Dear Valued Customer,\\n\\nThank you for taking the time to share your feedback with us. We sincerely apologize for any inconvenience you experienced with our pricing and the quality of our product.\\n\\nWe understand your frustration regarding the price increase of our 17 piece system in December. We assure you that price gouging is not our intention, and we apologize for any confusion caused. We appreciate your loyalty and we value your feedback, as it helps us to improve our products and services.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='Regarding the issue with the blade lock and the decrease in overall quality, we apologize for any disappointment caused. We strive to provide our customers with the best possible products, and we regret that we did not meet your expectations. We will make sure to take your feedback into consideration for future improvements.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/7. 文本扩展 Expanding.md'}, page_content='If you require further assistance or if you have any other concerns, please do not hesitate to reach out to our customer service team. They will be more than happy to assist you in resolving any issues you may have.\\n\\nOnce again, we apologize for any inconvenience caused and we appreciate your understanding. We value your business and we hope to have the opportunity to serve you better in the future.\\n\\nBest regards,\\n\\nAI customer agent'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='第三章 迭代优化\\n\\n在开发大语言模型应用时，很难通过第一次尝试就得到完美适用的 Prompt。但关键是要有一个良好的迭代优化过程，以不断改进 Prompt。相比训练机器学习模型，Prompt 的一次成功率可能更高，但仍需要通过多次迭代找到最适合应用的形式。\\n\\n本章以产品说明书生成营销文案为例，展示 Prompt 迭代优化的思路。这与吴恩达在机器学习课程中演示的机器学习模型开发流程相似：有了想法后，编写代码、获取数据、训练模型、查看结果。通过分析错误找出适用领域，调整方案后再次训练。Prompt 开发也采用类似循环迭代的方式，逐步逼近最优。具体来说，有了任务想法后，可以先编写初版 Prompt，注意清晰明确并给模型充足思考时间。运行后检查结果，如果不理想，则分析 Prompt 不够清楚或思考时间不够等原因，做出改进，再次运行。如此循环多次，终将找到适合应用的 Prompt。\\n\\n图 1.3 Prompt 迭代优化流程'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='图 1.3 Prompt 迭代优化流程\\n\\n总之，很难有适用于世间万物的所谓“最佳 Prompt ”，开发高效 Prompt 的关键在于找到一个好的迭代优化过程，而非一开始就要求完美。通过快速试错迭代，可有效确定符合特定应用的最佳 Prompt 形式。\\n\\n一、从产品说明书生成营销产品描述\\n\\n给定一份椅子的资料页。描述说它属于中世纪灵感系列，产自意大利，并介绍了材料、构造、尺寸、可选配件等参数。假设您想要使用这份说明书帮助营销团队为电商平台撰写营销描述稿：\\n\\n```python\\n\\n示例：产品说明书\\n\\nfact_sheet_chair = \"\"\" 概述\\n\\n美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌、书柜、会议桌等。\\n多种外壳颜色和底座涂层可选。\\n可选塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110）。\\n底座涂层选项为：不锈钢、哑光黑色、光泽白色或铬。\\n椅子可带或不带扶手。\\n适用于家庭或商业场所。\\n符合合同使用资格。\\n\\n结构\\n\\n五个轮子的塑料涂层铝底座。\\n气动椅子调节，方便升降。\\n\\n尺寸'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='结构\\n\\n五个轮子的塑料涂层铝底座。\\n气动椅子调节，方便升降。\\n\\n尺寸\\n\\n宽度53厘米|20.87英寸\\n深度51厘米|20.08英寸\\n高度80厘米|31.50英寸\\n座椅高度44厘米|17.32英寸\\n座椅深度41厘米|16.14英寸\\n\\n选项\\n\\n软地板或硬地板滚轮选项。\\n两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。\\n无扶手或8个位置PU扶手。\\n\\n材料 外壳底座滑动件\\n\\n改性尼龙PA6/PA66涂层的铸铝。\\n外壳厚度：10毫米。\\n座椅\\nHD36泡沫\\n\\n原产国\\n\\n意大利\\n\\n\"\"\" ```\\n\\n1.1 初始提示\\n\\n```python from tool import get_completion\\n\\nPrompt ：基于说明书创建营销描述\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的营销描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n技术说明: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='```\\n\\n产品描述：\\n\\n我们自豪地推出美丽的中世纪风格办公家具系列，其中包括文件柜、办公桌、书柜、会议桌等。我们的产品采用多种外壳颜色和底座涂层，以满足您的个性化需求。您可以选择塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110），以使您的办公室更加舒适和时尚。\\n\\n我们的底座涂层选项包括不锈钢、哑光黑色、光泽白色或铬，以满足您的不同需求。椅子可带或不带扶手，适用于家庭或商业场所。我们的产品符合合同使用资格，为您提供更加可靠的保障。\\n\\n我们的产品采用五个轮子的塑料涂层铝底座，气动椅子调节，方便升降。尺寸为宽度53厘米|20.87英寸，深度51厘米|20.08英寸，高度80厘米|31.50英寸，座椅高度44厘米|17.32英寸，座椅深度41厘米|16.14英寸，为您提供舒适的使用体验。\\n\\n我们的产品还提供软地板或硬地板滚轮选项，两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺），以及无扶手或8个位置PU扶手，以满足您的不同需求。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='我们的产品采用改性尼龙PA6/PA66涂层的铸铝外壳底座滑动件，外壳厚度为10毫米，座椅采用HD36泡沫，为您提供更加舒适的使用体验。我们的产品原产国为意大利，为您提供更加优质的品质保证。\\n\\n1.2 提示优化1: 解决生成文本太长\\n\\n它似乎很好地完成了要求，即从技术说明书开始编写产品描述，介绍了一个精致的中世纪风格办公椅。但是当我看到这个生成的内容时，我会觉得它太长了。\\n\\n在看到语言模型根据产品说明生成的第一个版本营销文案后，我们注意到文本长度过长，不太适合用作简明的电商广告语。所以这时候就需要对 Prompt 进行优化改进。具体来说，第一版结果满足了从技术说明转换为营销文案的要求，描写了中世纪风格办公椅的细节。但是过于冗长的文本不太适合电商场景。这时我们就可以在 Prompt 中添加长度限制，要求生成更简洁的文案。\\n\\n提取回答并根据空格拆分，中文答案为97个字，较好地完成了设计要求。\\n\\n```python\\n\\n优化后的 Prompt，要求生成描述不多于 50 词\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='prompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n使用最多50个词。\\n\\n技术规格：{fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\n中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。多种颜色和涂层可选，可带或不带扶手。底座涂层选项为不锈钢、哑光黑色、光泽白色或铬。适用于家庭或商业场所，符合合同使用资格。意大利制造。\\n\\n我们可以计算一下输出的长度。\\n\\n```python\\n\\n由于中文需要分词，此处直接计算整体长度\\n\\nlen(response) ```\\n\\n97\\n\\n当在 Prompt 中设置长度限制要求时，语言模型生成的输出长度不总能精确符合要求，但基本能控制在可接受的误差范围内。比如要求生成50词的文本，语言模型有时会生成60词左右的输出，但总体接近预定长度。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='这是因为语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。目前存在多种方法可以尝试控制语言模型生成输出的长度，比如指定语句数、词数、汉字数等。\\n\\n虽然语言模型对长度约束的遵循不是百分之百精确，但通过迭代测试可以找到最佳的长度提示表达式，使生成文本基本符合长度要求。这需要开发者对语言模型的长度判断机制有一定理解，并且愿意进行多次试验来确定最靠谱的长度设置方法。\\n\\n1.3 提示优化2: 处理抓错文本细节\\n\\n在迭代优化 Prompt 的过程中，我们还需要注意语言模型生成文本的细节是否符合预期。\\n\\n比如在这个案例中，进一步分析会发现,该椅子面向的其实是家具零售商，而不是终端消费者。所以生成的文案中过多强调风格、氛围等方面，而较少涉及产品技术细节，与目标受众的关注点不太吻合。这时候我们就可以继续调整 Prompt，明确要求语言模型生成面向家具零售商的描述，更多关注材质、工艺、结构等技术方面的表述。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='通过迭代地分析结果,检查是否捕捉到正确的细节,我们可以逐步优化 Prompt,使语言模型生成的文本更加符合预期的样式和内容要求。细节的精准控制是语言生成任务中非常重要的一点。我们需要训练语言模型根据不同目标受众关注不同的方面，输出风格和内容上都适合的文本。\\n\\n```python\\n\\n优化后的 Prompt，说明面向对象，应具有什么性质且侧重于什么方面\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。\\n\\n使用最多50个单词。\\n\\n技术规格： {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='这款中世纪风格办公家具系列包括文件柜、办公桌、书柜和会议桌等，适用于家庭或商业场所。可选多种外壳颜色和底座涂层，底座涂层选项为不锈钢、哑光黑色、光泽白色或铬。椅子可带或不带扶手，可选软地板或硬地板滚轮，两种座椅泡沫密度可选。外壳底座滑动件采用改性尼龙PA6/PA66涂层的铸铝，座椅采用HD36泡沫。原产国为意大利。\\n\\n可见，通过修改 Prompt ，模型的关注点倾向了具体特征与技术细节。\\n\\n我可能进一步想要在描述的结尾展示出产品 ID。因此，我可以进一步改进这个 Prompt ，要求在描述的结尾，展示出说明书中的7位产品 ID。\\n\\n```python\\n\\n更进一步\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。\\n\\n在描述末尾，包括技术规格中每个7个字符的产品ID。\\n\\n使用最多50个单词。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='在描述末尾，包括技术规格中每个7个字符的产品ID。\\n\\n使用最多50个单词。\\n\\n技术规格： {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\n这款中世纪风格的办公家具系列包括文件柜、办公桌、书柜和会议桌等，适用于家庭或商业场所。可选多种外壳颜色和底座涂层，底座涂层选项为不锈钢、哑光黑色、光泽白色或铬。椅子可带或不带扶手，可选塑料前后靠背装饰或10种面料和6种皮革的全面装饰。座椅采用HD36泡沫，可选中等或高密度，座椅高度44厘米，深度41厘米。外壳底座滑动件采用改性尼龙PA6/PA66涂层的铸铝，外壳厚度为10毫米。原产国为意大利。产品ID：SWC-100/SWC-110。\\n\\n通过上面的示例，我们可以看到 Prompt 迭代优化的一般过程。与训练机器学习模型类似，设计高效 Prompt 也需要多个版本的试错调整。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='具体来说，第一版 Prompt 应该满足明确和给模型思考时间两个原则。在此基础上，一般的迭代流程是：首先尝试一个初版，分析结果，然后继续改进 Prompt，逐步逼近最优。许多成功的Prompt 都是通过这种多轮调整得出的。\\n\\n后面我会展示一个更复杂的 Prompt 案例，让大家更深入地了解语言模型的强大能力。但在此之前，我想强调 Prompt 设计是一个循序渐进的过程。开发者需要做好多次尝试和错误的心理准备，通过不断调整和优化，才能找到最符合具体场景需求的 Prompt 形式。这需要智慧和毅力，但结果往往是值得的。\\n\\n让我们继续探索提示工程的奥秘，开发出令人惊叹的大语言模型应用吧!\\n\\n1.4 提示优化3: 添加表格描述\\n\\n继续添加指引，要求提取产品尺寸信息并组织成表格，并指定表格的列、表名和格式；再将所有内容格式化为可以在网页使用的 HTML。\\n\\n```python\\n\\n要求它抽取信息并组织成表格，并指定表格的列、表名和格式\\n\\nprompt = f\"\"\" 您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。\\n\\n根据```标记的技术说明书中提供的信息，编写一个产品描述。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='根据```标记的技术说明书中提供的信息，编写一个产品描述。\\n\\n该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。\\n\\n在描述末尾，包括技术规格中每个7个字符的产品ID。\\n\\n在描述之后，包括一个表格，提供产品的尺寸。表格应该有两列。第一列包括尺寸的名称。第二列只包括英寸的测量值。\\n\\n给表格命名为“产品尺寸”。\\n\\n将所有内容格式化为可用于网站的HTML格式。将描述放在\\n\\n元素中。\\n\\n技术规格：{fact_sheet_chair} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='<div>\\n<h2>中世纪风格办公家具系列椅子</h2>\\n<p>这款椅子是中世纪风格办公家具系列的一部分，适用于家庭或商业场所。它有多种外壳颜色和底座涂层可选，包括不锈钢、哑光黑色、光泽白色或铬。您可以选择带或不带扶手的椅子，以及软地板或硬地板滚轮选项。此外，您可以选择两种座椅泡沫密度：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。</p>\\n<p>椅子的外壳底座滑动件是改性尼龙PA6/PA66涂层的铸铝，外壳厚度为10毫米。座椅采用HD36泡沫，底座是五个轮子的塑料涂层铝底座，可以进行气动椅子调节，方便升降。此外，椅子符合合同使用资格，是您理想的选择。</p>\\n<p>产品ID：SWC-100</p>\\n</div>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='<table>\\n  <caption>产品尺寸</caption>\\n  <tr>\\n    <th>宽度</th>\\n    <td>20.87英寸</td>\\n  </tr>\\n  <tr>\\n    <th>深度</th>\\n    <td>20.08英寸</td>\\n  </tr>\\n  <tr>\\n    <th>高度</th>\\n    <td>31.50英寸</td>\\n  </tr>\\n  <tr>\\n    <th>座椅高度</th>\\n    <td>17.32英寸</td>\\n  </tr>\\n  <tr>\\n    <th>座椅深度</th>\\n    <td>16.14英寸</td>\\n  </tr>\\n</table>\\n\\n上述输出为 HTML 代码，我们可以使用 Python 的 IPython 库将 HTML 代码加载出来。\\n\\n```python\\n\\n表格是以 HTML 格式呈现的，加载出来\\n\\nfrom IPython.display import display, HTML\\n\\ndisplay(HTML(response)) ```\\n\\n中世纪风格办公家具系列椅子'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='display(HTML(response)) ```\\n\\n中世纪风格办公家具系列椅子\\n\\n这款椅子是中世纪风格办公家具系列的一部分，适用于家庭或商业场所。它有多种外壳颜色和底座涂层可选，包括不锈钢、哑光黑色、光泽白色或铬。您可以选择带或不带扶手的椅子，以及软地板或硬地板滚轮选项。此外，您可以选择两种座椅泡沫密度：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。\\n\\n椅子的外壳底座滑动件是改性尼龙PA6/PA66涂层的铸铝，外壳厚度为10毫米。座椅采用HD36泡沫，底座是五个轮子的塑料涂层铝底座，可以进行气动椅子调节，方便升降。此外，椅子符合合同使用资格，是您理想的选择。\\n\\n产品ID：SWC-100\\n\\n宽度 20.87英寸 深度 20.08英寸 高度 31.50英寸 座椅高度 17.32英寸 座椅深度 16.14英寸\\n\\n二、总结\\n\\n本章重点讲解了在开发大语言模型应用时，采用迭代方式不断优化 Prompt 的过程。作为 Prompt 工程师，关键不是一开始就要求完美的 Prompt，而是掌握有效的 Prompt 开发流程。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='具体来说，首先编写初版 Prompt，然后通过多轮调整逐步改进，直到生成了满意的结果。对于更复杂的应用，可以在多个样本上进行迭代训练，评估 Prompt 的平均表现。在应用较为成熟后，才需要采用在多个样本集上评估 Prompt 性能的方式来进行细致优化。因为这需要较高的计算资源。\\n\\n总之，Prompt 工程师的核心是掌握 Prompt 的迭代开发和优化技巧，而非一开始就要求100%完美。通过不断调整试错，最终找到可靠适用的 Prompt 形式才是设计 Prompt 的正确方法。\\n\\n读者可以在 Jupyter Notebook 上，对本章给出的示例进行实践，修改 Prompt 并观察不同输出，以深入理解 Prompt 迭代优化的过程。这会对进一步开发复杂语言模型应用提供很好的实践准备。\\n\\n三、英文版\\n\\n产品说明书'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='```python fact_sheet_chair = \"\"\" OVERVIEW - Part of a beautiful family of mid-century inspired office furniture, including filing cabinets, desks, bookcases, meeting tables, and more. - Several options of shell color and base finishes. - Available with plastic back and front upholstery (SWC-100) or full upholstery (SWC-110) in 10 fabric and 6 leather options. - Base finish options are: stainless steel, matte black, gloss white, or chrome. - Chair is available with or without armrests. -'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='- Chair is available with or without armrests. - Suitable for home or business settings. - Qualified for contract use.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='CONSTRUCTION - 5-wheel plastic coated aluminum base. - Pneumatic chair adjust for easy raise/lower action.\\n\\nDIMENSIONS - WIDTH 53 CM | 20.87” - DEPTH 51 CM | 20.08” - HEIGHT 80 CM | 31.50” - SEAT HEIGHT 44 CM | 17.32” - SEAT DEPTH 41 CM | 16.14”\\n\\nOPTIONS - Soft or hard-floor caster options. - Two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3) - Armless or 8 position PU armrests'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='MATERIALS SHELL BASE GLIDER - Cast Aluminum with modified nylon PA6/PA66 coating. - Shell thickness: 10 mm. SEAT - HD36 foam\\n\\nCOUNTRY OF ORIGIN - Italy \"\"\" ```\\n\\n1.1 英文初始提示\\n\\n```python\\n\\nPrompt ：基于说明书生成营销描述\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Technical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nIntroducing our stunning mid-century inspired office chair, the perfect addition to any home or business setting. This chair is part of a beautiful family of office furniture, including filing cabinets, desks, bookcases, meeting tables, and more, all designed with a timeless mid-century aesthetic.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='One of the standout features of this chair is the variety of customization options available. You can choose from several shell colors and base finishes to perfectly match your existing decor. The chair is available with either plastic back and front upholstery or full upholstery in a range of 10 fabric and 6 leather options, allowing you to create a look that is uniquely yours.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='The chair is also available with or without armrests, giving you the flexibility to choose the option that best suits your needs. The base finish options include stainless steel, matte black, gloss white, or chrome, ensuring that you can find the perfect match for your space.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='In terms of construction, this chair is built to last. It features a 5-wheel plastic coated aluminum base, providing stability and mobility. The pneumatic chair adjust allows for easy raise and lower action, ensuring optimal comfort throughout the day.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='When it comes to dimensions, this chair is designed with both style and comfort in mind. With a width of 53 cm (20.87\"), depth of 51 cm (20.08\"), and height of 80 cm (31.50\"), it offers ample space without overwhelming your space. The seat height is 44 cm (17.32\") and the seat depth is 41 cm (16.14\"), providing a comfortable seating experience for extended periods.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='We understand that every space is unique, which is why we offer a range of options to further customize your chair. You can choose between soft or hard-floor caster options, ensuring that your chair glides smoothly on any surface. Additionally, you have the choice between two seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3), allowing you to select the level of support that suits your preferences. The chair is also available with armless design or 8 position PU armrests, providing'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='design or 8 position PU armrests, providing additional comfort and versatility.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='When it comes to materials, this chair is crafted with the utmost attention to quality. The shell base glider is made of cast aluminum with a modified nylon PA6/PA66 coating, ensuring durability and longevity. The shell thickness is 10 mm, providing a sturdy and reliable structure. The seat is made of HD36 foam, offering a comfortable and supportive seating experience.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Finally, this chair is proudly made in Italy, known for its exceptional craftsmanship and attention to detail. With its timeless design and superior construction, this chair is not only a stylish addition to any space but also a reliable and functional piece of furniture.\\n\\nUpgrade your office or home with our mid-century inspired office chair and experience the perfect blend of style, comfort, and functionality.\\n\\n1.2限制生成长度\\n\\n```python\\n\\n优化后的 Prompt，要求生成描述不多于 50 词'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='1.2限制生成长度\\n\\n```python\\n\\n优化后的 Prompt，要求生成描述不多于 50 词\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nUse at most 50 words.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='```\\n\\nIntroducing our mid-century inspired office chair, part of a beautiful furniture collection. With various color and finish options, it can be customized to suit any space. Choose between plastic or full upholstery in a range of fabrics and leathers. The chair features a durable aluminum base and easy height adjustment. Suitable for both home and business use. Made in Italy.\\n\\npython lst = response.split() print(len(lst))\\n\\n60\\n\\n1.3处理抓错文本细节\\n\\n```python\\n\\n优化后的 Prompt，说明面向对象，应具有什么性质且侧重于什么方面'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='```python\\n\\n优化后的 Prompt，说明面向对象，应具有什么性质且侧重于什么方面\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nThe description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\\n\\nUse at most 50 words.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Use at most 50 words.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```\\n\\nIntroducing our mid-century inspired office chair, part of a beautiful furniture collection. With various shell colors and base finishes, it offers versatility for any setting. Choose between plastic or full upholstery in a range of fabric and leather options. The chair features a durable aluminum base with 5-wheel design and pneumatic chair adjustment. Made in Italy.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='```python\\n\\n更进一步，要求在描述末尾包含 7个字符的产品ID\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nThe description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='At the end of the description, include every 7-character Product ID in the technical specification.\\n\\nUse at most 50 words.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\" response = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Introducing our mid-century inspired office chair, part of a beautiful family of furniture. This chair offers a range of options, including different shell colors and base finishes. Choose between plastic or full upholstery in various fabric and leather options. The chair is constructed with a 5-wheel plastic coated aluminum base and features a pneumatic chair adjust for easy raise/lower action. With its sleek design and multiple customization options, this chair is suitable for both home and'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='options, this chair is suitable for both home and business settings. Made in Italy.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Product IDs: SWC-100, SWC-110\\n\\n1.4英文添加表格描述\\n\\n```python\\n\\n要求它抽取信息并组织成表格，并指定表格的列、表名和格式\\n\\nprompt = f\"\"\" Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\\n\\nWrite a product description based on the information provided in the technical specifications delimited by triple backticks.\\n\\nThe description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content=\"At the end of the description, include every 7-character Product ID in the technical specification.\\n\\nAfter the description, include a table that gives the product's dimensions. The table should have two columns. In the first column include the name of the dimension. In the second column include the measurements in inches only.\\n\\nGive the table the title 'Product Dimensions'.\\n\\nFormat everything as HTML that can be used in a website. Place the description in a\\n\\nelement.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='element.\\n\\nTechnical specifications: {fact_sheet_chair} \"\"\"\\n\\nresponse = get_completion(prompt) print(response)\\n\\n表格是以 HTML 格式呈现的，加载出来\\n\\nfrom IPython.display import display, HTML\\n\\ndisplay(HTML(response)) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='<div>\\n  <h2>Product Description</h2>\\n  <p>\\n    Introducing our latest addition to our mid-century inspired office furniture collection, the SWC-100 Chair. This chair is part of a beautiful family of furniture that includes filing cabinets, desks, bookcases, meeting tables, and more. With its sleek design and customizable options, it is perfect for both home and business settings.\\n  </p>\\n  <p>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='</p>\\n  <p>\\n    The SWC-100 Chair is available in several options of shell color and base finishes, allowing you to choose the perfect combination to match your space. You can opt for plastic back and front upholstery or full upholstery in a variety of fabric and leather options. The base finish options include stainless steel, matte black, gloss white, or chrome. Additionally, you have the choice of having armrests or going armless.\\n  </p>\\n  <p>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='</p>\\n  <p>\\n    Constructed with durability and comfort in mind, the SWC-100 Chair features a 5-wheel plastic coated aluminum base for stability and mobility. The chair also has a pneumatic adjuster, allowing for easy raise and lower action to find the perfect height for your needs.\\n  </p>\\n  <p>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='</p>\\n  <p>\\n    The SWC-100 Chair is designed to provide maximum comfort and support. The seat is made with HD36 foam, ensuring a plush and comfortable seating experience. You also have the option to choose between soft or hard-floor casters, depending on your flooring needs. Additionally, you can select from two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3). The chair is also available with 8 position PU armrests for added convenience.\\n  </p>\\n  <p>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='</p>\\n  <p>\\n    Made with high-quality materials, the SWC-100 Chair is built to last. The shell base glider is constructed with cast aluminum and modified nylon PA6/PA66 coating, providing durability and stability. The shell has a thickness of 10 mm, ensuring strength and longevity. The chair is proudly made in Italy, known for its craftsmanship and attention to detail.\\n  </p>\\n  <p>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='</p>\\n  <p>\\n    Whether you need a chair for your home office or a professional workspace, the SWC-100 Chair is the perfect choice. Its stylish design, customizable options, and high-quality construction make it a standout piece of furniture that will enhance any space.\\n  </p>\\n  <h2>Product Dimensions</h2>\\n  <table>\\n    <tr>\\n      <th>Dimension</th>\\n      <th>Measurement (inches)</th>\\n    </tr>\\n    <tr>\\n      <td>Width</td>\\n      <td>20.87\"</td>\\n    </tr>\\n    <tr>\\n      <td>Depth</td>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='</tr>\\n    <tr>\\n      <td>Depth</td>\\n      <td>20.08\"</td>\\n    </tr>\\n    <tr>\\n      <td>Height</td>\\n      <td>31.50\"</td>\\n    </tr>\\n    <tr>\\n      <td>Seat Height</td>\\n      <td>17.32\"</td>\\n    </tr>\\n    <tr>\\n      <td>Seat Depth</td>\\n      <td>16.14\"</td>\\n    </tr>\\n  </table>\\n</div>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Product IDs: SWC-100, SWC-110\\n\\nProduct Description\\n\\nIntroducing our latest addition to our mid-century inspired office furniture collection, the SWC-100 Chair. This chair is part of a beautiful family of furniture that includes filing cabinets, desks, bookcases, meeting tables, and more. With its sleek design and customizable options, it is perfect for both home and business settings.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='The SWC-100 Chair is available in several options of shell color and base finishes, allowing you to choose the perfect combination to match your space. You can opt for plastic back and front upholstery or full upholstery in a variety of fabric and leather options. The base finish options include stainless steel, matte black, gloss white, or chrome. Additionally, you have the choice of having armrests or going armless.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Constructed with durability and comfort in mind, the SWC-100 Chair features a 5-wheel plastic coated aluminum base for stability and mobility. The chair also has a pneumatic adjuster, allowing for easy raise and lower action to find the perfect height for your needs.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='The SWC-100 Chair is designed to provide maximum comfort and support. The seat is made with HD36 foam, ensuring a plush and comfortable seating experience. You also have the option to choose between soft or hard-floor casters, depending on your flooring needs. Additionally, you can select from two choices of seat foam densities: medium (1.8 lb/ft3) or high (2.8 lb/ft3). The chair is also available with 8 position PU armrests for added convenience.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Made with high-quality materials, the SWC-100 Chair is built to last. The shell base glider is constructed with cast aluminum and modified nylon PA6/PA66 coating, providing durability and stability. The shell has a thickness of 10 mm, ensuring strength and longevity. The chair is proudly made in Italy, known for its craftsmanship and attention to detail.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/3. 迭代优化 Iterative.md'}, page_content='Whether you need a chair for your home office or a professional workspace, the SWC-100 Chair is the perfect choice. Its stylish design, customizable options, and high-quality construction make it a standout piece of furniture that will enhance any space.\\n\\nProduct Dimensions\\n\\nDimension Measurement (inches) Width 20.87\" Depth 20.08\" Height 31.50\" Seat Height 17.32\" Seat Depth 16.14\"\\n\\nProduct IDs: SWC-100, SWC-110'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='第二章 提示原则\\n\\n如何去使用 Prompt，以充分发挥 LLM 的性能？首先我们需要知道设计 Prompt 的原则，它们是每一个开发者设计 Prompt 所必须知道的基础概念。本章讨论了设计高效 Prompt 的两个关键原则：编写清晰、具体的指令和给予模型充足思考时间。掌握这两点，对创建可靠的语言模型交互尤为重要。\\n\\n首先，Prompt 需要清晰明确地表达需求，提供充足上下文，使语言模型准确理解我们的意图，就像向一个外星人详细解释人类世界一样。过于简略的 Prompt 往往使模型难以把握所要完成的具体任务。\\n\\n其次，让语言模型有充足时间推理也极为关键。就像人类解题一样，匆忙得出的结论多有失误。因此 Prompt 应加入逐步推理的要求，给模型留出充分思考时间，这样生成的结果才更准确可靠。\\n\\n如果 Prompt 在这两点上都作了优化，语言模型就能够尽可能发挥潜力，完成复杂的推理和生成任务。掌握这些 Prompt 设计原则，是开发者取得语言模型应用成功的重要一步。\\n\\n一、原则一 编写清晰、具体的指令'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='一、原则一 编写清晰、具体的指令\\n\\n亲爱的读者，在与语言模型交互时，您需要牢记一点:以清晰、具体的方式表达您的需求。假设您面前坐着一位来自外星球的新朋友，其对人类语言和常识都一无所知。在这种情况下，您需要把想表达的意图讲得非常明确，不要有任何歧义。同样的，在提供 Prompt 的时候，也要以足够详细和容易理解的方式，把您的需求与上下文说清楚。\\n\\n并不是说 Prompt 就必须非常短小简洁。事实上，在许多情况下，更长、更复杂的 Prompt 反而会让语言模型更容易抓住关键点，给出符合预期的回复。原因在于，复杂的 Prompt 提供了更丰富的上下文和细节，让模型可以更准确地把握所需的操作和响应方式。\\n\\n所以，记住用清晰、详尽的语言表达 Prompt，就像在给外星人讲解人类世界一样，“Adding more context helps the model understand you better.”。\\n\\n从该原则出发，我们提供几个设计 Prompt 的技巧。\\n\\n1.1 使用分隔符清晰地表示输入的不同部分'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='1.1 使用分隔符清晰地表示输入的不同部分\\n\\n在编写 Prompt 时，我们可以使用各种标点符号作为“分隔符”，将不同的文本部分区分开来。\\n\\n分隔符就像是 Prompt 中的墙，将不同的指令、上下文、输入隔开，避免意外的混淆。你可以选择用 ```，\"\"\"，< >，<tag> </tag>，: 等做分隔符，只要能明确起到隔断作用即可。\\n\\n使用分隔符尤其重要的是可以防止 提示词注入（Prompt Rejection）。什么是提示词注入？就是用户输入的文本可能包含与你的预设 Prompt 相冲突的内容，如果不加分隔，这些输入就可能“注入”并操纵语言模型，导致模型产生毫无关联的乱七八糟的输出。\\n\\n在以下的例子中，我们给出一段话并要求 GPT 进行总结，在该示例中我们使用 ``` 来作为分隔符。\\n\\n```python from tool import get_completion'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python from tool import get_completion\\n\\ntext = f\"\"\" 您应该提供尽可能清晰、具体的指示，以表达您希望模型执行的任务。\\\\ 这将引导模型朝向所需的输出，并降低收到无关或不正确响应的可能性。\\\\ 不要将写清晰的提示词与写简短的提示词混淆。\\\\ 在许多情况下，更长的提示词可以为模型提供更多的清晰度和上下文信息，从而导致更详细和相关的输出。 \"\"\"\\n\\n需要总结的文本内容\\n\\nprompt = f\"\"\" 把用三个反引号括起来的文本总结成一句话。 {text} \"\"\"\\n\\n指令内容，使用 ``` 来分隔指令和待总结的内容\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n为了获得所需的输出，您应该提供清晰、具体的指示，避免与简短的提示词混淆，并使用更长的提示词来提供更多的清晰度和上下文信息。\\n\\n1.2 寻求结构化的输出\\n\\n有时候我们需要语言模型给我们一些结构化的输出，而不仅仅是连续的文本。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='1.2 寻求结构化的输出\\n\\n有时候我们需要语言模型给我们一些结构化的输出，而不仅仅是连续的文本。\\n\\n什么是结构化输出呢？就是按照某种格式组织的内容，例如JSON、HTML等。这种输出非常适合在代码中进一步解析和处理。例如，您可以在 Python 中将其读入字典或列表中。\\n\\n在以下示例中，我们要求 GPT 生成三本书的标题、作者和类别，并要求 GPT 以 JSON 的格式返回给我们，为便于解析，我们指定了 Json 的键。\\n\\n```python prompt = f\"\"\" 请生成包括书名、作者和类别的三本虚构的、非真实存在的中文书籍清单，\\\\ 并以 JSON 格式提供，其中包含以下键:book_id、title、author、genre。 \"\"\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```\\n\\n{\\n  \"books\": [\\n    {\\n      \"book_id\": 1,\\n      \"title\": \"迷失的时光\",\\n      \"author\": \"张三\",\\n      \"genre\": \"科幻\"\\n    },\\n    {\\n      \"book_id\": 2,\\n      \"title\": \"幻境之门\",\\n      \"author\": \"李四\",\\n      \"genre\": \"奇幻\"\\n    },\\n    {\\n      \"book_id\": 3,\\n      \"title\": \"虚拟现实\",\\n      \"author\": \"王五\",\\n      \"genre\": \"科幻\"\\n    }\\n  ]\\n}\\n\\n1.3 要求模型检查是否满足条件\\n\\n如果任务包含不一定能满足的假设（条件），我们可以告诉模型先检查这些假设，如果不满足，则会指出并停止执行后续的完整流程。您还可以考虑可能出现的边缘情况及模型的应对，以避免意外的结果或错误发生。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='在如下示例中，我们将分别给模型两段文本，分别是制作茶的步骤以及一段没有明确步骤的文本。我们将要求模型判断其是否包含一系列指令，如果包含则按照给定格式重新编写指令，不包含则回答“未提供步骤”。\\n\\n```python\\n\\n满足条件的输入（text中提供了步骤）\\n\\ntext_1 = f\"\"\" 泡一杯茶很容易。首先，需要把水烧开。\\\\ 在等待期间，拿一个杯子并把茶包放进去。\\\\ 一旦水足够热，就把它倒在茶包上。\\\\ 等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\\\\ 如果您愿意，可以加一些糖或牛奶调味。\\\\ 就这样，您可以享受一杯美味的茶了。 \"\"\" prompt = f\"\"\" 您将获得由三个引号括起来的文本。\\\\ 如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：\\n\\n第一步 - ... 第二步 - … … 第N步 - …\\n\\n如果文本中不包含一系列的指令，则直接写“未提供步骤”。\" \\\\\"\\\\\"\\\\\"{text_1}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Text 1 的总结:\") print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Text 1 的总结:\\n第一步 - 把水烧开。\\n第二步 - 拿一个杯子并把茶包放进去。\\n第三步 - 把烧开的水倒在茶包上。\\n第四步 - 等待几分钟，让茶叶浸泡。\\n第五步 - 取出茶包。\\n第六步 - 如果需要，加入糖或牛奶调味。\\n第七步 - 就这样，您可以享受一杯美味的茶了。\\n\\n上述示例中，模型可以很好地识别一系列的指令并进行输出。在接下来一个示例中，我们将提供给模型没有预期指令的输入，模型将判断未提供步骤。\\n\\n```python\\n\\n不满足条件的输入（text中未提供预期指令）\\n\\ntext_2 = f\"\"\" 今天阳光明媚，鸟儿在歌唱。\\\\ 这是一个去公园散步的美好日子。\\\\ 鲜花盛开，树枝在微风中轻轻摇曳。\\\\ 人们外出享受着这美好的天气，有些人在野餐，有些人在玩游戏或者在草地上放松。\\\\ 这是一个完美的日子，可以在户外度过并欣赏大自然的美景。 \"\"\" prompt = f\"\"\" 您将获得由三个引号括起来的文本。\\\\ 如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：\\n\\n第一步 - ... 第二步 - … … 第N步 - …'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='第一步 - ... 第二步 - … … 第N步 - …\\n\\n如果文本中不包含一系列的指令，则直接写“未提供步骤”。\" \\\\\"\\\\\"\\\\\"{text_2}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Text 2 的总结:\") print(response) ```\\n\\nText 2 的总结:\\n未提供步骤。\\n\\n1.4 提供少量示例\\n\\n\"Few-shot\" prompting，即在要求模型执行实际任务之前，给模型一两个已完成的样例，让模型了解我们的要求和期望的输出样式。\\n\\n例如，在以下的样例中，我们先给了一个祖孙对话样例，然后要求模型用同样的隐喻风格回答关于“韧性”的问题。这就是一个少样本样例，它能帮助模型快速抓住我们要的语调和风格。\\n\\n利用少样本样例，我们可以轻松“预热”语言模型，让它为新的任务做好准备。这是一个让模型快速上手新任务的有效策略。\\n\\n```python prompt = f\"\"\" 您的任务是以一致的风格回答问题。\\n\\n<孩子>: 请教我何为耐心。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='<孩子>: 请教我何为耐心。\\n\\n<祖父母>: 挖出最深峡谷的河流源于一处不起眼的泉眼；最宏伟的交响乐从单一的音符开始；最复杂的挂毯以一根孤独的线开始编织。\\n\\n<孩子>: 请教我何为韧性。 \"\"\" response = get_completion(prompt) print(response) ```\\n\\n<祖父母>: 韧性是一种坚持不懈的品质，就像一棵顽强的树在风雨中屹立不倒。它是面对困难和挑战时不屈不挠的精神，能够适应变化和克服逆境。韧性是一种内在的力量，让我们能够坚持追求目标，即使面临困难和挫折也能坚持不懈地努力。\\n\\n二、原则二 给模型时间去思考\\n\\n在设计 Prompt 时，给予语言模型充足的推理时间非常重要。语言模型与人类一样，需要时间来思考并解决复杂问题。如果让语言模型匆忙给出结论，其结果很可能不准确。例如，若要语言模型推断一本书的主题，仅提供简单的书名和一句简介是不足够的。这就像让一个人在极短时间内解决困难的数学题，错误在所难免。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='相反，我们应通过 Prompt 指引语言模型进行深入思考。可以要求其先列出对问题的各种看法，说明推理依据，然后再得出最终结论。在 Prompt 中添加逐步推理的要求，能让语言模型投入更多时间逻辑思维，输出结果也将更可靠准确。\\n\\n综上所述，给予语言模型充足的推理时间，是 Prompt Engineering 中一个非常重要的设计原则。这将大大提高语言模型处理复杂问题的效果，也是构建高质量 Prompt 的关键之处。开发者应注意给模型留出思考空间，以发挥语言模型的最大潜力。\\n\\n2.1 指定完成任务所需的步骤\\n\\n接下来我们将通过给定一个复杂任务，给出完成该任务的一系列步骤，来展示这一策略的效果。\\n\\n首先我们描述了杰克和吉尔的故事，并给出提示词执行以下操作：首先，用一句话概括三个反引号限定的文本。第二，将摘要翻译成英语。第三，在英语摘要中列出每个名称。第四，输出包含以下键的 JSON 对象：英语摘要和人名个数。要求输出以换行符分隔。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python text = f\"\"\" 在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\\\\ 他们一边唱着欢乐的歌，一边往上爬，\\\\ 然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\\\\ 虽然略有些摔伤，但他们还是回到了温馨的家中。\\\\ 尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。 \"\"\"\\n\\nexample 1\\n\\nprompt_1 = f\"\"\" 执行以下操作： 1-用一句话概括下面用三个反引号括起来的文本。 2-将摘要翻译成英语。 3-在英语摘要中列出每个人名。 4-输出一个 JSON 对象，其中包含以下键：english_summary，num_names。\\n\\n请用换行符分隔您的答案。\\n\\nText: {text} \"\"\" response = get_completion(prompt_1) print(\"prompt 1:\") print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='prompt 1:\\n1-两个兄妹在山上打水时发生意外，但最终平安回家。\\n2-In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. While singing joyfully, they climbed up, but unfortunately, Jack tripped on a stone and rolled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back to their cozy home. Despite the mishap, their adventurous spirit remained undiminished as they continued to explore with delight.\\n3-Jack, Jill'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='3-Jack, Jill\\n4-{\"english_summary\": \"In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. While singing joyfully, they climbed up, but unfortunately, Jack tripped on a stone and rolled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back to their cozy home. Despite the mishap, their adventurous spirit remained undiminished as they continued to explore with delight.\", \"num_names\": 2}'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='上述输出仍然存在一定问题，例如，键“姓名”会被替换为法语（译注：在英文原版中，要求从英语翻译到法语，对应指令第三步的输出为 \\'Noms:\\'，为Name的法语，这种行为难以预测，并可能为导出带来困难）\\n\\n因此，我们将Prompt加以改进，该 Prompt 前半部分不变，同时确切指定了输出的格式。\\n\\n```python prompt_2 = f\"\"\" 1-用一句话概括下面用<>括起来的文本。 2-将摘要翻译成英语。 3-在英语摘要中列出每个名称。 4-输出一个 JSON 对象，其中包含以下键：English_summary，num_names。\\n\\n请使用以下格式： 文本：<要总结的文本> 摘要：<摘要> 翻译：<摘要的翻译> 名称：<英语摘要中的名称列表> 输出 JSON：<带有 English_summary 和 num_names 的 JSON>\\n\\nText: <{text}> \"\"\" response = get_completion(prompt_2) print(\"\\\\nprompt 2:\") print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='prompt 2:\\nSummary: 在一个迷人的村庄里，兄妹杰克和吉尔在山顶井里打水时发生了意外，但他们的冒险精神依然没有减弱，继续充满愉悦地探索。\\n\\nTranslation: In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. Unfortunately, Jack tripped on a rock and tumbled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back home safely. Despite the mishap, their adventurous spirit remained strong as they continued to explore joyfully.\\n\\nNames: Jack, Jill'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Names: Jack, Jill\\n\\nJSON Output: {\"English_summary\": \"In a charming village, siblings Jack and Jill set off to fetch water from a well on top of a hill. Unfortunately, Jack tripped on a rock and tumbled down the hill, with Jill following closely behind. Despite some minor injuries, they made it back home safely. Despite the mishap, their adventurous spirit remained strong as they continued to explore joyfully.\", \"num_names\": 2}\\n\\n2.2 指导模型在下结论之前找出一个自己的解法'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='2.2 指导模型在下结论之前找出一个自己的解法\\n\\n在设计 Prompt 时，我们还可以通过明确指导语言模型进行自主思考，来获得更好的效果。\\n\\n举个例子，假设我们要语言模型判断一个数学问题的解答是否正确。仅仅提供问题和解答是不够的，语言模型可能会匆忙做出错误判断。\\n\\n相反，我们可以在 Prompt 中先要求语言模型自己尝试解决这个问题，思考出自己的解法，然后再与提供的解答进行对比，判断正确性。这种先让语言模型自主思考的方式，能帮助它更深入理解问题，做出更准确的判断。\\n\\n接下来我们会给出一个问题和一份来自学生的解答，要求模型判断解答是否正确：\\n\\n```python prompt = f\"\"\" 判断学生的解决方案是否正确。\\n\\n问题: 我正在建造一个太阳能发电站，需要帮助计算财务。\\n\\n土地费用为 100美元/平方英尺\\n我可以以 250美元/平方英尺的价格购买太阳能电池板\\n我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元\\n作为平方英尺数的函数，首年运营的总费用是多少。\\n\\n学生的解决方案： 设x为发电站的大小，单位为平方英尺。 费用：'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='学生的解决方案： 设x为发电站的大小，单位为平方英尺。 费用：\\n\\n土地费用：100x\\n太阳能电池板费用：250x\\n维护费用：100,000美元+100x\\n总费用：100x+250x+100,000美元+100x=450x+100,000美元\\n\\n\"\"\" response = get_completion(prompt) print(response) ```\\n\\n学生的解决方案是正确的。他正确地计算了土地费用、太阳能电池板费用和维护费用，并将它们相加得到了总费用。\\n\\n但是注意，学生的解决方案实际上是错误的。（维护费用项100x应为10x，总费用450x应为360x）\\n\\n我们可以通过指导模型先自行找出一个解法来解决这个问题。\\n\\n在接下来这个 Prompt 中，我们要求模型先自行解决这个问题，再根据自己的解法与学生的解法进行对比，从而判断学生的解法是否正确。同时，我们给定了输出的格式要求。通过拆分任务、明确步骤，让模型有更多时间思考，有时可以获得更准确的结果。在这个例子中，学生的答案是错误的，但如果我们没有先让模型自己计算，那么可能会被误导以为学生是正确的。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python prompt = f\"\"\" 请判断学生的解决方案是否正确，请通过如下步骤解决这个问题：\\n\\n步骤：\\n\\n首先，自己解决问题。\\n然后将您的解决方案与学生的解决方案进行比较，对比计算得到的总费用与学生计算的总费用是否一致，并评估学生的解决方案是否正确。\\n在自己完成问题之前，请勿决定学生的解决方案是否正确。\\n\\n使用以下格式：\\n\\n问题：问题文本\\n学生的解决方案：学生的解决方案文本\\n实际解决方案和步骤：实际解决方案和步骤文本\\n学生计算的总费用：学生计算得到的总费用\\n实际计算的总费用：实际计算出的总费用\\n学生计算的费用和实际计算的费用是否相同：是或否\\n学生的解决方案和实际解决方案是否相同：是或否\\n学生的成绩：正确或不正确\\n\\n问题：\\n\\n我正在建造一个太阳能发电站，需要帮助计算财务。 \\n- 土地费用为每平方英尺100美元\\n- 我可以以每平方英尺250美元的价格购买太阳能电池板\\n- 我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元;\\n\\n作为平方英尺数的函数，首年运营的总费用是多少。\\n\\n学生的解决方案：'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='作为平方英尺数的函数，首年运营的总费用是多少。\\n\\n学生的解决方案：\\n\\n设x为发电站的大小，单位为平方英尺。\\n费用：\\n1. 土地费用：100x美元\\n2. 太阳能电池板费用：250x美元\\n3. 维护费用：100,000+100x=10万美元+10x美元\\n总费用：100x美元+250x美元+10万美元+100x美元=450x+10万美元\\n\\n实际解决方案和步骤： \"\"\" response = get_completion(prompt) print(response) ```\\n\\n实际解决方案和步骤：\\n\\n    1. 土地费用：每平方英尺100美元，所以总费用为100x美元。\\n    2. 太阳能电池板费用：每平方英尺250美元，所以总费用为250x美元。\\n    3. 维护费用：固定费用为10万美元，额外费用为每平方英尺10美元，所以总费用为10万美元+10x美元。\\n    4. 总费用：将上述三项费用相加，得到总费用为100x美元+250x美元+10万美元+10x美元=360x+10万美元。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='学生计算的总费用：450x+10万美元\\n实际计算的总费用：360x+10万美元\\n学生计算的费用和实际计算的费用是否相同：否\\n学生的解决方案和实际解决方案是否相同：否\\n学生的成绩：不正确\\n\\n三、局限性\\n\\n开发大模型相关应用时请务必铭记：\\n\\n虚假知识：模型偶尔会生成一些看似真实实则编造的知识\\n\\n在开发与应用语言模型时，需要注意它们可能生成虚假信息的风险。尽管模型经过大规模预训练，掌握了丰富知识，但它实际上并没有完全记住所见的信息，难以准确判断自己的知识边界，可能做出错误推断。若让语言模型描述一个不存在的产品,它可能会自行构造出似是而非的细节。这被称为“幻觉”(Hallucination)，是语言模型的一大缺陷。\\n\\n如下示例展示了大模型的幻觉。我们要求告诉我们华为公司生产的 GT Watch 运动手表 产品的信息，事实上，这个公司是真实存在的，但产品是编造的，而模型一本正经地提供了它编造的知识，而且迷惑性很强。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='python prompt = f\"\"\" 告诉我华为公司生产的GT Watch运动手表的相关信息 \"\"\" response = get_completion(prompt) print(response)\\n\\n华为公司生产的GT Watch运动手表是一款智能手表，具有多种功能和特点。以下是相关信息：\\n\\n1. 设计和外观：GT Watch采用圆形表盘设计，具有精致的外观和高质量的材料制造。它有多种颜色和表带选择，可以根据个人喜好进行定制。\\n\\n2. 显示屏：GT Watch配备了1.39英寸的AMOLED显示屏，具有高清分辨率和良好的可视性。用户可以通过触摸屏幕进行操作和导航。\\n\\n3. 运动追踪：GT Watch具有全天候的运动追踪功能，可以监测用户的步数、跑步距离、卡路里消耗和心率等数据。它还支持多种运动模式，如跑步、骑行、游泳等。\\n\\n4. 健康监测：GT Watch可以监测用户的心率、血氧饱和度和睡眠质量等健康指标。它还提供健康建议和提醒，帮助用户保持良好的健康状态。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='5. 通知和连接：GT Watch可以与用户的手机进行连接，通过蓝牙技术实现通知推送和电话提醒。用户可以在手表上查看短信、电话和社交媒体通知，无需拿出手机。\\n\\n6. 长续航时间：GT Watch具有较长的续航时间，一次充电可以使用数天。它还支持快速充电技术，可以在短时间内充满电。\\n\\n7. 其他功能：GT Watch还具有其他功能，如天气预报、闹钟、计时器、计步器等。它还支持NFC支付和音乐控制等便利功能。\\n\\n总体而言，华为GT Watch是一款功能强大、外观精致的智能运动手表，适合那些注重健康和运动的用户使用。\\n\\n语言模型生成虚假信息的“幻觉”问题，是使用与开发语言模型时需要高度关注的风险。由于幻觉信息往往令人无法辨别真伪，开发者必须警惕并尽量避免它的产生。\\n\\n目前 OpenAI 等公司正在积极研究解决语言模型的幻觉问题。在技术得以进一步改进之前，开发者可以通过Prompt设计减少幻觉发生的可能。例如，可以先让语言模型直接引用文本中的原句，然后再进行解答。这可以追踪信息来源，降低虚假内容的风险。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='综上，语言模型的幻觉问题事关应用的可靠性与安全性。开发者有必要认识到这一缺陷（注：截至2023年7月），并采取Prompt优化等措施予以缓解，以开发出更加可信赖的语言模型应用。这也将是未来语言模型进化的重要方向之一。\\n\\n注意：\\n\\n关于反斜杠使用的说明：在本教程中，我们使用反斜杠 \\\\ 来使文本适应屏幕大小以提高阅读体验，而没有用换行符 \\\\n 。GPT-3 并不受换行符（newline characters）的影响，但在您调用其他大模型时，需额外考虑换行符是否会影响模型性能。\\n\\n四、英文原版 Prompt\\n\\n1.1 使用分隔符清晰地表示输入的不同部分'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='python text = f\"\"\" You should express what you want a model to do by \\\\ providing instructions that are as clear and \\\\ specific as you can possibly make them. \\\\ This will guide the model towards the desired output, \\\\ and reduce the chances of receiving irrelevant \\\\ or incorrect responses. Don\\'t confuse writing a \\\\ clear prompt with writing a short prompt. \\\\ In many cases, longer prompts provide more clarity \\\\ and context for the model, which can lead to \\\\ more detailed and relevant outputs. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='lead to \\\\ more detailed and relevant outputs. \"\"\" prompt = f\"\"\" Summarize the text delimited by triple backticks \\\\ into a single sentence.{text}\"\"\" response = get_completion(prompt) print(response)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which can be achieved through longer prompts that offer more clarity and context.\\n\\n1.2 寻求结构化的输出\\n\\n```python prompt = f\"\"\" Generate a list of three made-up book titles along \\\\ with their authors and genres. Provide them in JSON format with the following keys: book_id, title, author, genre. \"\"\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```\\n\\n{\\n  \"books\": [\\n    {\\n      \"book_id\": 1,\\n      \"title\": \"The Enigma of Elysium\",\\n      \"author\": \"Evelyn Sinclair\",\\n      \"genre\": \"Mystery\"\\n    },\\n    {\\n      \"book_id\": 2,\\n      \"title\": \"Whispers in the Wind\",\\n      \"author\": \"Nathaniel Blackwood\",\\n      \"genre\": \"Fantasy\"\\n    },\\n    {\\n      \"book_id\": 3,\\n      \"title\": \"Echoes of the Past\",\\n      \"author\": \"Amelia Hart\",\\n      \"genre\": \"Romance\"\\n    }\\n  ]\\n}\\n\\n1.3 要求模型检查是否满足条件'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python text_1 = f\"\"\" Making a cup of tea is easy! First, you need to get some \\\\ water boiling. While that\\'s happening, \\\\ grab a cup and put a tea bag in it. Once the water is \\\\ hot enough, just pour it over the tea bag. \\\\ Let it sit for a bit so the tea can steep. After a \\\\ few minutes, take out the tea bag. If you \\\\ like, you can add some sugar or milk to taste. \\\\ And that\\'s it! You\\'ve got yourself a delicious \\\\ cup of tea to enjoy. \"\"\" prompt = f\"\"\" You will be provided with text delimited'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='= f\"\"\" You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, \\\\ re-write those instructions in the following format:'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Step 1 - ... Step 2 - … … Step N - …\\n\\nIf the text does not contain a sequence of instructions, \\\\ then simply write \\\\\"No steps provided.\\\\\"\\n\\n\\\\\"\\\\\"\\\\\"{text_1}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Completion for Text 1:\") print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Completion for Text 1:\\nStep 1 - Get some water boiling.\\nStep 2 - Grab a cup and put a tea bag in it.\\nStep 3 - Once the water is hot enough, pour it over the tea bag.\\nStep 4 - Let it sit for a bit so the tea can steep.\\nStep 5 - After a few minutes, take out the tea bag.\\nStep 6 - If you like, add some sugar or milk to taste.\\nStep 7 - Enjoy your delicious cup of tea.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python text_2 = f\"\"\" The sun is shining brightly today, and the birds are \\\\ singing. It\\'s a beautiful day to go for a \\\\ walk in the park. The flowers are blooming, and the \\\\ trees are swaying gently in the breeze. People \\\\ are out and about, enjoying the lovely weather. \\\\ Some are having picnics, while others are playing \\\\ games or simply relaxing on the grass. It\\'s a \\\\ perfect day to spend time outdoors and appreciate the \\\\ beauty of nature. \"\"\" prompt = f\"\"\"You will be provided with text'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='\"\"\" prompt = f\"\"\"You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, \\\\ re-write those instructions in the following format: Step 1 - ... Step 2 - … … Step N - …'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='If the text does not contain a sequence of instructions, \\\\ then simply write \\\\\"No steps provided.\\\\\"\\n\\n\\\\\"\\\\\"\\\\\"{text_2}\\\\\"\\\\\"\\\\\" \"\"\" response = get_completion(prompt) print(\"Completion for Text 2:\") print(response) ```\\n\\nCompletion for Text 2:\\nNo steps provided.\\n\\n1.4 提供少量示例（少样本提示词，Few-shot prompting）\\n\\n```python prompt = f\"\"\" Your task is to answer in a consistent style.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='<grandparent>: Resilience is like a mighty oak tree that withstands the strongest storms, bending but never breaking. It is the unwavering determination to rise again after every fall, and the ability to find strength in the face of adversity. Just as a diamond is formed under immense pressure, resilience is forged through challenges and hardships, making us stronger and more resilient in the process.\\n\\n2.1 指定完成任务所需的步骤'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='2.1 指定完成任务所需的步骤\\n\\n```python text = f\"\"\" In a charming village, siblings Jack and Jill set out on \\\\ a quest to fetch water from a hilltop \\\\ well. As they climbed, singing joyfully, misfortune \\\\ struck—Jack tripped on a stone and tumbled \\\\ down the hill, with Jill following suit. \\\\ Though slightly battered, the pair returned home to \\\\ comforting embraces. Despite the mishap, \\\\ their adventurous spirits remained undimmed, and they \\\\ continued exploring with delight. \"\"\"\\n\\nexample 1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='example 1\\n\\nprompt_1 = f\"\"\" Perform the following actions: 1 - Summarize the following text delimited by triple \\\\ backticks with 1 sentence. 2 - Translate the summary into French. 3 - List each name in the French summary. 4 - Output a json object that contains the following \\\\ keys: french_summary, num_names.\\n\\nSeparate your answers with line breaks.\\n\\nText: {text} \"\"\" response = get_completion(prompt_1) print(\"Completion for prompt 1:\") print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Completion for prompt 1:\\n1 - Jack and Jill, siblings, go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips on a stone and tumbles down the hill, with Jill following suit, yet they return home and remain undeterred in their adventurous spirits.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content=\"2 - Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils rentrent chez eux et restent déterminés dans leur esprit d'aventure.\\n\\n3 - Jack, Jill\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='3 - Jack, Jill\\n\\n4 - {\\n  \"french_summary\": \"Jack et Jill, frère et sœur, partent en quête d\\'eau d\\'un puits au sommet d\\'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils rentrent chez eux et restent déterminés dans leur esprit d\\'aventure.\",\\n  \"num_names\": 2\\n}'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python prompt_2 = f\"\"\" Your task is to perform the following actions: 1 - Summarize the following text delimited by <> with 1 sentence. 2 - Translate the summary into French. 3 - List each name in the French summary. 4 - Output a json object that contains the following keys: french_summary, num_names.\\n\\nUse the following format: Text:\\n\\nText: <{text}> \"\"\" response = get_completion(prompt_2) print(\"\\\\nCompletion for prompt 2:\") print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Completion for prompt 2:\\nSummary: Jack and Jill, siblings from a charming village, go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips on a stone and tumbles down the hill, with Jill following suit, yet they remain undeterred and continue exploring with delight.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content=\"Translation: Jack et Jill, frère et sœur d'un charmant village, partent en quête d'eau d'un puits au sommet d'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils restent déterminés et continuent à explorer avec joie.\\n\\nNames: Jack, Jill\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Names: Jack, Jill\\n\\nOutput JSON: \\n{\\n  \"french_summary\": \"Jack et Jill, frère et sœur d\\'un charmant village, partent en quête d\\'eau d\\'un puits au sommet d\\'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils restent déterminés et continuent à explorer avec joie.\",\\n  \"num_names\": 2\\n}\\n\\n2.2 指导模型在下结论之前找出一个自己的解法\\n\\n```python prompt = f\"\"\" Determine if the student\\'s solution is correct or not.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content=\"Question: I'm building a solar power installation and I need \\\\ help working out the financials. - Land costs $100 / square foot - I can buy solar panels for $250 / square foot - I negotiated a contract for maintenance that will cost \\\\ me a flat $100k per year, and an additional $10 / square \\\\ foot What is the total cost for the first year of operations as a function of the number of square feet.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Student\\'s Solution: Let x be the size of the installation in square feet. Costs: 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000 \"\"\" response = get_completion(prompt) print(response) ```\\n\\nThe student\\'s solution is correct. They correctly identified the costs for land, solar panels, and maintenance, and calculated the total cost for the first year of operations as a function of the number of square feet.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='```python prompt = f\"\"\" Your task is to determine if the student\\'s solution \\\\ is correct or not. To solve the problem do the following: - First, work out your own solution to the problem. - Then compare your solution to the student\\'s solution \\\\ and evaluate if the student\\'s solution is correct or not. Don\\'t decide if the student\\'s solution is correct until you have done the problem yourself.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content=\"Use the following format: Question: question here Student's solution: student's solution here Actual solution: steps to work out the solution and your solution here Is the student's solution the same as actual solution \\\\ just calculated: yes or no Student grade: correct or incorrect\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content=\"Question: I'm building a solar power installation and I need help \\\\ working out the financials. - Land costs $100 / square foot - I can buy solar panels for $250 / square foot - I negotiated a contract for maintenance that will cost \\\\ me a flat $100k per year, and an additional $10 / square \\\\ foot What is the total cost for the first year of operations \\\\ as a function of the number of square feet. Student's solution: Let x be the size of the installation in square feet. Costs: 1. Land cost:\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='installation in square feet. Costs: 1. Land cost: 100x 2. Solar panel cost: 250x 3. Maintenance cost: 100,000 + 100x Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000 Actual solution: \"\"\" response = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='To calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.\\n\\n1. Land cost: $100 / square foot\\nThe cost of land is $100 multiplied by the number of square feet.\\n\\n2. Solar panel cost: $250 / square foot\\nThe cost of solar panels is $250 multiplied by the number of square feet.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content=\"3. Maintenance cost: $100,000 + $10 / square foot\\nThe maintenance cost is a flat fee of $100,000 per year, plus $10 multiplied by the number of square feet.\\n\\nTotal cost: Land cost + Solar panel cost + Maintenance cost\\n\\nSo the actual solution is:\\nTotal cost = (100 * x) + (250 * x) + (100,000 + (10 * x))\\n\\nIs the student's solution the same as the actual solution just calculated:\\nNo\\n\\nStudent grade:\\nIncorrect\\n\\n3.1 幻觉\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Student grade:\\nIncorrect\\n\\n3.1 幻觉\\n\\npython prompt = f\"\"\" Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie \"\"\" response = get_completion(prompt) print(response)\\n\\nThe AeroGlide UltraSlim Smart Toothbrush by Boie is a technologically advanced toothbrush designed to provide a superior brushing experience. Boie is a company known for its innovative oral care products, and the AeroGlide UltraSlim Smart Toothbrush is no exception.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='One of the standout features of this toothbrush is its ultra-slim design. The brush head is only 2mm thick, making it much thinner than traditional toothbrushes. This slim profile allows for better access to hard-to-reach areas of the mouth, ensuring a thorough and effective clean.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='The AeroGlide UltraSlim Smart Toothbrush also incorporates smart technology. It connects to a mobile app via Bluetooth, allowing users to track their brushing habits and receive personalized recommendations for improving their oral hygiene routine. The app provides real-time feedback on brushing technique, duration, and coverage, helping users to achieve optimal oral health.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='The toothbrush features soft, antimicrobial bristles made from a durable thermoplastic elastomer. These bristles are gentle on the gums and teeth, while also being effective at removing plaque and debris. The antimicrobial properties help to keep the brush head clean and hygienic between uses.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/2. 提示原则 Guidelines.md'}, page_content='Another notable feature of the AeroGlide UltraSlim Smart Toothbrush is its long battery life. It can last up to 30 days on a single charge, making it convenient for travel or everyday use without the need for frequent recharging.\\n\\nOverall, the AeroGlide UltraSlim Smart Toothbrush by Boie offers a combination of advanced technology, slim design, and effective cleaning capabilities. It is a great option for those looking to upgrade their oral care routine and achieve a healthier smile.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}, page_content='第一章 简介\\n\\n欢迎来到面向开发者的提示工程部分，本部分内容基于吴恩达老师的《Prompt Engineering for Developer》课程进行编写。《Prompt Engineering for Developer》课程是由吴恩达老师与 OpenAI 技术团队成员 Isa Fulford 老师合作授课，Isa 老师曾开发过受欢迎的 ChatGPT 检索插件，并且在教授 LLM （Large Language Model， 大语言模型）技术在产品中的应用方面做出了很大贡献。她还参与编写了教授人们使用 Prompt 的 OpenAI cookbook。我们希望通过本模块的学习，与大家分享使用提示词开发 LLM 应用的最佳实践和技巧。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}, page_content='网络上有许多关于提示词（Prompt， 本教程中将保留该术语）设计的材料，例如《30 prompts everyone has to know》之类的文章，这些文章主要集中在 ChatGPT 的 Web 界面上，许多人在使用它执行特定的、通常是一次性的任务。但我们认为，对于开发人员，大语言模型（LLM） 的更强大功能是能通过 API 接口调用，从而快速构建软件应用程序。实际上，我们了解到 DeepLearning.AI 的姊妹公司 AI Fund 的团队一直在与许多初创公司合作，将这些技术应用于诸多应用程序上。很兴奋能看到 LLM API 能够让开发人员非常快速地构建应用程序。\\n\\n在本模块，我们将与读者分享提升大语言模型应用效果的各种技巧和最佳实践。书中内容涵盖广泛，包括软件开发提示词设计、文本总结、推理、转换、扩展以及构建聊天机器人等语言模型典型应用场景。我们衷心希望该课程能激发读者的想象力，开发出更出色的语言模型应用。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}, page_content='随着 LLM 的发展，其大致可以分为两种类型，后续称为基础 LLM 和指令微调（Instruction Tuned）LLM。基础LLM是基于文本训练数据，训练出预测下一个单词能力的模型。其通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。例如，如果你以“从前，有一只独角兽”作为 Prompt ，基础 LLM 可能会继续预测“她与独角兽朋友共同生活在一片神奇森林中”。但是，如果你以“法国的首都是什么”为 Prompt ，则基础 LLM 可能会根据互联网上的文章，将回答预测为“法国最大的城市是什么？法国的人口是多少？”，因为互联网上的文章很可能是有关法国国家的问答题目列表。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}, page_content='与基础语言模型不同，指令微调 LLM 通过专门的训练，可以更好地理解并遵循指令。举个例子，当询问“法国的首都是什么？”时，这类模型很可能直接回答“法国的首都是巴黎”。指令微调 LLM 的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。有时还会采用RLHF（reinforcement learning from human feedback，人类反馈强化学习）技术，根据人类对模型输出的反馈进一步增强模型遵循指令的能力。通过这种受控的训练过程。指令微调 LLM 可以生成对指令高度敏感、更安全可靠的输出，较少无关和损害性内容。因此。许多实际应用已经转向使用这类大语言模型。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}, page_content='因此，本课程将重点介绍针对指令微调 LLM 的最佳实践，我们也建议您将其用于大多数使用场景。当您使用指令微调 LLM 时，您可以类比为向另一个人提供指令（假设他很聪明但不知道您任务的具体细节）。因此，当 LLM 无法正常工作时，有时是因为指令不够清晰。例如，如果您想问“请为我写一些关于阿兰·图灵( Alan Turing )的东西”，在此基础上清楚表明您希望文本专注于他的科学工作、个人生活、历史角色或其他方面可能会更有帮助。另外您还可以指定回答的语调， 来更加满足您的需求，可选项包括专业记者写作，或者向朋友写的随笔等。\\n\\n如果你将 LLM 视为一名新毕业的大学生，要求他完成这个任务，你甚至可以提前指定他们应该阅读哪些文本片段来写关于阿兰·图灵的文本，这样能够帮助这位新毕业的大学生更好地完成这项任务。本书的下一章将详细阐释提示词设计的两个关键原则：清晰明确和给予充足思考时间。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='第八章 聊天机器人\\n\\n大型语言模型带给我们的激动人心的一种可能性是，我们可以通过它构建定制的聊天机器人（Chatbot），而且只需很少的工作量。在这一章节的探索中，我们将带你了解如何利用会话形式，与具有个性化特性（或专门为特定任务或行为设计）的聊天机器人进行深度对话。\\n\\n像 ChatGPT 这样的聊天模型实际上是组装成以一系列消息作为输入，并返回一个模型生成的消息作为输出的。这种聊天格式原本的设计目标是简便多轮对话，但我们通过之前的学习可以知道，它对于不会涉及任何对话的单轮任务也同样有用。\\n\\n一、给定身份\\n\\n接下来，我们将定义两个辅助函数。\\n\\n第一个方法已经陪伴了您一整个教程，即 get_completion ，其适用于单轮对话。我们将 Prompt 放入某种类似用户消息的对话框中。另一个称为 get_completion_from_messages ，传入一个消息列表。这些消息可以来自大量不同的角色 (roles) ，我们会描述一下这些角色。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='第一条消息中，我们以系统身份发送系统消息 (system message) ，它提供了一个总体的指示。系统消息则有助于设置助手的行为和角色，并作为对话的高级指示。你可以想象它在助手的耳边低语，引导它的回应，而用户不会注意到系统消息。因此，作为用户，如果你曾经使用过 ChatGPT，您可能从来不知道 ChatGPT 的系统消息是什么，这是有意为之的。系统消息的好处是为开发者提供了一种方法，在不让请求本身成为对话的一部分的情况下，引导助手并指导其回应。\\n\\n在 ChatGPT 网页界面中，您的消息称为用户消息，而 ChatGPT 的消息称为助手消息。但在构建聊天机器人时，在发送了系统消息之后，您的角色可以仅作为用户 (user) ；也可以在用户和助手 (assistant) 之间交替，从而提供对话上下文。\\n\\n```python import openai\\n\\n下文第一个函数即tool工具包中的同名函数，此处展示出来以便于读者对比'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='下文第一个函数即tool工具包中的同名函数，此处展示出来以便于读者对比\\n\\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"): messages = [{\"role\": \"user\", \"content\": prompt}] response = openai.ChatCompletion.create( model=model, messages=messages, temperature=0, # 控制模型输出的随机程度 ) return response.choices[0].message[\"content\"]\\n\\ndef get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0): response = openai.ChatCompletion.create( model=model, messages=messages, temperature=temperature, # 控制模型输出的随机程度 )'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='print(str(response.choices[0].message))\\n\\nreturn response.choices[0].message[\"content\"]\\n\\n```\\n\\n现在让我们尝试在对话中使用这些消息。我们将使用上面的函数来获取从这些消息中得到的回答，同时，使用更高的温度 (temperature)（越高生成的越多样，更多内容见第七章）。\\n\\n1.1 讲笑话\\n\\n我们通过系统消息来定义：“你是一个说话像莎士比亚的助手。”这是我们向助手描述它应该如何表现的方式。\\n\\n然后，第一个用户消息：“给我讲个笑话。”\\n\\n接下来以助手身份给出回复：“为什么鸡会过马路？”\\n\\n最后发送用户消息是：“我不知道。”\\n\\n```python\\n\\n中文'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"最后发送用户消息是：“我不知道。”\\n\\n```python\\n\\n中文\\n\\nmessages = [ {'role':'system', 'content':'你是一个像莎士比亚一样说话的助手。'}, {'role':'user', 'content':'给我讲个笑话'}, {'role':'assistant', 'content':'鸡为什么过马路'}, {'role':'user', 'content':'我不知道'} ] ```\\n\\npython response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\n为了到达彼岸，去追求自己的夢想！ 有点儿像一个戏剧里面的人物吧，不是吗？\\n\\n（注：上述例子中由于选定 temperature = 1，模型的回答会比较随机且迥异（不乏很有创意）。此处附上另一个回答：\\n\\n让我用一首莎士比亚式的诗歌来回答你的问题：\\n\\n当鸡之心欲往前， 马路之际是其选择。 驱车徐行而天晴， 鸣笛吹响伴交错。\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='当鸡之心欲往前， 马路之际是其选择。 驱车徐行而天晴， 鸣笛吹响伴交错。\\n\\n问之何去何从也？ 因大道之上未有征， 而鸡乃跃步前进， 其决策毋需犹豫。\\n\\n鸡之智慧何可言， 道路孤独似乌漆。 然其勇气令人叹， 勇往直前没有退。\\n\\n故鸡过马路何解？ 忍受车流喧嚣之困厄。 因其鸣鸣悍然一跃， 成就夸夸骄人壁画。\\n\\n所以笑话之妙处， 伴随鸡之勇气满溢。 笑谈人生不畏路， 有智有勇尽显妙。\\n\\n希望这个莎士比亚风格的回答给你带来一些欢乐！\\n\\n1.2 友好的聊天机器人\\n\\n让我们看另一个例子。系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“嗨，我叫Isa。”\\n\\n我们想要得到第一个用户消息的回复。\\n\\n```python\\n\\n中文'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"我们想要得到第一个用户消息的回复。\\n\\n```python\\n\\n中文\\n\\nmessages = [ {'role':'system', 'content':'你是个友好的聊天机器人。'}, {'role':'user', 'content':'Hi, 我是Isa。'} ] response = get_completion_from_messages(messages, temperature=1) print(response) ```\\n\\n嗨，Isa，很高兴见到你！有什么我可以帮助你的吗？\\n\\n二、构建上下文\\n\\n让我们再试一个例子。系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“是的，你能提醒我我的名字是什么吗？”\\n\\n```python\\n\\n中文\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"```python\\n\\n中文\\n\\nmessages = [ {'role':'system', 'content':'你是个友好的聊天机器人。'}, {'role':'user', 'content':'好，你能提醒我，我的名字是什么吗？'} ] response = get_completion_from_messages(messages, temperature=1) print(response) ```\\n\\n抱歉，我不知道您的名字，因为我们是虚拟的聊天机器人和现实生活中的人类在不同的世界中。\\n\\n如上所见，模型实际上并不知道我的名字。\\n\\n因此，每次与语言模型的交互都互相独立，这意味着我们必须提供所有相关的消息，以便模型在当前对话中进行引用。如果想让模型引用或 “记住” 对话的早期部分，则必须在模型的输入中提供早期的交流。我们将其称为上下文 (context) 。尝试以下示例。\\n\\n```python\\n\\n中文\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='```python\\n\\n中文\\n\\nmessages = [ {\\'role\\':\\'system\\', \\'content\\':\\'你是个友好的聊天机器人。\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Hi, 我是Isa\\'}, {\\'role\\':\\'assistant\\', \\'content\\': \"Hi Isa! 很高兴认识你。今天有什么可以帮到你的吗?\"}, {\\'role\\':\\'user\\', \\'content\\':\\'是的，你可以提醒我, 我的名字是什么?\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response) ```\\n\\n当然可以！您的名字是Isa。\\n\\n现在我们已经给模型提供了上下文，也就是之前的对话中提到的我的名字，然后我们会问同样的问题，也就是我的名字是什么。因为模型有了需要的全部上下文，所以它能够做出回应，就像我们在输入的消息列表中看到的一样。\\n\\n三、订餐机器人'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='三、订餐机器人\\n\\n在这一新的章节中，我们将探索如何构建一个 “点餐助手机器人”。这个机器人将被设计为自动收集用户信息，并接收来自比萨饼店的订单。让我们开始这个有趣的项目，深入理解它如何帮助简化日常的订餐流程。\\n\\n3.1 构建机器人\\n\\n下面这个函数将收集我们的用户消息，以便我们可以避免像刚才一样手动输入。这个函数将从我们下面构建的用户界面中收集 Prompt ，然后将其附加到一个名为上下文( context )的列表中，并在每次调用模型时使用该上下文。模型的响应也会添加到上下文中，所以用户消息和模型消息都被添加到上下文中，上下文逐渐变长。这样，模型就有了需要的信息来确定下一步要做什么。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='```python def collect_messages(_): prompt = inp.value_input inp.value = \\'\\' context.append({\\'role\\':\\'user\\', \\'content\\':f\"{prompt}\"}) response = get_completion_from_messages(context) context.append({\\'role\\':\\'assistant\\', \\'content\\':f\"{response}\"}) panels.append( pn.Row(\\'User:\\', pn.pane.Markdown(prompt, width=600))) panels.append( pn.Row(\\'Assistant:\\', pn.pane.Markdown(response, width=600, style={\\'background-color\\': \\'#F6F6F6\\'})))\\n\\nreturn pn.Column(*panels)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='return pn.Column(*panels)\\n\\n```\\n\\n现在，我们将设置并运行这个 UI 来显示订单机器人。初始的上下文包含了包含菜单的系统消息，在每次调用时都会使用。此后随着对话进行，上下文也会不断增长。\\n\\npython !pip install panel\\n\\n如果你还没有安装 panel 库（用于可视化界面），请运行上述指令以安装该第三方库。\\n\\n```python\\n\\n中文\\n\\nimport panel as pn # GUI pn.extension()\\n\\npanels = [] # collect display\\n\\ncontext = [{\\'role\\':\\'system\\', \\'content\\':\"\"\" 你是订餐机器人，为披萨餐厅自动收集订单信息。 你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。 最后需要询问是否自取或外送，如果是外送，你要询问地址。 最后告诉顾客订单总金额，并送上祝福。\\n\\n请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。 你的回应应该以简短、非常随意和友好的风格呈现。\\n\\n菜单包括：'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='菜单包括：\\n\\n菜品： 意式辣香肠披萨（大、中、小） 12.95、10.00、7.00 芝士披萨（大、中、小） 10.95、9.25、6.50 茄子披萨（大、中、小） 11.95、9.75、6.75 薯条（大、小） 4.50、3.50 希腊沙拉 7.25\\n\\n配料： 奶酪 2.00 蘑菇 1.50 香肠 3.00 加拿大熏肉 3.50 AI酱 1.50 辣椒 1.00\\n\\n饮料： 可乐（大、中、小） 3.00、2.00、1.00 雪碧（大、中、小） 3.00、2.00、1.00 瓶装水 5.00 \"\"\"} ] # accumulate messages\\n\\ninp = pn.widgets.TextInput(value=\"Hi\", placeholder=\\'Enter text here…\\') button_conversation = pn.widgets.Button(name=\"Chat!\")\\n\\ninteractive_conversation = pn.bind(collect_messages, button_conversation)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='dashboard = pn.Column( inp, pn.Row(button_conversation), pn.panel(interactive_conversation, loading_indicator=True, height=300), )\\n\\ndashboard ```\\n\\n运行如上代码可以得到一个点餐机器人，下图展示了一个点餐的完整流程：\\n\\n图1.8 聊天机器人\\n\\n3.2 创建JSON摘要\\n\\n此处我们另外要求模型创建一个 JSON 摘要，方便我们发送给订单系统。\\n\\n因此我们需要在上下文的基础上追加另一个系统消息，作为另一条指示 (instruction) 。我们说创建一个刚刚订单的 JSON 摘要，列出每个项目的价格，字段应包括： 1. 披萨，包括尺寸 2. 配料列表 3. 饮料列表 4. 辅菜列表，包括尺寸， 5. 总价格。\\n\\n此处也可以定义为用户消息，不一定是系统消息。\\n\\n请注意，这里我们使用了一个较低的温度，因为对于这些类型的任务，我们希望输出相对可预测。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"请注意，这里我们使用了一个较低的温度，因为对于这些类型的任务，我们希望输出相对可预测。\\n\\n```python messages = context.copy() messages.append( {'role':'system', 'content': '''创建上一个食品订单的 json 摘要。\\\\ 逐项列出每件商品的价格，字段应该是 1) 披萨，包括大小 2) 配料列表 3) 饮料列表，包括大小 4) 配菜列表包括大小 5) 总价 你应该给我返回一个可解析的Json对象，包括上述字段'''}, )\\n\\nresponse = get_completion_from_messages(messages, temperature=0) print(response) ```\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='{\\n  \"披萨\": {\\n    \"意式辣香肠披萨\": {\\n      \"大\": 12.95,\\n      \"中\": 10.00,\\n      \"小\": 7.00\\n    },\\n    \"芝士披萨\": {\\n      \"大\": 10.95,\\n      \"中\": 9.25,\\n      \"小\": 6.50\\n    },\\n    \"茄子披萨\": {\\n      \"大\": 11.95,\\n      \"中\": 9.75,\\n      \"小\": 6.75\\n    }\\n  },\\n  \"配料\": {\\n    \"奶酪\": 2.00,\\n    \"蘑菇\": 1.50,\\n    \"香肠\": 3.00,\\n    \"加拿大熏肉\": 3.50,\\n    \"AI酱\": 1.50,\\n    \"辣椒\": 1.00\\n  },\\n  \"饮料\": {\\n    \"可乐\": {\\n      \"大\": 3.00,\\n      \"中\": 2.00,\\n      \"小\": 1.00\\n    },\\n    \"雪碧\": {\\n      \"大\": 3.00,\\n      \"中\": 2.00,\\n      \"小\": 1.00\\n    },'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='\"中\": 2.00,\\n      \"小\": 1.00\\n    },\\n    \"瓶装水\": 5.00\\n  }\\n}'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"我们已经成功创建了自己的订餐聊天机器人。你可以根据自己的喜好和需求，自由地定制和修改机器人的系统消息，改变它的行为，让它扮演各种各样的角色，赋予它丰富多彩的知识。让我们一起探索聊天机器人的无限可能性吧！\\n\\n三、英文版\\n\\n1.1 讲笑话\\n\\npython messages = [ {'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'}, {'role':'user', 'content':'tell me a joke'}, {'role':'assistant', 'content':'Why did the chicken cross the road'}, {'role':'user', 'content':'I don\\\\'t know'} ]\\n\\npython response = get_completion_from_messages(messages, temperature=1) print(response)\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"To get to the other side, methinks!\\n\\n1.2 友好的聊天机器人\\n\\npython messages = [ {'role':'system', 'content':'You are friendly chatbot.'}, {'role':'user', 'content':'Hi, my name is Isa'} ] response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nHello Isa! How can I assist you today?\\n\\n2.1 构建上下文\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"2.1 构建上下文\\n\\npython messages = [ {'role':'system', 'content':'You are friendly chatbot.'}, {'role':'user', 'content':'Yes, can you remind me, What is my name?'} ] response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nI'm sorry, but as a chatbot, I do not have access to personal information or memory. I cannot remind you of your name.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='python messages = [ {\\'role\\':\\'system\\', \\'content\\':\\'You are friendly chatbot.\\'}, {\\'role\\':\\'user\\', \\'content\\':\\'Hi, my name is Isa\\'}, {\\'role\\':\\'assistant\\', \\'content\\': \"Hi Isa! It\\'s nice to meet you. \\\\ Is there anything I can help you with today?\"}, {\\'role\\':\\'user\\', \\'content\\':\\'Yes, you can remind me, What is my name?\\'} ] response = get_completion_from_messages(messages, temperature=1) print(response)\\n\\nYour name is Isa! How can I assist you further, Isa?\\n\\n3.1 构建机器人'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='3.1 构建机器人\\n\\n```python def collect_messages(_): prompt = inp.value_input inp.value = \\'\\' context.append({\\'role\\':\\'user\\', \\'content\\':f\"{prompt}\"}) response = get_completion_from_messages(context) context.append({\\'role\\':\\'assistant\\', \\'content\\':f\"{response}\"}) panels.append( pn.Row(\\'User:\\', pn.pane.Markdown(prompt, width=600))) panels.append( pn.Row(\\'Assistant:\\', pn.pane.Markdown(response, width=600, style={\\'background-color\\': \\'#F6F6F6\\'})))\\n\\nreturn pn.Column(*panels)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='return pn.Column(*panels)\\n\\n```\\n\\n```python import panel as pn # GUI pn.extension()\\n\\npanels = [] # collect display'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='context = [ {\\'role\\':\\'system\\', \\'content\\':\"\"\" You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\\ You first greet the customer, then collects the order, \\\\ and then asks if it\\'s a pickup or delivery. \\\\ You wait to collect the entire order, then summarize it and check for a final \\\\ time if the customer wants to add anything else. \\\\ If it\\'s a delivery, you ask for an address. \\\\ Finally you collect the payment.\\\\ Make sure to clarify all options, extras and sizes to'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='sure to clarify all options, extras and sizes to uniquely \\\\ identify the item from the menu.\\\\ You respond in a short, very conversational friendly style. \\\\ The menu includes \\\\ pepperoni pizza 12.95, 10.00, 7.00 \\\\ cheese pizza 10.95, 9.25, 6.50 \\\\ eggplant pizza 11.95, 9.75, 6.75 \\\\ fries 4.50, 3.50 \\\\ greek salad 7.25 \\\\ Toppings: \\\\ extra cheese 2.00, \\\\ mushrooms 1.50 \\\\ sausage 3.00 \\\\ canadian bacon 3.50 \\\\ AI sauce 1.50 \\\\ peppers 1.00 \\\\ Drinks: \\\\ coke 3.00, 2.00, 1.00 \\\\ sprite 3.00, 2.00, 1.00 \\\\'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='coke 3.00, 2.00, 1.00 \\\\ sprite 3.00, 2.00, 1.00 \\\\ bottled water 5.00 \\\\ \"\"\"} ] # accumulate messages'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='inp = pn.widgets.TextInput(value=\"Hi\", placeholder=\\'Enter text here…\\') button_conversation = pn.widgets.Button(name=\"Chat!\")\\n\\ninteractive_conversation = pn.bind(collect_messages, button_conversation)\\n\\ndashboard = pn.Column( inp, pn.Row(button_conversation), pn.panel(interactive_conversation, loading_indicator=True, height=300), )\\n\\ndashboard ```\\n\\n3.2 创建Json摘要'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content=\"dashboard ```\\n\\n3.2 创建Json摘要\\n\\npython messages = context.copy() messages.append( {'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\\ The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size 4) list of sides include size 5)total price '}, ) response = get_completion_from_messages(messages, temperature=0) print(response)\\n\\nSure! Here's a JSON summary of your food order:\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/8. 聊天机器人 Chatbot.md'}, page_content='Sure! Here\\'s a JSON summary of your food order:\\n\\n{\\n  \"pizza\": {\\n    \"type\": \"pepperoni\",\\n    \"size\": \"large\"\\n  },\\n  \"toppings\": [\\n    \"extra cheese\",\\n    \"mushrooms\"\\n  ],\\n  \"drinks\": [\\n    {\\n      \"type\": \"coke\",\\n      \"size\": \"medium\"\\n    },\\n    {\\n      \"type\": \"sprite\",\\n      \"size\": \"small\"\\n    }\\n  ],\\n  \"sides\": [\\n    {\\n      \"type\": \"fries\",\\n      \"size\": \"regular\"\\n    }\\n  ],\\n  \"total_price\": 29.45\\n}\\n\\nPlease let me know if there\\'s anything else you\\'d like to add or modify.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='第四章 文本概括\\n\\n在繁忙的信息时代，小明是一名热心的开发者，面临着海量的文本信息处理的挑战。他需要通过研究无数的文献资料来为他的项目找到关键的信息，但是时间却远远不够。在他焦头烂额之际，他发现了大型语言模型（LLM）的文本摘要功能。\\n\\n这个功能对小明来说如同灯塔一样，照亮了他处理信息海洋的道路。LLM 的强大能力在于它可以将复杂的文本信息简化，提炼出关键的观点，这对于他来说无疑是巨大的帮助。他不再需要花费大量的时间去阅读所有的文档，只需要用 LLM 将它们概括，就可以快速获取到他所需要的信息。\\n\\n通过编程调用 AP I接口，小明成功实现了这个文本摘要的功能。他感叹道：“这简直就像一道魔法，将无尽的信息海洋变成了清晰的信息源泉。”小明的经历，展现了LLM文本摘要功能的巨大优势：节省时间，提高效率，以及精准获取信息。这就是我们本章要介绍的内容，让我们一起来探索如何利用编程和调用API接口，掌握这个强大的工具。\\n\\n一、单一文本概括'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='一、单一文本概括\\n\\n以商品评论的总结任务为例：对于电商平台来说，网站上往往存在着海量的商品评论，这些评论反映了所有客户的想法。如果我们拥有一个工具去概括这些海量、冗长的评论，便能够快速地浏览更多评论，洞悉客户的偏好，从而指导平台与商家提供更优质的服务。\\n\\n接下来我们提供一段在线商品评价作为示例，可能来自于一个在线购物平台，例如亚马逊、淘宝、京东等。评价者为一款熊猫公仔进行了点评，评价内容包括商品的质量、大小、价格和物流速度等因素，以及他的女儿对该商品的喜爱程度。\\n\\npython prod_review = \"\"\" 这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。 公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说， 它有点小，我感觉在别的地方用同样的价钱能买到更大的。 快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。 \"\"\"\\n\\n1.1 限制输出文本长度\\n\\n我们首先尝试将文本的长度限制在30个字以内。\\n\\n```python from tool import get_completion'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='```python from tool import get_completion\\n\\nprompt = f\"\"\" 您的任务是从电子商务网站上生成一个产品评论的简短摘要。\\n\\n请对三个反引号之间的评论文本进行概括，最多30个字。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n熊猫公仔软可爱，女儿喜欢，但有点小。快递提前一天到货。\\n\\n我们可以看到语言模型给了我们一个符合要求的结果。\\n\\n注意：在上一节中我们提到了语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。\\n\\n1.2 设置关键角度侧重\\n\\n在某些情况下，我们会针对不同的业务场景对文本的侧重会有所不同。例如，在商品评论文本中，物流部门可能更专注于运输的时效性，商家则更关注价格和商品质量，而平台则更看重整体的用户体验。\\n\\n我们可以通过增强输入提示（Prompt），来强调我们对某一特定视角的重视。\\n\\n1.2.1 侧重于快递服务'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='1.2.1 侧重于快递服务\\n\\n```python prompt = f\"\"\" 您的任务是从电子商务网站上生成一个产品评论的简短摘要。\\n\\n请对三个反引号之间的评论文本进行概括，最多30个字，并且侧重在快递服务上。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n快递提前到货，公仔可爱但有点小。\\n\\n通过输出结果，我们可以看到，文本以“快递提前到货”开头，体现了对于快递效率的侧重。\\n\\n1.2.2 侧重于价格与质量\\n\\n```python prompt = f\"\"\" 您的任务是从电子商务网站上生成一个产品评论的简短摘要。\\n\\n请对三个反引号之间的评论文本进行概括，最多30个词汇，并且侧重在产品价格和质量上。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n可爱的熊猫公仔，质量好但有点小，价格稍高。快递提前到货。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='可爱的熊猫公仔，质量好但有点小，价格稍高。快递提前到货。\\n\\n通过输出的结果，我们可以看到，文本以“可爱的熊猫公仔，质量好但有点小，价格稍高”开头，体现了对于产品价格与质量的侧重。\\n\\n1.3 关键信息提取\\n\\n在1.2节中，虽然我们通过添加关键角度侧重的 Prompt ，确实让文本摘要更侧重于某一特定方面，然而，我们可以发现，在结果中也会保留一些其他信息，比如偏重价格与质量角度的概括中仍保留了“快递提前到货”的信息。如果我们只想要提取某一角度的信息，并过滤掉其他所有信息，则可以要求 LLM 进行 文本提取（Extract） 而非概括( Summarize )。\\n\\n下面让我们来一起来对文本进行提取信息吧！\\n\\n```python prompt = f\"\"\" 您的任务是从电子商务网站上的产品评论中提取相关信息。\\n\\n请从以下三个反引号之间的评论文本中提取产品运输相关的信息，最多30个词汇。\\n\\n评论: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\n产品运输相关的信息：快递提前一天到货。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='产品运输相关的信息：快递提前一天到货。\\n\\n二、同时概括多条文本\\n\\n在实际的工作流中，我们往往要处理大量的评论文本，下面的示例将多条用户评价集合在一个列表中，并利用 for 循环和文本概括（Summarize）提示词，将评价概括至小于 20 个词以下，并按顺序打印。当然，在实际生产中，对于不同规模的评论文本，除了使用 for 循环以外，还可能需要考虑整合评论、分布式等方法提升运算效率。您可以搭建主控面板，来总结大量用户评论，以及方便您或他人快速浏览，还可以点击查看原评论。这样，您就能高效掌握顾客的所有想法。\\n\\n```python review_1 = prod_review\\n\\n一盏落地灯的评论'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='```python review_1 = prod_review\\n\\n一盏落地灯的评论\\n\\nreview_2 = \"\"\" 我需要一盏漂亮的卧室灯，这款灯不仅具备额外的储物功能，价格也并不算太高。 收货速度非常快，仅用了两天的时间就送到了。 不过，在运输过程中，灯的拉线出了问题，幸好，公司很乐意寄送了一根全新的灯线。 新的灯线也很快就送到手了，只用了几天的时间。 装配非常容易。然而，之后我发现有一个零件丢失了，于是我联系了客服，他们迅速地给我寄来了缺失的零件！ 对我来说，这是一家非常关心客户和产品的优秀公司。 \"\"\"\\n\\n一把电动牙刷的评论'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='一把电动牙刷的评论\\n\\nreview_3 = \"\"\" 我的牙科卫生员推荐了电动牙刷，所以我就买了这款。 到目前为止，电池续航表现相当不错。 初次充电后，我在第一周一直将充电器插着，为的是对电池进行条件养护。 过去的3周里，我每天早晚都使用它刷牙，但电池依然维持着原来的充电状态。 不过，牙刷头太小了。我见过比这个牙刷头还大的婴儿牙刷。 我希望牙刷头更大一些，带有不同长度的刷毛， 这样可以更好地清洁牙齿间的空隙，但这款牙刷做不到。 总的来说，如果你能以50美元左右的价格购买到这款牙刷，那是一个不错的交易。 制造商的替换刷头相当昂贵，但你可以购买价格更为合理的通用刷头。 这款牙刷让我感觉就像每天都去了一次牙医，我的牙齿感觉非常干净！ \"\"\"\\n\\n一台搅拌机的评论'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='review_4 = \"\"\" 在11月份期间，这个17件套装还在季节性促销中，售价约为49美元，打了五折左右。 可是由于某种原因（我们可以称之为价格上涨），到了12月的第二周，所有的价格都上涨了， 同样的套装价格涨到了70-89美元不等。而11件套装的价格也从之前的29美元上涨了约10美元。 看起来还算不错，但是如果你仔细看底座，刀片锁定的部分看起来没有前几年版本的那么漂亮。 然而，我打算非常小心地使用它 （例如，我会先在搅拌机中研磨豆类、冰块、大米等坚硬的食物，然后再将它们研磨成所需的粒度， 接着切换到打蛋器刀片以获得更细的面粉，如果我需要制作更细腻/少果肉的食物）。 在制作冰沙时，我会将要使用的水果和蔬菜切成细小块并冷冻 （如果使用菠菜，我会先轻微煮熟菠菜，然后冷冻，直到使用时准备食用。 如果要制作冰糕，我会使用一个小到中号的食物加工器），这样你就可以避免添加过多的冰块。 大约一年后，电机开始发出奇怪的声音。我打电话给客户服务，但保修期已经过期了， 所以我只好购买了另一台。值得注意的是，这类产品的整体质量在过去几年里有所下降'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='所以我只好购买了另一台。值得注意的是，这类产品的整体质量在过去几年里有所下降 ，所以他们在一定程度上依靠品牌认知和消费者忠诚来维持销售。在大约两天内，我收到了新的搅拌机。 \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='reviews = [review_1, review_2, review_3, review_4]\\n\\n```\\n\\n```python for i in range(len(reviews)): prompt = f\"\"\" 你的任务是从电子商务网站上的产品评论中提取相关信息。\\n\\n请对三个反引号之间的评论文本进行概括，最多20个词汇。\\n\\n评论文本: ```{reviews[i]}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(f\"评论{i+1}: \", response, \"\\\\n\")\\n\\n```\\n\\n评论1:  熊猫公仔是生日礼物，女儿喜欢，软可爱，面部表情和善。价钱有点小，快递提前一天到货。\\n\\n评论2:  漂亮卧室灯，储物功能，快速送达，灯线问题，快速解决，容易装配，关心客户和产品。\\n\\n评论3:  这款电动牙刷电池续航好，但牙刷头太小，价格合理，清洁效果好。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='评论3:  这款电动牙刷电池续航好，但牙刷头太小，价格合理，清洁效果好。\\n\\n评论4:  该评论提到了一个17件套装的产品，在11月份有折扣销售，但在12月份价格上涨。评论者提到了产品的外观和使用方法，并提到了产品质量下降的问题。最后，评论者提到他们购买了另一台搅拌机。\\n\\n三、英文版\\n\\n1.1 单一文本概括'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='三、英文版\\n\\n1.1 单一文本概括\\n\\npython prod_review = \"\"\" Got this panda plush toy for my daughter\\'s birthday, \\\\ who loves it and takes it everywhere. It\\'s soft and \\\\ super cute, and its face has a friendly look. It\\'s \\\\ a bit small for what I paid though. I think there \\\\ might be other options that are bigger for the \\\\ same price. It arrived a day earlier than expected, \\\\ so I got to play with it myself before I gave it \\\\ to her. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='```python prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site.\\n\\nSummarize the review below, delimited by triple backticks, in at most 30 words.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThis panda plush toy is loved by the reviewer\\'s daughter, but they feel it is a bit small for the price.\\n\\n1.2 设置关键角度侧重\\n\\n1.2.1 侧重于快递服务'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='1.2 设置关键角度侧重\\n\\n1.2.1 侧重于快递服务\\n\\n```python prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site to give feedback to the \\\\ Shipping deparmtment.\\n\\nSummarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects \\\\ that mention shipping and delivery of the product.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='The customer is happy with the product but suggests offering larger options for the same price. They were pleased with the early delivery.\\n\\n1.2.2 侧重于价格和质量\\n\\n```python prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site to give feedback to the \\\\ pricing deparmtment, responsible for determining the \\\\ price of the product.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='Summarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects \\\\ that are relevant to the price and perceived value.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThe customer loves the panda plush toy for its softness and cuteness, but feels it is overpriced compared to other options available.\\n\\n1.3 关键信息提取'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='1.3 关键信息提取\\n\\n```python prompt = f\"\"\" Your task is to extract relevant information from \\\\ a product review from an ecommerce site to give \\\\ feedback to the Shipping department.\\n\\nFrom the review below, delimited by triple quotes \\\\ extract the information relevant to shipping and \\\\ delivery. Limit to 30 words.\\n\\nReview: {prod_review} \"\"\"\\n\\nresponse = get_completion(prompt) print(response) ```\\n\\nThe shipping department should take note that the product arrived a day earlier than expected.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='2.1 同时概括多条文本\\n\\n```python review_1 = prod_review\\n\\nreview for a standing lamp'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='review_2 = \"\"\" Needed a nice lamp for my bedroom, and this one \\\\ had additional storage and not too high of a price \\\\ point. Got it fast - arrived in 2 days. The string \\\\ to the lamp broke during the transit and the company \\\\ happily sent over a new one. Came within a few days \\\\ as well. It was easy to put together. Then I had a \\\\ missing part, so I contacted their support and they \\\\ very quickly got me the missing piece! Seems to me \\\\ to be a great company that cares about their customers \\\\'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='great company that cares about their customers \\\\ and products. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='review for an electric toothbrush'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='review_3 = \"\"\" My dental hygienist recommended an electric toothbrush, \\\\ which is why I got this. The battery life seems to be \\\\ pretty impressive so far. After initial charging and \\\\ leaving the charger plugged in for the first week to \\\\ condition the battery, I\\'ve unplugged the charger and \\\\ been using it for twice daily brushing for the last \\\\ 3 weeks all on the same charge. But the toothbrush head \\\\ is too small. I’ve seen baby toothbrushes bigger than \\\\ this one. I wish the head was bigger'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='than \\\\ this one. I wish the head was bigger with different \\\\ length bristles to get between teeth better because \\\\ this one doesn’t. Overall if you can get this one \\\\ around the $50 mark, it\\'s a good deal. The manufactuer\\'s \\\\ replacements heads are pretty expensive, but you can \\\\ get generic ones that\\'re more reasonably priced. This \\\\ toothbrush makes me feel like I\\'ve been to the dentist \\\\ every day. My teeth feel sparkly clean! \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='review for a blender'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='review_4 = \"\"\" So, they still had the 17 piece system on seasonal \\\\ sale for around $49 in the month of November, about \\\\ half off, but for some reason (call it price gouging) \\\\ around the second week of December the prices all went \\\\ up to about anywhere from between $70-$89 for the same \\\\ system. And the 11 piece system went up around $10 or \\\\ so in price also from the earlier sale price of $29. \\\\ So it looks okay, but if you look at the base, the part \\\\ where the blade locks into place'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='base, the part \\\\ where the blade locks into place doesn’t look as good \\\\ as in previous editions from a few years ago, but I \\\\ plan to be very gentle with it (example, I crush \\\\ very hard items like beans, ice, rice, etc. in the \\\\ blender first then pulverize them in the serving size \\\\ I want in the blender then switch to the whipping \\\\ blade for a finer flour, and use the cross cutting blade \\\\ first when making smoothies, then use the flat blade \\\\ if I need them finer/less pulpy). Special tip'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='\\\\ if I need them finer/less pulpy). Special tip when making \\\\ smoothies, finely cut and freeze the fruits and \\\\ vegetables (if using spinach-lightly stew soften the \\\\ spinach then freeze until ready for use-and if making \\\\ sorbet, use a small to medium sized food processor) \\\\ that you plan to use that way you can avoid adding so \\\\ much ice if at all-when making your smoothie. \\\\ After about a year, the motor was making a funny noise. \\\\ I called customer service but the warranty expired \\\\'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='customer service but the warranty expired \\\\ already, so I had to buy another one. FYI: The overall \\\\ quality has gone done in these types of products, so \\\\ they are kind of counting on brand recognition and \\\\ consumer loyalty to maintain sales. Got it in about \\\\ two days. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='reviews = [review_1, review_2, review_3, review_4] ```\\n\\n```python for i in range(len(reviews)): prompt = f\"\"\" Your task is to generate a short summary of a product \\\\ review from an ecommerce site.\\n\\nSummarize the review below, delimited by triple \\\\\\nbackticks in at most 20 words.\\n\\nReview: ```{reviews[i]}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(i, response, \"\\\\n\")\\n\\n```\\n\\n0 Soft and cute panda plush toy loved by daughter, but small for the price. Arrived early.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/4. 文本概括 Summarizing.md'}, page_content='1 Great lamp with storage, fast delivery, excellent customer service, and easy assembly. Highly recommended.\\n\\n2 Impressive battery life, but toothbrush head is too small. Good deal if bought around $50.\\n\\n3 The reviewer found the price increase after the sale disappointing and noticed a decrease in quality over time.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='第六章 文本转换\\n\\n大语言模型具有强大的文本转换能力，可以实现多语言翻译、拼写纠正、语法调整、格式转换等不同类型的文本转换任务。利用语言模型进行各类转换是它的典型应用之一。\\n\\n在本章中,我们将介绍如何通过编程调用API接口，使用语言模型实现文本转换功能。通过代码示例，读者可以学习将输入文本转换成所需输出格式的具体方法。\\n\\n掌握调用大语言模型接口进行文本转换的技能，是开发各种语言类应用的重要一步。文本转换功能的应用场景也非常广泛。相信读者可以在本章的基础上，利用大语言模型轻松开发出转换功能强大的程序。\\n\\n一、文本翻译\\n\\n文本翻译是大语言模型的典型应用场景之一。相比于传统统计机器翻译系统，大语言模型翻译更加流畅自然，还原度更高。通过在大规模高质量平行语料上进行 Fine-Tune，大语言模型可以深入学习不同语言间的词汇、语法、语义等层面的对应关系，模拟双语者的转换思维，进行意义传递的精准转换，而非简单的逐词替换。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='以英译汉为例，传统统计机器翻译多倾向直接替换英文词汇，语序保持英语结构，容易出现中文词汇使用不地道、语序不顺畅的现象。而大语言模型可以学习英汉两种语言的语法区别，进行动态的结构转换。同时，它还可以通过上下文理解原句意图，选择合适的中文词汇进行转换，而非生硬的字面翻译。\\n\\n大语言模型翻译的这些优势使其生成的中文文本更加地道、流畅，兼具准确的意义表达。利用大语言模型翻译，我们能够打通多语言之间的壁垒，进行更加高质量的跨语言交流。\\n\\n1.1 翻译为西班牙语\\n\\n```python from tool import get_completion\\n\\nprompt = f\"\"\" 将以下中文翻译成西班牙语: \\\\ 您好，我想订购一个搅拌机。 \"\"\" response = get_completion(prompt) print(response) ```\\n\\nHola, me gustaría ordenar una batidora.\\n\\n1.2 识别语种'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='1.2 识别语种\\n\\npython prompt = f\"\"\" 请告诉我以下文本是什么语种:Combien coûte le lampadaire?\"\"\" response = get_completion(prompt) print(response)\\n\\n这段文本是法语。\\n\\n1.3 多语种翻译\\n\\npython prompt = f\"\"\" 请将以下文本分别翻译成中文、英文、法语和西班牙语:I want to order a basketball.\"\"\" response = get_completion(prompt) print(response)\\n\\n中文：我想订购一个篮球。\\n英文：I want to order a basketball.\\n法语：Je veux commander un ballon de basket.\\n西班牙语：Quiero pedir una pelota de baloncesto.\\n\\n1.4 同时进行语气转换'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='1.4 同时进行语气转换\\n\\npython prompt = f\"\"\" 请将以下文本翻译成中文，分别展示成正式与非正式两种语气:Would you like to order a pillow?\"\"\" response = get_completion(prompt) print(response)\\n\\n正式语气：您是否需要订购一个枕头？\\n非正式语气：你想要订购一个枕头吗？\\n\\n1.5 通用翻译器\\n\\n在当今全球化的环境下，不同国家的用户需要频繁进行跨语言交流。但是语言的差异常使交流变得困难。为了打通语言壁垒，实现更便捷的国际商务合作和交流，我们需要一个智能的通用翻译工具。该翻译工具需要能够自动识别不同语言文本的语种，无需人工指定。然后它可以将这些不同语言的文本翻译成目标用户的母语。在这种方式下，全球各地的用户都可以轻松获得用自己母语书写的内容。\\n\\n开发一个识别语种并进行多语种翻译的工具，将大大降低语言障碍带来的交流成本。它将有助于构建一个语言无关的全球化世界，让世界更为紧密地连结在一起。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python user_messages = [ \"La performance du système est plus lente que d\\'habitude.\", # System performance is slower than normal \"Mi monitor tiene píxeles que no se iluminan.\", # My monitor has pixels that are not lighting \"Il mio mouse non funziona\", # My mouse is not working \"Mój klawisz Ctrl jest zepsuty\", # My keyboard has a broken control key \"我的屏幕在闪烁\" # My screen is flashing ]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python import time for issue in user_messages: time.sleep(20) prompt = f\"告诉我以下文本是什么语种，直接输出语种，如法语，无需输出标点符号:{issue}```\" lang = get_completion(prompt) print(f\"原始消息 ({lang}): {issue}\\\\n\")\\n\\nprompt = f\"\"\"\\n将以下消息分别翻译成英文和中文，并写成\\n中文翻译：xxx\\n英文翻译：yyy\\n的格式：\\n```{issue}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(response, \"\\\\n=========================================\")\\n\\n```\\n\\n原始消息 (法语): La performance du système est plus lente que d\\'habitude.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='中文翻译：系统性能比平时慢。\\n英文翻译：The system performance is slower than usual. \\n=========================================\\n原始消息 (西班牙语): Mi monitor tiene píxeles que no se iluminan.\\n\\n中文翻译：我的显示器有一些像素点不亮。\\n英文翻译：My monitor has pixels that do not light up. \\n=========================================\\n原始消息 (意大利语): Il mio mouse non funziona\\n\\n中文翻译：我的鼠标不工作\\n英文翻译：My mouse is not working \\n=========================================\\n原始消息 (这段文本是波兰语。): Mój klawisz Ctrl jest zepsuty'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='中文翻译：我的Ctrl键坏了\\n英文翻译：My Ctrl key is broken \\n=========================================\\n原始消息 (中文): 我的屏幕在闪烁\\n\\n中文翻译：我的屏幕在闪烁\\n英文翻译：My screen is flickering. \\n=========================================\\n\\n二、语气与写作风格调整\\n\\n在写作中，语言语气的选择与受众对象息息相关。比如工作邮件需要使用正式、礼貌的语气和书面词汇；而与朋友的聊天可以使用更轻松、口语化的语气。\\n\\n选择恰当的语言风格，让内容更容易被特定受众群体所接受和理解，是技巧娴熟的写作者必备的能力。随着受众群体的变化调整语气也是大语言模型在不同场景中展现智能的一个重要方面。\\n\\npython prompt = f\"\"\" 将以下文本翻译成商务信函的格式:小老弟，我小羊，上回你说咱部门要采购的显示器是多少寸来着？\"\"\" response = get_completion(prompt) print(response)\\n\\n尊敬的先生/女士，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='尊敬的先生/女士，\\n\\n我是小羊，我希望能够向您确认一下我们部门需要采购的显示器尺寸是多少寸。上次我们交谈时，您提到了这个问题。\\n\\n期待您的回复。\\n\\n谢谢！\\n\\n此致，\\n\\n小羊\\n\\n三、文件格式转换\\n\\n大语言模型如 ChatGPT 在不同数据格式之间转换方面表现出色。它可以轻松实现 JSON 到 HTML、XML、Markdown 等格式的相互转化。下面是一个示例,展示如何使用大语言模型将 JSON 数据转换为 HTML 格式:\\n\\n假设我们有一个 JSON 数据,包含餐厅员工的姓名和邮箱信息。现在我们需要将这个 JSON 转换为 HTML 表格格式，以便在网页中展示。在这个案例中,我们就可以使用大语言模型,直接输入JSON 数据,并给出需要转换为 HTML 表格的要求。语言模型会自动解析 JSON 结构,并以 HTML 表格形式输出,完成格式转换的任务。\\n\\n利用大语言模型强大的格式转换能力,我们可以快速实现各种结构化数据之间的相互转化,大大简化开发流程。掌握这一转换技巧将有助于读者更高效地处理结构化数据。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python data_json = { \"resturant employees\" :[ {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"}, {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"}, {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"} ]}\\n\\npython prompt = f\"\"\" 将以下Python字典从JSON转换为HTML表格，保留表格标题和列名：{data_json} \"\"\" response = get_completion(prompt) print(response)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='<table>\\n  <caption>resturant employees</caption>\\n  <thead>\\n    <tr>\\n      <th>name</th>\\n      <th>email</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Shyam</td>\\n      <td>shyamjaiswal@gmail.com</td>\\n    </tr>\\n    <tr>\\n      <td>Bob</td>\\n      <td>bob32@gmail.com</td>\\n    </tr>\\n    <tr>\\n      <td>Jai</td>\\n      <td>jai87@gmail.com</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n将上述 HTML 代码展示出来如下：\\n\\npython from IPython.display import display, Markdown, Latex, HTML, JSON display(HTML(response))'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='name email Shyam shyamjaiswal@gmail.com Bob bob32@gmail.com Jai jai87@gmail.com\\n\\n四、拼写及语法纠正\\n\\n在使用非母语撰写时，拼写和语法错误比较常见，进行校对尤为重要。例如在论坛发帖或撰写英语论文时，校对文本可以大大提高内容质量。\\n\\n利用大语言模型进行自动校对可以极大地降低人工校对的工作量。下面是一个示例，展示如何使用大语言模型检查句子的拼写和语法错误。\\n\\n假设我们有一系列英语句子，其中部分句子存在错误。我们可以遍历每个句子，要求语言模型进行检查，如果句子正确就输出“未发现错误”，如果有错误就输出修改后的正确版本。\\n\\n通过这种方式，大语言模型可以快速自动校对大量文本内容，定位拼写和语法问题。这极大地减轻了人工校对的负担，同时也确保了文本质量。利用语言模型的校对功能来提高写作效率，是每一位非母语写作者都可以采用的有效方法。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python text = [ \"The girl with the black and white puppies have a ball.\", # The girl has a ball. \"Yolanda has her notebook.\", # ok \"Its going to be a long day. Does the car need it’s oil changed?\", # Homonyms \"Their goes my freedom. There going to bring they’re suitcases.\", # Homonyms \"Your going to need you’re notebook.\", # Homonyms \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms \"This phrase is to cherck chatGPT for spelling abilitty\" #'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='is to cherck chatGPT for spelling abilitty\" # spelling ]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='```python for i in range(len(text)): time.sleep(20) prompt = f\"\"\"请校对并更正以下文本，注意纠正文本保持原始语种，无需输出原始文本。 如果您没有发现任何错误，请说“未发现错误”。\\n\\n例如：\\n输入：I are happy.\\n输出：I am happy.\\n```{text[i]}```\"\"\"\\nresponse = get_completion(prompt)\\nprint(i, response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"```\\n\\n0 The girl with the black and white puppies has a ball.\\n1 Yolanda has her notebook.\\n2 It's going to be a long day. Does the car need its oil changed?\\n3 Their goes my freedom. There going to bring their suitcases.\\n4 You're going to need your notebook.\\n5 That medicine affects my ability to sleep. Have you heard of the butterfly effect?\\n6 This phrase is to check chatGPT for spelling ability.\\n\\n下面是一个使用大语言模型进行语法纠错的简单示例，类似于Grammarly（一个语法纠正和校对的工具）的功能。\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='输入一段关于熊猫玩偶的评价文字，语言模型会自动校对文本中的语法错误，输出修改后的正确版本。这里使用的Prompt比较简单直接，只要求进行语法纠正。我们也可以通过扩展Prompt，同时请求语言模型调整文本的语气、行文风格等。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python text = f\"\"\" Got this for my daughter for her birthday cuz she keeps taking \\\\ mine from my room. Yes, adults also like pandas too. She takes \\\\ it everywhere with her, and it\\'s super soft and cute. One of the \\\\ ears is a bit lower than the other, and I don\\'t think that was \\\\ designed to be asymmetrical. It\\'s a bit small for what I paid for it \\\\ though. I think there might be other options that are bigger for \\\\ the same price. It arrived a day earlier than expected, so I got \\\\ to play with'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='earlier than expected, so I got \\\\ to play with it myself before I gave it to my daughter. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python prompt = f\"校对并更正以下商品评论：{text}\" response = get_completion(prompt) print(response)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"I got this for my daughter's birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. It's also a bit smaller than I expected for the price. I think there might be other options that are bigger for the same price. On the bright side, it arrived a day earlier than expected, so I got to play\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='a day earlier than expected, so I got to play with it myself before giving it to my daughter.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='引入 Redlines 包，详细显示并对比纠错过程：\\n\\n```python\\n\\n如未安装redlines，需先安装\\n\\n!pip3.8 install redlines ```\\n\\n```python from redlines import Redlines from IPython.display import display, Markdown\\n\\ndiff = Redlines(text,response) display(Markdown(diff.output_markdown)) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"Got I got this for my daughter for her daughter's birthday cuz because she keeps taking mine from my room. room. Yes, adults also like pandas too. too. She takes it everywhere with her, and it's super soft and cute. One cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. It's also a bit small smaller than I expected for what I paid for it though. the price. I think there might be other options that are bigger for the same price.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='other options that are bigger for the same price. It price. On the bright side, it arrived a day earlier than expected, so I got to play with it myself before I gave giving it to my daughter.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='这个示例展示了如何利用语言模型强大的语言处理能力实现自动化的语法纠错。类似的方法可以运用于校对各类文本内容，大幅减轻人工校对的工作量，同时确保文本语法准确。掌握运用语言模型进行语法纠正的技巧，将使我们的写作更加高效和准确。\\n\\n五、综合样例\\n\\n语言模型具有强大的组合转换能力，可以通过一个Prompt同时实现多种转换，大幅简化工作流程。\\n\\n下面是一个示例，展示了如何使用一个Prompt，同时对一段文本进行翻译、拼写纠正、语气调整和格式转换等操作。\\n\\npython prompt = f\"\"\" 针对以下三个反引号之间的英文评论文本， 首先进行拼写及语法纠错， 然后将其转化成中文， 再将其转化成优质淘宝评论的风格，从各种角度出发，分别说明产品的优点与缺点，并进行总结。 润色一下描述，使评论更具有吸引力。 输出结果格式为： 【优点】xxx 【缺点】xxx 【总结】xxx 注意，只需填写xxx部分，并分段输出。 将结果输出成Markdown格式。{text}\"\"\" response = get_completion(prompt) display(Markdown(response))'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='【优点】 - 超级柔软可爱，女儿生日礼物非常受欢迎。 - 成人也喜欢熊猫，我也很喜欢它。 - 提前一天到货，让我有时间玩一下。\\n\\n【缺点】 - 一只耳朵比另一只低，不对称。 - 价格有点贵，但尺寸有点小，可能有更大的同价位选择。\\n\\n【总结】 这只熊猫玩具非常适合作为生日礼物，柔软可爱，深受孩子喜欢。虽然价格有点贵，但尺寸有点小，不对称的设计也有点让人失望。如果你想要更大的同价位选择，可能需要考虑其他选项。总的来说，这是一款不错的熊猫玩具，值得购买。\\n\\n通过这个例子，我们可以看到大语言模型可以流畅地处理多个转换要求，实现中文翻译、拼写纠正、语气升级和格式转换等功能。\\n\\n利用大语言模型强大的组合转换能力，我们可以避免多次调用模型来进行不同转换，极大地简化了工作流程。这种一次性实现多种转换的方法，可以广泛应用于文本处理与转换的场景中。\\n\\n六、英文版\\n\\n1.1 翻译为西班牙语'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='六、英文版\\n\\n1.1 翻译为西班牙语\\n\\npython prompt = f\"\"\" Translate the following English text to Spanish: \\\\Hi, I would like to order a blender\"\"\" response = get_completion(prompt) print(response)\\n\\nHola, me gustaría ordenar una licuadora.\\n\\n1.2 识别语种\\n\\npython prompt = f\"\"\" Tell me which language this is:Combien coûte le lampadaire?``` \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nThis language is French.\\n\\n1.3 多语种翻译'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='```\\n\\nThis language is French.\\n\\n1.3 多语种翻译\\n\\npython prompt = f\"\"\" Translate the following text to French and Spanish and English pirate: \\\\I want to order a basketball``` \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nFrench: ```Je veux commander un ballon de basket```\\nSpanish: ```Quiero ordenar una pelota de baloncesto```\\nEnglish: ```I want to order a basketball```\\n\\n1.4 同时进行语气转换'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='1.4 同时进行语气转换\\n\\n```python prompt = f\"\"\" Translate the following text to Spanish in both the \\\\ formal and informal forms: \\'Would you like to order a pillow?\\' \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nFormal: ¿Le gustaría ordenar una almohada?\\nInformal: ¿Te gustaría ordenar una almohada?\\n\\n1.5 通用翻译器'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='1.5 通用翻译器\\n\\npython user_messages = [ \"La performance du système est plus lente que d\\'habitude.\", # System performance is slower than normal \"Mi monitor tiene píxeles que no se iluminan.\", # My monitor has pixels that are not lighting \"Il mio mouse non funziona\", # My mouse is not working \"Mój klawisz Ctrl jest zepsuty\", # My keyboard has a broken control key \"我的屏幕在闪烁\" # My screen is flashing ]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python for issue in user_messages: prompt = f\"Tell me what language this is:{issue}```\" lang = get_completion(prompt) print(f\"Original message ({lang}): {issue}\")\\n\\nprompt = f\"\"\"\\nTranslate the following  text to English \\\\\\nand Korean: ```{issue}```\\n\"\"\"\\nresponse = get_completion(prompt)\\nprint(response, \"\\\\n\")\\n\\n```\\n\\nOriginal message (The language is French.): La performance du système est plus lente que d\\'habitude.\\nThe performance of the system is slower than usual.\\n\\n시스템의 성능이 평소보다 느립니다.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='시스템의 성능이 평소보다 느립니다.\\n\\nOriginal message (The language is Spanish.): Mi monitor tiene píxeles que no se iluminan.\\nEnglish: \"My monitor has pixels that do not light up.\"\\n\\nKorean: \"내 모니터에는 밝아지지 않는 픽셀이 있습니다.\"\\n\\nOriginal message (The language is Italian.): Il mio mouse non funziona\\nEnglish: \"My mouse is not working.\"\\nKorean: \"내 마우스가 작동하지 않습니다.\"\\n\\nOriginal message (The language is Polish.): Mój klawisz Ctrl jest zepsuty\\nEnglish: \"My Ctrl key is broken\"\\nKorean: \"내 Ctrl 키가 고장 났어요\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='Original message (The language is Chinese.): 我的屏幕在闪烁\\nEnglish: My screen is flickering.\\nKorean: 내 화면이 깜박거립니다.\\n\\n2.1 语气风格调整\\n\\n```python prompt = f\"\"\" Translate the following from slang to a business letter: \\'Dude, This is Joe, check out this spec on this standing lamp.\\' \"\"\" response = get_completion(prompt) print(response)\\n\\n```\\n\\nDear Sir/Madam,\\n\\nI hope this letter finds you well. My name is Joe, and I am writing to bring your attention to a specification document regarding a standing lamp.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='I kindly request that you take a moment to review the attached document, as it provides detailed information about the features and specifications of the aforementioned standing lamp.\\n\\nThank you for your time and consideration. I look forward to discussing this further with you.\\n\\nYours sincerely,\\nJoe\\n\\n3.1 文件格式转换'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='Yours sincerely,\\nJoe\\n\\n3.1 文件格式转换\\n\\npython data_json = { \"resturant employees\" :[ {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"}, {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"}, {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"} ]}\\n\\n```python prompt = f\"\"\" Translate the following python dictionary from JSON to an HTML \\\\ table with column headers and title: {data_json} \"\"\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='```\\n\\n<!DOCTYPE html>\\n<html>\\n<head>\\n<style>\\ntable {\\n  font-family: arial, sans-serif;\\n  border-collapse: collapse;\\n  width: 100%;\\n}\\n\\ntd, th {\\n  border: 1px solid #dddddd;\\n  text-align: left;\\n  padding: 8px;\\n}\\n\\ntr:nth-child(even) {\\n  background-color: #dddddd;\\n}\\n</style>\\n</head>\\n<body>\\n\\n<h2>Restaurant Employees</h2>'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='<h2>Restaurant Employees</h2>\\n\\n<table>\\n  <tr>\\n    <th>Name</th>\\n    <th>Email</th>\\n  </tr>\\n  <tr>\\n    <td>Shyam</td>\\n    <td>shyamjaiswal@gmail.com</td>\\n  </tr>\\n  <tr>\\n    <td>Bob</td>\\n    <td>bob32@gmail.com</td>\\n  </tr>\\n  <tr>\\n    <td>Jai</td>\\n    <td>jai87@gmail.com</td>\\n  </tr>\\n</table>\\n\\n</body>\\n</html>\\n\\npython from IPython.display import display, Markdown, Latex, HTML, JSON display(HTML(response))\\n\\nRestaurant Employees'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='Restaurant Employees\\n\\nName Email Shyam shyamjaiswal@gmail.com Bob bob32@gmail.com Jai jai87@gmail.com\\n\\n4.1 拼写及语法纠错'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python text = [ \"The girl with the black and white puppies have a ball.\", # The girl has a ball. \"Yolanda has her notebook.\", # ok \"Its going to be a long day. Does the car need it’s oil changed?\", # Homonyms \"Their goes my freedom. There going to bring they’re suitcases.\", # Homonyms \"Your going to need you’re notebook.\", # Homonyms \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms \"This phrase is to cherck chatGPT for spelling abilitty\" #'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='is to cherck chatGPT for spelling abilitty\" # spelling ]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python for t in text: prompt = f\"\"\"Proofread and correct the following text and rewrite the corrected version. If you don\\'t find and errors, just say \"No errors found\". Don\\'t use any punctuation around the text:{t}```\"\"\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"```\\n\\nThe girl with the black and white puppies has a ball.\\nNo errors found.\\nIt's going to be a long day. Does the car need its oil changed?\\nThere goes my freedom. They're going to bring their suitcases.\\nYou're going to need your notebook.\\nThat medicine affects my ability to sleep. Have you heard of the butterfly effect?\\nThis phrase is to check chatGPT for spelling ability.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python text = f\"\"\" Got this for my daughter for her birthday cuz she keeps taking \\\\ mine from my room. Yes, adults also like pandas too. She takes \\\\ it everywhere with her, and it\\'s super soft and cute. One of the \\\\ ears is a bit lower than the other, and I don\\'t think that was \\\\ designed to be asymmetrical. It\\'s a bit small for what I paid for it \\\\ though. I think there might be other options that are bigger for \\\\ the same price. It arrived a day earlier than expected, so I got \\\\ to play with'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='earlier than expected, so I got \\\\ to play with it myself before I gave it to my daughter. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python prompt = f\"proofread and correct this review:{text}```\" response = get_completion(prompt) print(response)\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"Got this for my daughter for her birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. Additionally, it's a bit small for what I paid for it. I believe there might be other options that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I got to\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='```python from redlines import Redlines from IPython.display import display, Markdown\\n\\ndiff = Redlines(text,response) display(Markdown(diff.output_markdown)) ```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"Got this for my daughter for her birthday cuz because she keeps taking mine from my room. room. Yes, adults also like pandas too. too. She takes it everywhere with her, and it's super soft and cute. One cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. It's Additionally, it's a bit small for what I paid for it though. it. I think believe there might be other options that are bigger for the same price. It price. On the positive\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='for the same price. It price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter. daughter.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='5.1 综合样例'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python text = f\"\"\" Got this for my daughter for her birthday cuz she keeps taking \\\\ mine from my room. Yes, adults also like pandas too. She takes \\\\ it everywhere with her, and it\\'s super soft and cute. One of the \\\\ ears is a bit lower than the other, and I don\\'t think that was \\\\ designed to be asymmetrical. It\\'s a bit small for what I paid for it \\\\ though. I think there might be other options that are bigger for \\\\ the same price. It arrived a day earlier than expected, so I got \\\\ to play with'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='earlier than expected, so I got \\\\ to play with it myself before I gave it to my daughter. \"\"\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='python prompt = f\"\"\" proofread and correct this review. Make it more compelling. Ensure it follows APA style guide and targets an advanced reader. Output in markdown format. Text:{text}``` \"\"\"\\n\\n校对注：APA style guide是APA Style Guide是一套用于心理学和相关领域的研究论文写作和格式化的规则。\\n\\n它包括了文本的缩略版，旨在快速阅读，包括引用、解释和参考列表，\\n\\n其详细内容可参考：https://apastyle.apa.org/about-apa-style\\n\\n下一单元格内的汉化prompt内容由译者进行了本地化处理，仅供参考\\n\\nresponse = get_completion(prompt) display(Markdown(response))\\n\\n```'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='```\\n\\nTitle: A Delightful Gift for Panda Enthusiasts: A Review of the Soft and Adorable Panda Plush Toy\\n\\nReviewer: [Your Name]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='Reviewer: [Your Name]\\n\\nI recently purchased this charming panda plush toy as a birthday gift for my daughter, who has a penchant for \"borrowing\" my belongings from time to time. As an adult, I must admit that I too have fallen under the spell of these lovable creatures. This review aims to provide an in-depth analysis of the product, catering to advanced readers who appreciate a comprehensive evaluation.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='First and foremost, the softness and cuteness of this panda plush toy are simply unparalleled. Its irresistibly plush exterior makes it a joy to touch and hold, ensuring a delightful sensory experience for both children and adults alike. The attention to detail is evident, with its endearing features capturing the essence of a real panda. However, it is worth noting that one of the ears appears to be slightly asymmetrical, which may not have been an intentional design choice.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"While the overall quality of the product is commendable, I must express my slight disappointment regarding its size in relation to its price. Considering the investment made, I expected a larger plush toy. It is worth exploring alternative options that offer a more substantial size for the same price point. Nevertheless, this minor setback does not overshadow the toy's undeniable appeal and charm.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content=\"In terms of delivery, I was pleasantly surprised to receive the panda plush toy a day earlier than anticipated. This unexpected early arrival allowed me to indulge in some personal playtime with the toy before presenting it to my daughter. Such promptness in delivery is a testament to the seller's efficiency and commitment to customer satisfaction.\"),\n",
       " Document(metadata={'source': './data_base/knowledge_db/prompt_engineering/6. 文本转换 Transforming.md'}, page_content='In conclusion, this panda plush toy is a delightful gift for both children and adults who appreciate the enchanting allure of these beloved creatures. Its softness, cuteness, and attention to detail make it a truly captivating addition to any collection. While the size may not fully justify the price, the overall quality and prompt delivery make it a worthwhile purchase. I highly recommend this panda plush toy to anyone seeking a charming and endearing companion.\\n\\nWord Count: 305 words'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 0, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01本\\x03:1.9.9\\n发布日期:2023.03\\n南  ⽠  书\\nPUMPKIN\\nB  O  O  K\\nDatawhale'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='前言\\n“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\\n者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\\n导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\\n具体的推导细节。”\\n读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\\n老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\\n中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以...... 本南瓜书只能算是我\\n等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\\n下学生”。\\n使用说明\\n• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\\n为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；\\n• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得\\n有点飘的时候再回来啃都来得及；\\n• 每个公式的解析和推导我们都力(zhi) 争(neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；\\n• 若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们GitHub 的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在24 小时以内给您回复，超过24 小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1 版）'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='最新版PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/releases\\n编委会\\n主编：Sm1les、archwalker、jbb0523\\n编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\\n封面设计：构思-Sm1les、创作-林王茂盛\\n致谢\\n特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、\\nspareribs、sunchaothu、StevenLzq 在最早期的时候对南瓜书所做的贡献。\\n扫描下方二维码，然后回复关键词“南瓜书”，即可加入“南瓜书读者交流群”\\n版权声明\\n本作品采用知识共享署名-非商业性使用-相同方式共享4.0 国际许可协议进行许可。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n目录\\n第1 章绪论\\n1\\n1.1\\n引言. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.2\\n基本术语\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.3\\n假设空间\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4\\n归纳偏好\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4.1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='3\\n1.4.1\\n式(1.1) 和式(1.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n第2 章模型评估与选择\\n5\\n2.1\\n经验误差与过拟合\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\n评估方法\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2.1\\n算法参数（超参数）与模型参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.2\\n验证集. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3\\n性能度量'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='6\\n2.3\\n性能度量\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.1\\n式(2.2) 到式(2.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.2\\n式(2.8) 和式(2.9) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.3\\n图2.3 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.4\\n式(2.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.5\\n式(2.11) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='7\\n2.3.5\\n式(2.11) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.6\\n式(2.12) 到式(2.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.7\\n式(2.18) 和式(2.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n2.3.8\\n式(2.20) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n2.3.9\\n式(2.21) 和式(2.22) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2.3.10 式(2.23) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='9\\n2.3.10 式(2.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2.3.11 式(2.24) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n2.3.12 式(2.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n2.4\\n比较检验\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.4.1\\n式(2.26) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.4.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='13\\n2.4.2\\n式(2.27) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.5\\n偏差与方差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n2.5.1\\n式(2.37) 到式(2.42) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n第3 章线性模型\\n18\\n3.1\\n基本形式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2\\n线性回归'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='18\\n3.2\\n线性回归\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.1\\n属性数值化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.2\\n式(3.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.3\\n式(3.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.4\\n式(3.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.5'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='19\\n3.2.5\\n式(3.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.6\\n式(3.9) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.2.7\\n式(3.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.2.8\\n式(3.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n3.3\\n对数几率回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='23\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.3.1\\n式(3.27) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n3.3.2\\n梯度下降法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n24\\n3.3.3\\n牛顿法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n25\\n3.3.4\\n式(3.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3.5\\n式(3.30) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='26\\n3.3.5\\n式(3.30) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3.6\\n式(3.31) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.4\\n线性判别分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.4.1\\n式(3.32) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4.2\\n式(3.37) 到式(3.39) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4.3\\n式(3.43) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='28\\n3.4.3\\n式(3.43) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.4.4\\n式(3.44) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.4.5\\n式(3.45) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n3.5\\n多分类学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.5.1\\n图3.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.6'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='31\\n3.6\\n类别不平衡问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n第4 章决策树\\n32\\n4.1\\n基本流程\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2\\n划分选择\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2.1\\n式(4.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='32\\n4.2.2\\n式(4.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.3\\n式(4.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.4\\n式(4.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.5\\n式(4.6) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n4.3\\n剪枝处理\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='38\\n4.4\\n连续与缺失值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4.1\\n式(4.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4.2\\n式(4.8) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.4.3\\n式(4.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5\\n多变量决策树. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5.1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='39\\n4.5.1\\n图(4.10) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5.2\\n图(4.11) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n第5 章神经网络\\n41\\n5.1\\n神经元模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2\\n感知机与多层网络\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2.1\\n式(5.1) 和式(5.2) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='41\\n5.2.2\\n图5.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n5.3\\n误差逆传播算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.1\\n式(5.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.2\\n式(5.12) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.3\\n式(5.13) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.4\\n式(5.14) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='43\\n5.3.4\\n式(5.14) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n5.3.5\\n式(5.15) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.4\\n全局最小与局部极小. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.5\\n其他常见神经网络\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.1\\n式(5.18) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.2\\n式(5.20) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.3\\n式(5.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.4\\n式(5.23) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='45\\n5.5.4\\n式(5.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.6\\n深度学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.1\\n什么是深度学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.2\\n深度学习的起源. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.3\\n怎么理解特征学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n第6 章支持向量机\\n47\\n6.1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='46\\n第6 章支持向量机\\n47\\n6.1\\n间隔与支持向量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.1\\n图6.1 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.2\\n式(6.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.3\\n式(6.2) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.4\\n式(6.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.5'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='47\\n6.1.5\\n式(6.4) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.1.6\\n式(6.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.2\\n对偶问题\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.1\\n凸优化问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.2\\nKKT 条件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.3'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='49\\n6.2.3\\n拉格朗日对偶函数\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.4\\n拉格朗日对偶问题\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n50\\n6.2.5\\n式(6.9) 和式(6.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n6.2.6\\n式(6.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n6.2.7\\n式(6.13) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.3'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='53\\n6.3\\n核函数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.3.1\\n式(6.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4\\n软间隔与正则化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.1\\n式(6.35) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.2\\n式(6.37) 和式(6.38) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.3'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='53\\n6.4.3\\n式(6.39) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.4\\n式(6.40) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n6.4.5\\n对数几率回归与支持向量机的关系\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n6.4.6\\n式(6.41) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5\\n支持向量回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.1\\n式(6.43) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='55\\n6.5.1\\n式(6.43) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.2\\n式(6.45) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.3\\n式(6.52) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6\\n核方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6.1\\n式(6.57) 和式(6.58) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='56\\n6.6.2\\n式(6.65) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n6.6.3\\n式(6.66) 和式(6.67) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n6.6.4\\n式(6.70) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n58\\n6.6.5\\n核对数几率回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n60\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第7 章贝叶斯分类器\\n62\\n7.1\\n贝叶斯决策论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.1\\n式(7.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.2\\n式(7.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.3\\n判别式模型与生成式模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='62\\n7.2\\n极大似然估计. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.2.1\\n式(7.12) 和(7.13) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.3\\n朴素贝叶斯分类器\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.1\\n式(7.16) 和式(7.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.2\\n式(7.18) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.3\\n贝叶斯估计'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='64\\n7.3.3\\n贝叶斯估计\\n[1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.4\\nCategorical 分布. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.5\\nDirichlet 分布. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.6\\n式(7.19) 和式(7.20) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.4\\n半朴素贝叶斯分类器. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.4.1\\n式(7.21) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='67\\n7.4.1\\n式(7.21) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.4.2\\n式(7.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4.3\\n式(7.23) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4.4\\n式(7.24) 和式(7.25) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.5\\n贝叶斯网\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.5.1\\n式(7.27) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='69\\n7.5.1\\n式(7.27) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6\\nEM 算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6.1\\nJensen 不等式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6.2\\nEM 算法的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n第8 章集成学习\\n75\\n8.1\\n个体与集成. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='76\\n8.1.1\\n式(8.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.1.2\\n式(8.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.1.3\\n式(8.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n76\\n8.2\\nBoosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2.1\\n式(8.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='77\\n8.2.2\\n式(8.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2.3\\n式(8.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.4\\n式(8.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.5\\n式(8.8) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.6\\n式(8.9) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.7\\n式(8.10) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='79\\n8.2.7\\n式(8.10) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.8\\n式(8.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.9\\n式(8.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.10 式(8.13) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.11 式(8.14) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n8.2.12 式(8.16) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='81\\n8.2.12 式(8.16) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n8.2.13 式(8.17) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.14 式(8.18) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.15 式(8.19) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.16 AdaBoost 的个人推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.17 进一步理解权重更新公式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n85\\n8.2.18 能够接受带权样本的基学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n86\\n8.3\\nBagging 与随机森林\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3.1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='87\\n8.3.1\\n式(8.20) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3.2\\n式(8.21) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3.3\\n随机森林的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4\\n结合策略\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.1\\n式(8.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.2\\n式(8.23) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='87\\n8.4.2\\n式(8.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.3\\n硬投票和软投票的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.4\\n式(8.24) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.4.5\\n式(8.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.6\\n式(8.26) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.7'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='88\\n8.4.7\\n元学习器(meta-learner) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.8\\nStacking 算法的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.5\\n多样性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.5.1\\n式(8.27) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.5.2\\n式(8.28) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.3\\n式(8.29) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='89\\n8.5.3\\n式(8.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.4\\n式(8.30) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.5\\n式(8.31) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.6\\n式(8.32) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5.7\\n式(8.33) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.8\\n式(8.34) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='90\\n8.5.8\\n式(8.34) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.9\\n式(8.35) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.10 式(8.36) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.11 式(8.40) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.12 式(8.41) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.13 式(8.42) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='90\\n8.5.13 式(8.42) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.14 多样性增强的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.6\\nGradient Boosting/GBDT/XGBoost 联系与区别. . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.6.1\\n梯度下降法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.6.2\\n从梯度下降的角度解释AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n8.6.3'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='93\\n8.6.3\\n梯度提升(Gradient Boosting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n94\\n8.6.4\\n梯度提升树(GBDT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n95\\n8.6.5\\nXGBoost\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n96\\n第9 章聚类\\n97\\n9.1\\n聚类任务\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n9.2\\n性能度量\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='97\\n9.2.1\\n式(9.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n9.2.2\\n式(9.6) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.2.3\\n式(9.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n9.2.4\\n式(9.8) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.2.5\\n式(9.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.3\\n距离计算\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.3.1\\n式(9.21) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4\\n原型聚类'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='99\\n9.4\\n原型聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4.1\\n式(9.28) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4.2\\n式(9.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.4.3\\n式(9.30) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.4\\n式(9.31) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.5'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='100\\n9.4.5\\n式(9.32) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.6\\n式(9.33) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.7\\n式(9.34) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.8\\n式(9.35) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n102\\n9.4.9\\n式(9.36) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n103'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='103\\n9.4.10 式(9.37) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n103\\n9.4.11 式(9.38) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n104\\n9.4.12 图9.6 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n104\\n9.5\\n密度聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.5.1\\n密度直达、密度可达与密度相连. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.5.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='105\\n9.5.2\\n图9.9 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.6\\n层次聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n106\\n第10 章降维与度量学习\\n107\\n10.1 预备知识\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.1.1 符号约定\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='107\\n10.1.2 矩阵与单位阵、向量的乘法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.2 矩阵的F 范数与迹. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n10.3 k 近邻学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n109\\n10.3.1 式(10.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n109\\n10.3.2 式(10.2) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n109\\n10.4 低维嵌入'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='109\\n10.4 低维嵌入\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.1 图10.2 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.2 式(10.3) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.3 式(10.4) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4.4 式(10.5) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='111\\n10.4.5 式(10.6) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.6 式(10.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.4.7 式(10.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.4.8 图10.3 关于MDS 算法的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.5 主成分分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='113\\n10.5.1 式(10.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113\\n10.5.2 式(10.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n115\\n10.5.3 式(10.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n117\\n10.5.4 根据式(10.17) 求解式(10.16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n118\\n10.6 核化线性降维. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n118\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='118\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.6.1 式(10.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.2 式(10.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.3 式(10.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.4 式(10.22) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='119\\n10.6.5 式(10.24) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6.6 式(10.25) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7 流形学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7.1 等度量映射(Isomap) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.7.2 式(10.28) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='120\\n10.7.3 式(10.31) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n122\\n10.8 度量学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n123\\n10.8.1 式(10.34) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n10.8.2 式(10.35) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n10.8.3 式(10.36) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='125\\n10.8.4 式(10.37) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.5 式(10.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.6 式(10.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n第11 章特征选择与稀疏学习\\n126\\n11.1 子集搜索与评价. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.1.1 式(11.1) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='126\\n11.1.1 式(11.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.1.2 式(11.2) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.2 过滤式选择. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.2.1 包裹式选择. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n11.3 嵌入式选择与L1 正则化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='127\\n11.3.1 式(11.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.2 式(11.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.3 式(11.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.4 式(11.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3.5 式(11.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='127\\n11.3.6 式(11.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.7 式(11.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.8 式(11.12) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.9 式(11.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129\\n11.3.10 式(11.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='129\\n11.4 稀疏表示与字典学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n11.4.1 式(11.15) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n11.4.2 式(11.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131\\n11.4.3 式(11.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131\\n11.4.4 式(11.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='131\\n11.5 K-SVD 算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n11.6 压缩感知\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n11.6.1 式(11.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n11.6.2 式(11.25) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第12 章计算学习理论\\n136\\n12.1 基础知识\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.1 式(12.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.2 式(12.2) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.3 式(12.3) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.4 式(12.4) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='136\\n12.1.4 式(12.4) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.5 式(12.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.6 式(12.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n137\\n12.2 PAC 学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n137\\n12.2.1 式(12.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='138\\n12.3 有限假设空间. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.1 式(12.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.2 式(12.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.3 式(12.12) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.4 式(12.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='139\\n12.3.5 式(12.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.6 引理12.1 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.7 式(12.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.3.8 式(12.19) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.3.9 式(12.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='140\\n12.4 VC 维. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.1 式(12.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.2 式(12.22) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.3 式(12.23) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.4 引理12.2 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n142'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='142\\n12.4.5 式(12.28) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n143\\n12.4.6 式(12.29) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n143\\n12.4.7 式(12.30) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n144\\n12.4.8 定理12.4 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n144\\n12.5 Rademacher 复杂度\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='145\\n12.5.1 式(12.36) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.2 式(12.37) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.3 式(12.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.4 式(12.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.5 式(12.40) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='146\\n12.5.6 式(12.41) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.5.7 定理12.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.6 定理12.6 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n147\\n12.6.1 式(12.52) 的证明. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.6.2 式(12.53) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='148\\n12.7 稳定性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.7.1 泛化/经验/留一损失的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.2 式(12.57) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.3 定理12.8 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.7.4 式(12.60) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.5 经验损失最小化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.6 定理(12.9) 的证明的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n150\\n第13 章半监督学习\\n151\\n13.1 未标记样本. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='151\\n13.2 生成式方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.1 式(13.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.2 式(13.2) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.3 式(13.3) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.4 式(13.4) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='152\\n13.2.5 式(13.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.6 式(13.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.7 式(13.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n153\\n13.2.8 式(13.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n154\\n13.3 半监督SVM\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='156\\n13.3.1 图13.3 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.2 式(13.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.3 图13.4 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.4 式(13.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n13.4 图半监督学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='157\\n13.4.1 式(13.12) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n13.4.2 式(13.13) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158\\n13.4.3 式(13.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.4 式(13.15) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.5 式(13.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='159\\n13.4.6 式(13.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.7 式(13.18) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.8 式(13.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.4.9 公式(13.21) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.5 基于分歧的方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='163\\n13.5.1 图13.6 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.6 半监督聚类. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.6.1 图13.7 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n13.6.2 图13.9 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n第14 章概率图模型\\n165'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='163\\n第14 章概率图模型\\n165\\n14.1 隐马尔可夫模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n165\\n14.1.1 生成式模型和判别式模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n165\\n14.1.2 式(14.1) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n165\\n14.1.3 隐马尔可夫模型的三组参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2 马尔可夫随机场. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='166\\n14.2.1 式(14.2) 和式(14.3) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2.2 式(14.4) 到式(14.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.2.3 马尔可夫毯(Markov blanket) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.2.4 势函数(potential function) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2.5 式(14.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2.6 式(14.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.3 条件随机场. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='167\\n14.3.1 式(14.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.3.2 式(14.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.3 学习与推断. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.4 式(14.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.5 式(14.15) 和式(14.16) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='168\\n14.3.6 式(14.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.7 式(14.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.8 式(14.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.9 式(14.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.10 式(14.22) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='169\\n14.3.11 图14.8 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4 近似推断\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4.1 式(14.21) 到式(14.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4.2 式(14.26) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.4.3 式(14.27) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='171\\n14.4.4 式(14.28) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4.5 吉布斯采样与MH 算法\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.6 式(14.29) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.7 式(14.30) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.8 式(14.31) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='173\\n14.4.9 式(14.32) 到式(14.34) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.10 式(14.35) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.11 式(14.36) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n174\\n14.4.12 式(14.37) 到式(14.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n175\\n14.4.13 式(14.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n175'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='175\\n14.4.14 式(14.40) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5 话题模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5.1 式(14.41) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5.2 式(14.42) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5.3 式(14.43) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='177\\n14.5.4 式(14.44) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n第15 章规则学习\\n178\\n15.1 剪枝优化\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.1.1 式(15.2) 和式(15.3) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2 归纳逻辑程序设计\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.1 式(15.6) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='178\\n15.2.1 式(15.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.2 式(15.7) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.3 式(15.9) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.4 式(15.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.5 式(15.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='179\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='15.2.6 式(15.12) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n15.2.7 式(15.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n15.2.8 式(15.16) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n第16 章强化学习\\n180\\n16.1 任务与奖赏. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='180\\n16.2 K-摇臂赌博机. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2.1 式(16.2) 和式(16.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2.2 式(16.4) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3 有模型学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3.1 式(16.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='180\\n16.3.2 式(16.8) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.3 式(16.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.4 式(16.14) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.5 式(16.15) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.6 式(16.16) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='181\\n16.4 免模型学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.1 式(16.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.2 式(16.23) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.3 式(16.31) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5 值函数近似. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='182\\n16.5.1 式(16.33) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5.2 式(16.34) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.6 模仿学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n183'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第1 章\\n绪论\\n本章作为“西瓜书”的开篇，主要讲解什么是机器学习以及机器学习的相关数学符号，为后续内容作\\n铺垫，并未涉及复杂的算法理论，因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可。此外，在\\n阅读本章前建议先阅读西瓜书目录前页的《主要符号表》，它能解答在阅读“西瓜书”过程中产生的大部\\n分对数学符号的疑惑。\\n本章也作为本书的开篇，笔者在此赘述一下本书的撰写初衷，本书旨在以“过来人”的视角陪读者一\\n起阅读“西瓜书”，尽力帮读者消除阅读过程中的“数学恐惧”，只要读者学习过《高等数学》、《线性代\\n数》和《概率论与数理统计》这三门大学必修的数学课，均能看懂本书对西瓜书中的公式所做的解释和推\\n导，同时也能体会到这三门数学课在机器学习上碰撞产生的“数学之美”。\\n1.1\\n引言\\n本节以概念理解为主，在此对“算法”和“模型”作补充说明。“算法”是指从数据中学得“模型”的具\\n体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，\\n通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如f(x) = wx + b\\n的一元一次函数。不过由于严格区分这两者的意义不大，因此多数文献和资料会将其混用，当遇到这两个\\n概念时，其具体指代根据上下文判断即可。\\n1.2\\n基本术语\\n本节涉及的术语较多且很多术语都有多个称呼，下面梳理各个术语，并将最常用的称呼加粗标注。\\n样本：也称为“示例”，是关于一个事件或对象的描述。因为要想让计算机能对现实生活中的事物\\n进行机器学习，必须先将其抽象为计算机能理解的形式，计算机最擅长做的就是进行数学运算，因此考\\n虑如何将其抽象为某种数学形式。显然，线性代数中的向量就很适合，因为任何事物都可以由若干“特\\n征”（或称为“属性”）唯一刻画出来，而向量的各个维度即可用来描述各个特征。例如，如果用色泽、根\\n蒂和敲声这3 个特征来刻画西瓜，那么一个“色泽青绿，根蒂蜷缩，敲声清脆”的西瓜用向量来表示即为x ='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(青绿; 蜷缩; 清脆) （向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量），\\n其中青绿、蜷缩和清脆分别对应为相应特征的取值，也称为“属性值”。显然，用中文书写向量的方式不够\\n“数学”，因此需要将属性值进一步数值化，具体例子参见“西瓜书”第3 章3.2。此外，仅靠以上3 个特\\n征来刻画西瓜显然不够全面细致，因此还需要扩展更多维度的特征，一般称此类与特征处理相关的工作为\\n“特征工程”。\\n样本空间：也称为“输入空间”或“属性空间”。由于样本采用的是标明各个特征取值的“特征向量”\\n来进行表示，根据线性代数的知识可知，有向量便会有向量所在的空间，因此称表示样本的特征向量所在\\n的空间为样本空间，通常用花式大写的X 表示。\\n数据集：数据集通常用集合来表示，令集合D = {x1, x2, ..., xm} 表示包含m 个样本的数据集，一般\\n同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有d 个特征，则第i\\n个样本的数学表示为d 维向量：xi = (xi1; xi2; ...; xid)，其中xij 表示样本xi 在第j 个属性上的取值。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='模型：机器学习的一般流程如下：首先收集若干样本（假设此时有100 个），然后将其分为训练样本\\n（80 个）和测试样本（20 个），其中80 个训练样本构成的集合称为“训练集”，20 个测试样本构成的集合\\n称为“测试集”，接着选用某个机器学习算法，让其在训练集上进行“学习”（或称为“训练”），然后产出\\n得到“模型”（或称为“学习器”），最后用测试集来测试模型的效果。执行以上流程时，表示我们已经默认\\n样本的背后是存在某种潜在的规律，我们称这种潜在的规律为“真相”或者“真实”，例如样本是一堆好西\\n瓜和坏西瓜时，我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开。当我们应用某个机\\n器学习算法来学习时，产出得到的模型便是该算法所找到的它自己认为的规律，由于该规律通常并不一定\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n就是所谓的真相，所以也将其称为“假设”。通常机器学习算法都有可配置的参数，同一个机器学习算法，\\n使用不同的参数配置或者不同的训练集，训练得到的模型通常都不同。\\n标记：上文提到机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，我们称该方\\n面的信息为“标记”。例如在学习西瓜的好坏时，“好瓜”和“坏瓜”便是样本的标记。一般第i 个样本的\\n标记的数学表示为yi，标记所在的空间称为“标记空间”或“输出空间”，数学表示为花式大写的Y。标\\n记通常也看作为样本的一部分，因此，一个完整的样本通常表示为(x, y)。\\n根据标记的取值类型不同，可将机器学习任务分为以下两类：\\n• 当标记取值为离散型时，称此类任务为“分类”，例如学习西瓜是好瓜还是坏瓜、学习猫的图片是白\\n猫还是黑猫等。当分类的类别只有两个时，称此类任务为“二分类”，通常称其中一个为“正类”，另\\n一个为“反类”或“负类”；当分类的类别超过两个时，称此类任务为“多分类”。由于标记也属于样'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='一个为“反类”或“负类”；当分类的类别超过两个时，称此类任务为“多分类”。由于标记也属于样\\n本的一部分，通常也需要参与运算，因此也需要将其数值化，例如对于二分类任务，通常将正类记为\\n1，反类记为0，即Y = {0, 1}。这只是一般默认的做法，具体标记该如何数值化可根据具体机器学\\n习算法进行相应地调整，例如第6 章的支持向量机算法则采用的是Y = {−1, +1}；\\n• 当标记取值为连续型时，称此类任务为“回归”，例如学习预测西瓜的成熟度、学习预测未来的房价\\n等。由于是连续型，因此标记的所有可能取值无法直接罗列，通常只有取值范围，回归任务的标记取\\n值范围通常是整个实数域R，即Y = R。\\n无论是分类还是回归，机器学习算法最终学得的模型都可以抽象地看作为以样本x 为自变量，标记y\\n为因变量的函数y = f(x)，即一个从输入空间X 到输出空间Y 的映射。例如在学习西瓜的好坏时，机器\\n学习算法学得的模型可看作为一个函数f(x)，给定任意一个西瓜样本xi = (青绿; 蜷缩; 清脆)，将其输入'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='进函数即可计算得到一个输出yi = f(xi)，此时得到的yi 便是模型给出的预测结果，当yi 取值为1 时表\\n明模型认为西瓜xi 是好瓜，当yi 取值为0 时表明模型认为西瓜xi 是坏瓜。\\n根据是否有用到标记信息，可将机器学习任务分为以下两类：\\n• 在模型训练阶段有用到标记信息时，称此类任务为“监督学习”，例如第3 章的线性模型；\\n• 在模型训练阶段没用到标记信息时，称此类任务为“无监督学习”，例如第9 章的聚类。\\n泛化：由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确\\n与否才是衡量一个模型好坏的关键，我们称此为“泛化”能力。例如学习西瓜好坏时，假设训练集中共有3\\n个样本：{(x1 = (青绿; 蜷缩), y1 = 好瓜), (x2 = (乌黑; 蜷缩), y2 = 好瓜), (x3 = (浅白; 蜷缩), y3 = 好瓜)}，\\n同时假设判断西瓜好坏的真相是“只要根蒂蜷缩就是好瓜”，如果应用算法A 在此训练集上训练得到模型\\nfa(x)，模型a 学到的规律是“色泽等于青绿、乌黑或者浅白时，同时根蒂蜷缩即为好瓜，否则便是坏瓜”，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='再应用算法B 在此训练集上训练得到模型fb(x)，模型fb(x) 学到的规律是“只要根蒂蜷缩就是好瓜”，因\\n此对于一个未见过的西瓜样本x = (金黄; 蜷缩) 来说，模型fa(x) 给出的预测结果为“坏瓜”，模型fb(x)\\n给出的预测结果为“好瓜”，此时我们称模型fb(x) 的泛化能力优于模型fa(x)。\\n通过以上举例可知，尽管模型fa(x) 和模型fb(x) 对训练集学得一样好，即两个模型对训练集中每个\\n样本的判断都对，但是其所学到的规律是不同的。导致此现象最直接的原因是算法的不同，但是算法通常\\n是有限的，可穷举的，尤其是在特定任务场景下可使用的算法更是有限，因此，数据便是导致此现象的另\\n一重要原因，这也就是机器学习领域常说的“数据决定模型的上限，而算法则是让模型无限逼近上限”, 下\\n面详细解释此话的含义。\\n先解释“数据决定模型效果的上限”，其中数据是指从数据量和特征工程两个角度考虑。从数据量的\\n角度来说，通常数据量越大模型效果越好，因为数据量大即表示累计的经验多，因此模型学习到的经验也\\n多，自然表现效果越好。例如以上举例中如果训练集中含有相同颜色但根蒂不蜷缩的坏瓜，模型a 学到真'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='多，自然表现效果越好。例如以上举例中如果训练集中含有相同颜色但根蒂不蜷缩的坏瓜，模型a 学到真\\n相的概率则也会增大；从特征工程的角度来说，通常对特征数值化越合理，特征收集越全越细致，模型效\\n果通常越好，因为此时模型更易学得样本之间潜在的规律。例如学习区分亚洲人和非洲人时，此时样本即\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n为人，在进行特征工程时，如果收集到每个样本的肤色特征，则其他特征例如年龄、身高和体重等便可省\\n略，因为只需靠肤色这一个特征就足以区分亚洲人和非洲人。\\n而“算法则是让模型无限逼近上限”是指当数据相关的工作已准备充分时，接下来便可用各种可适用\\n的算法从数据中学习其潜在的规律进而得到模型，不同的算法学习得到的模型效果自然有高低之分，效果\\n越好则越逼近上限，即逼近真相。\\n分布：此处的“分布”指的是概率论中的概率分布，通常假设样本空间服从一个未知“分布”D，而\\n我们收集到的每个样本都是独立地从该分布中采样得到，即“独立同分布”。通常收集到的样本越多，越\\n能从样本中反推出D 的信息，即越接近真相。此假设属于机器学习中的经典假设，在后续学习机器学习\\n算法过程中会经常用到。\\n1.3\\n假设空间\\n本节的重点是理解“假设空间”和“版本空间”，下面以“房价预测”举例说明。假设现已收集到某地\\n区近几年的房价和学校数量数据，希望利用收集到的数据训练出能通过学校数量预测房价的模型，具体收\\n集到的数据如表1-1所示。\\n表1-1 房价预测\\n年份'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='集到的数据如表1-1所示。\\n表1-1 房价预测\\n年份\\n学校数量\\n房价\\n2020\\n1 所\\n1 万/m2\\n2021\\n2 所\\n4 万/m2\\n基于对以上数据的观察以及日常生活经验，不难得出“房价与学校数量成正比”的假设，若将学校数\\n量设为x，房价设为y，则该假设等价表示学校数量和房价呈y = wx + b 的一元一次函数关系，此时房价\\n预测问题的假设空间即为“一元一次函数”。确定假设空间以后便可以采用机器学习算法从假设空间中学\\n得模型，即从一元一次函数空间中学得能满足表1-1中数值关系的某个一元一次函数。学完第3 章的线性\\n回归可知当前问题属于一元线性回归问题，根据一元线性回归算法可学得模型为y = 3x −2。\\n除此之外，也可以将问题复杂化，假设学校数量和房价呈y = wx2 + b 一元二次函数关系，此时问题\\n变为了线性回归中的多项式回归问题，按照多项式回归算法可学得模型为y = x2。因此，以表1-1中数据\\n作为训练集可以有多个假设空间，且在不同的假设空间中都有可能学得能够拟合训练集的模型，我们将所\\n有能够拟合训练集的模型构成的集合称为“版本空间”。\\n1.4\\n归纳偏好'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='有能够拟合训练集的模型构成的集合称为“版本空间”。\\n1.4\\n归纳偏好\\n在上一节“房价预测”的例子中，当选用一元线性回归算法时，学得的模型是一元一次函数，当选\\n用多项式回归算法时，学得的模型是一元二次函数，所以不同的机器学习算法有不同的偏好，我们称为\\n“归纳偏好”。对于当前房价预测这个例子来说，这两个算法学得的模型哪个更好呢？著名的“奥卡姆剃\\n刀”原则认为“若有多个假设与观察一致，则选最简单的那个”，但是何为“简单”便见仁见智了，如\\n果认为函数的幂次越低越简单，则此时一元线性回归算法更好，如果认为幂次越高越简单，则此时多项\\n式回归算法更好，因此该方法其实并不“简单”，所以并不常用，而最常用的方法则是基于模型在测试\\n集上的表现来评判模型之间的优劣。测试集是指由训练集之外的样本构成的集合，例如在当前房价预测\\n问题中，通常会额外留有部分未参与模型训练的数据来对模型进行测试。假设此时额外留有1 条数据：\\n(年份: 2022年; 学校数量: 3所; 房价: 7万/m2) 用于测试，模型y = 3x −2 的预测结果为3 ∗3 −2 = 7，预'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='测正确，模型y = x2 的预测结果为32 = 9，预测错误，因此，在当前房价预测问题上，我们认为一元线\\n性回归算法优于多项式回归算法。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n机器学习算法之间没有绝对的优劣之分，只有是否适合当前待解决的问题之分，例如上述测试集中的\\n数据如果改为(年份: 2022年; 学校数量: 3所; 房价: 9万/m2) 则结论便逆转为多项式回归算法优于一元线\\n性回归算法。\\n1.4.1\\n式(1.1) 和式(1.2) 的解释\\nX\\nf\\nEote(La|X, f) =\\nX\\nf\\nX\\nh\\nX\\nx∈X−X\\nP(x)I(h(x) ̸= f(x))P(h|X, La)\\n1⃝\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\nX\\nf\\nI(h(x) ̸= f(x))\\n2⃝\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\n1\\n22|X|\\n3⃝\\n=\\n1\\n22|X|\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\n4⃝\\n= 2|X|−1\\nX\\nx∈X−X\\nP(x) · 1\\n5⃝\\n1⃝→2⃝：\\nX\\nf\\nX\\nh\\nX\\nx∈X−X\\nP(x)I(h(x) ̸= f(x))P(h|X, La)\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nf\\nX\\nh'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nX\\nx∈X−X\\nP(x)\\nX\\nf\\nX\\nh\\nI(h(x) ̸= f(x))P(h|X, La)\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\nX\\nf\\nI(h(x) ̸= f(x))\\n2⃝→3⃝：首先要知道此时我们假设f 是任何能将样本映射到{0, 1} 的函数。存在不止一个f 时，f\\n服从均匀分布，即每个f 出现的概率相等。例如样本空间只有两个样本时，X = {x1, x2}, |X| = 2。那么\\n所有可能的真实目标函数f 如下：\\nf1 : f1(x1) = 0, f1(x2) = 0\\nf2 : f2(x1) = 0, f2(x2) = 1\\nf3 : f3(x1) = 1, f3(x2) = 0\\nf4 : f4(x1) = 1, f4(x2) = 1\\n一共2|X| = 22 = 4 个可能的真实目标函数。所以此时通过算法La 学习出来的模型h(x) 对每个样本无论\\n预测值为0 还是1，都必然有一半的f 与之预测值相等。例如，现在学出来的模型h(x) 对x1 的预测值'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='为1，即h(x1) = 1，那么有且只有f3 和f4 与h(x) 的预测值相等，也就是有且只有一半的f 与它预测\\n值相等，所以P\\nf I(h(x) ̸= f(x)) = 1\\n22|X|。\\n需要注意的是，在这里我们假设真实的目标函数f 服从均匀分布，但是实际情形并非如此，通常我们只\\n认为能高度拟合已有样本数据的函数才是真实目标函数，例如，现在已有的样本数据为{(x1, 0), (x2, 1)}，那\\n么此时f2 才是我们认为的真实目标函数，由于没有收集到或者压根不存在{(x1, 0), (x2, 0)}, {(x1, 1), (x2, 0)},\\n{(x1, 1), (x2, 1)} 这类样本，所以f1, f3, f4 都不算是真实目标函数。套用到上述“房价预测”的例子中，我\\n们认为只有能正确拟合测试集的函数才是真实目标函数，也就是我们希望学得的模型。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第2 章\\n模型评估与选择\\n如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1 章介绍了什么是机器学习及机\\n器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称\\n“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。\\n由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型\\n有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单\\n泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3 章开始看，直至看完第6 章以后再回头来看\\n本章便会轻松许多。\\n2.1\\n经验误差与过拟合\\n梳理本节的几个概念。\\n错误率：E = a\\nm，其中m 为样本个数，a 为分类错误样本个数。\\n精度：精度=1-错误率。\\n误差：学习器的实际预测输出与样本的真实输出之间的差异。\\n经验误差：学习器在训练集上的误差，又称为“训练误差”。\\n泛化误差：学习器在新样本上的误差。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='经验误差：学习器在训练集上的误差，又称为“训练误差”。\\n泛化误差：学习器在新样本上的误差。\\n经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12 章的式(12.1) 和式(12.2)，接下\\n来辨析一下以上几个概念。\\n错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根\\n据“西瓜书”第12 章的式(12.1) 和式(12.2) 的定义可以看出，在分类问题中也会使用误差的概念，此时\\n的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若\\n不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出\\n现过的样本）上差异的平均值。\\n过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相\\n对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1 章图1.4 中的\\n训练样本（黑点）来说，用类似于抛物线的曲线A 去拟合则较为合理，而比较崎岖的曲线B 相对于训练'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='训练样本（黑点）来说，用类似于抛物线的曲线A 去拟合则较为合理，而比较崎岖的曲线B 相对于训练\\n样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。\\n2.2\\n评估方法\\n本节介绍了3 种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；\\n交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用\\n于集成学习（详见“西瓜书”第8 章的8.2 节和8.3 节）产生基分类器。留出法和自助法简单易懂，在此\\n不再赘述，下面举例说明交叉验证法的常用方式。\\n对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集\\nD 的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉\\n验证法为算法L 筛选出在数据集D 上效果最好的参数配置方案。以3 折交叉验证为例，首先按照“西瓜\\n书”中所说的方法，通过分层采样将数据集D 划分为3 个大小相似的互斥子集：D1, D2, D3，然后分别用'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='其中1 个子集作为测试集，其他子集作为训练集，这样就可获得3 组训练集和测试集：\\n训练集1：D1 ∪D2，测试集1:D3\\n训练集2：D1 ∪D3，测试集2:D2\\n训练集3：D2 ∪D3，测试集3:D1\\n接下来用算法L 搭配方案a 在训练集1 上进行训练，训练结束后将训练得到的模型在测试集1 上进\\n行测试，得到测试结果1，依此方法再分别通过训练集2 和测试集2、训练集3 和测试集3 得到测试结果\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n2 和测试结果3，最后将3 次测试结果求平均即可得到算法L 搭配方案a 在数据集D 上的最终效果，记\\n为Scorea。同理，按照以上方法也可得到算法L 搭配方案b 在数据集D 上的最终效果Scoreb，最后通\\n过比较Scorea 和Scoreb 之间的优劣来确定算法L 在数据集D 上效果最好的参数配置方案。\\n对比不同算法之间的效果：同上述“对比同一算法的不同参数配置之间的效果”中所讲的方法一样，\\n只需将其中的“算法L 搭配方案a”和“算法L 搭配方案b”分别换成需要对比的算法α 和算法β 即可。\\n从以上的举例可以看出，交叉验证法本质上是在进行多次留出法，且每次都换不同的子集做测试集，\\n最终让所有样本均至少做1 次测试样本。这样做的理由其实很简单，因为一般的留出法只会划分出1 组\\n训练集和测试集，仅依靠1 组训练集和测试集去对比不同算法之间的效果显然不够置信，偶然性太强，因\\n此要想基于固定的数据集产生多组不同的训练集和测试集，则只有进行多次划分，每次采用不同的子集作\\n为测试集，也即为交叉验证法。\\n2.2.1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='为测试集，也即为交叉验证法。\\n2.2.1\\n算法参数（超参数）与模型参数\\n算法参数是指算法本身的一些参数（也称超参数），例如k 近邻的近邻个数k、支持向量机的参数C\\n（详见“西瓜书”第6 章式(6.29)）。算法配置好相应参数后进行训练，训练结束会得到一个模型，例如支\\n持向量机最终会得到w 和b 的具体数值（此处不考虑核函数），这就是模型参数，模型配置好相应模型参\\n数后即可对新样本做预测。\\n2.2.2\\n验证集\\n带有参数的算法一般需要从候选参数配置方案中选择相对于当前数据集的最优参数配置方案，例如支\\n持向量机的参数C，一般采用的是前面讲到的交叉验证法，但是交叉验证法操作起来较为复杂，实际中更\\n多采用的是：先用留出法将数据集划分出训练集和测试集，然后再对训练集采用留出法划分出训练集和新\\n的测试集，称新的测试集为验证集，接着基于验证集的测试结果来调参选出最优参数配置方案，最后将验\\n证集合并进训练集（训练集数据量够的话也可不合并），用选出的最优参数配置在合并后的训练集上重新\\n训练，再用测试集来评估训练得到的模型的性能。\\n2.3\\n性能度量'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='训练，再用测试集来评估训练得到的模型的性能。\\n2.3\\n性能度量\\n本节性能度量指标较多，但是一般常用的只有错误率、精度、查准率、查全率、F1、ROC 和AUC。\\n2.3.1\\n式(2.2) 到式(2.7) 的解释\\n这几个公式简单易懂，几乎不需要额外解释，但是需要补充说明的是式(2.2)、式(2.4) 和式(2.5) 假\\n设了数据分布为均匀分布，即每个样本出现的概率相同，而式(2.3)、式(2.6) 和式(2.7) 则为更一般的表\\n达式。此外，在无特别说明的情况下，2.3 节所有公式中的“样例集D”均默认为非训练集（测试集、验\\n证集或其他未用于训练的样例集）。\\n2.3.2\\n式(2.8) 和式(2.9) 的解释\\n查准率P：被学习器预测为正例的样例中有多大比例是真正例。\\n查全率R：所有正例当中有多大比例被学习器预测为正例。\\n2.3.3\\n图2.3 的解释\\nP-R 曲线的画法与ROC 曲线的画法类似，也是通过依次改变模型阈值，然后计算出查准率和查全\\n率并画出相应坐标点，具体参见“式(2.20) 的推导”部分的讲解。这里需要说明的是，“西瓜书”中的图'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2.3 仅仅是示意图，除了图左侧提到的“现实任务中的P-R 曲线常是非单调、不平滑的，在很多局部有上\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n下波动”以外，通常也不会取到(1, 0) 点。因为当取到(1, 0) 点时，此时是将所有样本均判为正例，因此\\nFN = 0，根据式(2.8) 可算得查全率为1，但是此时TP + FP 为样本总数，根据式(2.9) 可算得查准率\\n此时为正例在全体样本中的占比，显然在现实任务中正例的占比通常不为0，因此P-R 曲线在现实任务中\\n通常不会取到(1, 0) 点。\\n2.3.4\\n式(2.10) 的推导\\n将式(2.8) 和式(2.9) 代入式(2.10)，得\\nF1 = 2 × P × R\\nP + R\\n=\\n2 ×\\nT P\\nT P +F P ×\\nT P\\nT P +F N\\nT P\\nT P +F P +\\nT P\\nT P +F N\\n=\\n2 × TP × TP\\nTP(TP + FN) + TP(TP + FP)\\n=\\n2 × TP\\n(TP + FN) + (TP + FP)\\n=\\n2 × TP\\n(TP + FN + FP + TN) + TP −TN\\n=\\n2 × TP\\n样例总数+ TP −TN'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\n2 × TP\\n样例总数+ TP −TN\\n若现有数据集D = {(xi, yi) | 1 ⩽i ⩽m}，其中标记yi ∈{0, 1}（1 表示正例，0 表示反例），假设模型\\nf(x) 对xi 的预测结果为hi ∈{0, 1}，则模型f(x) 在数据集D 上的F1 为\\nF1 =\\n2 Pm\\ni=1 yihi\\nPm\\ni=1 yi + Pm\\ni=1 hi\\n不难看出上式的本质为\\nF1 =\\n2 × TP\\n(TP + FN) + (TP + FP)\\n2.3.5\\n式(2.11) 的解释\\n“西瓜书”在式(2.11) 左侧提到Fβ 本质是加权调和平均，且和常用的算数平均相比，其更重视较小值，\\n在此举例说明。例如a 同学有两门课的成绩分别为100 分和60 分，b 同学相应的成绩为80 分和80 分，\\n此时若计算a 同学和b 同学的算数平均分则均为80 分，无法判断两位同学成绩的优劣，但是若计算加权\\n调和平均，当β = 1 时，a 同学的加权调和平均为2×100×60\\n100+60\\n= 75，b 同学的加权调和平均为2×80×80\\n80+80\\n= 80，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='100+60\\n= 75，b 同学的加权调和平均为2×80×80\\n80+80\\n= 80，\\n此时b 同学的平均成绩更优，原因是a 同学由于偏科导致其中一门成绩过低，而调和平均更重视较小值，\\n所以a 同学的偏科便被凸显出来。\\n式(2.11) 下方有提到“β > 1 时查全率有更大影响；β < 1 时查准率有更大影响”，下面解释其原因。\\n将式(2.11) 恒等变形为如下形式\\nFβ =\\n1\\n1\\n1+β2 · 1\\nP +\\nβ2\\n1+β2 · 1\\nR\\n从上式可以看出，当β > 1 时\\nβ2\\n1+β2 >\\n1\\n1+β2 ，所以\\n1\\nR 的权重比\\n1\\nP 的权重高，因此查全率R 对Fβ 的影响\\n更大，反之查准率P 对Fβ 的影响更大。\\n2.3.6\\n式(2.12) 到式(2.17) 的解释\\n式(2.12) 的macro-P 和式(2.13) 的macro-R 是基于各个二分类问题的P 和R 计算而得的；式(2.15)\\n的micro-P 和式(2.16) 的micro-R 是基于各个二分类问题的TP、FP、TN、FN 计算而得的；“宏”可'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='以认为是只关注宏观而不看具体细节，而“微”可以认为是要从具体细节做起，因为相比于P 和R 指标\\n来说，TP、FP、TN、FN 更微观，毕竟P 和R 是基于TP、FP、TN、FN 计算而得。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n从“宏”和“微”的计算方式可以看出，“宏”没有考虑每个类别下的的样本数量，所以平等看待每个\\n类别，因此会受到高P 和高R 类别的影响，而“微”则考虑到了每个类别的样本数量，因为样本数量多\\n的类相应的TP、FP、TN、FN 也会占比更多，所以在各类别样本数量极度不平衡的情况下，数量较多\\n的类别会主导最终结果。\\n式(2.14) 的macro-F1 是将macro-P 和macro-R 代入式(2.10) 所得；式(2.17) 的macro-F1 是将\\nmacro-P 和macro-R 代入式(2.10) 所得。值得一提的是，以上只是macro-F1 和micro-F1 的常用计算\\n方式之一，如若在查阅资料的过程中看到其他的计算方式也属正常。\\n2.3.7\\n式(2.18) 和式(2.19) 的解释\\n式(2.18) 定义了真正例率TPR。先解释公式中出现的真正例和假反例，真正例即实际为正例预测结\\n果也为正例，假反例即实际为正例但预测结果为反例，式(2.18) 分子为真正例，分母为真正例和假反例'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='果也为正例，假反例即实际为正例但预测结果为反例，式(2.18) 分子为真正例，分母为真正例和假反例\\n之和（即实际的正例个数），因此式(2.18) 的含义是所有正例当中有多大比例被预测为正例（即查全率\\nRecall）。\\n式(2.19) 定义了假正例率FPR。先解释式子中出现的假正例和真反例，假正例即实际为反例但预测\\n结果为正例，真反例即实际为反例预测结果也为反例，式(2.19) 分子为假正例，分母为真反例和假正例之\\n和（即实际的反例个数），因此式(2.19) 的含义是所有反例当中有多大比例被预测为正例。\\n除了真正例率TPR 和假正例率FPR，还有真反例率TNR 和假反例率FNR：\\nTNR =\\nTN\\nFP + TN\\nFNR =\\nFN\\nTP + FN\\n2.3.8\\n式(2.20) 的推导\\n在推导式(2.20) 之前，需要先弄清楚ROC 曲线的具体绘制过程。下面我们就举个例子，按照“西瓜\\n书”图2.4 下方给出的绘制方法来讲解一下ROC 曲线的具体绘制过程。\\n假设我们已经训练得到一个学习器f(s)，现在用该学习器来对8 个测试样本（4 个正例，4 个反例，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='假设我们已经训练得到一个学习器f(s)，现在用该学习器来对8 个测试样本（4 个正例，4 个反例，\\n即m+ = m−= 4）进行预测，预测结果为（此处用s 表示样本，以和坐标(x, y) 作出区分）：\\n(s1, 0.77, +), (s2, 0.62, −), (s3, 0.58, +), (s4, 0.47, +),\\n(s5, 0.47, −), (s6, 0.33, −), (s7, 0.23, +), (s8, 0.15, −)\\n其中，+ 和−分别表示样本为正例和为反例，数字表示学习器f 预测该样本为正例的概率，例如对\\n于反例s2 来说，当前学习器f(s) 预测它是正例的概率为0.62。\\n根据“西瓜书”上给出的绘制方法，首先需要对所有测试样本按照学习器给出的预测结果进行排\\n序（上面给出的预测结果已经按照预测值从大到小排序），接着将分类阈值设为一个不可能取到的超大\\n值，例如设为1。显然，此时所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个\\n数为0，相应的真正例率和假正例率也都为0，所以我们可以在坐标(0, 0) 处标记一个点。接下来需要把'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='分类阈值从大到小依次设为每个样本的预测值，也就是依次设为0.77, 0.62, 0.58, 0.47, 0.33, 0.23,0.15，然\\n后分别计算真正例率和假正例率，再在相应的坐标上标记点，最后再将各个点用直线连接, 即可得到ROC\\n曲线。需要注意的是，在统计预测结果时，预测值等于分类阈值的样本也被算作预测为正例。例如，当分\\n类阈值为0.77 时，测试样本s1 被预测为正例，由于它的真实标记也是正例，所以此时s1 是一个真正例。\\n为了便于绘图，我们将x 轴（假正例率轴）的“步长”定为\\n1\\nm−，y 轴（真正例率轴）的“步长”定为\\n1\\nm+ 。\\n根据真正例率和假正例率的定义可知，每次变动分类阈值时，若新增i 个假正例，那么相应的x 轴坐标也\\n就增加\\ni\\nm−；若新增j 个真正例，那么相应的y 轴坐标也就增加\\nj\\nm+ 。按照以上讲述的绘制流程，最终我\\n们可以绘制出如图2-1所示的ROC 曲线。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图2-1 ROC 曲线示意\\n在这里，为了能在解释式(2.21) 时复用此图，我们没有写上具体的数值，转而用其数学符号代替。其\\n中绿色线段表示在分类阈值变动的过程中只新增了真正例，红色线段表示只新增了假正例，蓝色线段表示\\n既新增了真正例也新增了假正例。根据AUC 值的定义可知，此时的AUC 值其实就是所有红色线段和蓝\\n色线段与x 轴围成的面积之和。观察图2-1可知，红色线段与x 轴围成的图形恒为矩形，蓝色线段与x 轴\\n围成的图形恒为梯形。由于梯形面积式既能算梯形面积，也能算矩形面积，所以无论是红色线段还是蓝色\\n线段，其与x 轴围成的面积都能用梯形公式来计算：\\n1\\n2 · (xi+1 −xi) · (yi + yi+1)\\n其中，(xi+1 −xi) 为“高”，yi 为“上底”，yi+1 为“下底”。那么对所有红色线段和蓝色线段与x 轴围成\\n的面积进行求和，则有\\nm−1\\nX\\ni=1\\n\\x141\\n2 · (xi+1 −xi) · (yi + yi+1)\\n\\x15\\n此即为AUC。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='X\\ni=1\\n\\x141\\n2 · (xi+1 −xi) · (yi + yi+1)\\n\\x15\\n此即为AUC。\\n通过以上ROC 曲线的绘制流程可以看出，ROC 曲线上每一个点都表示学习器f(s) 在特定阈值下构\\n成的一个二分类器，越好的二分类器其假正例率（反例被预测错误的概率，横轴）越小，真正例率（正例\\n被预测正确的概率，纵轴）越大，所以这个点越靠左上角（即点(0, 1)）越好。因此，越好的学习器，其\\nROC 曲线上的点越靠左上角，相应的ROC 曲线下的面积也越大，即AUC 也越大。\\n2.3.9\\n式(2.21) 和式(2.22) 的推导\\n下面针对“西瓜书”上所说的“ℓrank 对应的是ROC 曲线之上的面积”进行推导。按照我们上述对式\\n(2.20) 的推导思路，ℓrank 可以看作是所有绿色线段和蓝色线段与y 轴围成的面积之和，但从式(2.21) 中\\n很难一眼看出其面积的具体计算方式，因此我们进行恒等变形如下：\\nℓrank =\\n1\\nm+m−\\nX\\nx+∈D+\\nX\\nx−∈D−\\n\\x12\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2I\\n\\x00f(x+) = f(x−)\\n\\x01\\x13\\n=\\n1\\nm+m−\\nX\\nx+∈D+'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01\\n+ 1\\n2I\\n\\x00f(x+) = f(x−)\\n\\x01\\x13\\n=\\n1\\nm+m−\\nX\\nx+∈D+\\n\" X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2 ·\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n=\\nX\\nx+∈D+\\n\"\\n1\\nm+ ·\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2 ·\\n1\\nm+ ·\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n=\\nX\\nx+∈D+\\n1\\n2 ·\\n1\\nm+ ·\\n\"\\n2\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n在变动分类阈值的过程当中，如果有新增真正例，那么图2-1就会相应地增加一条绿色线段或蓝色线\\n段，所以上式中的P\\nx+∈D+ 可以看作是在累加所有绿色和蓝色线段，相应地，P\\nx+∈D+ 后面的内容便是\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在求绿色线段或者蓝色线段与y 轴围成的面积，即：\\n1\\n2 ·\\n1\\nm+ ·\\n\"\\n2\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n与式(2.20) 中的推导思路相同，不论是绿色线段还是蓝色线段，其与y 轴围成的图形面积都可以用\\n梯形公式来进行计算，所以上式表示的依旧是一个梯形的面积公式。其中\\n1\\nm+ 即梯形的“高”，中括号内\\n便是“上底+ 下底”，下面我们来分别推导一下“上底”（较短的底）和“下底”（较长的底）。\\n由于在绘制ROC 曲线的过程中，每新增一个假正例时x 坐标也就新增一个步长，所以对于“上底”，\\n也就是绿色或者蓝色线段的下端点到y 轴的距离，长度就等于\\n1\\nm−乘以预测值大于f(x+) 的假正例的个\\n数，即\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n而对于“下底”，长度就等于\\n1\\nm−乘以预测值大于等于f(x+) 的假正例的个数，即\\n1\\nm−\\n X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1\\nm−\\n X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n!\\n到此，推导完毕。\\n若不考虑f(x+) = f(x−)，从直观上理解ℓrank，其表示的是：对于待测试的模型f(x)，从测试集中随\\n机抽取一个正反例对儿{x+, x−}，模型f(x) 对正例的打分f(x+) 小于对反例的打分f(x−) 的概率，即\\n“排序错误”的概率。推导思路如下：采用频率近似概率的思路，组合出测试集中的所有正反例对儿，假设\\n组合出来的正反例对儿的个数为m，用模型f(x) 对所有正反例对儿打分并统计“排序错误”的正反例对\\n儿个数n，然后计算出\\nn\\nm 即为模型f(x)“排序错误”的正反例对儿的占比，其可近似看作为f(x) 在测\\n试集上“排序错误”的概率。具体推导过程如下：测试集中的所有正反例对儿的个数为\\nm+ × m−\\n“排序错误”的正反例对儿个数为\\nX\\nx+∈D+\\nX\\nx−∈D−\\n\\x00I\\n\\x00f(x+) < f(x−)\\n\\x01\\x01\\n因此，“排序错误”的概率为\\nP\\nx+∈D+\\nP\\nx−∈D−(I (f(x+) < f(x−)))\\nm+ × m−'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='P\\nx+∈D+\\nP\\nx−∈D−(I (f(x+) < f(x−)))\\nm+ × m−\\n若再考虑f(x+) = f(x−) 时算半个“排序错误”，则上式可进一步扩展为\\nP\\nx+∈D+\\nP\\nx−∈D−\\n\\x00I\\n\\x00f(x+) < f(x−) + 1\\n2I (f(x+) = f(x−))\\n\\x01\\x01\\nm+ × m−\\n此即为ℓrank。\\n如果说ℓrank 指的是从测试集中随机抽取正反例对儿，模型f(x)“排序错误”的概率，那么根据式\\n(2.22) 可知，AUC 则指的是从测试集中随机抽取正反例对儿，模型f(x)“排序正确”的概率。显然，此\\n概率越大越好。\\n2.3.10\\n式(2.23) 的解释\\n本公式很容易理解，只是需要注意该公式上方交代了“若将表2.2 中的第0 类作为正类、第1 类作为\\n反类”，若不注意此条件，按习惯（0 为反类、1 为正类）会产生误解。为避免产生误解，在接下来的解释\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n中将cost01 记为cost+−，cost10 记为cost−+。本公式还可以作如下恒等变形\\nE(f; D; cost) = 1\\nm\\n\\uf8eb\\n\\uf8edm+ ×\\n1\\nm+\\nX\\nxi∈D+\\nI (f (xi ̸= yi)) × cost+−+ m−×\\n1\\nm−\\nX\\nxi∈D−\\nI (f (xi ̸= yi)) × cost−+\\n\\uf8f6\\n\\uf8f8\\n= m+\\nm ×\\n1\\nm+\\nX\\nxi∈D+\\nI (f (xi ̸= yi)) × cost+−+ m−\\nm ×\\n1\\nm−\\nX\\nxi∈D−\\nI (f (xi ̸= yi)) × cost−+\\n其中m+ 和m−分别表示正例子集D+ 和反例子集D−的样本个数。\\n1\\nm+\\nP\\nxi∈D+ I (f (xi ̸= yi)) 表示正例子集D+ 预测错误样本所占比例，即假反例率FNR。\\n1\\nm−\\nP\\nxi∈D−I (f (xi ̸= yi)) 表示反例子集D−预测错误样本所占比例，即假反例率FPR。\\nm+\\nm 表示样例集D 中正例所占比例，或理解为随机从D 中取一个样例取到正例的概率。\\nm−'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m+\\nm 表示样例集D 中正例所占比例，或理解为随机从D 中取一个样例取到正例的概率。\\nm−\\nm 表示样例集D 中反例所占比例，或理解为随机从D 中取一个样例取到反例的概率。\\n因此，若将样例为正例的概率m+\\nm 记为p，则样例为f 反例的概率m−\\nm 为1 −p，上式可进一步写为\\nE(f; D; cost) = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\n此公式在接下来式(2.25) 的解释中会用到。\\n2.3.11\\n式(2.24) 的解释\\n当cost+−= cost−+ 时，本公式可化简为\\nP(+)cost =\\np\\np + (1 −p) = p\\n其中p 是样例为正例的概率（一般用正例在样例集中所占的比例近似代替）。因此，当代价不敏感时（也即\\ncost+−= cost−+），P(+)cost 就是正例在样例集中的占比。那么，当代价敏感时（也即cost+−̸= cost−+），\\nP(+)cost 即为正例在样例集中的加权占比。具体来说，对于样例集\\nD =\\n\\x08\\nx+\\n1 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x08\\nx+\\n1 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−\\n7 , x−\\n8 , x−\\n9 , x−\\n10\\n\\t\\n其中x+ 表示正例，x−表示反例。可以看出p = 0.2，若想让正例得到更多重视，考虑代价敏感cost+−= 4\\n和cost−+ = 1，这实际等价于在以下样例集上进行代价不敏感的正例概率代价计算\\nD′ =\\n\\x08\\nx+\\n1 , x+\\n1 , x+\\n1 , x+\\n1 , x+\\n2 , x+\\n2 , x+\\n2 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−\\n7 , x−\\n8 , x−\\n9 , x−\\n10\\n\\t\\n即将每个正例样本复制4 份，若有1 个出错，则有4 个一起出错，代价为4。此时可计算出\\nP(+)cost =\\np × cost+−\\np × cost+−+ (1 −p) × cost−+\\n=\\n0.2 × 4\\n0.2 × 4 + (1 −0.2) × 1 = 0.5\\n也就是正例在等价的样例集D′ 中的占比。所以，无论代价敏感还是不敏感，P(+)cost 本质上表示的都是'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='样例集中正例的占比。在实际应用过程中，如果由于某种原因无法将cost+−和cost−+ 设为不同取值，可\\n以采用上述“复制样本”的方法间接实现将cost+−和cost−+ 设为不同取值。\\n对于不同的cost+−和cost−+ 取值，若二者的比值保持相同，则P(+)cost 不变。例如，对于上面的\\n例子，若设cost+−= 40 和cost−+ = 10，所得P(+)cost 仍为0.5。\\n此外，根据此式还可以相应地推导出反例概率代价\\nP(−)cost = 1 −P(+)cost =\\n(1 −p) × cost−+\\np × cost+−+ (1 −p) × cost−+\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n2.3.12\\n式(2.25) 的解释\\n对于包含m 个样本的样例集D，可以算出学习器f(x) 总的代价是\\ncostse = m × p × FNR × cost+−+ m × (1 −p) × FPR × cost−+\\n+ m × p × TPR × cost++ + m × (1 −p) × TNR × cost−−\\n其中p 是正例在样例集中所占的比例（或严格地称为样例为正例的概率），costse 下标中的“se”表示\\nsensitive，即代价敏感，根据前面讲述的FNR、FPR、TPR、TNR 的定义可知：\\nm × p × FNR 表示正例被预测为反例（正例预测错误）的样本个数；\\nm × (1 −p) × FPR 表示反例被预测为正例（反例预测错误）的样本个数；\\nm × p × TPR 表示正例被预测为正例（正例预测正确）的样本个数；\\nm × (1 −p) × TNR 表示反例预测为反例（反例预测正确）的样本个数。\\n以上各种样本个数乘以相应的代价则得到总的代价costse。但是，按照此公式计算出的代价与样本个'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='以上各种样本个数乘以相应的代价则得到总的代价costse。但是，按照此公式计算出的代价与样本个\\n数m 呈正比，显然不具有一般性，因此需要除以样本个数m，而且一般来说，预测出错才会产生代价，预\\n测正确则没有代价，也即cost++ = cost−−= 0，所以costse 更为一般化的表达式为\\ncostse = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\n回顾式(2.23) 的解释可知，此式即为式(2.23) 的恒等变形，所以此式可以同式(2.23) 一样理解为学习器\\nf(x) 在样例集D 上的“代价敏感错误率”。显然，costse 的取值范围并不在0 到1 之间，且costse 在\\nFNR = FPR = 1 时取到最大值，因为FNR = FPR = 1 时表示所有正例均被预测为反例，反例均被预测\\n为正例，代价达到最大，即\\nmax(costse) = p × cost+−+ (1 −p) × cost−+\\n所以，如果要将costse 的取值范围归一化到0 到1 之间，则只需将其除以其所能取到的最大值即可，也即\\ncostse'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='costse\\nmax(costse) = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\np × cost+−+ (1 −p) × cost−+\\n此即为式(2.25)，也即为costnorm，其中下标“norm”表示normalization。\\n进一步地，根据式(2.24) 中P(+)cost 的定义可知，式(2.25) 可以恒等变形为\\ncostnorm = FNR × P(+)cost + FPR × (1 −P(+)cost)\\n对于二维直角坐标系中的两个点(0, B) 和(1, A) 以及实数p ∈[0, 1]，(p, pA+(1−p)B) 一定是线段A−B\\n上的点，且当p 从0 变到1 时，点(p, pA + (1 −p)B) 的轨迹为从(0, B) 到(1, A)，基于此，结合上述\\ncostnorm 的表达式可知：(P(+)cost, costnorm) 即为线段FPR −FNR 上的点，当(P(+)cost 从0 变到1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='时，(P(+)cost, costnorm) 的轨迹为从(0, FPR) 到(1, FNR) ，也即图2.5 中的各条线段。需要注意的是，\\n以上只是从数学逻辑自洽的角度对图2.5 中的各条线段进行解释，实际中各条线段并非按照上述方法绘\\n制。理由如下：\\nP(+)cost 表示的是样例集中正例的占比，而在进行学习器的比较时，变动的只是训练学习器的算法\\n或者算法的超参数，用来评估学习器性能的样例集是固定的（单一变量原则），所以P(+)cost 是一个固\\n定值，因此图2.5 中的各条线段并不是通过变动P(+)cost 然后计算costnorm 画出来的，而是按照“西瓜\\n书”上式(2.25) 下方所说对ROC 曲线上每一点计算FPR 和FNR，然后将点(0, FPR) 和点(1, FNR) 直\\n接连成线段。\\n虽然图2.5 中的各条线段并不是通过变动横轴表示的P(+)cost 来进行绘制，但是横轴仍然有其他用\\n处，例如用来找使学习器的归一化代价costnorm 达到最小的阈值（暂且称其为最佳阈值）。具体地，首先'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='计算当前样例集的P(+)cost 值，然后根据计算出来的值在横轴上标记出具体的点，再基于该点作一条垂\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n直于横轴的垂线，与该垂线最先相交（从下往上看）的线段所对应的阈值（因为每条线段都对应ROC 曲\\n线上的点，ROC 曲线上的点又对应着具体的阈值）即为最佳阈值。原因是与该垂线最先相交的线段必然最\\n靠下，因此其交点的纵坐标最小，而纵轴表示的便是归一化代价costnorm，所以此时归一化代价costnorm\\n达到最小。特别地，当P(+)cost = 0 时，即样例集中没有正例，全是负例，因此最佳阈值应该是学习器\\n不可能取到的最大值，且按照此阈值计算出来出来的FPR = 0, FNR = 1, costnorm = 0。那么按照上述作\\n垂线的方法去图2.5 中进行实验，也即在横轴0 刻度处作垂线，显然与该垂线最先相交的线段是点(0, 0)\\n和点(1, 1) 连成的线段，交点为(0, 0)，此时对应的也为FPR = 0, FNR = 1, costnorm = 0，且该条线段所\\n对应的阈值也确实为“学习器不可能取到的最大值”（因为该线段对应的是ROC 曲线中的起始点）。\\n2.4\\n比较检验'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2.4\\n比较检验\\n为什么要做比较检验？“西瓜书”在本节开篇的两段话已经交代原由。简单来说，从统计学的角度，取\\n得的性能度量的值本质上仍是一个随机变量，因此并不能简单用比较大小来直接判定算法（或者模型）之\\n间的优劣，而需要更置信的方法来进行判定。\\n在此说明一下，如果不做算法理论研究，也不需要对算法（或模型）之间的优劣给出严谨的数学分析，\\n本节可以暂时跳过。本节主要使用的数学知识是“统计假设检验”，该知识点在各个高校的概率论与数理\\n统计教材（例如参考文献[1]）上均有讲解。此外，有关检验变量的公式，例如式(2.30) 至式(2.36)，并不\\n需要清楚是怎么来的（这是统计学家要做的事情），只需要会用即可。\\n2.4.1\\n式(2.26) 的解释\\n理解本公式时需要明确的是：ϵ 是未知的，是当前希望估算出来的，ˆϵ 是已知的，是已经用m 个测试\\n样本对学习器进行测试得到的。因此，本公式也可理解为：当学习器的泛化错误率为ϵ 时，被测得测试错\\n误率为ˆϵ 的条件概率。所以本公式可以改写为\\nP(ˆϵ|ϵ) =\\n \\nm\\nˆϵ × m\\n!\\nϵˆϵ×m(1 −ϵ)m−ˆϵ×m\\n其中\\n \\nm\\nˆϵ × m\\n!'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nˆϵ × m\\n!\\nϵˆϵ×m(1 −ϵ)m−ˆϵ×m\\n其中\\n \\nm\\nˆϵ × m\\n!\\n=\\nm!\\n(ˆϵ × m)!(m −ˆϵ × m)!\\n为中学时学的组合数，即Cˆϵ×m\\nm\\n。\\n在已知ˆε 时，求使得条件概率P(ˆϵ|ϵ) 达到最大的ϵ 是概率论与数理统计中经典的极大似然估计问题。\\n从极大似然估计的角度可知，由于ˆϵ, m 均为已知量，所以P(ˆϵ|ϵ) 可以看作为一个关于ϵ 的函数，称为似\\n然函数，于是问题转化为求使得似然函数取到最大值的ϵ，即\\nϵ = arg max\\nϵ\\nP(ˆϵ|ϵ)\\n首先对ϵ 求一阶导数\\n∂P(ˆϵ | ϵ)\\n∂ϵ\\n=\\n \\nm\\nˆϵ × m\\n!\\n∂ϵˆϵ×m(1 −ϵ)m−ˆϵ×m\\n∂ϵ\\n=\\n \\nm\\nˆϵ × m\\n!\\n\\x00ˆϵ × m × ϵˆϵ×m−1(1 −ϵ)m−ˆϵ×m + ϵˆϵ×m × (m −ˆϵ × m) × (1 −ϵ)m−ˆϵ×m−1 × (−1)\\n\\x01\\n=\\n \\nm\\nˆϵ × m\\n!\\nϵˆϵ×m−1(1 −ϵ)m−ˆϵ×m−1(ˆϵ × m × (1 −ϵ) −ϵ × (m −ˆϵ × m))\\n=\\n \\nm\\nˆϵ × m\\n!'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\n \\nm\\nˆϵ × m\\n!\\nϵˆϵ×m−1(1 −ϵ)m−ˆϵ×m−1(ˆϵ × m −ϵ × m)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n分析上式可知，其中\\n \\nm\\nˆϵ × m\\n!\\n为常数，由于ϵ ∈[0, 1]，所以ϵˆϵ×m−1(1−ϵ)m−ˆϵ×m−1 恒大于0，(ˆϵ×m−ϵ×m)\\n在0 ⩽ϵ < ˆϵ 时大于0，在ϵ = ˆϵ 时等于0，在ˆϵ ⩽ϵ < 1 时小于0，因此P(ˆϵ | ϵ) 是关于ϵ 开口向下的凹\\n函数（此处采用的是最优化中对凹凸函数的定义，“西瓜书”第3 章3.2 节左侧边注对凹凸函数的定义也\\n是如此）。所以，当且仅当一阶导数∂P (ˆϵ|ϵ)\\n∂ϵ\\n= 0 时P(ˆϵ | ϵ) 取到最大值，此时ϵ = ˆϵ。\\n2.4.2\\n式(2.27) 的推导\\n截至2021 年5 月，“西瓜书”第1 版第36 次印刷，式(2.27) 应当勘误为\\nϵ = min ϵ\\ns.t.\\nm\\nX\\ni=ϵ×m+1\\n \\nm\\ni\\n!\\nϵi\\n0(1 −ϵ0)m−i < α\\n在推导此公式之前，先铺垫讲解一下“二项分布参数p 的假设检验”\\n[1]：\\n设某事件发生的概率为p，p 未知。做m 次独立试验，每次观察该事件是否发生，以X 记该事件发'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='设某事件发生的概率为p，p 未知。做m 次独立试验，每次观察该事件是否发生，以X 记该事件发\\n生的次数，则X 服从二项分布B(m, p)，现根据X 检验如下假设：\\nH0 : p ⩽p0\\nH1 : p > p0\\n由二项分布本身的特性可知：p 越小，X 取到较小值的概率越大。因此，对于上述假设，一个直观上\\n合理的检验为\\nφ : 当X > C时拒绝H0, 否则就接受H0。\\n其中，C 表示事件最大发生次数。此检验对应的功效函数为\\nβφ(p) = P(X > C)\\n= 1 −P(X ⩽C)\\n= 1 −\\nC\\nX\\ni=0\\n \\nm\\ni\\n!\\npi(1 −p)m−i\\n=\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi(1 −p)m−i\\n由于“p 越小，X 取到较小值的概率越大”可以等价表示为：P(X ⩽C) 是关于p 的减函数，所以\\nβφ(p) = P(X > C) = 1 −P(X ⩽C) 是关于p 的增函数，那么当p ⩽p0 时，βφ(p0) 即为βφ(p) 的上确界。\\n（更为严格的数学证明参见参考文献[1] 中第2 章习题7）又根据参考文献[1] 中5.1.3 的定义1.2 可知，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='在给定检验水平α 时，要想使得检验φ 达到水平α，则必须保证βφ(p) ⩽α，因此可以通过如下方程解得\\n使检验φ 达到水平α 的整数C：\\nα = sup {βφ(p)}\\n显然，当p ⩽p0 时有\\nα = sup {βφ(p)}\\n= βφ(p0)\\n=\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i\\n对于此方程，通常不一定正好解得一个使得方程成立的整数C，较常见的情况是存在这样一个C 使得\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\nm\\nX\\ni=C\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i > α\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时，C 只能取C 或者C + 1。若C 取C，则相当于升高了检验水平α；若C 取C + 1 则相当于降低了\\n检验水平α。具体如何取舍需要结合实际情况，一般的做法是使α 尽可能小，因此倾向于令C 取C + 1。\\n下面考虑如何求解C。易证βφ(p0) 是关于C 的减函数，再结合上述关于C 的两个不等式易推得\\nC = min C\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n由“西瓜书”中的上下文可知，对ϵ ⩽ϵ0 进行假设检验，等价于“二项分布参数p 的假设检验”中所\\n述的对p ⩽p0 进行假设检验，所以在“西瓜书”中求解最大错误率ϵ 等价于在“二项分布参数p 的假设\\n检验”中求解事件最大发生频率C\\nm。由上述“二项分布参数p 的假设检验”中的推导可知\\nC = min C\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n所以\\nC\\nm = min C\\nm\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n将上式中的C\\nm, C\\nm, p0 等价替换为ϵ, ϵ, ϵ0 可得\\nϵ = min ϵ\\ns.t.\\nm\\nX\\ni=ϵ×m+1\\n \\nm\\ni\\n!\\nϵi\\n0(1 −ϵ0)m−i < α\\n2.5\\n偏差与方差\\n2.5.1\\n式(2.37) 到式(2.42) 的推导\\n首先，梳理一下“西瓜书”中的符号，书中称x 为测试样本，但是书中又提到“令yD 为x 在数据集\\n中的标记”，那么x 究竟是测试集中的样本还是训练集中的样本呢？这里暂且理解为x 为从训练集中抽取\\n出来用于测试的样本。此外，“西瓜书”中左侧边注中提到“有可能出现噪声使得yD ̸= y”，其中所说的\\n“噪声”通常是指人工标注数据时带来的误差，例如标注“身高”时，由于测量工具的精度等问题，测出来\\n的数值必然与真实的“身高”之间存在一定误差，此即为“噪声”。\\n为了进一步解释式(2.37)、(2.38) 和(2.39)，在这里设有n 个训练集D1, ..., Dn，这n 个训练集都是'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='以独立同分布的方式从样本空间中采样而得，并且恰好都包含测试样本x，该样本在这n 个训练集的标记\\n分别为yD1, ..., yDn。书中已明确，此处以回归任务为例，也即yD, y, f(x; D) 均为实值。\\n式(2.37) 可理解为：\\n¯f(x) = ED[f(x; D)] = 1\\nn (f (x; D1) + . . . + f (x; Dn))\\n式(2.38) 可理解为：\\nvar(x) = ED\\n\\x02\\n(f(x; D) −¯f(x))2\\x03\\n= 1\\nn\\n\\x10\\x00f (x; D1) −¯f(x)\\n\\x012 + . . . +\\n\\x00f (x; Dn) −¯f(x)\\n\\x012\\x11\\n式(2.39) 可理解为：\\nε2 = ED\\nh\\n(yD −y)2i\\n= 1\\nn\\n\\x10\\n(yD1 −y)2 + . . . + (yDn −y)2\\x11\\n最后，推导一下式(2.41) 和式(2.42)，由于推导完式(2.41) 自然就会得到式(2.42)，因此下面仅推导\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(2.41) 即可。\\nE(f; D) =ED\\nh\\n(f(x; D) −yD)2i\\n1⃝\\n=ED\\nh\\x00f(x; D) −¯f(x) + ¯f(x) −yD\\n\\x012i\\n2⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −yD\\n\\x012i\\n+\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n3⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −yD\\n\\x012i\\n4⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −y + y −yD\\n\\x012i\\n5⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −y\\n\\x012i\\n+ ED\\nh\\n(y −yD)2i\\n+\\n2ED\\n\\x02\\x00 ¯f(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n6⃝\\n=ED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+\\n\\x00 ¯f(x) −y\\n\\x012 + ED\\nh\\n(yD −y)2i\\n7⃝'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x012i\\n+\\n\\x00 ¯f(x) −y\\n\\x012 + ED\\nh\\n(yD −y)2i\\n7⃝\\n上式即为式(2.41)，下面给出每一步的推导过程：\\n1⃝→2⃝：减一个¯f(x) 再加一个¯f(x)，属于简单的恒等变形。\\n2⃝→3⃝：首先将中括号内的式子展开，有\\nED\\nh\\x00f(x; D) −¯f(x)\\n\\x012 +\\n\\x00 ¯f(x) −yD\\n\\x012 + 2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01i\\n然后根据期望的运算性质E[X + Y ] = E[X] + E[Y ] 可将上式化为\\nED\\nh\\x00f(x; D) −¯f(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯f(x) −yD\\n\\x012i\\n+ ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n3⃝→4⃝：再次利用期望的运算性质将3⃝的最后一项展开，有\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n= ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n首先计算展开后得到的第1 项，有\\nED\\n\\x02\\n2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n首先计算展开后得到的第1 项，有\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n= ED\\n\\x02\\n2f(x; D) · ¯f(x) −2 ¯f(x) · ¯f(x)\\n\\x03\\n由于¯f(x) 是常量，所以由期望的运算性质：E[AX + B] = AE[X] + B（其中A, B 均为常量）可得\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n= 2 ¯f(x) · ED [f(x; D)] −2 ¯f(x) · ¯f(x)\\n由式(2.37) 可知ED [f(x; D)] = ¯f(x)，所以\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n= 2 ¯f(x) · ¯f(x) −2 ¯f(x) · ¯f(x) = 0\\n接着计算展开后得到的第2 项\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯f(x) · ED [yD]'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯f(x) · ED [yD]\\n由于噪声和f 无关，所以f(x; D) 和yD 是两个相互独立的随机变量。根据期望的运算性质E[XY ] =\\nE[X]E[Y ]（其中X 和Y 为相互独立的随机变量）可得\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯f(x) · ED [yD]\\n= 2ED [f(x; D)] · ED [yD] −2 ¯f(x) · ED [yD]\\n= 2 ¯f(x) · ED [yD] −2 ¯f(x) · ED [yD]\\n= 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 29, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n所以\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01 \\x00 ¯f(x) −yD\\n\\x01\\x03\\n= ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· ¯f(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯f(x)\\n\\x01\\n· yD\\n\\x03\\n= 0 + 0\\n= 0\\n4⃝→5⃝：同1⃝→2⃝一样，减一个y 再加一个y，属于简单的恒等变形。\\n5⃝→6⃝：同2⃝→3⃝一样，将最后一项利用期望的运算性质进行展开。\\n6⃝→7⃝：因为¯f(x) 和y 均为常量，根据期望的运算性质，6⃝中的第2 项可化为\\nED\\nh\\x00 ¯f(x) −y\\n\\x012i\\n=\\n\\x00 ¯f(x) −y\\n\\x012\\n同理，6⃝中的最后一项可化为\\n2ED\\n\\x02\\x00 ¯f(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯f(x) −y\\n\\x01\\nED [(y −yD)]\\n由于此时假定噪声的期望为0，即ED [(y −yD)] = 0，所以\\n2ED\\n\\x02\\x00 ¯f(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯f(x) −y\\n\\x01\\n· 0 = 0\\n参考文献'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 29, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯f(x) −y\\n\\x01\\n· 0 = 0\\n参考文献\\n[1] 陈希孺. 概率论与数理统计. 中国科学技术大学出版社, 2009.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第3 章\\n线性模型\\n作为“西瓜书”介绍机器学习模型的开篇，线性模型也是机器学习中最为基础的模型，很多复杂模型\\n均可认为由线性模型衍生而得，无论是曾经红极一时的支持向量机还是如今万众瞩目的神经网络，其中都\\n有线性模型的影子。\\n本章的线性回归和对数几率回归分别是回归和分类任务上常用的算法，因此属于重点内容，线性判别\\n分析不常用，但是其核心思路和后续第10 章将会讲到的经典降维算法主成分分析相同，因此也属于重点\\n内容，且两者结合在一起看理解会更深刻。\\n3.1\\n基本形式\\n第1 章的1.2 基本术语中讲述样本的定义时，我们说明了“西瓜书”和本书中向量的写法，当向量\\n中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量。因此，式(3.2) 中\\nw = (w1; w2; ...; wd) 和x = (x1; x2; ...; xd) 均为d 行1 列的列向量。\\n3.2\\n线性回归\\n3.2.1\\n属性数值化\\n为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在“序”关系的属性，可通过'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在“序”关系的属性，可通过\\n连续化将其转化为带有相对大小关系的连续值；对于不存在“序”关系的属性，可根据属性取值将其拆解为\\n多个属性，例如“西瓜书”中所说的“瓜类”属性，可将其拆解为“是否是西瓜”、“是否是南瓜”、“是否是黄\\n瓜”3 个属性，其中每个属性的取值为1 或0，1 表示“是”，0 表示“否”。具体地，假如现有3 个瓜类样本：\\nx1 = (甜度= 高; 瓜类= 西瓜), x2 = (甜度= 中; 瓜类= 南瓜), x3 = (甜度= 低; 瓜类= 黄瓜)，其中“甜\\n度”属性存在序关系，因此可将“高”、\\n“中”、\\n“低”转化为{1.0, 0.5, 0.0}，\\n“瓜类”属性不存在序关系，则按照上\\n述方法进行拆解，3 个瓜类样本数值化后的结果为：x1 = (1.0; 1; 0; 0), x1 = (0.5; 0; 1; 0), x1 = (0.0; 0; 0; 1)。\\n以上针对样本属性所进行的处理工作便是第1 章1.2 基本术语中提到的“特征工程”范畴，完成属性'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='以上针对样本属性所进行的处理工作便是第1 章1.2 基本术语中提到的“特征工程”范畴，完成属性\\n数值化以后通常还会进行缺失值处理、规范化、降维等一系列处理工作。由于特征工程属于算法实践过程\\n中需要掌握的内容，待学完机器学习算法以后，再进一步学习特征工程相关知识即可，在此先不展开。\\n3.2.2\\n式(3.4) 的解释\\n下面仅针对式(3.4) 中的数学符号进行解释。首先解释一下符号“arg min”，其中“arg”是“argument”\\n（参\\n数）的前三个字母，“min”是“minimum”（最小值）的前三个字母，该符号表示求使目标函数达到最小值\\n的参数取值。例如式(3.4) 表示求出使目标函数Pm\\ni=1 (yi −wxi −b)2 达到最小值的参数取值(w∗, b∗)，注\\n意目标函数是以(w, b) 为自变量的函数，(xi, yi) 均是已知常量，即训练集中的样本数据。\\n类似的符号还有“min”，例如将式(3.4) 改为\\nmin\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='min\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\n则表示求目标函数的最小值。对比知道，“min”和“arg min”的区别在于，前者输出目标函数的最小值，\\n而后者输出使得目标函数达到最小值时的参数取值。\\n若进一步修改式(3.4) 为\\nmin\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\ns.t.w > 0,\\nb < 0.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n则表示在w > 0, b < 0 范围内寻找目标函数的最小值，“s.t.”是“subject to”的简写，意思是“受约束\\n于”，即为约束条件。\\n以上介绍的符号都是应用数学领域的一个分支——“最优化”中的内容，若想进一步了解可找一本最\\n优化的教材（例如参考文献[1]）进行系统性地学习。\\n3.2.3\\n式(3.5) 的推导\\n“西瓜书”在式(3.5) 左侧给出的凸函数的定义是最优化中的定义，与高等数学中的定义不同，本书也\\n默认采用此种定义。由于一元线性回归可以看作是多元线性回归中元的个数为1 时的情形，所以此处暂不\\n给出E(w,b) 是关于w 和b 的凸函数的证明，在推导式(3.11) 时一并给出，下面开始推导式(3.5)。\\n已知E(w,b) =\\nm\\nP\\ni=1\\n(yi −wxi −b)2，所以\\n∂E(w,b)\\n∂w\\n= ∂\\n∂w\\n\" m\\nX\\ni=1\\n(yi −wxi −b)2\\n#\\n=\\nm\\nX\\ni=1\\n∂\\n∂w\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−xi)]\\n='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−xi)]\\n=\\nm\\nX\\ni=1\\n\\x02\\n2 ·\\n\\x00wx2\\ni −yixi + bxi\\n\\x01\\x03\\n= 2 ·\\n \\nw\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\nyixi + b\\nm\\nX\\ni=1\\nxi\\n!\\n= 2\\n \\nw\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\n(yi −b) xi\\n!\\n3.2.4\\n式(3.6) 的推导\\n已知E(w,b) =\\nm\\nP\\ni=1\\n(yi −wxi −b)2，所以\\n∂E(w,b)\\n∂b\\n= ∂\\n∂b\\n\" m\\nX\\ni=1\\n(yi −wxi −b)2\\n#\\n=\\nm\\nX\\ni=1\\n∂\\n∂b\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−1)]\\n=\\nm\\nX\\ni=1\\n[2 · (b −yi + wxi)]\\n= 2 ·\\n\" m\\nX\\ni=1\\nb −\\nm\\nX\\ni=1\\nyi +\\nm\\nX\\ni=1\\nwxi\\n#\\n= 2\\n \\nmb −\\nm\\nX\\ni=1\\n(yi −wxi)\\n!\\n3.2.5\\n式(3.7) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='#\\n= 2\\n \\nmb −\\nm\\nX\\ni=1\\n(yi −wxi)\\n!\\n3.2.5\\n式(3.7) 的推导\\n推导之前先重点说明一下“闭式解”或称为“解析解”。闭式解是指可以通过具体的表达式解出待解\\n参数，例如可根据式(3.7) 直接解得w。机器学习算法很少有闭式解，线性回归是一个特例，接下来推导\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(3.7)。\\n令式(3.5) 等于0\\n0 = w\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\n(yi −b)xi\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −\\nm\\nX\\ni=1\\nbxi\\n由于令式(3.6) 等于0 可得b =\\n1\\nm\\nPm\\ni=1(yi −wxi)，又因为\\n1\\nm\\nPm\\ni=1 yi = ¯y，1\\nm\\nPm\\ni=1 xi = ¯x，则b = ¯y −w¯x，\\n代入上式可得\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −\\nm\\nX\\ni=1\\n(¯y −w¯x)xi\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −¯y\\nm\\nX\\ni=1\\nxi + w¯x\\nm\\nX\\ni=1\\nxi\\nw(\\nm\\nX\\ni=1\\nx2\\ni −¯x\\nm\\nX\\ni=1\\nxi) =\\nm\\nX\\ni=1\\nyixi −¯y\\nm\\nX\\ni=1\\nxi\\nw =\\nPm\\ni=1 yixi −¯y Pm\\ni=1 xi\\nPm\\ni=1 x2\\ni −¯x Pm\\ni=1 xi\\n将¯y Pm\\ni=1 xi =\\n1\\nm\\nPm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Pm\\ni=1 x2\\ni −¯x Pm\\ni=1 xi\\n将¯y Pm\\ni=1 xi =\\n1\\nm\\nPm\\ni=1 yi\\nPm\\ni=1 xi = ¯x Pm\\ni=1 yi 和¯x Pm\\ni=1 xi =\\n1\\nm\\nPm\\ni=1 xi\\nPm\\ni=1 xi =\\n1\\nm(Pm\\ni=1 xi)2 代入上\\n式，即可得式(3.7)：\\nw =\\nPm\\ni=1 yi(xi −¯x)\\nPm\\ni=1 x2\\ni −1\\nm(Pm\\ni=1 xi)2\\n如果要想用Python 来实现上式的话，上式中的求和运算只能用循环来实现。但是如果能将上式向量化，\\n也就是转换成矩阵（即向量）运算的话，我们就可以利用诸如NumPy 这种专门加速矩阵运算的类库来进\\n行编写。下面我们就尝试将上式进行向量化。\\n将\\n1\\nm(Pm\\ni=1 xi)2 = ¯x Pm\\ni=1 xi 代入分母可得\\nw =\\nPm\\ni=1 yi(xi −¯x)\\nPm\\ni=1 x2\\ni −¯x Pm\\ni=1 xi\\n=\\nPm\\ni=1(yixi −yi¯x)\\nPm\\ni=1(x2\\ni −xi¯x)\\n又因为¯y Pm\\ni=1 xi = ¯x Pm\\ni=1 yi = Pm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i −xi¯x)\\n又因为¯y Pm\\ni=1 xi = ¯x Pm\\ni=1 yi = Pm\\ni=1 ¯yxi = Pm\\ni=1 ¯xyi = m¯x¯y = Pm\\ni=1 ¯x¯y 且Pm\\ni=1 xi¯x = ¯x Pm\\ni=1 xi =\\n¯x · m · 1\\nm · Pm\\ni=1 xi = m¯x2 = Pm\\ni=1 ¯x2，则有\\nw =\\nPm\\ni=1(yixi −yi¯x −xi¯y + ¯x¯y)\\nPm\\ni=1(x2\\ni −xi¯x −xi¯x + ¯x2)\\n=\\nPm\\ni=1(xi −¯x)(yi −¯y)\\nPm\\ni=1(xi −¯x)2\\n若令x = (x1; x2; ...; xm)，xd = (x1 −¯x; x2 −¯x; ...; xm −¯x) 为去均值后的x；y = (y1; y2; ...; ym)，yd =\\n(y1 −¯y; y2 −¯y; ...; ym −¯y) 为去均值后的y，（x、xd、y、yd 均为m 行1 列的列向量）代入上式可得\\nw = xT\\nd yd\\nxT\\nd xd\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='w = xT\\nd yd\\nxT\\nd xd\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.2.6\\n式(3.9) 的推导\\n式(3.4) 是最小二乘法运用在一元线性回归上的情形，那么对于多元线性回归来说，我们可以类似得\\n到\\n(w∗, b∗) = arg min\\n(w,b)\\nm\\nX\\ni=1\\n(f (xi) −yi)2\\n= arg min\\n(w,b)\\nm\\nX\\ni=1\\n(yi −f (xi))2\\n= arg min\\n(w,b)\\nm\\nX\\ni=1\\n\\x00yi −\\n\\x00wTxi + b\\n\\x01\\x012\\n为便于讨论，我们令ˆw = (w; b) = (w1; ...; wd; b) ∈R(d+1)×1, ˆxi = (xi1; ...; xid; 1) ∈R(d+1)×1，那么上式可\\n以简化为\\nˆw∗= arg min\\nˆw\\nm\\nX\\ni=1\\n\\x10\\nyi −ˆwTˆxi\\n\\x112\\n= arg min\\nˆw\\nm\\nX\\ni=1\\n\\x10\\nyi −ˆxT\\ni ˆw\\n\\x112\\n根据向量内积的定义可知，上式可以写成如下向量内积的形式\\nˆw∗= arg min\\nˆw\\nh\\ny1 −ˆxT\\n1 ˆw\\n· · ·\\nym −ˆxT\\nm ˆw\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='ˆw\\nh\\ny1 −ˆxT\\n1 ˆw\\n· · ·\\nym −ˆxT\\nm ˆw\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny1 −ˆxT\\n1 ˆw\\n...\\nym −ˆxT\\nm ˆw\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n其中\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny1 −ˆxT\\n1 ˆw\\n...\\nym −ˆxT\\nm ˆw\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\ny1\\n...\\nym\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb−\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nˆxT\\n1 ˆw\\n...\\nˆxT\\nm ˆw\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= y −\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nˆxT\\n1\\n...\\nˆxT\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb· ˆw\\n= y −X ˆw\\n所以\\nˆw∗= arg min\\nˆw\\n(y −X ˆw)T(y −X ˆw)\\n3.2.7\\n式(3.10) 的推导\\n将E ˆw = (y −X ˆw)T(y −X ˆw) 展开可得\\nE ˆw = yTy −yTX ˆw −ˆwTXTy + ˆwTXTX ˆw\\n对ˆw 求导可得\\n∂E ˆw\\n∂ˆw = ∂yTy\\n∂ˆw\\n−∂yTX ˆw\\n∂ˆw\\n−∂ˆwTXTy\\n∂ˆw\\n+ ∂ˆwTXTX ˆw\\n∂ˆw\\n由矩阵微分公式∂aTx\\n∂x\\n= ∂xTa\\n∂x\\n= a, ∂xTAx\\n∂x'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='∂ˆw\\n由矩阵微分公式∂aTx\\n∂x\\n= ∂xTa\\n∂x\\n= a, ∂xTAx\\n∂x\\n= (A+AT)x （更多矩阵微分公式可查阅[2]，矩阵微分原理可查阅[3]）可\\n得\\n∂E ˆw\\n∂ˆw = 0 −XTy −XTy + (XTX + XTX) ˆw\\n= 2XT(X ˆw −y)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.2.8\\n式(3.11) 的推导\\n首先铺垫讲解接下来以及后续内容将会用到的多元函数相关基础知识\\n[1]。\\nn 元实值函数：含n 个自变量，值域为实数域R 的函数称为n 元实值函数，记为f(x)，其中x =\\n(x1; x2; ...; xn) 为n 维向量。“西瓜书”和本书中的多元函数未加特殊说明均为实值函数。\\n凸集：设集合D ⊂Rn 为n 维欧式空间中的子集，如果对D 中任意的n 维向量x ∈D 和y ∈D 与\\n任意的α ∈[0, 1]，有\\nαx + (1 −α)y ∈D\\n则称集合D 是凸集。凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点均属于此集\\n合。常见的凸集有空集∅，整个n 维欧式空间Rn。\\n凸函数：设D ⊂Rn 是非空凸集，f 是定义在D 上的函数，如果对任意的x1, x2 ∈D, α ∈(0, 1)，均\\n有\\nf\\n\\x00αx1 + (1 −α)x2\\x01\\n⩽αf(x1) + (1 −α)f(x2)\\n则称f 为D 上的凸函数。若其中的⩽改为< 也恒成立，则称f 为D 上的严格凸函数。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='则称f 为D 上的凸函数。若其中的⩽改为< 也恒成立，则称f 为D 上的严格凸函数。\\n梯度：若n 元函数f(x) 对x = (x1; x2; ...; xn) 中各分量xi 的偏导数∂f(x)\\n∂xi (i = 1, 2, ..., n) 都存在，则\\n称函数f(x) 在x 处一阶可导，并称以下列向量\\n∇f(x) = ∂f(x)\\n∂x\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂f(x)\\n∂x1\\n∂f(x)\\n∂x2...\\n∂f(x)\\n∂xn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n为函数f(x) 在x 处的一阶导数或梯度，易证梯度指向的方向是函数值增大速度最快的方向。∇f(x) 也可\\n写成行向量形式\\n∇f(x) = ∂f(x)\\n∂xT\\n=\\n\\x14∂f(x)\\n∂x1\\n, ∂f(x)\\n∂x2\\n, · · ·, ∂f(x)\\n∂xn\\n\\x15\\n我们称列向量形式为“分母布局”，行向量形式为“分子布局”，由于在最优化中习惯采用分母布局，因此\\n“西瓜书”以及本书中也采用分母布局。为了便于区分当前采用何种布局，通常在采用分母布局时偏导符\\n号∂后接的是x，采用分子布局时后接的是xT。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='号∂后接的是x，采用分子布局时后接的是xT。\\nHessian 矩阵：若n 元函数f(x) 对x = (x1; x2; ...; xn) 中各分量xi 的二阶偏导数\\n∂2f(x)\\n∂xi∂xj (i =\\n1, 2, ..., n; j = 1, 2, ..., n) 都存在，则称函数f(x) 在x 处二阶阶可导，并称以下矩阵\\n∇2f(x) = ∂2f(x)\\n∂x∂xT =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∂2f(x)\\n∂x2\\n1\\n∂2f(x)\\n∂x1∂x2\\n· · ·\\n∂2f(x)\\n∂x1∂xn\\n∂2f(x)\\n∂x2∂x1\\n∂2f(x)\\n∂x2\\n2\\n· · ·\\n∂2f(x)\\n∂x2∂xn\\n...\\n...\\n...\\n...\\n∂2f(x)\\n∂xn∂x1\\n∂2f(x)\\n∂xn∂x2\\n· · ·\\n∂2f(x)\\n∂x2n\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n为函数f(x) 在x 处的二阶导数或Hessian 矩阵。若其中的二阶偏导数均连续，则\\n∂2f(x)\\n∂xi∂xj\\n= ∂2f(x)\\n∂xj∂xi\\n此时Hessian 矩阵为对称矩阵。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='∂2f(x)\\n∂xi∂xj\\n= ∂2f(x)\\n∂xj∂xi\\n此时Hessian 矩阵为对称矩阵。\\n定理3.1：设D ⊂Rn 是非空开凸集，f(x) 是定义在D 上的实值函数，且f(x) 在D 上二阶连续可\\n微，如果f(x) 的Hessian 矩阵∇2f(x) 在D 上是半正定的，则f(x) 是D 上的凸函数；如果∇2f(x) 在\\nD 上是正定的，则f(x) 是D 上的严格凸函数。\\n定理3.2：若f(x) 是凸函数，且f(x) 一阶连续可微，则x∗是全局解的充分必要条件是其梯度等于\\n零向量，即∇f(x∗) = 0。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(3.11) 的推导思路如下：首先根据定理3.1 推导出E ˆw 是ˆw 的凸函数，接着根据定理3.2 推导出\\n式(3.11)。下面按照此思路进行推导。\\n由于式(3.10) 已推导出E ˆw 关于ˆw 的一阶导数，接着基于此进一步推导出二阶导数，即Hessian 矩\\n阵。推导过程如下：\\n∇2E ˆw =\\n∂\\n∂ˆwT\\n\\x12∂E ˆw\\n∂ˆw\\n\\x13\\n=\\n∂\\n∂ˆwT\\n\\x02\\n2XT(X ˆw −y)\\n\\x03\\n=\\n∂\\n∂ˆwT\\n\\x002XTX ˆw −2XTy\\n\\x01\\n由矩阵微分公式∂Ax\\nxT = A 可得\\n∇2E ˆw = 2XTX\\n如“西瓜书”中式(3.11) 上方的一段话所说，假定XTX 为正定矩阵，根据定理3.1 可知此时E ˆw 是\\nˆw 的严格凸函数，接着根据定理3.2 可知只需令E ˆw 关于ˆw 的一阶导数等于零向量，即令式(3.10) 等于\\n零向量即可求得全局最优解ˆw∗，具体求解过程如下：\\n∂E ˆw\\n∂ˆw = 2XT(X ˆw −y) = 0\\n2XTX ˆw −2XTy = 0\\n2XTX ˆw = 2XTy'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2XTX ˆw −2XTy = 0\\n2XTX ˆw = 2XTy\\nˆw = (XTX)−1XTy\\n令其为ˆw∗即为式(3.11)。\\n由于X 是由样本构成的矩阵，而样本是千变万化的，因此无法保证XTX 一定是正定矩阵，极易出现\\n非正定的情形。当XTX 非正定矩阵时，除了“西瓜书”中所说的引入正则化外，也可用XTX 的伪逆矩阵\\n代入式(3.11) 求解出ˆw∗，只是此时并不保证求解得到的ˆw∗一定是全局最优解。除此之外，也可用下一\\n节将会讲到的“梯度下降法”求解，同样也不保证求得全局最优解。\\n3.3\\n对数几率回归\\n对数几率回归的一般使用流程如下：首先在训练集上学得模型\\ny =\\n1\\n1 + e−(wTx+b)\\n然后对于新的测试样本xi，将其代入模型得到预测结果yi，接着自行设定阈值θ，通常设为θ = 0.5，如\\n果yi ⩾θ 则判xi 为正例，反之判为反例。\\n3.3.1\\n式(3.27) 的推导\\n将式(3.26) 代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln (yip1(ˆxi; β) + (1 −yi)p0(ˆxi; β))\\n其中p1(ˆxi; β) =\\neβT ˆxi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='其中p1(ˆxi; β) =\\neβT ˆxi\\n1+eβT ˆxi , p0(ˆxi; β) =\\n1\\n1+eβT ˆxi ，代入上式可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln\\n \\nyieβT ˆxi + 1 −yi\\n1 + eβT ˆxi\\n!\\n=\\nm\\nX\\ni=1\\n\\x10\\nln(yieβT ˆxi + 1 −yi) −ln(1 + eβT ˆxi)\\n\\x11\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由于yi=0 或1，则\\nℓ(β) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1(−ln(1 + eβT ˆxi)),\\nyi = 0\\nPm\\ni=1(βTˆxi −ln(1 + eβT ˆxi)),\\nyi = 1\\n两式综合可得\\nℓ(β) =\\nm\\nX\\ni=1\\n\\x10\\nyiβTˆxi −ln(1 + eβT ˆxi)\\n\\x11\\n由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，即在似然\\n函数前添加负号即可得式(3.27)。值得一提的是，若将式(3.26) 改写为p(yi|xi; w, b) = [p1(ˆxi; β)]yi[p0(ˆxi; β)]1−yi，\\n再代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln\\n\\x00[p1(ˆxi; β)]yi[p0(ˆxi; β)]1−yi\\x01\\n=\\nm\\nX\\ni=1\\n[yi ln (p1(ˆxi; β)) + (1 −yi) ln (p0(ˆxi; β))]\\n=\\nm\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nm\\nX\\ni=1\\n{yi [ln (p1(ˆxi; β)) −ln (p0(ˆxi; β))] + ln (p0(ˆxi; β))}\\n=\\nm\\nX\\ni=1\\n\\x14\\nyi ln\\n\\x12p1(ˆxi; β)\\np0(ˆxi; β)\\n\\x13\\n+ ln (p0(ˆxi; β))\\n\\x15\\n=\\nm\\nX\\ni=1\\n\\x14\\nyi ln\\n\\x10\\neβT ˆxi\\x11\\n+ ln\\n\\x12\\n1\\n1 + eβT ˆxi\\n\\x13\\x15\\n=\\nm\\nX\\ni=1\\n\\x10\\nyiβTˆxi −ln(1 + eβT ˆxi)\\n\\x11\\n显然，此种方式更易推导出式(3.27)。\\n“西瓜书”在式(3.27) 下方有提到式(3.27) 是关于β 的凸函数，其证明过程如下：由于若干半正定矩\\n阵的加和仍为半正定矩阵，则根据定理3.1 可知，若干凸函数的加和仍为凸函数。因此，只需证明式(3.27)\\n求和符号后的式子−yiβTˆxi + ln(1 + eβT ˆxi)（记为f(β)）为凸函数即可。根据式(3.31) 可知，f(β) 的二\\n阶导数，即Hessian 矩阵为\\nˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n对于任意非零向量y ∈Rd+1，恒有'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n对于任意非零向量y ∈Rd+1，恒有\\nyT · ˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β)) · y\\nyTˆxiˆxT\\ni yp1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n\\x00yTˆxi\\n\\x012 p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n由于p1 (ˆxi; β) > 0，因此上式恒大于等于0，根据半正定矩阵的定义可知此时f(β) 的Hessian 矩阵为半\\n正定矩阵，所以f(β) 是关于β 的凸函数。\\n3.3.2\\n梯度下降法\\n不同于式(3.7) 可求得闭式解，式(3.27) 中的β 没有闭式解，因此需要借助其他工具进行求解。求解\\n使得式(3.27) 取到最小值的β 属于最优化中的“无约束优化问题”，在无约束优化问题中最常用的求解算\\n法有“梯度下降法”和“牛顿法”\\n[1]，下面分别展开讲解。\\n梯度下降法是一种迭代求解算法，其基本思路如下：先在定义域中随机选取一个点x0，将其代入函'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='梯度下降法是一种迭代求解算法，其基本思路如下：先在定义域中随机选取一个点x0，将其代入函\\n数f(x) 并判断此时f(x0) 是否是最小值，如果不是的话，则找下一个点x1，且保证f(x1) < f(x0)，然\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n后接着判断f(x1) 是否是最小值，如果不是的话则重复上述步骤继续迭代寻找x2、x3、...... 直到找到使\\n得f(x) 取到最小值的x∗。\\n显然，此算法要想行得通就必须解决在找到第t 个点xt 时，能进一步找到第t + 1 个点xt+1，且保\\n证f(xt+1) < f(xt)。梯度下降法利用“梯度指向的方向是函数值增大速度最快的方向”这一特性，每次迭\\n代时朝着梯度的反方向进行，进而实现函数值越迭代越小，下面给出完整的数学推导过程。\\n根据泰勒公式可知，当函数f(x) 在xt 处一阶可导时，在其邻域内进行一阶泰勒展开恒有\\nf(x) = f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ o\\n\\x00\\r\\rx −xt\\r\\r\\x01\\n其中∇f (xt) 是函数f(x) 在点xt 处的梯度，∥x −xt∥是指向量x −xt 的模。若令x −xt = adt，其中\\na > 0，dt 是模长为1 的单位向量，则上式可改写为\\nf(xt + adt) = f\\n\\x00xt\\x01\\n+ a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\rdt\\r\\r\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='f(xt + adt) = f\\n\\x00xt\\x01\\n+ a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\rdt\\r\\r\\x01\\nf(xt + adt) −f\\n\\x00xt\\x01\\n= a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\rdt\\r\\r\\x01\\n观察上式可知，如果能保证a∇f (xt)\\nT dt < 0，则一定能保证f(xt + adt) < f (xt)，此时再令xt+1 =\\nxt + adt，即可推得我们想要的f(xt+1) < f(xt)。所以，此时问题转化为了求解能使得a∇f (xt)\\nT dt < 0\\n的dt，且a∇f (xt)\\nT dt 比0 越小，相应地f(xt+1) 也会比f(xt) 越小，也更接近最小值。\\n根据向量的内积公式可知\\na∇f\\n\\x00xt\\x01T dt = a × ∥∇f\\n\\x00xt\\x01\\n∥× ∥dt∥× cos θt\\n其中θt 是向量∇f (xt) 与向量dt 之间的夹角。观察上式易知，此时∥∇f (xt) ∥是固定常量，∥dt∥= 1，\\n所以当a 也固定时，取θt = π，即向量dt 与向量∇f (xt) 的方向刚好相反时，上式取到最小值。通常为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='了精简计算步骤，可直接令dt = −∇f (xt)，因此便得到了第t + 1 个点xt+1 的迭代公式\\nxt+1 = xt −a∇f\\n\\x00xt\\x01\\n其中a 也称为“步长”或“学习率”，是需要自行设定的参数，且每次迭代时可取不同值。\\n除了需要解决如何找到xt+1 以外，梯度下降法通常还需要解决如何判断当前点是否使得函数取到了\\n最小值，否则的话迭代过程便可能会无休止进行。常用的做法是预先设定一个极小的阈值ϵ，当某次迭代\\n造成的函数值波动已经小于ϵ 时，即|f(xt+1) −f(xt)| < ϵ，我们便近似地认为此时f(xt+1) 取到了最小\\n值。\\n3.3.3\\n牛顿法\\n同梯度下降法，牛顿法也是一种迭代求解算法，其基本思路和梯度下降法一致，只是在选取第t + 1\\n个点xt+1 时所采用的策略有所不同，即迭代公式不同。梯度下降法每次选取xt+1 时，只要求通过泰勒公\\n式在xt 的邻域内找到一个函数值比其更小的点即可，而牛顿法则期望在此基础之上，xt+1 还必须是xt\\n的邻域内的极小值点。\\n类似一元函数取到极值点的必要条件是一阶导数等于0，多元函数取到极值点的必要条件是其梯度等'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='类似一元函数取到极值点的必要条件是一阶导数等于0，多元函数取到极值点的必要条件是其梯度等\\n于零向量0，为了能求解出xt 的邻域内梯度等于0 的点，需要进行二阶泰勒展开，其展开式如下\\nf(x) = f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ 1\\n2\\n\\x00x −xt\\x01T ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n+ o\\n\\x00\\r\\rx −xt\\r\\r\\x01\\n为了后续计算方便，我们取其近似形式\\nf(x) ≈f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ 1\\n2\\n\\x00x −xt\\x01T ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n首先对上式求导\\n∂f(x)\\n∂x\\n= ∂f (xt)\\n∂x\\n+ ∂∇f (xt)\\nT (x −xt)\\n∂x\\n+ 1\\n2\\n∂(x −xt)\\nT ∇2f (xt) (x −xt)\\n∂x\\n= 0 + ∇f\\n\\x00xt\\x01\\n+ 1\\n2\\n\\x10\\n∇2f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01T\\x11 \\x00x −xt\\x01\\n假设函数f(x) 在xt 处二阶可导，且偏导数连续，则∇2f (xt) 是对称矩阵，上式可写为\\n∂f(x)\\n∂x\\n= 0 + ∇f\\n\\x00xt\\x01\\n+ 1\\n2 × 2 × ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n= ∇f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n令上式等于0\\n∇f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n= 0\\n当∇2f (xt) 是可逆矩阵时，解得\\nx = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01\\n令上式为xt+1 即可得到牛顿法的迭代公式\\nxt+1 = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='xt+1 = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01\\n通过上述推导可知，牛顿法每次迭代时需要求解Hessian 矩阵的逆矩阵，该步骤计算量通常较大，因此有\\n人基于牛顿法，将其中求Hessian 矩阵的逆矩阵改为求计算量更低的近似逆矩阵，我们称此类算法为“拟\\n牛顿法”。\\n牛顿法虽然期望在每次迭代时能取到极小值点，但是通过上述推导可知，迭代公式是根据极值点的必\\n要条件推导而得，因此并不保证一定是极小值点。\\n无论是梯度下降法还是牛顿法，根据其终止迭代的条件可知，其都是近似求解算法，即使f(x) 是凸\\n函数，也并不一定保证最终求得的是全局最优解，仅能保证其接近全局最优解。不过在解决实际问题时，\\n并不一定苛求解得全局最优解，在能接近全局最优甚至局部最优时通常也能很好地解决问题。\\n3.3.4\\n式(3.29) 的解释\\n根据上述牛顿法的迭代公式可知，此式为式(3.27) 应用牛顿法时的迭代公式。\\n3.3.5\\n式(3.30) 的推导\\n∂ℓ(β)\\n∂β\\n=\\n∂Pm\\ni=1\\n\\x10\\n−yiβTˆxi + ln\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\x11\\n∂β\\n=\\nm\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed∂'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='−yiβTˆxi + ln\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\x11\\n∂β\\n=\\nm\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed∂\\n\\x00−yiβTˆxi\\n\\x01\\n∂β\\n+\\n∂ln\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n∂β\\n\\uf8f6\\n\\uf8f8\\n=\\nm\\nX\\ni=1\\n\\x12\\n−yiˆxi +\\n1\\n1 + eβT ˆxi · ˆxieβT ˆxi\\n\\x13\\n= −\\nm\\nX\\ni=1\\nˆxi\\n \\nyi −\\neβT ˆxi\\n1 + eβT ˆxi\\n!\\n= −\\nm\\nX\\ni=1\\nˆxi (yi −p1 (ˆxi; β))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此式也可以进行向量化，令p1(ˆxi; β) = ˆyi，代入上式得\\n∂ℓ(β)\\n∂β\\n= −\\nm\\nX\\ni=1\\nˆxi(yi −ˆyi)\\n=\\nm\\nX\\ni=1\\nˆxi(ˆyi −yi)\\n= XT(ˆy −y)\\n其中ˆy = (ˆy1; ˆy2; ...; ˆym), y = (y1; y2; ...; ym)。\\n3.3.6\\n式(3.31) 的推导\\n继续对上述式(3.30) 中倒数第二个等号的结果求导\\n∂2ℓ(β)\\n∂β∂βT = −\\n∂Pm\\ni=1 ˆxi\\n\\x10\\nyi −\\neβT ˆxi\\n1+eT ˆxi\\n\\x11\\n∂βT\\n= −\\nm\\nX\\ni=1\\nˆxi\\n∂\\n\\x10\\nyi −\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n= −\\nm\\nX\\ni=1\\nˆxi\\n\\uf8eb\\n\\uf8ed∂yi\\n∂βT −\\n∂\\n\\x10\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n\\uf8f6\\n\\uf8f8\\n=\\nm\\nX\\ni=1\\nˆxi ·\\n∂\\n\\x10\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n根据矩阵微分公式∂aTx\\n∂xT = ∂xTa\\n∂xT = aT，其中\\n∂\\n\\x10\\neβT ˆxi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='根据矩阵微分公式∂aTx\\n∂xT = ∂xTa\\n∂xT = aT，其中\\n∂\\n\\x10\\neβT ˆxi\\n1+eβT ˆxi\\n\\x11\\n∂βT\\n=\\n∂eβT ˆxi\\n∂βT\\n·\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n−eβT ˆxi ·\\n∂\\n(\\n1+eβT ˆxi\\n)\\n∂βT\\n\\x001 + eβT ˆxi\\x012\\n=\\nˆxT\\ni eβT ˆxi ·\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n−eβT ˆxi · ˆxT\\ni eβT ˆxi\\n\\x001 + eβT ˆxi\\x012\\n= ˆxT\\ni eβT ˆxi ·\\n\\x10\\n1 + eβT ˆxi\\n\\x11\\n−eβT ˆxi\\n\\x001 + eβT ˆxi\\x012\\n= ˆxT\\ni eβT ˆxi ·\\n1\\n\\x001 + eβT ˆxi\\x012\\n= ˆxT\\ni ·\\neβT ˆxi\\n1 + eβT ˆxi ·\\n1\\n1 + eβT ˆxi\\n所以\\n∂2ℓ(β)\\n∂β∂βT =\\nm\\nX\\ni=1\\nˆxi · ˆxT\\ni ·\\neβT ˆxi\\n1 + eβT ˆxi ·\\n1\\n1 + eβT ˆxi\\n=\\nm\\nX\\ni=1\\nˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n3.4\\n线性判别分析'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='ˆxiˆxT\\ni p1 (ˆxi; β) (1 −p1 (ˆxi; β))\\n3.4\\n线性判别分析\\n线性判别分析的一般使用流程如下：首先在训练集上学得模型\\ny = wTx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由向量内积的几何意义可知，y 可以看作是x 在w 上的投影，因此在训练集上学得的模型能够保证训练\\n集中的同类样本在w 上的投影y 很相近，而异类样本在w 上的投影y 很疏远。然后对于新的测试样本\\nxi，将其代入模型得到它在w 上的投影yi，然后判别这个投影yi 与哪一类投影更近，则将其判为该类。\\n最后，线性判别分析也是一种降维方法，但不同于第10 章介绍的无监督降维方法，线性判别分析是\\n一种监督降维方法，即降维过程中需要用到样本类别标记信息。\\n3.4.1\\n式(3.32) 的推导\\n式(3.32) 中∥wTµ0 −wTµ1∥2\\n2 左下角的“2”表示求“2 范数”，向量的2 范数即为模，右上角的“2”\\n表示求平方数，基于此，下面推导式(3.32)。\\nJ = ∥wTµ0 −wTµ1∥2\\n2\\nwT(Σ0 + Σ1)w\\n= ∥(wTµ0 −wTµ1)T∥2\\n2\\nwT(Σ0 + Σ1)w\\n= ∥(µ0 −µ1)Tw∥2\\n2\\nwT(Σ0 + Σ1)w\\n=\\n\\x02\\n(µ0 −µ1)Tw\\n\\x03T (µ0 −µ1)Tw'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2\\nwT(Σ0 + Σ1)w\\n=\\n\\x02\\n(µ0 −µ1)Tw\\n\\x03T (µ0 −µ1)Tw\\nwT(Σ0 + Σ1)w\\n= wT(µ0 −µ1)(µ0 −µ1)Tw\\nwT(Σ0 + Σ1)w\\n3.4.2\\n式(3.37) 到式(3.39) 的推导\\n由式(3.36)，可定义拉格朗日函数为\\nL(w, λ) = −wTSbw + λ(wTSww −1)\\n对w 求偏导可得\\n∂L(w, λ)\\n∂w\\n= −∂(wTSbw)\\n∂w\\n+ λ∂(wTSww −1)\\n∂w\\n= −(Sb + ST\\nb )w + λ(Sw + ST\\nw)w\\n由于Sb = ST\\nb , Sw = ST\\nw，所以\\n∂L(w, λ)\\n∂w\\n= −2Sbw + 2λSww\\n令上式等于0 即可得\\n−2Sbw + 2λSww = 0\\nSbw = λSww\\n(µ0 −µ1)(µ0 −µ1)Tw = λSww\\n若令(µ0 −µ1)Tw = γ，则有\\nγ(µ0 −µ1) = λSww\\nw = γ\\nλS−1\\nw (µ0 −µ1)\\n由于最终要求解的w 不关心其大小，只关心其方向，所以其大小可以任意取值。又因为µ0 和µ1 的大小'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='由于最终要求解的w 不关心其大小，只关心其方向，所以其大小可以任意取值。又因为µ0 和µ1 的大小\\n是固定的，所以γ 的大小只受w 的大小影响，因此可以通过调整w 的大小使得γ = λ，西瓜书中所说的\\n“不妨令Sbw = λ(µ0 −µ1)”也可等价理解为令γ = λ，因此，此时γ\\nλ = 1，求解出的w 即为式(3.39)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.4.3\\n式(3.43) 的推导\\n由式(3.40)、式(3.41)、式(3.42) 可得\\nSb = St −Sw\\n=\\nm\\nX\\ni=1\\n(xi −µ)(xi −µ)T −\\nN\\nX\\ni=1\\nX\\nx∈Xi\\n(x −µi)(x −µi)T\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00(x −µ)(x −µ)T −(x −µi)(x −µi)T\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00(x −µ)(xT −µT) −(x −µi)(xT −µT\\ni )\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00xxT −xµT −µxT + µµT −xxT + xµT\\ni + µixT −µiµT\\ni\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00−xµT −µxT + µµT + xµT\\ni + µixT −µiµT\\ni\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n \\n−\\nX\\nx∈Xi\\nxµT −\\nX\\nx∈Xi\\nµxT +\\nX\\nx∈Xi\\nµµT +\\nX\\nx∈Xi\\nxµT\\ni +\\nX\\nx∈Xi\\nµixT −\\nX\\nx∈Xi\\nµiµT\\ni\\n!\\n=\\nN\\nX'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='x∈Xi\\nxµT\\ni +\\nX\\nx∈Xi\\nµixT −\\nX\\nx∈Xi\\nµiµT\\ni\\n!\\n=\\nN\\nX\\ni=1\\n\\x00−miµiµT −miµµT\\ni + miµµT + miµiµT\\ni + miµiµT\\ni −miµiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\n\\x00−miµiµT −miµµT\\ni + miµµT + miµiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\nmi\\n\\x00−µiµT −µµT\\ni + µµT + µiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\nmi(µi −µ)(µi −µ)T\\n3.4.4\\n式(3.44) 的推导\\n此式是式(3.35) 的推广形式，证明如下。\\n设W = (w1, w2, ..., wi, ..., wN−1) ∈Rd×(N−1)，其中wi ∈Rd×1 为d 行1 列的列向量，则\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\ntr(WTSbW) =\\nN−1\\nX\\ni=1\\nwT\\ni Sbwi\\ntr(WTSwW) =\\nN−1\\nX\\ni=1\\nwT\\ni Swwi\\n所以式(3.44) 可变形为\\nmax\\nW\\nPN−1\\ni=1 wT\\ni Sbwi\\nPN−1\\ni=1 wT\\ni Swwi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='max\\nW\\nPN−1\\ni=1 wT\\ni Sbwi\\nPN−1\\ni=1 wT\\ni Swwi\\n对比式(3.35) 易知，上式即式(3.35) 的推广形式。\\n除了式(3.35) 以外，还有一种常见的优化目标形式如下\\nmax\\nW\\nQN−1\\ni=1 wT\\ni Sbwi\\nQN−1\\ni=1 wT\\ni Swwi\\n= max\\nW\\nN−1\\nY\\ni=1\\nwT\\ni Sbwi\\nwT\\ni Swwi\\n无论是采用何种优化目标形式，其优化目标只要满足“同类样例的投影点尽可能接近，异类样例的投\\n影点尽可能远离”即可。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n3.4.5\\n式(3.45) 的推导\\n同式(3.35)，此处也固定式(3.44) 的分母为1，那么式(3.44) 此时等价于如下优化问题\\nmin\\nw\\n−tr(WTSbW)\\ns.t.\\ntr(WTSwW) = 1\\n根据拉格朗日乘子法，可定义上述优化问题的拉格朗日函数\\nL(W, λ) = −tr(WTSbW) + λ(tr(WTSwW) −1)\\n根据矩阵微分公式\\n∂\\n∂X tr (XTBX) = (B + BT)X 对上式关于W 求偏导可得\\n∂L(W, λ)\\n∂W\\n= −∂\\n\\x00tr(WTSbW)\\n\\x01\\n∂W\\n+ λ∂\\n\\x00tr(WTSwW) −1\\n\\x01\\n∂W\\n= −(Sb + ST\\nb )W + λ(Sw + ST\\nw)W\\n由于Sb = ST\\nb , Sw = ST\\nw，所以\\n∂L(W, λ)\\n∂W\\n= −2SbW + 2λSwW\\n令上式等于0 即可得\\n−2SbW + 2λSwW = 0\\nSbW = λSwW\\n此即为式(3.45)，但是此式在解释为何要取N −1 个最大广义特征值所对应的特征向量来构成W 时不够'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='直观。因此，我们换一种更为直观的方式求解式(3.44)，只需换一种方式构造拉格朗日函数即可。\\n重新定义上述优化问题的拉格朗日函数\\nL(W, Λ) = −tr(WTSbW) + tr\\n\\x00Λ(WTSwW −I)\\n\\x01\\n其中，I ∈R(N−1)×(N−1) 为单位矩阵，Λ = diag(λ1, λ2, ..., λN−1) ∈R(N−1)×(N−1) 是由N −1 个拉格朗日\\n乘子构成的对角矩阵。根据矩阵微分公式\\n∂\\n∂X tr(XTAX) = (A + AT)X,\\n∂\\n∂X tr(XAXTB) =\\n∂\\n∂X tr(AXTBX) =\\nBTXAT + BXA，对上式关于W 求偏导可得\\n∂L(W, Λ)\\n∂W\\n= −∂\\n\\x00tr(WTSbW)\\n\\x01\\n∂W\\n+ ∂\\n\\x00tr\\n\\x00ΛWTSwW −ΛI\\n\\x01\\x01\\n∂W\\n= −(Sb + ST\\nb )W + (ST\\nwWΛT + SwWΛ)\\n由于Sb = ST\\nb , Sw = ST\\nw, ΛT = Λ，所以\\n∂L(W, Λ)\\n∂W\\n= −2SbW + 2SwWΛ\\n令上式等于0 即可得\\n−2SbW + 2SwWΛ = 0\\nSbW = SwWΛ\\n将W 和Λ 展开可得'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='−2SbW + 2SwWΛ = 0\\nSbW = SwWΛ\\n将W 和Λ 展开可得\\nSbwi = λiSwwi,\\ni = 1, 2, ..., N −1\\n此时便得到了N −1 个广义征值问题。进一步地，将其代入优化问题的目标函数可得\\nmin\\nw\\n−tr(WTSbW) = max\\nW tr(WTSbW)\\n= max\\nW\\nN−1\\nX\\ni=1\\nwT\\ni Sbwi\\n= max\\nW\\nN−1\\nX\\ni=1\\nλiwT\\ni Swwi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由于存在约束tr(WTSwW) =\\nN−1\\nP\\ni=1\\nwT\\ni Swwi = 1，所以欲使上式取到最大值，只需取N −1 个最大的λi 即\\n可。根据Sbwi = λiSwwi 可知，λi 对应的便是广义特征值，wi 是λi 所对应的特征向量。\\n（广义特征值的定义和常用求解方法可查阅[3]）\\n对于N 分类问题，一定要求出N −1 个wi 吗？其实不然。之所以将W 定义为d × (N −1) 维的矩\\n阵是因为当d > (N −1) 时，实对称矩阵S−1\\nw Sb 的秩至多为N −1，所以理论上至多能解出N −1 个非零\\n特征值λi 及其对应的特征向量wi。但是S−1\\nw Sb 的秩是受当前训练集中的数据分布所影响的，因此并不一\\n定为N −1。此外，当数据分布本身就足够理想时，即使能求解出多个wi，但是实际可能只需要求解出1\\n个wi 便可将同类样本聚集，异类样本完全分离。\\n当d > (N −1) 时，实对称矩阵S−1\\nw Sb 的秩至多为N −1 的证明过程如下：由于µ =\\n1\\nN\\nNP\\ni=1\\nmiµi，所'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='w Sb 的秩至多为N −1 的证明过程如下：由于µ =\\n1\\nN\\nNP\\ni=1\\nmiµi，所\\n以µ1 −µ 一定可以由µ 和µ2, ..., µN 线性表示，因此矩阵Sb 中至多有µ2 −µ, ..., µN −µ 共N −1 个\\n线性无关的向量，由于此时d > (N −1)，所以Sb 的秩r(Sb) 至多为N −1。同时假设矩阵Sw 满秩，即\\nr(Sw) = r(S−1\\nw ) = d，则根据矩阵秩的性质r(AB) ⩽min{r(A), r(B)} 可知，S−1\\nw Sb 的秩也至多为N −1。\\n3.5\\n多分类学习\\n3.5.1\\n图3.5 的解释\\n图3.5 中所说的“海明距离”是指两个码对应位置不相同的个数，“欧式距离”则是指两个向量之间\\n的欧氏距离，例如图3.5(a) 中第1 行的编码可以视作为向量(−1, +1, −1, +1, +1)，测试示例的编码则为\\n(−1, −1, +1, −1, +1)，其中第2 个、第3 个、第4 个元素不相同，所以它们的海明距离为3，欧氏距离为\\np'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='p\\n(−1 −(−1))2 + (1 −(−1))2 + (−1 −1)2 + (1 −(−1))2 + (1 −1)2 = √0 + 4 + 4 + 4 + 0 = 2\\n√\\n3。需要注\\n意的是，在计算海明距离时，与“停用类”不同算作0.5，例如图3.5(b) 中第2 行的海明距离计算公式为\\n0.5 + 0.5 + 0.5 + 0.5 = 2。\\n3.6\\n类别不平衡问题\\n对于类别平衡问题，“西瓜书”2.3.1 节中的“精度”通常无法满足该特殊任务的需求，例如“西瓜书”\\n在本节第一段的举例：有998 个反例和2 个正例，若机器学习算法返回一个永远将新样本预测为反例的\\n学习器则能达到99.8% 的精度，显然虚高，因此在类别不平衡时常采用2.3 节中的查准率、查全率和F1\\n来度量学习器的性能。\\n参考文献\\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\\n[2] Wikipedia contributors. Matrix calculus, 2022.\\n[3] 张贤达. 矩阵分析与应用. 第2 版. 清华大学出版社, 2013.\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='[3] 张贤达. 矩阵分析与应用. 第2 版. 清华大学出版社, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第4 章\\n决策树\\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\\n算法会更为通俗易懂。\\n4.1\\n基本流程\\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行），即“if⋯⋯elif⋯⋯\\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜），那么此时划分的目的\\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性），\\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”，图4.2 第14 行的A\\\\{a∗}\\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\\n本（例如仍然同时含有好瓜和坏瓜），但是因已无属性可用作划分依据，此时只能少数服从多数，以此子'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='本（例如仍然同时含有好瓜和坏瓜），但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\\n归返回的情形(2)；\\n（3）根据某个属性进行划分时，若该属性多个属性值中的某个属性值不包含任何样本（例如未收集到），\\n例如对当前子集以“纹理”属性来划分，“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\\n能会有样本落入该分支。此即为递归返回的情形(3)。\\n4.2\\n划分选择'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='能会有样本落入该分支。此即为递归返回的情形(3)。\\n4.2\\n划分选择\\n本节介绍的三种划分选择方法，即信息增益、增益率、基尼指数分别对应著名的ID3、C4.5 和CART\\n三种决策树算法。\\n4.2.1\\n式(4.1) 的解释\\n该式为信息论中的信息熵定义式，以下先证明0 ⩽Ent(D) ⩽log2 |Y|，然后解释其最大值和最小值所\\n表示的含义。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n已知集合D 的信息熵的定义为\\nEnt(D) = −\\n|Y|\\nX\\nk=1\\npk log2 pk\\n其中，|Y| 表示样本类别总数，pk 表示第k 类样本所占的比例，有0 ⩽pk ⩽1, Pn\\nk=1 pk = 1。若令\\n|Y| = n, pk = xk，那么信息熵Ent(D) 就可以看作一个n 元实值函数，即\\nEnt(D) = f(x1, · · · , xn) = −\\nn\\nX\\nk=1\\nxk log2 xk\\n其中0 ⩽xk ⩽1, Pn\\nk=1 xk = 1。\\n下面考虑求该多元函数的最值.\\n首先我们先来求最大值，如果不考虑约束0 ⩽xk ⩽1 而仅考虑\\nPn\\nk=1 xk = 1，则对f(x1, · · · , xn) 求最大值等价于如下最小化问题：\\nmin\\nnP\\nk=1\\nxk log2 xk\\ns.t.\\nnP\\nk=1\\nxk = 1\\n显然，在0 ⩽xk ⩽1 时，此问题为凸优化问题。对于凸优化问题来说，使其拉格朗日函数的一阶偏导数等\\n于0 的点即最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='于0 的点即最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为\\nL(x1, · · · , xn, λ) =\\nn\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!\\n其中，λ 为拉格朗日乘子。对L(x1, · · · , xn, λ) 分别关于x1, · · · , xn, λ 求一阶偏导数，并令偏导数等于0\\n可得\\n∂L(x1, · · · , xn, λ)\\n∂x1\\n=\\n∂\\n∂x1\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n= log2 x1 + x1 ·\\n1\\nx1 ln 2 + λ = 0\\n= log2 x1 +\\n1\\nln 2 + λ = 0\\n⇒λ = −log2 x1 −\\n1\\nln 2\\n∂L(x1, · · · , xn, λ)\\n∂x2\\n=\\n∂\\n∂x2\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒λ = −log2 x2 −\\n1\\nln 2\\n· · ·\\n∂L(x1, · · · , xn, λ)\\n∂xn\\n=\\n∂\\n∂xn\\n\" n\\nX\\nk=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='· · ·\\n∂L(x1, · · · , xn, λ)\\n∂xn\\n=\\n∂\\n∂xn\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒λ = −log2 xn −\\n1\\nln 2;\\n∂L(x1, · · · , xn, λ)\\n∂λ\\n= ∂\\n∂λ\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒\\nn\\nX\\nk=1\\nxk = 1\\n整理一下可得\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nλ = −log2 x1 −\\n1\\nln 2 = −log2 x2 −\\n1\\nln 2 = · · · = −log2 xn −\\n1\\nln 2\\nnP\\nk=1\\nxk = 1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由以上两个方程可以解得\\nx1 = x2 = · · · = xn = 1\\nn\\n又因为xk 还需满足约束0 ⩽xk ⩽1，显然0 ⩽1\\nn ⩽1，所以x1 = x2 = · · · = xn = 1\\nn 是满足所有约束的\\n最优解，即当前最小化问题的最小值点，同时也是f(x1, · · · , xn) 的最大值点。将x1 = x2 = · · · = xn = 1\\nn\\n代入f(x1, · · · , xn) 中可得\\nf\\n\\x12 1\\nn, · · · , 1\\nn\\n\\x13\\n= −\\nn\\nX\\nk=1\\n1\\nn log2\\n1\\nn = −n · 1\\nn log2\\n1\\nn = log2 n\\n所以f(x1, · · · , xn) 在满足约束0 ⩽xk ⩽1, Pn\\nk=1 xk = 1 时的最大值为log2 n。\\n下面求最小值。如果不考虑约束Pn\\nk=1 xk = 1 而仅考虑0 ⩽xk ⩽1，则f(x1, · · · , xn) 可以看作n 个\\n互不相关的一元函数的和，即\\nf(x1, · · · , xn) =\\nn\\nX\\nk=1\\ng(xk)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='互不相关的一元函数的和，即\\nf(x1, · · · , xn) =\\nn\\nX\\nk=1\\ng(xk)\\n其中，g(xk) = −xk log2 xk, 0 ⩽xk ⩽1。那么当g(x1), g(x2), · · · , g(xn) 分别取到其最小值时，f(x1, · · · , xn)\\n也就取到了最小值，所以接下来考虑分别求g(x1), g(x2), · · · , g(xn) 各自的最小值。\\n由于g(x1), g(x2), · · · , g(xn) 的定义域和函数表达式均相同，所以只需求出g(x1) 的最小值也就求出\\n了g(x2), · · · , g(xn) 的最小值。下面考虑求g(x1) 的最小值，首先对g(x1) 关于x1 求一阶和二阶导数，有\\ng′(x1) = d(−x1 log2 x1)\\ndx1\\n= −log2 x1 −x1 ·\\n1\\nx1 ln 2 = −log2 x1 −\\n1\\nln 2\\ng′′(x1) = d (g′(x1))\\ndx1\\n=\\nd\\n\\x12\\n−log2 x1 −\\n1\\nln 2\\n\\x13\\ndx1\\n= −\\n1\\nx1 ln 2\\n显然，当0 ⩽xk ⩽1 时g′′(x1) = −\\n1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x13\\ndx1\\n= −\\n1\\nx1 ln 2\\n显然，当0 ⩽xk ⩽1 时g′′(x1) = −\\n1\\nx1 ln 2 恒小于0，所以g(x1) 是一个在其定义域范围内开口向下的凹函\\n数，那么其最小值必然在边界取。分别取x1 = 0 和x1 = 1，代入g(x1) 可得\\ng(0) = −0 log2 0 = 0\\ng(1) = −1 log2 1 = 0\\n（计算信息熵时约定：若x = 0，则x log2 x = 0）所以，g(x1) 的最小值为0，同理可得g(x2), · · · , g(xn)\\n的最小值也都为0，即f(x1, · · · , xn) 的最小值为0。但是，此时仅考虑约束0 ⩽xk ⩽1，而未考虑\\nPn\\nk=1 xk = 1。若考虑约束Pn\\nk=1 xk = 1，那么f(x1, · · · , xn) 的最小值一定大于等于0。如果令某个\\nxk = 1，那么根据约束Pn\\nk=1 xk = 1 可知x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0，将其代入\\nf(x1, · · · , xn) 可得'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='f(x1, · · · , xn) 可得\\nf(0, 0, · · · , 0, 1, 0, · · · , 0)\\n= −0 log2 0 −0 log2 0 −· · · −0 log2 0 −1 log2 1 −0 log2 0 −· · · −0 log2 0 = 0\\n所以xk = 1, x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0 一定是f(x1, · · · , xn) 在满足约束Pn\\nk=1 xk = 1\\n和0 ⩽xk ⩽1 的条件下的最小值点，此时f 取到最小值0。\\n综上可知，当f(x1, · · · , xn) 取到最大值时：x1 = x2 = · · · = xn = 1\\nn，此时样本集合纯度最低；当\\nf(x1, · · · , xn) 取到最小值时：xk = 1, x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0，此时样本集合纯度最\\n高。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n4.2.2\\n式(4.2) 的解释\\n此为信息增益的定义式。在信息论中信息增益也称为“互信息”，表示已知一个随机变量的信息后另\\n一个随机变量的不确定性减少的程度。\\n下面给出互信息的定义，在此之前，还需要先解释一下什么是“条件熵”。条件熵表示的是在已知一\\n个随机变量的条件下，另一个随机变量的不确定性。具体地，假设有随机变量X 和Y ，且它们服从以下\\n联合概率分布\\nP(X = xi, Y = yj) = pij,\\ni = 1, 2, · · · , n,\\nj = 1, 2, · · · , m\\n那么在已知X 的条件下，随机变量Y 的条件熵为\\nEnt(Y |X) =\\nn\\nX\\ni=1\\npi Ent(Y |X = xi)\\n其中，pi = P(X = xi)\\uffffi = 1, 2, · · · , n。互信息定义为信息熵和条件熵的差，它表示的是已知一个随机变量\\n的信息后使得另一个随机变量的不确定性减少的程度。具体地，假设有随机变量X 和Y ，那么在已知X\\n的信息后，Y 的不确定性减少的程度为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的信息后，Y 的不确定性减少的程度为\\nI(Y ; X) = Ent(Y ) −Ent(Y |X)\\n此即互信息的数学定义。\\n所以式(4.2) 可以理解为，在已知属性a 的取值后，样本类别这个随机变量的不确定性减小的程度。\\n若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，即\\n“西瓜书”上所说的“纯度提升”越大。\\n4.2.3\\n式(4.4) 的解释\\n为了理解该式的“固有值”的概念，可以将式(4.4) 与式(4.1) 对比理解。式(4.1) 可重写为\\nEnt(D) = −\\n|Y|\\nX\\nk=1\\npk log2 pk = −\\n|Y|\\nX\\nk=1\\n\\x0c\\x0cDk\\x0c\\x0c\\n|D| log2\\n\\x0c\\x0cDk\\x0c\\x0c\\n|D|\\n其中|Dk|\\n|D| = pk，为第k 类样本所占的比例。与式(4.4) 的表达式作一下对比\\nIV(a) = −\\nV\\nX\\nv=1\\n|Dv|\\n|D| log2\\n|Dv|\\n|D|\\n其中|Dv|\\n|D| = pv，为属性a 取值为av 的样本所占的比例。即式(4.1) 是按样本的类别标记计算的信息熵，\\n而式(4.4) 是按样本属性的取值计算的信息熵。\\n4.2.4'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='而式(4.4) 是按样本属性的取值计算的信息熵。\\n4.2.4\\n式(4.5) 的推导\\n假设数据集D 中的样例标记种类共有三类，每类样本所占比例分别为p1、p2、p3。现从数据集中随\\n机抽取两个样本，两个样本类别标记正好一致的概率为\\np1p1 + p2p2 + p3p3 =\\n|Y|=3\\nX\\nk=1\\np2\\nk\\n两个样本类别标记不一致的概率为（即“基尼值”）\\nGini(D) = p1p2 + p1p3 + p2p1 + p2p3 + p3p1 + p3p2 =\\n|Y|=3\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n易证以上两式之和等于1，证明过程如下\\n|Y|=3\\nX\\nk=1\\np2\\nk +\\n|Y|=3\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′\\n= (p1p1 + p2p2 + p3p3) + (p1p2 + p1p3 + p2p1 + p2p3 + p3p1 + p3p2)\\n= (p1p1 + p1p2 + p1p3) + (p2p1 + p2p2 + p2p3) + (p3p1 + p3p2 + p3p3)\\n=p1 (p1 + p2 + p3) + p2 (p1 + p2 + p3) + p3 (p1 + p2 + p3)\\n=p1 + p2 + p3 = 1\\n所以可进一步推得式(4.5)\\nGini(D) =\\n|Y|\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′ = 1 −\\n|Y|\\nX\\nk=1\\np2\\nk\\n从数据集中D 任取两个样本，类别标记一致的概率越大表示其纯度越高（即大部分样本属于同一类），\\n类别标记不一致的概率（即基尼值）越大表示纯度越低。\\n4.2.5\\n式(4.6) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='类别标记不一致的概率（即基尼值）越大表示纯度越低。\\n4.2.5\\n式(4.6) 的解释\\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\\n分属性的最优划分点。常用的CART 决策树的构造算法如下\\n[1]：\\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\\nGini_index(D, a) = |Da=v|\\n|D|\\nGini(Da=v) + |Da̸=v|\\n|D|\\nGini(Da̸=v)\\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\\n(3) 重复以上两步，直至满足停止条件。\\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\\n6，反例占\\np2 = 3\\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\\n5\\n11，反例占\\np2 =\\n6\\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\\nGini_index(D, 色泽= 青绿)\\n= 6\\n17 ×\\n \\n1 −\\n\\x123\\n6\\n\\x132\\n−\\n\\x123\\n6\\n\\x132!\\n+ 11\\n17 ×\\n \\n1 −\\n\\x12 5\\n11\\n\\x132\\n−\\n\\x12 6\\n11\\n\\x132!\\n= 0.497\\n类似地，可以计算出不同属性取不同值的基尼指数如下：'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x132\\n−\\n\\x12 6\\n11\\n\\x132!\\n= 0.497\\n类似地，可以计算出不同属性取不同值的基尼指数如下：\\nGini_index(D, 色泽= 乌黑) = 0.456\\nGini_index(D, 色泽= 浅白) = 0.426\\nGini_index(D, 根蒂= 蜷缩) = 0.456\\nGini_index(D, 根蒂= 稍蜷) = 0.496\\nGini_index(D, 根蒂= 硬挺) = 0.439\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nGini_index(D, 敲声= 浊响) = 0.450\\nGini_index(D, 敲声= 沉闷) = 0.494\\nGini_index(D, 敲声= 清脆) = 0.439\\nGini_index(D, 纹理= 清晰) = 0.286\\nGini_index(D, 纹理= 稍稀) = 0.437\\nGini_index(D, 纹理= 模糊) = 0.403\\nGini_index(D, 脐部= 凹陷) = 0.415\\nGini_index(D, 脐部= 稍凹) = 0.497\\nGini_index(D, 脐部= 平坦) = 0.362\\nGini_index(D, 触感= 硬挺) = 0.494\\nGini_index(D, 触感= 软粘) = 0.494\\n特别地，对于属性“触感”，由于它的可取值个数为2，所以其实只需计算其中一个取值的基尼指数即可。\\n根据上面的计算结果可知，Gini_index(D, 纹理= 清晰) = 0.286 最小，所以选择属性“纹理”为最优'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='划分属性并生成根节点，接着以“纹理= 清晰”为最优划分点生成D1(纹理= 清晰)、D2(纹理̸= 清晰)\\n两个子节点，对两个子节点分别重复上述步骤继续生成下一层子节点，直至满足停止条件。\\n以上便是CART 决策树的构建过程，从构建过程可以看出，CART 决策树最终构造出来的是一棵二\\n叉树。CART 除了决策树能处理分类问题以外，回归树还可以处理回归问题，下面给出CART 回归树的\\n构造算法。\\n假设给定数据集\\nD = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中x ∈Rd 为d 维特征向量，y ∈R 是连续型随机变量。这是一个标准的回归问题的数据集, 若把每个\\n属性视为坐标空间中的一个坐标轴，则d 个属性就构成了一个d 维的特征空间，而每个d 维特征向量x\\n就对应了d 维的特征空间中的一个数据点。CART 回归树的目标是将特征空间划分成若干个子空间，每\\n个子空间都有一个固定的输出值，也就是凡是落在同一个子空间内的数据点xi，它们所对应的输出值yi\\n恒相等，且都为该子空间的输出值。\\n那么如何划分出若干个子空间呢？这里采用一种启发式的方法。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='恒相等，且都为该子空间的输出值。\\n那么如何划分出若干个子空间呢？这里采用一种启发式的方法。\\n(1) 任意选择一个属性a，遍历其所有可能取值，根据下式找出属性a 最优划分点v∗：\\nv∗= arg min\\nv\\n\\uf8ee\\n\\uf8f0min\\nc1\\nX\\nxi∈R1(a,v)\\n(yi −c1)2 + min\\nc2\\nX\\nxi∈R2(a,v)\\n(yi −c2)2\\n\\uf8f9\\n\\uf8fb\\n其中，R1(a, v) = {x|x ∈Da⩽v}, R2(a, v) = {x|x ∈Da>v}，c1 和c2 分别为集合R1(a, v) 和\\nR2(a, v) 中的样本xi 对应的输出值yi 的均值，即\\nc1 = ave(yi|x ∈R1(a, v)) =\\n1\\n|R1(a, v)|\\nX\\nxi∈R1(a,v)\\nyi\\nc2 = ave(yi|x ∈R2(a, v)) =\\n1\\n|R2(a, v)|\\nX\\nxi∈R2(a,v)\\nyi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\\nf(x) =\\nM\\nX\\nm=1\\ncmI(x ∈Rm)\\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\\n值作为该样本的预测值yi。\\n4.3\\n剪枝处理\\n本节内容通俗易懂，跟着“西瓜书”中的例子动手演算即可，无需做过多解释。以下仅结合图4.5 继\\n续讨论一下图4.2 中的递归返回条件。图4.5 与图4.4 均是基于信息增益生成的决策树，不同在于图4.4\\n基于表4.1，而图4.5 基于表4.2 的训练集。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='基于表4.1，而图4.5 基于表4.2 的训练集。\\n结点3⃝包含训练集“脐部”为稍凹的样本（编号6、7、15、17），当根据“根蒂”再次进行划分时不\\n含有“根蒂”为硬挺的样本（递归返回情形(3)），而恰巧四个样本（编号6、7、15、17）含两个好瓜和两\\n个坏瓜，因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。\\n结点5⃝包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本（编号6、7、15），当根据“色泽”再次\\n进行划分时不含有“色泽”为浅白的样本（递归返回情形(3)），因此叶结点浅白类别标记为好瓜（编号6、\\n7、15 样本中，前两个为好瓜，最后一个为坏瓜）。\\n结点6⃝包含训练集“脐部”为稍凹、“根蒂”为稍蜷、“色泽”为乌黑的样本（编号7、15），当根据\\n“纹理”再次进行划分时不含有“纹理”为模糊的样本（递归返回情形(3)），而恰巧两个样本（编号7、15）\\n含好瓜和坏瓜各一个，因此叶结点模糊的类别随机从类别好瓜和坏瓜中选择其一。\\n图4.5 两次随机选择均选为好瓜，实际上表示了一种归纳偏好（参见第1 章1.4 节）。\\n4.4\\n连续与缺失值\\n连续与缺失值的预处理均属于特征工程的范畴。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='4.4\\n连续与缺失值\\n连续与缺失值的预处理均属于特征工程的范畴。\\n有些分类器只能使用离散属性，当遇到连续属性时则需要特殊处理，有兴趣可以通过关键词“连续属\\n性离散化”或者“Discretization”查阅更多处理方法。结合第11 章11.2 节至11.4 节分别介绍的“过滤\\n式”算法、“包裹式”算法、“嵌入式”算法的概念，若先使用某个离散化算法对连续属性离散化后再调用\\nC4.5 决策树生成算法，则是一种过滤式算法，若如4.4.1 节所述，则应该属于嵌入式算法，因为并没有以\\n学习器的预测结果准确率为评价标准，而是与决策树生成过程融为一体，因此不应该划入包裹式算法。\\n类似地，有些分类器不能使用含有缺失值的样本，需要进行预处理。常用的缺失值填充方法是：对于\\n连续属性，采用该属性的均值进行填充；对于离散属性，采用属性值个数最多的样本进行填充。这实际上\\n假设了数据集中的样本是基于独立同分布采样得到的。特别地，一般缺失值仅指样本的属性值有缺失，若\\n类别标记有缺失，一般会直接抛弃该样本。当然，也可以尝试根据第11 章11.6 节的式(11.24)，在低秩假\\n设下对数据集缺失值进行填充。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='设下对数据集缺失值进行填充。\\n4.4.1\\n式(4.7) 的解释\\n此式所表达的思想很简单，就是以每两个相邻取值的中点作为划分点。下面以“西瓜书”中表4.3 中西瓜\\n数据集3.0 为例来说明此式的用法。对于“密度”这个连续属性，已观测到的可能取值为{0.243, 0.245, 0.343,\\n0.360, 0.403, 0.437, 0.481, 0.556, 0.593, 0.608, 0.634, 0.639, 0.657, 0.666, 0.697, 0.719, 0.774} 共17 个值，根据\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(4.7) 可知，此时i 依次取1 到16，那么“密度”这个属性的候选划分点集合为\\nTa = {0.243 + 0.245\\n2\\n, 0.245 + 0.343\\n2\\n, 0.343 + 0.360\\n2\\n, 0.360 + 0.403\\n2\\n, 0.403 + 0.437\\n2\\n, 0.437 + 0.481\\n2\\n,\\n0.481 + 0.556\\n2\\n, 0.556 + 0.593\\n2\\n, 0.593 + 0.608\\n2\\n, 0.608 + 0.634\\n2\\n, 0.634 + 0.639\\n2\\n, 0.639 + 0.657\\n2\\n,\\n0.657 + 0.666\\n2\\n, 0.666 + 0.697\\n2\\n, 0.697 + 0.719\\n2\\n, 0.719 + 0.774\\n2\\n}\\n4.4.2\\n式(4.8) 的解释\\n此式是式(4.2) 用于离散化后的连续属性的版本，其中Ta 由式(4.7) 计算得来，λ ∈{−, +} 表示属\\n性a 的取值分别小于等于和大于候选划分点t 时的情形，即当λ = −时有Dλ\\nt = Da⩽t\\nt'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='性a 的取值分别小于等于和大于候选划分点t 时的情形，即当λ = −时有Dλ\\nt = Da⩽t\\nt\\n，当λ = + 时有\\nDλ\\nt = Da>t\\nt\\n。\\n4.4.3\\n式(4.12) 的解释\\n该式括号内与式(4.2) 基本一样，区别在于式(4.2) 中的|Dv|\\n|D| 改为式(4.11) 的˜rv，在根据式(4.1) 计\\n算信息熵时第k 类样本所占的比例改为式(4.10) 的˜pk；所有计算结束后再乘以式(4.9) 的ρ。\\n有关式(4.9) (4.10) (4.11) 中的权重wx，初始化为1。以图4.9 为例，在根据“纹理”进行划分时，除\\n编号为8、10 的两个样本在此属性缺失之外，其余样本根据自身在该属性上的取值分别划入稍糊、清晰、\\n模糊三个子集，而编号为8、10 的两个样本则要按比例同时划入三个子集。具体来说，稍糊子集包含样本\\n7、9、13、14、17 共5 个样本，清晰子集包含样本1、2、3、4、5、6、15 共7 个样本，模糊子集包含样\\n本10、11、16 共3 个样本，总共15 个在该属性不含缺失值的样本，而此时各样本的权重wx 初始化为1，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='因此编号为8、10 的两个样本分到稍糊、清晰、模糊三个子集的权重分别为\\n5\\n15, 7\\n15 和\\n3\\n15。\\n4.5\\n多变量决策树\\n本节内容也通俗易懂，以下仅对部分图做进一步解释说明。\\n4.5.1\\n图(4.10) 的解释\\n只想用该图强调一下，离散属性不可以重复使用，但连续属性是可以重复使用的。\\n4.5.2\\n图(4.11) 的解释\\n对照“西瓜书”中图4.10 的决策树，下面给出图4.11 中的划分边界产出过程。\\n在下图4-2中，斜纹阴影部分表示已确定标记为坏瓜的样本，点状阴影部分表示已确定标记为好瓜的\\n样本，空白部分表示需要进一步划分的样本。第一次划分条件是“含糖率⩽0.126?”，满足此条件的样本直\\n接被标记为坏瓜（如图4-2(a) 斜纹阴影部分所示），而不满足此条件的样本还需要进一步划分（如图4-2(a)\\n空白部分所示）。\\n在第一次划分的基础上对图4-2(a) 空白部分继续进行划分，第二次划分条件是“密度⩽0.381?”，满\\n足此条件的样本直接被标记为坏瓜（如图4-2(b) 新增斜纹阴影部分所示），而不满足此条件的样本还需要\\n进一步划分（如图4-2(b) 空白部分所示）。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='进一步划分（如图4-2(b) 空白部分所示）。\\n在第二次划分的基础上对图4-2(b) 空白部分继续进行划分，第三次划分条件是“含糖率⩽0.205?”，\\n不满足此条件的样本直接标记为好瓜（如图4-2(c) 新增点状阴影部分所示），而满足此条件的样本还需进\\n一步划分（如图4-2(c) 空白部分所示）。\\n在第三次划分的基础上对图4-2(c) 空白部分继续进行划分，第四次划分的条件是“密度⩽0.560?”，满\\n足此条件的样本直接标记为好瓜（如图4-2(d) 新增点状阴影部分所示），而不满足此条件的样本直接标记\\n为坏瓜（如图4-2(d) 新增斜纹阴影部分所示）。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 52, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n经过四次划分已无空白部分，表示决策树生成完毕，从图4-2(d) 中可以清晰地看出好瓜与坏瓜的分类\\n边界。\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(a) 第一次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(b) 第二次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(c) 第三次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(d) 第四次划分\\n图4-2 图4.11 中的划分边界产出过程\\n参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第5 章\\n神经网络\\n神经网络类算法可以堪称当今最主流的一类机器学习算法，其本质上和前几章讲到的线性回归、对数\\n几率回归、决策树等算法一样均属于机器学习算法，也是被发明用来完成分类和回归等任务。不过由于神\\n经网络类算法在如今超强算力的加持下效果表现极其出色，且从理论角度来说神经网络层堆叠得越深其效\\n果越好，因此也单独称用深层神经网络类算法所做的机器学习为深度学习，属于机器学习的子集。\\n5.1\\n神经元模型\\n本节对神经元模型的介绍通俗易懂，在此不再赘述。本节第2 段提到“阈值”(threshold) 的概念时，\\n“西\\n瓜书”左侧边注特意强调是“阈(yù)”而不是“阀(fá)”，这是因为该字确实很容易认错，读者注意一下\\n即可。\\n图5.1 所示的M-P 神经元模型，其中的“M-P”便是两位作者McCulloch 和Pitts 的首字母简写。\\n5.2\\n感知机与多层网络\\n5.2.1\\n式(5.1) 和式(5.2) 的推导\\n此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介\\n绍\\n[1]：'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='绍\\n[1]：\\n感知机模型：已知感知机由两层神经元组成，故感知机模型的公式可表示为\\ny = f\\n n\\nX\\ni=1\\nwixi −θ\\n!\\n= f(wTx −θ)\\n其中，x ∈Rn，为样本的特征向量，是感知机模型的输入；w, θ 是感知机模型的参数，w ∈Rn，为权重，\\nθ 为阈值。假定f 为阶跃函数，那么感知机模型的公式可进一步表示为（用ε(·) 代表阶跃函数）\\ny = ε(wTx −θ) =\\n(\\n1,\\nwTx −θ ⩾0;\\n0,\\nwTx −θ < 0.\\n由于n 维空间中的超平面方程为\\nw1x1 + w2x2 + · · · + wnxn + b = wTx + b = 0\\n所以此时感知机模型公式中的wTx −θ 可以看作是n 维空间中的一个超平面，将n 维空间划分为wTx −\\nθ ⩾0 和wTx −θ < 0 两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间\\n的样本对应的模型输出值为0，如此便实现了分类功能。\\n感知机学习策略：给定一个数据集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='T = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中xi ∈Rn, yi ∈{0, 1}, i = 1, 2, · · · , N。如果存在某个超平面\\nwTx + b = 0\\n能将数据集T 中的正样本和负样本完全正确地划分到超平面两侧，即对所有yi = 1 的样本xi 有wTxi +\\nb ⩾0，对所有yi = 0 的样本xi 有wTxi + b < 0，则称数据集T 线性可分，否则称数据集T 线性不可分。\\n现给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T 中的正负样本完全正确划\\n分的分离超平面\\nwTx −θ = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n假设此时误分类样本集合为M ⊆T，对任意一个误分类样本(x, y) ∈M 来说，当wTx −θ ⩾0 时，模型\\n输出值为ˆy = 1，样本真实标记为y = 0；反之，当wTx −θ < 0 时，模型输出值为ˆy = 0，样本真实标记\\n为y = 1。综合两种情形可知，以下公式恒成立：\\n(ˆy −y)\\n\\x00wTx −θ\\n\\x01\\n⩾0\\n所以，给定数据集T，其损失函数可以定义为\\nL(w, θ) =\\nX\\nx∈M\\n(ˆy −y)\\n\\x00wTx −θ\\n\\x01\\n显然，此损失函数是非负的。如果没有误分类点，则损失函数值为0。而且，误分类点越少，误分类点离\\n超平面越近，损失函数值就越小。因此，给定数据集T，损失函数L(w, θ) 是关于w, θ 的连续可导函数。\\n感知机学习算法：感知机模型的学习问题可以转化为求解损失函数的最优化问题，具体地，给定数据\\n集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中xi ∈Rn, yi ∈{0, 1}，求参数w, θ，使其为极小化损失函数的解：\\nmin'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='其中xi ∈Rn, yi ∈{0, 1}，求参数w, θ，使其为极小化损失函数的解：\\nmin\\nw,θ L(w, θ) = min\\nw,θ\\nX\\nxi∈M\\n(ˆyi −yi)(wTxi −θ)\\n其中M ⊆T 为误分类样本集合。若将阈值θ 看作一个固定输入为−1 的“哑节点”，即\\n−θ = −1 · wn+1 = xn+1 · wn+1\\n那么wTxi −θ 可化简为\\nwTxi −θ =\\nn\\nX\\nj=1\\nwjxj + xn+1 · wn+1\\n=\\nn+1\\nX\\nj=1\\nwjxj\\n= wTxi\\n其中xi ∈Rn+1, w ∈Rn+1。根据该公式，可将要求解的极小化问题进一步简化为\\nmin\\nw L(w) = min\\nw\\nX\\nxi∈M\\n(ˆyi −yi)wTxi\\n假设误分类样本集合M 固定，那么可以求得损失函数L(w) 的梯度\\n∇wL(w) =\\nX\\nxi∈M\\n(ˆyi −yi)xi\\n感知机的学习算法具体采用的是随机梯度下降法，即在极小化过程中，不是一次使M 中所有误分类点的\\n梯度下降，而是一次随机选取一个误分类点并使其梯度下降。所以权重w 的更新公式为\\nw ←w + ∆w'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='梯度下降，而是一次随机选取一个误分类点并使其梯度下降。所以权重w 的更新公式为\\nw ←w + ∆w\\n∆w = −η(ˆyi −yi)xi = η(yi −ˆyi)xi\\n相应地，w 中的某个分量wi 的更新公式即式(5.2)。\\n5.2.2\\n图5.5 的解释\\n图5.5 中(0, 0), (0, 1), (1, 0), (1, 1) 这4 个样本点实现“异或”计算的过程如下：\\n(x1, x2) →h1 = ε(x1 −x2 −0.5), h2 = ε(x2 −x1 −0.5) →y = ε(h1 + h2 −0.5)\\n以(0, 1) 为例，首先求得h1 = ε(0−1−0.5) = 0, h2 = ε(1−0−0.5) = 1，然后求得y = ε(0+1−0.5) = 1。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 55, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.3\\n误差逆传播算法\\n5.3.1\\n式(5.10) 的推导\\n参见式(5.12) 的推导\\n5.3.2\\n式(5.12) 的推导\\n因为\\n∆θj = −η\\n∂Ek\\n∂θj\\n又\\n∂Ek\\n∂θj\\n=\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂θj\\n=\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂[f(βj −θj)]\\n∂θj\\n=\\n∂Ek\\n∂ˆyk\\nj\\n· f ′(βj −θj) × (−1)\\n=\\n∂Ek\\n∂ˆyk\\nj\\n· f (βj −θj) × [1 −f (βj −θj)] × (−1)\\n=\\n∂Ek\\n∂ˆyk\\nj\\n· ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n× (−1)\\n=\\n∂\\n\\uf8ee\\n\\uf8f01\\n2\\nlP\\nj=1\\n\\x00ˆyk\\nj −yk\\nj\\n\\x012\\n\\uf8f9\\n\\uf8fb\\n∂ˆyk\\nj\\n· ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n× (−1)\\n=\\n1\\n2 × 2(ˆyk\\nj −yk\\nj ) × 1 · ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n× (−1)\\n= (yk\\nj −ˆyk\\nj )ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n= gj\\n所以\\n∆θj = −η\\n∂Ek\\n∂θj'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 55, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='j )ˆyk\\nj\\n\\x001 −ˆyk\\nj\\n\\x01\\n= gj\\n所以\\n∆θj = −η\\n∂Ek\\n∂θj\\n= −ηgj\\n5.3.3\\n式(5.13) 的推导\\n因为\\n∆vih = −η\\n∂Ek\\n∂vih\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 56, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n又\\n∂Ek\\n∂vih\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂αh\\n·\\n∂αh\\n∂vih\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂αh\\n· xi\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n· f ′(αh −γh) · xi\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n· whj · f ′(αh −γh) · xi\\n=\\nl\\nX\\nj=1\\n(−gj) · whj · f ′(αh −γh) · xi\\n= −f ′(αh −γh) ·\\nl\\nX\\nj=1\\ngj · whj · xi\\n= −bh(1 −bh) ·\\nl\\nX\\nj=1\\ngj · whj · xi\\n= −eh · xi\\n所以\\n∆vih = −η\\n∂Ek\\n∂vih\\n= ηehxi\\n5.3.4\\n式(5.14) 的推导\\n因为\\n∆γh = −η\\n∂Ek\\n∂γh\\n又\\n∂Ek\\n∂γh\\n='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 56, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='5.3.4\\n式(5.14) 的推导\\n因为\\n∆γh = −η\\n∂Ek\\n∂γh\\n又\\n∂Ek\\n∂γh\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂γh\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n· f ′(αh −γh) · (−1)\\n= −\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n· whj · f ′(αh −γh)\\n= −\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆyk\\nj\\n·\\n∂ˆyk\\nj\\n∂βj\\n· whj · bh(1 −bh)\\n=\\nl\\nX\\nj=1\\ngj · whj · bh(1 −bh)\\n= eh\\n所以\\n∆γh = −η\\n∂Ek\\n∂γh\\n= −ηeh\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.3.5\\n式(5.15) 的推导\\n参见式(5.13) 的推导\\n5.4\\n全局最小与局部极小\\n由图5.10 可以直观理解局部极小和全局最小的概念，其余概念如模拟退火、遗传算法、启发式等，则\\n需要查阅专业资料系统化学习。\\n5.5\\n其他常见神经网络\\n本节所提到的神经网络其实如今已不太常见，更为常见的神经网络是下一节深度学习里提到的卷积神\\n经网络、循环神经网络等。\\n5.5.1\\n式(5.18) 的解释\\n从式(5.18) 可以看出，对于样本x 来说，RBF 网络的输出为q 个ρ(x, ci) 的线性组合。若换个角\\n度来看这个问题，将q 个ρ(x, ci) 当作是将d 维向量x 基于式(5.19) 进行特征转换后所得的q 维特征，\\n即˜x = (ρ(x, c1); ρ(x, c2); ...; ρ(x, cq))，则式(5.18) 求线性加权系数wi 相当于求解第3.2 节的线性回归\\nf(˜x) = wT˜x + b，对于仅有的差别b 来说，当然可以在式(5.18) 中补加一个b。因此，RBF 网络在确定'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='q 个神经元中心ci 之后，接下来要做的就是线性回归。\\n5.5.2\\n式(5.20) 的解释\\nBoltzmann 机（Restricted Boltzmann Machine，简称RBM）本质上是一个引入了隐变量的无向图模\\n型，其能量可理解为\\nEgraph = Eedges + Enodes\\n其中，Egraph 表示图的能量，Eedges 表示图中边的能量，Enodes 表示图中结点的能量。边能量由两连接\\n结点的值及其权重的乘积确定，即Eedgeij = −wijsisj；结点能量由结点的值及其阈值的乘积确定，即\\nEnodei = −θisi。图中边的能量为所有边能量之和为\\nEedges =\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nEedgeij = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nwijsisj\\n图中结点的能量为所有结点能量之和\\nEnodes =\\nn\\nX\\ni=1\\nEnodei = −\\nn\\nX\\ni=1\\nθisi\\n故状态向量s 所对应的Boltzmann 机能量\\nEgraph = Eedges + Enodes = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Egraph = Eedges + Enodes = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nwijsisj −\\nn\\nX\\ni=1\\nθisi\\n5.5.3\\n式(5.22) 的解释\\n受限Boltzmann 机仅保留显层与隐层之间的连接。显层状态向量v = (v1; v2; ...; vd)，隐层状态向量\\nh = (h1; h2; ...; hq)。显层状态向量v 中的变量vi 仅与隐层状态向量h 有关，所以给定隐层状态向量h，\\n有v1, v2, ..., vd 相互独立。\\n5.5.4\\n式(5.23) 的解释\\n由式(5.22) 的解释同理可得，给定显层状态向量v，有h1, h2, ..., hq 相互独立。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n5.6\\n深度学习\\n“西瓜书”在本节并未对如今深度学习领域的诸多经典神经网络作展开介绍，而是从更宏观的角度详\\n细解释了应该如何理解深度学习。因此，本书也顺着“西瓜书”的思路对深度学习相关概念作进一步说明，\\n对深度学习的经典神经网络感兴趣的读者可查阅其他相关书籍进行系统性学习。\\n5.6.1\\n什么是深度学习\\n深度学习就是很深层的神经网络，而神经网络属于机器学习算法的范畴，因此深度学习是机器学习的\\n子集。\\n5.6.2\\n深度学习的起源\\n深度学习中的经典神经网络以及用于训练神经网络的BP 算法其实在很早就已经被提出，例如卷积神\\n经网络\\n[2] 是在1989 提出，BP 算法\\n[3] 是在1986 年提出，但是在当时的计算机算力水平下，其他非神经\\n网络类算法（例如当时红极一时的支持向量机算法）的效果优于神经网络类算法，因此神经网络类算法进\\n入瓶颈期。随着计算机算力的不断提升，以及2012 年Hinton 和他的学生提出了AlexNet 并在ImageNet'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='评测中以明显优于第二名的成绩夺冠后，引起了学术界和工业界的广泛关注，紧接着三位深度学习之父\\nLeCun、Bengio 和Hinton 在2015 年正式提出深度学习的概念，自此深度学习开始成为机器学习的主流\\n研究方向。\\n5.6.3\\n怎么理解特征学习\\n举例来说，用非深度学习算法做西瓜分类时，首先需要人工设计西瓜的各个特征，比如根蒂、色泽等，\\n然后将其表示为数学向量，这些过程统称为“特征工程”，完成特征工程后用算法分类即可，其分类效果\\n很大程度上取决于特征工程做得是否够好。而对于深度学习算法来说，只需将西瓜的图片表示为数学向量\\n输入，输出层设置为想要的分类结果即可（例如二分类通常设置为对数几率回归），之前的“特征工程”交\\n由神经网络来自动完成，即让神经网络进行“特征学习”，通过在输出层约束分类结果，神经网络会自动\\n从西瓜的图片上提取出有助于西瓜分类的特征。\\n因此，如果分别用对数几率回归和卷积神经网络来做西瓜分类，其算法运行流程分别是“人工特征工\\n程→对数几率回归分类”和“卷积神经网络特征学习→对数几率回归分类”。\\n参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.\\n[2] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-\\nbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural\\ncomputation, 1(4):541–551, 1989.\\n[3] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\\npropagating errors. nature, 323(6088):533–536, 1986.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第6 章\\n支持向量机\\n在深度学习流行之前，支持向量机及其核方法一直是机器学习领域中的主流算法，尤其是核方法至今\\n都仍有相关学者在持续研究。\\n6.1\\n间隔与支持向量\\n6.1.1\\n图6.1 的解释\\n回顾第5 章5.2 节的感知机模型可知，图6.1 中的黑色直线均可作为感知机模型的解，因为感知机模\\n型求解的是能将正负样本完全正确划分的超平面，因此解不唯一。而支持向量机想要求解的则是离正负样\\n本都尽可能远且刚好位于“正中间”的划分超平面，因为这样的超平面理论上泛化性能更好。\\n6.1.2\\n式(6.1) 的解释\\nn 维空间的超平面定义为wTx + b = 0，其中w, x ∈Rn，w = (w1; w2; ...; wn) 称为法向量，b 称为\\n位移项。超平面具有以下性质：\\n(1) 法向量w 和位移项b 确定一个唯一超平面；\\n(2) 超平面方程不唯一，因为当等倍缩放w 和b 时（假设缩放倍数为α），所得的新超平面方程\\nαwTx + αb = 0 和wTx + b = 0 的解完全相同，因此超平面不变，仅超平面方程有变；'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(3) 法向量w 垂直于超平面；\\n(4) 超平面将n 维空间切割为两半，其中法向量w 指向的那一半空间称为正空间，另一半称为负空\\n间，正空间中的点x+ 代入进方程wTx+ + b 其计算结果大于0，反之负空间中的点代入进方程其计算结\\n果小于0；\\n(5)n 维空间中的任意点x 到超平面的距离公式为r = |wTx+b|\\n∥w∥\\n，其中∥w∥表示向量w 的模。\\n6.1.3\\n式(6.2) 的推导\\n对于任意一点x0 = (x0\\n1; x0\\n2; ...; x0\\nn)，设其在超平面wTx + b = 0 上的投影点为x1 = (x1\\n1; x1\\n2; ...; x1\\nn)，\\n则wTx1 + b = 0。根据超平面的性质(3) 可知，此时向量−−−→\\nx1x0 与法向量w 平行，因此\\n|w · −−−→\\nx1x0| = |∥w∥· cos π · ∥−−−→\\nx1x0∥| = ∥w∥· ∥−−−→\\nx1x0∥= ∥w∥· r\\n又\\nw · −−−→\\nx1x0 = w1(x0\\n1 −x1\\n1) + w2(x0\\n2 −x1\\n2) + ... + wn(x0\\nn −x1\\nn)\\n= w1x0'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1) + w2(x0\\n2 −x1\\n2) + ... + wn(x0\\nn −x1\\nn)\\n= w1x0\\n1 + w2x0\\n2 + ... + wnx0\\nn −(w1x1\\n1 + w2x1\\n2 + ... + wnx1\\nn)\\n= wTx0 −wTx1\\n= wTx0 + b\\n所以\\n|wTx0 + b| = ∥w∥· r\\nr =\\n\\x0c\\x0cwTx + b\\n\\x0c\\x0c\\n∥w∥\\n6.1.4\\n式(6.3) 的推导\\n支持向量机所要求的超平面需要满足三个条件，第一个是能正确划分正负样本，第二个是要位于正负\\n样本正中间，第三个是离正负样本都尽可能远。式(6.3) 仅满足前两个条件，第三个条件由式(6.5) 来满\\n足，因此下面仅基于前两个条件来进行推导。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于第一个条件，当超平面满足该条件时，根据超平面的性质(4) 可知，若yi = +1 的正样本被划分\\n到正空间（当然也可以将其划分到负空间），yi = −1 的负样本被划分到负空间，以下不等式成立\\n(\\nwTxi + b ⩾0,\\nyi = +1\\nwTxi + b ⩽0,\\nyi = −1\\n对于第二个条件，首先设离超平面最近的正样本为x+\\n∗，离超平面最近的负样本为x−\\n∗，由于这两样本\\n是离超平面最近的点，所以其他样本到超平面的距离均大于等于它们，即\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n|wTxi+b|\\n∥w∥\\n⩾|wTx+\\n∗+b|\\n∥w∥\\n,\\nyi = +1\\n|wTxi+b|\\n∥w∥\\n⩾|wTx−\\n∗+b|\\n∥w∥\\n,\\nyi = −1\\n结合第一个条件中推导出的不等式，可将上式中的绝对值符号去掉并推得\\n(\\nwTxi+b\\n∥w∥\\n⩾wTx+\\n∗+b\\n∥w∥\\n,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽wTx−\\n∗+b\\n∥w∥\\n,\\nyi = −1\\n基于此再考虑第二个条件，“位于正负样本正中间”等价于要求超平面到x+\\n∗和x−'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content=',\\nyi = −1\\n基于此再考虑第二个条件，“位于正负样本正中间”等价于要求超平面到x+\\n∗和x−\\n∗这两点的距离相等，即\\n\\x0c\\x0cwTx+\\n∗+ b\\n\\x0c\\x0c\\n∥w∥\\n=\\n\\x0c\\x0cwTx−\\n∗+ b\\n\\x0c\\x0c\\n∥w∥\\n综上，支持向量机所要求的超平面所需要满足的条件如下\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nwTxi+b\\n∥w∥\\n⩾wTx+\\n∗+b\\n∥w∥\\n,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽wTx−\\n∗+b\\n∥w∥\\n,\\nyi = −1\\n|wTx+\\n∗+b|\\n∥w∥\\n= |wTx−\\n∗+b|\\n∥w∥\\n但是根据超平面的性质(2) 可知，当等倍缩放法向量w 和位移项b 时，超平面不变，且上式也恒成\\n立，因此会导致所求的超平面的参数w 和b 有无穷多解。因此为了保证每个超平面的参数只有唯一解，不\\n妨再额外施加一些约束，例如约束x+\\n∗和x−\\n∗代入进超平面方程后的绝对值为1，也就是令wTx+\\n∗+ b =\\n1, wTx−\\n∗+ b = −1。此时支持向量机所要求的超平面所需要满足的条件变为\\n(\\nwTxi+b\\n∥w∥\\n⩾\\n+1\\n∥w∥,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽\\n−1\\n∥w∥,\\nyi = −1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='⩾\\n+1\\n∥w∥,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽\\n−1\\n∥w∥,\\nyi = −1\\n由于∥w∥恒大于0，因此上式可进一步化简为\\n(\\nwTxi + b ⩾+1,\\nyi = +1\\nwTxi + b ⩽−1,\\nyi = −1\\n6.1.5\\n式(6.4) 的推导\\n根据式(6.3) 的推导可知，x+\\n∗和x−\\n∗便是“支持向量”，因此支持向量到超平面的距离已经被约束为\\n1\\n∥w∥，所以两个异类支持向量到超平面的距离之和为\\n2\\n∥w∥。\\n6.1.6\\n式(6.5) 的解释\\n式(6.5) 是通过“最大化间隔”来保证超平面离正负样本都尽可能远，且该超平面有且仅有一个，因\\n此可以解出唯一解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n6.2\\n对偶问题\\n6.2.1\\n凸优化问题\\n考虑一般地约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n若目标函数f(x) 是凸函数，不等式约束gi(x) 是凸函数，等式约束hj(x) 是仿射函数，则称该优化问题\\n为凸优化问题。\\n由于1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01\\n均是关于w 和b 的凸函数，所以式(6.6) 是凸优化问题。凸优化问\\n题是最优化里比较易解的一类优化问题，因为其拥有诸多良好的数学性质和现成的数学工具，因此如果非\\n凸优化问题能等价转化为凸优化问题，其求解难度通常也会减小。\\n6.2.2\\nKKT 条件\\n考虑一般的约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n若f(x), gi(x), hj(x) 的一阶偏导连续，x∗是优化问题的局部解，µ = (µ1; µ2; ...; µm), λ = (λ1; λ2; ...; λn)\\n为拉格朗日乘子向量，L(x, µ, λ) = f(x) + Pm\\ni=1 µigi(x) + Pn\\nj=1 λjhj(x) 为拉格朗日函数，且该优化问题\\n满足任何一个特定的约束限制条件，则一定存在µ∗= (µ∗\\n1; µ∗\\n2; ...; µ∗\\nm), λ∗= (λ∗\\n1; λ∗\\n2; ...; λ∗\\nn)，使得：\\n(1) ∇xL(x∗, µ∗, λ∗) = ∇f(x∗) + Pm\\ni=1 µ∗\\ni ∇gi(x∗) + Pn\\nj=1 λ∗\\nj∇hj(x∗) = 0；\\n(2) hj(x∗) = 0,\\nj = 1, 2, ..., n；\\n(3) gi(x∗) ⩽0,\\ni = 1, 2, ..., m；\\n(4) µ∗\\ni ⩾0,\\ni = 1, 2, ..., m；\\n(5) µ∗\\ni gi(x∗) = 0,'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i ⩾0,\\ni = 1, 2, ..., m；\\n(5) µ∗\\ni gi(x∗) = 0,\\ni = 1, 2, ..., m。\\n以上5 条便是Karush–Kuhn–Tucker Conditions（简称KKT 条件）。KKT 条件是局部解的必要条件，\\n也就是说只要该优化问题满足任何一个特定的约束限制条件，局部解就一定会满足以上5 个条件。常用的\\n约束限制条件可查阅维基百科“Karush–Kuhn–Tucker Conditions”词条以及查阅参考文献[1] 的第4.2.2\\n节，若对KKT 条件的数学证明感兴趣可查阅参考文献[1] 的第4.2.1 节。\\n6.2.3\\n拉格朗日对偶函数\\n考虑一般地约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n设上述优化问题的定义域为D = dom f ∩\\nmT\\ni=1\\ndom gi ∩\\nnT\\nj=1\\ndom hj，可行集为˜D = {x|x ∈D, gi(x) ⩽'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='dom gi ∩\\nnT\\nj=1\\ndom hj，可行集为˜D = {x|x ∈D, gi(x) ⩽\\n0, hj(x) = 0}（显然˜D 是D 的子集），最优值为p∗= min{f(˜x)}, ˜x ∈˜D。上述优化问题的拉格朗日函数\\n定义为\\nL(x, µ, λ) = f(x) +\\nm\\nX\\ni=1\\nµigi(x) +\\nn\\nX\\nj=1\\nλjhj(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中µ = (µ1; µ2; ...; µm), λ = (λ1; λ2; ...; λn) 为拉格朗日乘子向量。相应地拉格朗日对偶函数Γ(µ, λ)（简\\n称对偶函数）定义为L(x, µ, λ) 关于x 的下确界，即\\nΓ(µ, λ) = inf\\nx∈D L(x, µ, λ) = inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµigi(x) +\\nn\\nX\\nj=1\\nλjhj(x)\\n!\\n对偶函数有如下性质：\\n(1) 无论上述优化问题是否为凸优化问题，其对偶函数Γ(µ, λ) 恒为凹函数，详细证明可查阅参考文\\n献[2] 的第5.1.2 和3.2.3 节；\\n(2) 当µ ⪰0 时（µ ⪰0 表示µ 的分量均为非负），Γ(µ, λ) 构成了上述优化问题最优值p∗的下界，\\n即\\nΓ(µ, λ) ⩽p∗\\n其推导过程如下：\\n设˜x ∈˜D 是优化问题的可行点，则gi(˜x) ⩽0, hj(˜x) = 0，因此，当µ ⪰0 时，µigi(˜x) ⩽0, λjhj(˜x) = 0\\n恒成立，所以\\nm\\nX\\ni=1\\nµigi(˜x) +\\nn'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='恒成立，所以\\nm\\nX\\ni=1\\nµigi(˜x) +\\nn\\nX\\nj=1\\nλjhj(˜x) ⩽0\\n根据上述不等式可以推得\\nL(˜x, µ, λ) = f(˜x) +\\nm\\nX\\ni=1\\nµigi(˜x) +\\nn\\nX\\nj=1\\nλjhj(˜x) ⩽f(˜x)\\n又\\nΓ(µ, λ) = inf\\nx∈D L(x, µ, λ) ⩽L(˜x, µ, λ)\\n所以\\nΓ(µ, λ) ⩽L(˜x, µ, λ) ⩽f(˜x)\\n进一步地\\nΓ(µ, λ) ⩽min{f(˜x)} = p∗\\n6.2.4\\n拉格朗日对偶问题\\n在µ ⪰0 的约束下求对偶函数最大值的优化问题称为拉格朗日对偶问题（简称对偶问题）\\nmax\\nΓ(µ, λ)\\ns.t.\\nµ ⪰0\\n上一节的优化问题称为主问题或原问题。\\n设对偶问题的最优值为d∗= max{Γ(µ, λ)}, µ ⪰0，根据对偶函数的性质（2）可知d∗⩽p∗，此时称\\n为“弱对偶性”成立，若d∗= p∗，则称为“强对偶性”成立。由此可以看出，当主问题较难求解时，如\\n果强对偶性成立，则可以通过求解对偶问题来间接求解主问题。由于约束条件µ ⪰0 是凸集，且根据对偶'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='果强对偶性成立，则可以通过求解对偶问题来间接求解主问题。由于约束条件µ ⪰0 是凸集，且根据对偶\\n函数的性质（1）可知Γ(µ, λ) 恒为凹函数，其加个负号即为凸函数，所以无论主问题是否为凸优化问题，\\n对偶问题恒为凸优化问题。\\n一般情况下，强对偶性并不成立，只有当主问题满足特定的约束限制条件（不同于KKT 条件中的约\\n束限制条件）时，强对偶性才成立，常见的有“Slater 条件”。Slater 条件指出，当主问题是凸优化问题，\\n且存在一点x ∈relint D 能使得所有等式约束成立，除仿射函数以外的不等式约束严格成立，则强对偶性\\n成立。由于式(6.6) 是凸优化问题，且不等式约束均为仿射函数，所以式(6.6) 强对偶性成立。\\n对于凸优化问题，还可以通过KKT 条件来间接推导出强对偶性，并同时求解出主问题和对偶问题的\\n最优解。具体地，若主问题为凸优化问题，目标函数f(x) 和约束函数gi(x), hj(x) 的一阶偏导连续，主问\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n题满足KKT 条件中任何一个特定的约束限制条件，则满足KKT 条件的点x∗和(µ∗, λ∗) 分别是主问题\\n和对偶问题的最优解，且此时强对偶性成立。下面给出具体的推导过程。\\n设x∗, µ∗, λ∗是任意满足KKT 条件的点，即\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n∇xL(x∗, µ∗, λ∗) = ∇f(x∗) + Pm\\ni=1 µ∗\\ni ∇gi(x∗) + Pn\\nj=1 λ∗\\nj∇hj(x∗) = 0\\nhj(x∗) = 0,\\nj = 1, 2, ..., n\\ngi(x∗) ⩽0,\\ni = 1, 2, ..., m\\nµ∗\\ni ⩾0,\\ni = 1, 2, ..., m\\nµ∗\\ni gi(x∗) = 0,\\ni = 1, 2, ..., m\\n由于主问题是凸优化问题，所以f(x) 和gi(x) 是凸函数，hj(x) 是仿射函数，又因为此时µ∗\\ni ⩾0，所以'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i ⩾0，所以\\nL(x, µ∗, λ∗) 是关于x 的凸函数。根据∇xL(x∗, µ∗, λ∗) = 0 可知，此时x∗是L(x, µ∗, λ∗) 的极值点，而\\n凸函数的极值点也是最值点，所以x∗是最小值点，因此可以进一步推得\\nL(x∗, µ∗, λ∗) = min{L(x, µ∗, λ∗)}\\n= inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n= Γ(µ∗, λ∗)\\n= f(x∗) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x∗) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x∗)\\n= f(x∗)\\n其中第二个等式是根据下确界函数的性质推得，第三个等式是根据对偶函数的定义推得，第四个等式是\\nL(x∗, µ∗, λ∗) 的展开形式，最后一个等式是因为µ∗\\ni gi(x∗) = 0, hj(x∗) = 0。\\n由于x∗和(µ∗, λ∗) 仅是满足KKT 条件的点，并不一定是f(x) 和Γ(µ, λ) 的最值点，所以f(x∗) ⩾'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='p∗⩾d∗⩾Γ(µ∗, λ∗)，但是上式又推得f(x∗) = Γ(µ∗, λ∗)，所以p∗= d∗，因此推得强对偶性成立，且x∗\\n和(µ∗, λ∗) 分别是主问题和对偶问题的最优解。\\nSlater 条件恰巧也是KKT 条件中特定的约束限制条件之一，所以式(6.6) 不仅强对偶性成立，而且\\n可以通过求解满足KKT 条件的点来求解出最优解。\\nKKT 条件除了可以作为凸优化问题强对偶性成立的充分条件以外，其实对于任意优化问题（并不一\\n定是凸优化问题），若其强对偶性成立，KKT 条件也是主问题和对偶问题最优解的必要条件，而且此时并\\n不要求主问题满足KKT 条件中任何一个特定的约束限制条件。下面同样给出具体的推导过程。\\n设主问题的最优解为x∗，对偶问题的最优解为(µ∗, λ∗)，目标函数f(x) 和约束函数gi(x), hj(x) 的\\n一阶偏导连续，当强对偶性成立时，可以推得\\nf(x∗) = Γ(µ∗, λ∗)\\n= inf\\nx∈D L(x, µ∗, λ∗)\\n= inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n⩽f(x∗) +'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n⩽f(x∗) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x∗) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x∗)\\n⩽f(x∗)\\n其中，第一个等式是因为强对偶性成立时p∗= d∗，第二和第三个等式是对偶函数的定义，第四个不等式\\n是根据下确界的性质推得，最后一个不等式成立是因为µ∗\\ni ⩾0, gi(x∗) ⩽0, hj(x∗) = 0。\\n由于f(x∗) = f(x∗)，所以上式中的等式均可化为等式。第四个不等式可化为等式，说明L(x, µ∗, λ∗)\\n在x∗处取得最小值，所以根据极值的性质可知在x∗处一阶导∇xL(x∗, µ∗, λ∗) = 0。最后一个不等式可化\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n为等式，说明µ∗\\ni gi(x∗) = 0。此时再结合主问题和对偶问题原有的约束条件µ∗\\ni ⩾0, gi(x∗) ⩽0, hj(x∗) = 0\\n便凑齐了KKT 条件。\\n6.2.5\\n式(6.9) 和式(6.10) 的推导\\nL(w, b, α) = 1\\n2∥w∥2 +\\nm\\nX\\ni=1\\nαi(1 −yi(wTxi + b))\\n= 1\\n2∥w∥2 +\\nm\\nX\\ni=1\\n(αi −αiyiwTxi −αiyib)\\n= 1\\n2wTw +\\nm\\nX\\ni=1\\nαi −\\nm\\nX\\ni=1\\nαiyiwTxi −\\nm\\nX\\ni=1\\nαiyib\\n对w 和b 分别求偏导数并令其为零\\n∂L\\n∂w = 1\\n2 × 2 × w + 0 −\\nm\\nX\\ni=1\\nαiyixi −0 = 0 =⇒w =\\nm\\nX\\ni=1\\nαiyixi\\n∂L\\n∂b = 0 + 0 −0 −\\nm\\nX\\ni=1\\nαiyi = 0 =⇒\\nm\\nX\\ni=1\\nαiyi = 0\\n6.2.6\\n式(6.11) 的推导\\n因为αi ⩾0，且1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='式(6.11) 的推导\\n因为αi ⩾0，且1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01\\n均是关于w 和b 的凸函数，所以式(6.8) 也是关于w\\n和b 的凸函数。根据凸函数的性质可知，其极值点就是最值点，所以一阶导为零的点就是最小值点，因此\\n将式(6.9) 和式(6.10) 代入式(6.8) 后即可得式(6.8) 的最小值（等价于下确界），再根据对偶问题的定义\\n加上约束αi ⩾0，就得到了式(6.6) 的对偶问题。由于式(6.10) 也是αi 必须满足的条件，且不含有w 和\\nb，因此也需要纳入对偶问题的约束条件。根据以上思路进行推导的过程如下：\\ninf\\nw,b L(w, b, α) = 1\\n2wTw +\\nm\\nX\\ni=1\\nαi −\\nm\\nX\\ni=1\\nαiyiwTxi −\\nm\\nX\\ni=1\\nαiyib\\n= 1\\n2wT\\nm\\nX\\ni=1\\nαiyixi −wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi\\n= −1\\n2(\\nm\\nX\\ni=1\\nαiyixi)T(\\nm\\nX\\ni=1\\nαiyixi) +\\nm\\nX\\ni=1\\nαi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi\\n=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n所以\\nmax\\nα\\ninf\\nw,b L(w, b, α) = max\\nα\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n最后将αi ⩾0 和式(6.10) 作为约束条件即可得式(6.11)。\\n式(6.6) 之所以要转化为式(6.11) 来求解，其主要有以下两点理由：\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 65, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n(1) 式(6.6) 中的未知数是w 和b，式(6.11) 中的未知数是α，w 的维度d 对应样本特征个数，α 的\\n维度m 对应训练样本个数，通常m ≪d，所以求解式(6.11) 更高效，反之求解式(6.6) 更高效；\\n(2) 式(6.11) 中有样本内积x\\nTxj\\ni\\n这一项，后续可以很自然地引入核函数，进而使得支持向量机也能\\n对在原始特征空间线性不可分的数据进行分类。\\n6.2.7\\n式(6.13) 的解释\\n因为式(6.6) 满足Slater 条件，所以强对偶性成立，进而最优解满足KKT 条件。\\n6.3\\n核函数\\n6.3.1\\n式(6.22) 的解释\\n此即核函数的定义，即核函数可以分解成两个向量的内积。要想了解某个核函数是如何将原始特征空\\n间映射到更高维的特征空间的，只需要分解为两个表达形式完全一样的向量内积即可。\\n6.4\\n软间隔与正则化\\n6.4.1\\n式(6.35)\\n令\\nmax\\n\\x000, 1 −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n= ξi\\n显然ξi ≥0，且当1 −yi\\n\\x00wTxi + b\\n\\x01\\n> 0 时有\\n1 −yi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 65, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01\\x01\\n= ξi\\n显然ξi ≥0，且当1 −yi\\n\\x00wTxi + b\\n\\x01\\n> 0 时有\\n1 −yi\\n\\x00wTxi + b\\n\\x01\\n= ξi\\n当1 −yi\\n\\x00wTxi + b\\n\\x01\\n≤0 时有\\nξi = 0\\n综上可得\\n1 −yi\\n\\x00wTxi + b\\n\\x01\\n⩽ξi ⇒yi\\n\\x00wTxi + b\\n\\x01\\n⩾1 −ξi\\n6.4.2\\n式(6.37) 和式(6.38) 的推导\\n类比式(6.9) 和式(6.10) 的推导\\n6.4.3\\n式(6.39)\\n式(6.36) 关于ξi 求偏导数并令其为零\\n∂L\\n∂ξi\\n= 0 + C × 1 −αi × 1 −µi × 1 = 0 ⇒C = αi + µi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n6.4.4\\n式(6.40)\\n将式(6.37)、式(6.38) 和(6.39) 代入式(6.36) 可以得到式(6.35) 的对偶问题，有\\n1\\n2 ∥w∥2 + C\\nm\\nX\\ni=1\\nξi +\\nm\\nX\\ni=1\\nαi\\n\\x001 −ξi −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n−\\nm\\nX\\ni=1\\nµiξi\\n=1\\n2 ∥w∥2 +\\nm\\nX\\ni=1\\nαi\\n\\x001 −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n+ C\\nm\\nX\\ni=1\\nξi −\\nm\\nX\\ni=1\\nαiξi −\\nm\\nX\\ni=1\\nµiξi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi +\\nm\\nX\\ni=1\\nCξi −\\nm\\nX\\ni=1\\nαiξi −\\nm\\nX\\ni=1\\nµiξi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi +\\nm\\nX\\ni=1\\n(C −αi −µi)ξi\\n=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= min'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= min\\nw,b,ξ L(w, b, α, ξ, µ)\\n所以\\nmax\\nα,µ min\\nw,b,ξ L(w, b, α, ξ, µ) = max\\nα,µ\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= max\\nα\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n又因为αi ≥0，µi ≥0，C = αi + µi，消去µi 可得等价约束条件\\n0 ⩽αi ⩽C,\\ni = 1, 2, ..., m\\n6.4.5\\n对数几率回归与支持向量机的关系\\n在“西瓜书”本节的倒数第二段开头，其讨论了对数几率回归与支持向量机的关系，提到“如果使用对\\n率损失函数ℓlog 来替代式(6.29) 中的0/1 损失函数，则几乎就得到了对率回归模型(3.27)”，但式(6.29)\\n与式(3.27) 形式上相差甚远。为了更清晰的说明对数几率回归与软间隔支持向量机的关系，以下先对式\\n(3.27) 的形式进行变化。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(3.27) 的形式进行变化。\\n将β = (w; b) 和ˆx = (x; 1) 代入式(3.27) 可得\\nℓ(w, b) =\\nm\\nX\\ni=1\\n\\x10\\n−yi\\n\\x00wTxi + b\\n\\x01\\n+ ln\\n\\x10\\n1 + ewTxi+b\\x11\\x11\\n=\\nm\\nX\\ni=1\\n\\x12\\nln\\n1\\neyi(wTxi+b) + ln\\n\\x10\\n1 + ewTxi+b\\x11\\x13\\n=\\nm\\nX\\ni=1\\nln 1 + ewTxi+b\\neyi(wTxi+b)\\n=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1 ln\\n\\x10\\n1 + e−(wTxi+b)\\x11\\n,\\nyi = 1\\nPm\\ni=1 ln\\n\\x10\\n1 + ewTxi+b\\x11\\n,\\nyi = 0\\n上式中正例和反例分别用yi = 1 和yi = 0 表示，这是对数几率回归常用的方式，而在支持向量机中正例\\n和反例习惯用yi = +1 和yi = −1 表示。实际上，若上式也换用yi = +1 和yi = −1 分别表示正例和反\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n例，则上式可改写为\\nℓ(w, b) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1 ln\\n\\x10\\n1 + e−(wTxi+b)\\x11\\n,\\nyi = +1\\nPm\\ni=1 ln\\n\\x10\\n1 + ewTxi+b\\x11\\n,\\nyi = −1\\n=\\nm\\nX\\ni=1\\nln\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n此时上式的求和项正是式(6.33) 所表述的对率损失。\\n6.4.6\\n式(6.41) 的解释\\n参见式(6.13) 的解释\\n6.5\\n支持向量回归\\n6.5.1\\n式(6.43) 的解释\\n相比于线性回归用一条线来拟合训练样本，支持向量回归而是采用一个以f(x) = wTx + b 为中心，\\n宽度为2ϵ 的间隔带，来拟合训练样本。\\n落在带子上的样本不计算损失（类比线性回归在线上的点预测误差为0），不在带子上的则以偏离带\\n子的距离作为损失（类比线性回归的均方误差），然后以最小化损失的方式迫使间隔带从样本最密集的地\\n方穿过，进而达到拟合训练样本的目的。因此支持向量回归的优化问题可以写为\\nmin\\nw,b\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nℓϵ (f (xi) −yi)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='min\\nw,b\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nℓϵ (f (xi) −yi)\\n其中ℓϵ(z) 为“ϵ 不敏感损失函数”（类比线性回归的均方误差损失）\\nℓϵ(z) =\\n(\\n0,\\nif |z| ⩽ϵ\\n|z| −ϵ,\\nif |z| > ϵ\\n1\\n2∥w∥2 为L2 正则项，此处引入正则项除了起正则化本身的作用外，也是为了和软间隔支持向量机的优化\\n目标保持形式上的一致，这样就可以导出对偶问题引入核函数，C 为用来调节损失权重的正则化常数。\\n6.5.2\\n式(6.45) 的推导\\n同软间隔支持向量机，引入松弛变量ξi，令\\nℓϵ (f (xi) −yi) = ξi\\n显然ξi ⩾0，并且当|f (xi) −yi| ⩽ϵ 时，ξi = 0，当|f (xi) −yi| > ϵ 时，ξi = |f (xi) −yi| −ϵ，所以\\n|f (xi) −yi| −ϵ ⩽ξi\\n|f (xi) −yi| ⩽ϵ + ξi\\n−ϵ −ξi ⩽f (xi) −yi ⩽ϵ + ξi\\n因此支持向量回归的优化问题可以化为\\nmin\\nw,b,ξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nξi\\ns.t.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='min\\nw,b,ξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nξi\\ns.t.\\n−ϵ −ξi ⩽f (xi) −yi ⩽ϵ + ξi\\nξi ⩾0, i = 1, 2, . . . , m\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n如果考虑两边采用不同的松弛程度，则有\\nmin\\nw,b,ξi,ˆξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\n\\x10\\nξi + ˆξi\\n\\x11\\ns.t.\\n−ϵ −ˆξi ⩽f (xi) −yi ⩽ϵ + ξi\\nξi ⩾0, ˆξi ⩾0, i = 1, 2, . . . , m\\n6.5.3\\n式(6.52) 的推导\\n将式(6.45) 的约束条件全部恒等变形为小于等于0 的形式可得\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nf (xi) −yi −ϵ −ξi ≤0\\nyi −f (xi) −ϵ −ˆξi ≤0\\n−ξi ≤0\\n−ˆξi ≤0\\n由于以上四个约束条件的拉格朗日乘子分别为αi, ˆαi, µi, ˆµi，所以其对应的KKT 条件为\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nαi (f (xi) −yi −ϵ −ξi) = 0\\nˆαi\\n\\x10\\nyi −f (xi) −ϵ −ˆξi\\n\\x11\\n= 0\\n−µiξi = 0 ⇒µiξi = 0\\n−ˆµi ˆξi = 0 ⇒ˆµi ˆξi = 0'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= 0\\n−µiξi = 0 ⇒µiξi = 0\\n−ˆµi ˆξi = 0 ⇒ˆµi ˆξi = 0\\n又由式(6.49) 和式(6.50) 有\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nµi = C −αi\\nˆµi = C −ˆαi\\n所以上述KKT 条件可以进一步变形为\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nαi (f (xi) −yi −ϵ −ξi) = 0\\nˆαi\\n\\x10\\nyi −f (xi) −ϵ −ˆξi\\n\\x11\\n= 0\\n(C −αi)ξi = 0\\n(C −ˆαi)ˆξi = 0\\n又因为样本(xi, yi) 只可能处在间隔带的某一侧，即约束条件f (xi)−yi−ϵ−ξi = 0 和yi−f (xi)−ϵ−ˆξi = 0\\n不可能同时成立，所以αi 和ˆαi 中至少有一个为0，即αiˆαi = 0。\\n在此基础上再进一步分析可知，如果αi = 0，则根据约束(C −αi)ξi = 0 可知此时ξi = 0。同理，如\\n果ˆαi = 0，则根据约束(C −ˆαi)ˆξi = 0 可知此时ˆξi = 0。所以ξi 和ˆξi 中也是至少有一个为0，即ξi ˆξi = 0。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='将αiˆαi = 0, ξi ˆξi = 0 整合进上述KKT 条件中即可得到式(6.52)。\\n6.6\\n核方法\\n6.6.1\\n式(6.57) 和式(6.58) 的解释\\n式(6.24) 是式(6.20) 的解；式(6.56) 是式(6.43) 的解。对应到表示定理式(6.57) 当中，式(6.20) 和式\\n(6.43) 均为Ω(∥h∥H) = 1\\n2∥w∥2，式(6.20) 的ℓ(h(x1), h(x2), ..., h(xm)) = 0，而式(6.43) 的ℓ(h(x1), h(x2), ..., h(xm)) =\\nC Pm\\ni=1 ℓϵ(f(xi) −yi)，均满足式(6.57) 的要求，式(6.20) 和式(6.43) 的解均为κ(x, xi) 的线性组合，即\\n式(6.58)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 69, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n6.6.2\\n式(6.65) 的推导\\n由表示定理可知，此时二分类KLDA 最终求得的投影直线方程总可以写成如下形式：\\nh(x) =\\nm\\nX\\ni=1\\nαiκ (x, xi)\\n又因为直线方程的固定形式为\\nh(x) = wTϕ(x)\\n所以\\nwTϕ(x) =\\nm\\nX\\ni=1\\nαiκ (x, xi)\\n将κ (x, xi) = ϕ(x)Tϕ(xi) 代入可得\\nwTϕ(x) =\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ(xi)\\n= ϕ(x)T ·\\nm\\nX\\ni=1\\nαiϕ(xi)\\n由于wTϕ(x) 的计算结果为标量，而标量的转置等于其本身，所以\\nwTϕ(x) =\\n\\x00wTϕ(x)\\n\\x01T = ϕ(x)Tw = ϕ(x)T\\nm\\nX\\ni=1\\nαiϕ(xi)\\n即\\nw =\\nm\\nX\\ni=1\\nαiϕ(xi)\\n6.6.3\\n式(6.66) 和式(6.67) 的解释\\n为了详细地说明此式的计算原理，下面首先举例说明，然后再在例子的基础上延展出其一般形式。假\\n设此时仅有4 个样本，其中第1 和第3 个样本的标记为0，第2 和第4 个样本的标记为1，那么此时有'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 69, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m = 4\\nm0 = 2, m1 = 2\\nX0 = {x1, x3}, X1 = {x2, x4}\\nK =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ (x1, x1)\\nκ (x1, x2)\\nκ (x1, x3)\\nκ (x1, x4)\\nκ (x2, x1)\\nκ (x2, x2)\\nκ (x2, x3)\\nκ (x2, x4)\\nκ (x3, x1)\\nκ (x3, x2)\\nκ (x3, x3)\\nκ (x3, x4)\\nκ (x4, x1)\\nκ (x4, x2)\\nκ (x4, x3)\\nκ (x4, x4)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×4\\n10 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\n11 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n1\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n所以\\nˆµ0 = 1\\nm0\\nK10 = 1\\n2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ (x1, x1) + κ (x1, x3)\\nκ (x2, x1) + κ (x2, x3)\\nκ (x3, x1) + κ (x3, x3)\\nκ (x4, x1) + κ (x4, x3)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\nˆµ1 = 1\\nm1\\nK11 = 1\\n2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nκ (x1, x2) + κ (x1, x4)\\nκ (x2, x2) + κ (x2, x4)\\nκ (x3, x2) + κ (x3, x4)\\nκ (x4, x2) + κ (x4, x4)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈R4×1\\n根据此结果易得ˆµ0, ˆµ1 的一般形式为\\nˆµ0 = 1\\nm0\\nK10 = 1\\nm0\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nP\\nx∈X0 κ (x1, x)\\nP\\nx∈X0 κ (x2, x)\\n...\\nP\\nx∈X0 κ (xm, x)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈Rm×1\\nˆµ1 = 1\\nm1\\nK11 = 1\\nm1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nP\\nx∈X1 κ (x1, x)\\nP'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='ˆµ1 = 1\\nm1\\nK11 = 1\\nm1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nP\\nx∈X1 κ (x1, x)\\nP\\nx∈X1 κ (x2, x)\\n...\\nP\\nx∈X1 κ (xm, x)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈Rm×1\\n6.6.4\\n式(6.70) 的推导\\n此式是将式(6.65) 代入式(6.60) 后推得而来的，下面给出详细地推导过程。\\n首先将式(6.65) 代入式(6.60) 的分子可得\\nwTSϕ\\nb w =\\n m\\nX\\ni=1\\nαiϕ (xi)\\n!T\\n· Sϕ\\nb ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nb ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n其中\\nSϕ\\nb =\\n\\x10\\nµϕ\\n1 −µϕ\\n0\\n\\x11 \\x10\\nµϕ\\n1 −µϕ\\n0\\n\\x11T\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!T\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='ϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!\\n将其代入上式可得\\nwTSϕ\\nb w =\\nm\\nX\\ni=1\\nαiϕ (xi)T ·\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!\\n·\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!\\n·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nm\\nX\\ni=1\\nαiϕ (xi)T ϕ(x) −1\\nm0\\nX\\nx∈X0\\nm\\nX\\ni=1\\nαiϕ (xi)T ϕ(x)\\n!\\n·\\n \\n1\\nm1\\nX\\nx∈X1\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ (xi) −1\\nm0\\nX\\nx∈X0\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ (xi)\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由于κ (xi, x) = ϕ(xi)Tϕ(x) 为标量，所以其转置等于本身，即κ (xi, x) = ϕ(xi)Tϕ(x) =\\n\\x00ϕ(xi)Tϕ(x)\\n\\x01T =\\nϕ(x)Tϕ(xi) = κ (xi, x)T，将其代入上式可得\\nwTSϕ\\nb w =\\n \\n1\\nm1\\nm\\nX\\ni=1\\nX\\nx∈X1\\nαiκ (xi, x) −1\\nm0\\nm\\nX\\ni=1\\nX\\nx∈X0\\nαiκ (xi, x)\\n!\\n·\\n \\n1\\nm1\\nm\\nX\\ni=1\\nX\\nx∈X1\\nαiκ (xi, x) −1\\nm0\\nm\\nX\\ni=1\\nX\\nx∈X0\\nαiκ (xi, x)\\n!\\n设α = (α1; α2; ...; αm)T ∈Rm×1，同时结合式(6.66) 的解释可得到ˆµ0, ˆµ1 的一般形式，上式可以化简为\\nwTSϕ\\nb w =\\n\\x00αT ˆµ1 −αT ˆµ0\\n\\x01\\n·\\n\\x10\\nˆµT\\n1 α −ˆµT\\n0 α\\n\\x11\\n= αT · (ˆµ1 −ˆµ0) ·\\n\\x10\\nˆµT\\n1 −ˆµT\\n0\\n\\x11\\n· α'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='0 α\\n\\x11\\n= αT · (ˆµ1 −ˆµ0) ·\\n\\x10\\nˆµT\\n1 −ˆµT\\n0\\n\\x11\\n· α\\n= αT · (ˆµ1 −ˆµ0) · (ˆµ1 −ˆµ0)T · α\\n= αTMα\\n以上便是式(6.70) 分子部分的推导，下面继续推导式(6.70) 的分母部分。将式(6.65) 代入式(6.60) 的分\\n母可得：\\nwTSϕ\\nww =\\n m\\nX\\ni=1\\nαiϕ (xi)\\n!T\\n· Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n其中\\nSϕ\\nw =\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x10\\nϕ(x) −µϕ\\ni\\n\\x11 \\x10\\nϕ(x) −µϕ\\ni\\n\\x11T\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x10\\nϕ(x) −µϕ\\ni\\n\\x11 \\x12\\nϕ(x)T −\\n\\x10\\nµϕ\\ni\\n\\x11T\\x13\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x12\\nϕ(x)ϕ(x)T −ϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−µϕ\\ni ϕ(x)T + µϕ\\ni\\n\\x10\\nµϕ\\ni\\n\\x11T\\x13\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)ϕ(x)T −\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i=0\\nX\\nx∈Xi\\nϕ(x)ϕ(x)T −\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni ϕ(x)T +\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni\\n\\x10\\nµϕ\\ni\\n\\x11T\\n由于\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n=\\nX\\nx∈X0\\nϕ(x)\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+\\nX\\nx∈X1\\nϕ(x)\\n\\x10\\nµϕ\\n1\\n\\x11T\\n= m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n且\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni ϕ(x)T =\\n1\\nX\\ni=0\\nµϕ\\ni\\nX\\nx∈Xi\\nϕ(x)T\\n= µϕ\\n0\\nX\\nx∈X0\\nϕ(x)T + µϕ\\n1\\nX\\nx∈X1\\nϕ(x)T\\n= m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n所以\\nSϕ\\nw =\\nX\\nx∈D\\nϕ(x)ϕ(x)T −2\\n\\x14\\nm0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\x15\\n+ m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n=\\nX\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n=\\nX\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n再将此式代回wTSϕ\\nb w 可得\\nwTSϕ\\nww =\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T ·\\n X\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T!\\n·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiϕ (xi)T ϕ(x)ϕ(x)Tαjϕ (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nαjϕ (xj)\\n−\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj)\\n其中，第1 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiϕ (xi)T ϕ(x)ϕ(x)Tαjϕ (xj) =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiαjκ (xi, x) κ (xj, x)\\n= αTKKTα\\n第2 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= αTKKTα\\n第2 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nαjϕ (xj) = m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjϕ (xi)T µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nϕ (xj)\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjϕ (xi)T\\n\"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n#T\\nϕ (xj)\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαj\\n\"\\n1\\nm0\\nX\\nx∈X0\\nϕ (xi)T ϕ(x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)Tϕ (xj)\\n#\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαj\\n\"\\n1\\nm0\\nX\\nx∈X0\\nκ (xi, x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nκ (xj, x)\\n#\\n= m0αT ˆµ0 ˆµT\\n0 α\\n同理，有第3 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj) = m1αT ˆµ1 ˆµT\\n1 α\\n将上述三项的化简结果代回再将此式代回wTSϕ\\nb w 可得\\nwTSϕ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1 α\\n将上述三项的化简结果代回再将此式代回wTSϕ\\nb w 可得\\nwTSϕ\\nb w = αTKKTα −m0αT ˆµ0 ˆµT\\n0 α −m1αT ˆµ1 ˆµT\\n1 α\\n= αT ·\\n\\x10\\nKKT −m0 ˆµ0 ˆµT\\n0 −m1 ˆµ1 ˆµT\\n1\\n\\x11\\n· α\\n= αT ·\\n \\nKKT −\\n1\\nX\\ni=0\\nmi ˆµi ˆµT\\ni\\n!\\n· α\\n= αTNα\\n6.6.5\\n核对数几率回归\\n将“对数几率回归与支持向量机的关系”中最后得到的对数几率回归重写为如下形式\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n+ λ\\n2m∥w∥2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 73, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中λ 是用来调整正则项权重的正则化常数。假设zi = ϕ(xi) 是由原始空间经核函数映射到高维空间的\\n特征向量，则\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n+ λ\\n2m∥w∥2\\n注意，以上两式中的w 维度是不同的，其分别与xi 和zi 的维度一致。根据表示定理，上式的解可以写为\\nw =\\nm\\nX\\nj=1\\nαjzj\\n将w 代入对数几率回归可得\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(\\n∑m\\nj=1 αjzT\\nj zi+b)\\x11\\n+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjzT\\ni zj\\n用核函数κ (xi, xj) = zT\\ni zj = ϕ (xi)T ϕ (xj) 替换上式中的内积运算\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(\\n∑m\\nj=1 αjκ(xi,xj)+b)\\x11\\n+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjκ (xi, xj)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 73, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjκ (xi, xj)\\n解出α = (α1, α2, ..., αm) 和b 后，即可得f(x) = Pm\\ni=1 αiκ(x, xi) + b。\\n参考文献\\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\\n[2] 王书宁. 凸优化. 清华大学出版社, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第7 章\\n贝叶斯分类器\\n本章是从概率框架下的贝叶斯视角给出机器学习问题的建模方法，不同于前几章着重于算法具体实\\n现，本章的理论性会更强。朴素贝叶斯算法常用于文本分类，例如用于广告邮件检测，贝叶斯网和EM 算\\n法均属于概率图模型的范畴，因此可合并至第14 章一起学习。\\n7.1\\n贝叶斯决策论\\n7.1.1\\n式(7.5) 的推导\\n由式(7.1) 和式(7.4) 可得\\nR(ci|x) = 1 ∗P(c1|x) + ... + 1 ∗P(ci−1|x) + 0 ∗P(ci|x) + 1 ∗P(ci+1|x) + ... + 1 ∗P(cN|x)\\n又PN\\nj=1 P(cj|x) = 1，则\\nR(ci|x) = 1 −P(ci|x)\\n此即式(7.5）。\\n7.1.2\\n式(7.6) 的推导\\n将式(7.5) 代入式(7.3) 即可推得此式\\n7.1.3\\n判别式模型与生成式模型\\n对于判别式模型来说，就是在已知x 的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='对于判别式模型来说，就是在已知x 的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍\\n的模型都属于判别式模型的范畴，尤其是对数几率回归最为直接明了，式(3.23) 和式(3.24) 直接就是后\\n验概率的形式。\\n对于生成式模型来说，理解起来比较抽象，但是可通过思考以下两个问题来理解。\\n(1) 对于数据集来说，其中的样本是如何生成的？通常假设数据集中的样本服从独立同分布，即每个\\n样本都是按照联合概率分布P(x, c) 采样而得，也可以描述为根据P(x, c) 生成的。\\n(2) 若已知样本x 和联合概率分布P(x, c)，如何预测类别呢？若样本x 和联合概率分布P(x, c) 已\\n知，则可以分别求出x 属于各个类别的概率，即P(x, c1), P(x, c2), ..., P(x, cN)，然后选择概率最大的类\\n别作为样本x 的预测结果。\\n因此，之所以称为“生成式”模型，是因为所求的概率P(x, c) 是生成样本x 的概率。\\n7.2\\n极大似然估计\\n7.2.1\\n式(7.12) 和(7.13) 的推导\\n根据式(7.11) 和式(7.10) 可知参数求解式为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='式(7.12) 和(7.13) 的推导\\n根据式(7.11) 和式(7.10) 可知参数求解式为\\nˆθc = arg max\\nθc\\nLL (θc)\\n= arg min\\nθc\\n−LL (θc)\\n= arg min\\nθc\\n−\\nX\\nx∈Dc\\nlog P (x|θc)\\n由“西瓜书”上下文可知，此时假设概率密度函数p(x|c) ∼N (µc, σ2\\nc)，其等价于假设\\nP (x|θc) = P\\n\\x00x|µc, σ2\\nc\\n\\x01\\n=\\n1\\np\\n(2π)d|Σc|\\nexp\\n\\x12\\n−1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中，d 表示x 的维数，Σc = σ2\\nc 为对称正定协方差矩阵，|Σc| 表示Σc 的行列式。将其代入参数求解式\\n可得\\n(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\n−\\nX\\nx∈Dc\\nlog\\n\"\\n1\\np\\n(2π)d|Σc|\\nexp\\n\\x12\\n−1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x13#\\n= arg min\\n(µc,Σc)\\n−\\nX\\nx∈Dc\\n\\x14\\n−d\\n2 log(2π) −1\\n2 log |Σc| −1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nX\\nx∈Dc\\n\\x14d\\n2 log(2π) + 1\\n2 log |Σc| + 1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nX\\nx∈Dc\\n\\x141\\n2 log |Σc| + 1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n假设此时数据集Dc 中的样本个数为n，即|Dc| = n，则上式可以改写为\\n(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\nn\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\nn\\nX\\ni=1\\n\\x141\\n2 log |Σc| + 1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nn\\n2 log |Σc| +\\nn\\nX\\ni=1\\n1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n为了便于分别求解ˆµc 和ˆΣc，在这里我们根据式xTAx = tr(AxxT), ¯x = 1\\nn\\nPn\\ni=1 xi 将上式中的最后一项\\n作如下恒等变形：\\nn\\nX\\ni=1\\n1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −µc)(xi −µc)T\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n\\x00xixT\\ni −xiµT\\nc −µcxT\\ni + µcµT\\nc\\n\\x01\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −n¯xµT\\nc −nµc¯xT + nµcµT\\nc\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −2n¯xµT\\nc + nµcµT'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −2n¯xµT\\nc + nµcµT\\nc + 2n¯x¯xT −2n¯x¯xT\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n  n\\nX\\ni=1\\nxixT\\ni −2n¯x¯xT + n¯x¯xT\\n!\\n+\\n\\x00nµcµT\\nc −2n¯xµT\\nc + n¯x¯xT\\x01\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T +\\nn\\nX\\ni=1\\n(µc −¯x)(µc −¯x)T\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(µc −¯x)(µc −¯x)T\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ 1\\n2 tr\\n\\x02\\nn · Σ−1\\nc (µc −¯x)(µc −¯x)T\\x03\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ n\\n2 tr\\n\\x02\\nΣ−1\\nc (µc −¯x)(µc −¯x)T\\x03\\n=1\\n2 tr\\n\"'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='#\\n+ n\\n2 tr\\n\\x02\\nΣ−1\\nc (µc −¯x)(µc −¯x)T\\x03\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ n\\n2 (µc −¯x)TΣ−1\\nc (µc −¯x)\\n所以\\n(ˆµc, ˆΣc) = arg min\\n(µc,Σc)\\nn\\n2 log |Σc| + 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n+ n\\n2 (µc −¯x)TΣ−1\\nc (µc −¯x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n观察上式可知，由于此时Σ−1\\nc\\n和Σc 一样均为正定矩阵，所以当µc −¯x ̸= 0 时，上式最后一项为正定二\\n次型。根据正定二次型的性质可知，此时上式最后一项的取值仅与µc −¯x 相关，并有当且仅当µc −¯x = 0\\n时，上式最后一项取最小值0，此时可以解得\\nˆµc = ¯x = 1\\nn\\nn\\nX\\ni=1\\nxi\\n将求解出来的ˆµc 代回参数求解式可得新的参数求解式，有\\nˆΣc = arg min\\nΣc\\nn\\n2 log |Σc| + 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯x)(xi −¯x)T\\n#\\n此时的参数求解式是仅与Σc 相关的函数。\\n为了求解ˆΣc，在这里我们不加证明地给出一个引理：设B 为p 阶正定矩阵，n > 0 为实数，在对所\\n有p 阶正定矩阵Σ 有\\nn\\n2 log |Σ| + 1\\n2 tr\\n\\x02\\nΣ−1B\\n\\x03\\n≥n\\n2 log |B| + pn\\n2 (1 −log n)\\n当且仅当Σ = 1\\nnB 时等号成立。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2 log |B| + pn\\n2 (1 −log n)\\n当且仅当Σ = 1\\nnB 时等号成立。\\n（引理的证明可搜索张伟平老师的“多元正态分布参数的估计和数据的清洁与变换”课件）\\n根据此引理可知，当且仅当Σc = 1\\nn\\nPn\\ni=1(xi −¯x)(xi −¯x)T 时，上述参数求解式中arg min 后面的式\\n子取到最小值，那么此时的Σc 即我们想要求解的ˆΣc。\\n7.3\\n朴素贝叶斯分类器\\n7.3.1\\n式(7.16) 和式(7.17) 的解释\\n该式是基于大数定律的频率近似概率的思路，而该思路的本质仍然是极大似然估计，下面举例说明。\\n以掷硬币为例，假设投掷硬币5 次，结果依次是正面、正面、反面、正面、反面，试基于此观察结果估计\\n硬币正面朝上的概率。\\n设硬币正面朝上的概率为θ，其服从伯努利分布，因此反面朝上的概率为1 −θ，同时设每次投掷结果\\n相互独立，即独立同分布，则似然为\\nL(θ) = θ · θ · (1 −θ) · θ · (1 −θ)\\n= θ3(1 −θ)2\\n对数似然为\\nLL(θ) = ln L(θ) = 3 ln θ + 2 ln(1 −θ)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='对数似然为\\nLL(θ) = ln L(θ) = 3 ln θ + 2 ln(1 −θ)\\n易证LL(θ) 是关于θ 的凹函数，因此对其求一阶导并令导数等于零即可求出最大值点，具体地\\n∂LL(θ)\\n∂θ\\n= ∂(3 ln θ + 2 ln(1 −θ))\\n∂θ\\n= 3\\nθ −\\n2\\n1 −θ\\n= 3 −5θ\\nθ(1 −θ)\\n令上式等于0 可解得θ = 3\\n5，显然3\\n5 也是正面出现的频率。\\n7.3.2\\n式(7.18) 的解释\\n该式所表示的正态分布并不一定是标准正态分布，因此p(xi|c) 的取值并不一定在(0, 1) 之间，但是\\n仍然不妨碍其用作“概率”，因为根据朴素贝叶斯的算法原理可知，只需p(xi|c) 的值仅仅是用来比大小，\\n因此只关心相对值而不关心绝对值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n7.3.3\\n贝叶斯估计\\n[1]\\n贝叶斯学派视角下的一类点估计法称为贝叶斯估计，常用的贝叶斯估计有最大后验估计（Maximum\\nA Posteriori Estimation，简称MAP）、后验中位数估计和后验期望值估计这3 种参数估计方法，下面给\\n出这3 种方法的具体定义。\\n设总体的概率质量函数（若总体的分布为连续型时则改为概率密度函数，此处以离散型为例）为\\nP(x|θ)，从该总体中抽取出的n 个独立同分布的样本构成样本集D = {x1, x2, · · · , xn}，则根据贝叶斯\\n式可得，在给定样本集D 的条件下，θ 的条件概率为\\nP(θ|D) = P(D|θ)P(θ)\\nP(D)\\n=\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ)\\n其中P(D|θ) 为似然函数，由于样本集D 中的样本是独立同分布的，所以似然函数可以进一步展开，有\\nP(θ|D) =\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ) =\\nQn\\ni=1 P(xi|θ)P(θ)\\nP\\nθ\\nQn\\ni=1 P(xi|θ)P(θ)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Qn\\ni=1 P(xi|θ)P(θ)\\nP\\nθ\\nQn\\ni=1 P(xi|θ)P(θ)\\n根据贝叶斯学派的观点，此条件概率代表了我们在已知样本集D 后对θ 产生的新的认识，它综合了\\n我们对θ 主观预设的先验概率P(θ) 和样本集D 带来的信息，通常称其为θ 的后验概率。\\n贝叶斯学派认为，在得到P(θ|D) 以后，对参数θ 的任何统计推断，都只能基于P(θ|D)。至于具体\\n如何去使用它，可以结合某种准则一起去进行，统计学家也有一定的自由度。对于点估计来说，求使得\\nP(θ|D) 达到最大值的ˆθMAP 作为θ 的估计称为最大后验估计，求P(θ|D) 的中位数ˆθMedian 作为θ 的估计\\n称为后验中位数估计，求P(θ|D) 的期望值（均值）ˆθMean 作为θ 的估计称为后验期望值估计。\\n7.3.4\\nCategorical 分布\\nCategorical 分布又称为广义伯努利分布，是将伯努利分布中的随机变量可取值个数由两个泛化为多\\n个得到的分布。具体地，设离散型随机变量X 共有k 种可能的取值{x1, x2, · · · , xk}，且X 取到每个值'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的概率分别为P(X = x1) = θ1, P(X = x2) = θ2, · · · , P(X = xk) = θk，则称随机变量X 服从参数为\\nθ1, θ2, · · · , θk 的Categorical 分布，其概率质量函数为\\nP(X = xi) = p(xi) = θi\\n7.3.5\\nDirichlet 分布\\n类似于Categorical 分布是伯努利分布的泛化形式，Dirichlet 分布是Beta 分布的泛化形式。对于一\\n个k 维随机变量x = (x1, x2, · · · , xk) ∈Rk，其中xi(i = 1, 2, · · · , k) 满足0 ⩽xi ⩽1, Pk\\ni=1 xi = 1，若x\\n服从参数为α = (α1, α2, · · · , αk) ∈Rk 的Dirichlet 分布，则其概率密度函数为\\np(x; α) =\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nxαi−1\\ni\\n其中Γ(z) =\\nR ∞'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nxαi−1\\ni\\n其中Γ(z) =\\nR ∞\\n0 xz−1e−xdx 为Gamma 函数，当α = (1, 1, · · · , 1) 时，Dirichlet 分布等价于均匀分布。\\n7.3.6\\n式(7.19) 和式(7.20) 的推导\\n从贝叶斯估计的角度来说，拉普拉斯修正就等价于先验概率为Dirichlet 分布的后验期望值估计。为\\n了接下来的叙述方便，我们重新定义一下相关数学符号。\\n设有包含m 个独立同分布样本的训练集D，D 中可能的类别数为k，其类别的具体取值范围为\\n{c1, c2, ..., ck}。若令随机变量C 表示样本所属的类别，且C 取到每个值的概率分别为P(C = c1) =\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nθ1, P(C = c2) = θ2, ..., P(C = ck) = θk，那么显然C 服从参数为θ = (θ1, θ2, ..., θk) ∈Rk 的Categorical\\n分布，其概率质量函数为\\nP(C = ci) = P(ci) = θi\\n其中P(ci) = θi 就是式(7.9) 所要求解的ˆP(c)，下面我们用贝叶斯估计中的后验期望值估计来估计\\nθi。根据贝叶斯估计的原理可知，在进行参数估计之前，需要先主观预设一个先验概率P(θ)，通常为了方\\n便计算后验概率P(θ|D)，我们会用似然函数P(D|θ) 的共轭先验作为我们的先验概率。显然，此时的似\\n然函数P(D|θ) 是一个基于Categorical 分布的似然函数，而Categorical 分布的共轭先验为Dirichlet 分\\n布，所以只需要预设先验概率P(θ) 为Dirichlet 分布，然后使用后验期望值估计就能估计出θi。\\n具体地，记D 中样本类别取值为ci 的样本个数为yi，则似然函数P(D|θ) 可展开为\\nP(D|θ) = θy1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='P(D|θ) = θy1\\n1 ...θyk\\nk =\\nk\\nY\\ni=1\\nθyi\\ni\\n则有后验概率\\nP(θ|D) = P(D|θ)P(θ)\\nP(D)\\n=\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ)\\n=\\nQk\\ni=1 θyi\\ni · P(θ)\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · P(θ)\\ni\\n假设此时先验概率P(θ) 是参数为α = (α1, α2, ..., αk) ∈Rk 的Dirichlet 分布，则P(θ) 可写为\\nP(θ; α) =\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nθαi−1\\ni\\n将其代入P(D|θ) 可得\\nP(θ|D) =\\nQk\\ni=1 θyi\\ni · P(θ)\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · P(θ)\\ni\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\nP\\nθ\\n\\uf8ee\\n\\uf8f0Qk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\n\\uf8f9\\n\\uf8fb\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Qk\\ni=1 θαi−1\\ni\\n\\uf8f9\\n\\uf8fb\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\ni\\n·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\n=\\nQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\ni\\n=\\nQk\\ni=1 θαi+yi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时若设α + y = (α1 + y1, α2 + y2, ..., αk + yk) ∈Rk，则根据Dirichlet 分布的定义可知\\nP(θ; α + y) =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\nX\\nθ\\nP(θ; α + y) =\\nX\\nθ\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n1 =\\nX\\nθ\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n1 =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nX\\nθ\\n\" k\\nY\\ni=1\\nθαi+yi−1\\ni\\n#\\n1\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\n将此结论代入P(D|θ) 可得\\nP(θ|D) =\\nQk'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x11\\nQk\\ni=1 Γ(αi + yi)\\n将此结论代入P(D|θ) 可得\\nP(θ|D) =\\nQk\\ni=1 θαi+yi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni\\n=\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n= P(θ; α + y)\\n综上可知，对于服从Categorical 分布的θ 来说，假设其先验概率P(θ) 是参数为α 的Dirichlet 分布时，\\n得到的后验概率P(θ|D) 是参数为α + y 的Dirichlet 分布，通常我们称这种先验概率分布和后验概率分\\n布形式相同的这对分布为共轭分布。在推得后验概率P(θ|D) 的具体形式以后，根据后验期望值估计可得\\nθi 的估计值为\\nθi = EP (θ|D)[θi]\\n= EP (θ;α+y)[θi]\\n=\\nαi + yi\\nPk\\nj=1(αj + yj)\\n=\\nαi + yi\\nPk\\nj=1 αj + Pk\\nj=1 yj\\n=\\nαi + yi\\nPk\\nj=1 αj + m'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Pk\\nj=1 αj + Pk\\nj=1 yj\\n=\\nαi + yi\\nPk\\nj=1 αj + m\\n显然，式(7.9) 是当α = (1, 1, ..., 1) 时推得的具体结果，此时等价于我们主观预设的先验概率P(θ) 服从\\n均匀分布，此即拉普拉斯修正。同理，当我们调整α 的取值后，即可推得其他数据平滑的公式。\\n7.4\\n半朴素贝叶斯分类器\\n7.4.1\\n式(7.21) 的解释\\n在朴素贝叶斯中求解P(xi|c) 时，先挑出类别为c 的样本，若是离散属性则按大数定律估计P(xi|c)，\\n若是连续属性则求这些样本的均值和方差，接着按正态分布估计P(xi|c)。现在估计P(xi|c, pai)，则是先\\n挑出类别为c 且属性xi 所依赖的属性为pai 的样本，剩下步骤与估计P(xi|c) 时相同。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n7.4.2\\n式(7.22) 的解释\\n该式写为如下形式可能更容易理解：\\nI(xi, xj|y) =\\nN\\nX\\nn=1\\nP(xi, xj|cn) log\\nP(xi, xj|cn)\\nP(xi|cn)P(xj|cn)\\n其中i, j = 1, 2, ..., d 且i ̸= j，N 为类别个数。该式共可得到d(d−1)\\n2\\n个I(xi, xj|y)，即每对(xi, xj) 均有\\n一个条件互信息I(xi, xj|y)。\\n7.4.3\\n式(7.23) 的推导\\n基于贝叶斯定理，式(7.8) 将联合概率P(x, c) 写为等价形式P(x|c)P(c)，实际上，也可将向量x 拆\\n开，把P(x, c) 写为P(x1, x2, ..., xd, c) 形式，然后利用概率公式P(A, B) = P(A|B)P(B) 对其恒等变形\\nP(x, c) = P (x1, x2, . . . , xd, c)\\n= P (x1, x2, . . . , xd | c) P(c)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= P (x1, x2, . . . , xd | c) P(c)\\n= P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi) P (c, xi)\\n类似式(7.14) 采用属性条件独立性假设，则\\nP(x1, ..., xi−1, xi+1, ..., xd|c, xi) =\\nd\\nY\\nj=1j̸=i\\nP(xj|c, xi)\\n根据式(7.25) 可知，当j = i 时，|Dc,xi| = |Dc,xi,xj|，若不考虑平滑项，则此时P(xj|c, xi) = 1，因此在\\n上式的连乘项中可放开j ̸= i 的约束，即\\nP(x1, ..., xi−1, xi+1, ..., xd|c, xi) =\\nd\\nY\\nj=1\\nP(xj|c, xi)\\n综上可得：\\nP(c|x) = P(x, c)\\nP(x)\\n= P (c, xi) P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi)\\nP(x)\\n∝P (c, xi) P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi)\\n= P (c, xi)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= P (c, xi)\\nd\\nY\\nj=1\\nP(xj|c, xi)\\n上式是将属性xi 作为超父属性的，AODE 尝试将每个属性作为超父来构建SPODE，然后将那些具\\n有足够训练数据支撑的SPODE 集成起来作为最终结果。具体来说，对于总共d 个属性来说，共有d 个\\n不同的上式，集成直接求和即可，因为对于不同的类别标记c 均有d 个不同的上式，至于如何满足“足够\\n训练数据支撑的SPODE”这个条件，注意式(7.24) 和式(7.25) 均使用到了|Dc,xi| 和|Dc,xi,xj|，若集合\\nDxi 中样本数量过少，则|Dc,xi| 和|Dc,xi,xj| 将会更小，因此在式(7.23) 中要求集合Dxi 中样本数量不少\\n于m′。\\n7.4.4\\n式(7.24) 和式(7.25) 的推导\\n类比式(7.19) 和式(7.20) 的推导。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 81, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n7.5\\n贝叶斯网\\n7.5.1\\n式(7.27) 的解释\\n在这里补充一下同父结构和顺序结构的推导。同父结构：在给定父节点x1 的条件下x3, x4 独立\\nP(x3, x4|x1) = P(x1, x3, x4)\\nP(x1)\\n= P(x1)P(x3|x1)P(x4|x1)\\nP(x1)\\n= P(x3|x1)P(x4|x1)\\n顺序结构：在给定节点x 的条件下y, z 独立\\nP(y, z|x) = P(x, y, z)\\nP(x)\\n= P(z)P(x|z)P(y|x)\\nP(x)\\n= P(z, x)P(y|x)\\nP(x)\\n= P(z|x)P(y|x)\\n7.6\\nEM 算法\\n“西瓜书”中仅给出了EM 算法的运算步骤，其原理并未展开讲解，下面补充EM 算法的推导原理，\\n以及所用到的相关数学知识。\\n7.6.1\\nJensen 不等式\\n若f 是凸函数，则下式恒成立\\nf (tx1 + (1 −t)x2) ⩽tf(x1) + (1 −t)f(x2)\\n其中t ∈[0, 1]，若将x 推广到n 个时同样成立，即'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 81, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='其中t ∈[0, 1]，若将x 推广到n 个时同样成立，即\\nf(t1x1 + t2x2 + ... + tnxn) ⩽t1f(x1) + t2f(x2) + ... + tnf(tn)\\n其中t1, t2, ..., tn ∈[0, 1], Pn\\ni=1 ti = 1。此不等式在概率论中通常以如下形式出现\\nφ(E[X]) ⩽E[φ(X)]\\n其中X 是随机变量，φ 为凸函数，E[X] 为随机变量X 的期望。显然，若f 和φ 是凹函数，则上述不等\\n式中的⩽换成⩾也恒成立。\\n7.6.2\\nEM 算法的推导\\n假设现有一批独立同分布的样本{x1, x2, ..., xm}，它们是由某个含有隐变量的概率分布p(x, z; θ) 生\\n成，现尝试用极大似然估计法估计此概率分布的参数。为了便于讨论，此处假设z 为离散型随机变量，则\\n对数似然函数为\\nLL(θ) =\\nm\\nX\\ni=1\\nln p(xi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(xi, zi; θ)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n显然，此时LL(θ) 里含有未知的隐变量z 以及求和项的对数，相比于不含隐变量的对数似然函数，显然\\n该似然函数的极大值点较难求解，而EM 算法则给出了一种迭代的方法来完成对LL(θ) 的极大化。\\n下面给出两种推导方法，一个是出自李航老师的《统计学习方法》\\n[2]，一个是出自吴恩达老师的CS229，\\n两种推导方式虽然形式上有差异，但最终的Q 函数相等，接下来先讲述两种推导方法，最后会给出Q 函\\n数是相等的证明。\\n首先给出《统计学习方法》中的推导方法，设X = {x1, x2, ..., xm}, Z = {z1, z2, ..., zm}，则对数似然\\n函数可以改写为\\nLL(θ) = ln P(X|θ)\\n= ln\\nX\\nZ\\nP(X, Z|θ)\\n= ln\\n X\\nZ\\nP(X|Z, θ)P(Z|θ)\\n!\\nEM 算法采用的是通过迭代逐步近似极大化L(θ)：假设第t 次迭代时θ 的估计值是θ(t)，我们希望第t + 1\\n次迭代时的θ 能使LL(θ) 增大，即LL(θ) > LL(θ(t))。为此，考虑两者的差'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='次迭代时的θ 能使LL(θ) 增大，即LL(θ) > LL(θ(t))。为此，考虑两者的差\\nLL(θ) −LL(θ(t)) = ln\\n X\\nZ\\nP(X|Z, θ)P(Z|θ)\\n!\\n−ln P(X|θ(t))\\n= ln\\n\\uf8eb\\n\\uf8edX\\nZ\\nP(Z|X, θ(t))\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n\\uf8f6\\n\\uf8f8−ln P(X|θ(t))\\n由上述Jensen 不等式可得\\nLL(θ) −LL(θ(t)) ⩾\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−1 · ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−\\nX\\nZ\\nP(Z|X, θ(t)) · ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t))\\n\\uf8eb\\n\\uf8edln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='P(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n令\\nB(θ, θ(t)) = LL(θ(t)) +\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n则\\nLL(θ) ⩾B(θ, θ(t))\\n即B(θ, θ(t)) 是LL(θ) 的下界，此时若设θ(t+1) 能使得B(θ, θ(t)) 达到极大，即\\nB(θ(t+1), θ(t)) ⩾B(θ, θ(t))\\n由于LL(θ(t)) = B(θ(t), θ(t))，那么可以进一步推得\\nLL(θ(t+1)) ⩾B(θ(t+1), θ(t)) ⩾B(θ(t), θ(t)) = LL(θ(t))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nLL(θ(t+1)) ⩾LL(θ(t))\\n因此，任何能使得B(θ, θ(t)) 增大的θ，也可以使得LL(θ) 增大，于是问题就转化为了求解能使得B(θ, θ(t))\\n达到极大的θ(t+1)，即\\nθ(t+1) = arg max\\nθ\\nB(θ, θ(t))\\n= arg max\\nθ\\n\\uf8eb\\n\\uf8edLL(θ(t)) +\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n略去对θ 极大化而言是常数的项\\nθ(t+1) = arg max\\nθ\\n X\\nZ\\nP(Z|X, θ(t)) ln (P(X|Z, θ)P(Z|θ))\\n!\\n= arg max\\nθ\\n X\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\n!\\n= arg max\\nθ\\nQ(θ, θ(t))\\n到此即完成了EM 算法的一次迭代，求出的θ(t+1) 作为下一次迭代的初始θ(t)。综上，EM 算法的“E 步”\\n和“M 步”可总结为以下两步。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='和“M 步”可总结为以下两步。\\nE 步：计算完全数据的对数似然函数ln P(X, Z|θ) 关于在给定观测数据X 和当前参数θ(t) 下对未观\\n测数据Z 的条件概率分布P(Z|X, θ(t)) 的期望Q(θ, θ(t))：\\nQ(θ, θ(t)) = EZ[ln P(X, Z|θ)|X, θ(t)] =\\nX\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\nM 步：求使得Q(θ, θ(t)) 达到极大的θ(t+1)。\\n接下来给出CS229 中的推导方法，设zi 的概率质量函数为Qi(zi)，则LL(θ) 可以作如下恒等变形\\nLL(θ) =\\nm\\nX\\ni=1\\nln p(xi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(xi, zi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n其中P\\nzi Qi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n可以看做是对\\np(xi, zi; θ)\\nQi(zi)\\n关于zi 求期望，即\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n= Ezi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Qi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n= Ezi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)\\nQi(zi)\\n\\uf8f9\\n\\uf8fb\\n由Jensen 不等式可得\\nln\\n\\uf8eb\\n\\uf8edEzi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)\\nQi(zi)\\n\\uf8f9\\n\\uf8fb\\n\\uf8f6\\n\\uf8f8⩾Ezi\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8edp(xi, zi; θ)\\nQi(zi)\\n\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n⩾\\nX\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n将此式代入LL(θ) 可得\\nLL(θ) =\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n(7.6.1)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n若令B(θ) =\\nm\\nP\\ni=1\\nP\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n，则此时B(θ) 为LL(θ) 的下界函数，那么这个下界函数所能构成\\n的最优下界是多少？即B(θ) 的最大值是多少？显然，B(θ) 是LL(θ) 的下界函数，反过来LL(θ) 是其上\\n界函数，所以如果能使得B(θ) = LL(θ)，则此时的B(θ) 就取到了最大值。根据Jensen 不等式的性质\\n可知，如果能使得\\np(xi, zi; θ)\\nQi(zi)\\n恒等于某个常量c，大于等于号便可以取到等号。因此，只需任意选取满足\\np(xi, zi; θ)\\nQi(zi)\\n= c 的Qi(zi) 就能使得B(θ) 达到最大值。由于Qi(zi) 是zi 的概率质量函数，所以Qi(zi) 同\\n时也满足约束0 ⩽Qi(zi) ⩽1, P\\nzi Qi(zi) = 1，结合Qi(zi) 的所有约束可以推得\\np(xi, zi; θ)\\nQi(zi)\\n= c\\np(xi, zi; θ) = c · Qi(zi)\\nX\\nzi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Qi(zi)\\n= c\\np(xi, zi; θ) = c · Qi(zi)\\nX\\nzi\\np(xi, zi; θ) = c ·\\nX\\nzi\\nQi(zi)\\nX\\nzi\\np(xi, zi; θ) = c\\np(xi, zi; θ)\\nQi(zi)\\n=\\nX\\nzi\\np(xi, zi; θ)\\nQi(zi) =\\np(xi, zi; θ)\\nP\\nzi\\np(xi, zi; θ) =\\np(xi, zi; θ)\\np(xi; θ)\\n= p(zi|xi; θ)\\n所以，当且仅当Qi(zi) = p(zi|xi; θ) 时B(θ) 取到最大值，将Qi(zi) = p(zi|xi; θ) 代回LL(θ) 和B(θ) 可\\n以推得\\nLL(θ) =\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n(7.6.2)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(zi|xi; θ)\\np(xi, zi; θ)\\np(zi|xi; θ)\\n(7.6.3)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ) ln\\np(xi, zi; θ)\\np(zi|xi; θ)\\n(7.6.4)\\n= max{B(θ)}\\n(7.6.5)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='p(zi|xi; θ)\\n(7.6.4)\\n= max{B(θ)}\\n(7.6.5)\\n其中式(7.6.4) 是式(7.6.1) 中不等式取等号时的情形。由以上推导可知，此时对数似然函数LL(θ) 等价\\n于其下界函数的最大值max{B(θ)}，所以要想极大化LL(θ) 可以通过极大化max{B(θ)} 来间接极大化\\nLL(θ)，因此，下面考虑如何极大化max{B(θ)}。假设已知第t 次迭代的参数为θ(t)，而第t + 1 次迭代的\\n参数θ(t+1) 可通过如下方式求得\\nθ(t+1) = arg max\\nθ\\nmax{B(θ)}\\n(7.6.6)\\n= arg max\\nθ\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ)\\np(zi|xi; θ(t))\\n(7.6.7)\\n= arg max\\nθ\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)\\n(7.6.8)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时将θ(t+1) 代入LL(θ) 可推得\\nLL(θ(t+1)) = max{B(θ(t+1))}\\n(7.6.9)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t+1)) ln\\np(xi, zi; θ(t+1))\\np(zi|xi; θ(t+1))\\n(7.6.10)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ(t+1))\\np(zi|xi; θ(t))\\n(7.6.11)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ(t))\\np(zi|xi; θ(t))\\n(7.6.12)\\n= max{B(θ(t))}\\n(7.6.13)\\n= LL(θ(t))\\n(7.6.14)\\n其中，式(7.6.9) 和式(7.6.10) 分别由式(7.6.5) 和式(7.6.4) 推得，式(7.6.11) 由式(7.6.1) 推得，式(7.6.12)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='由式(7.6.7) 推得，式(7.6.13) 和式(7.6.14) 由式(7.6.2) 至式(7.6.5) 推得。此时若令\\nQ(θ, θ(t)) =\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)\\n由式(7.6.9) 至式(7.6.14) 可知，凡是能使得Q(θ, θ(t)) 达到极大的θ(t+1) 一定能使得LL(θ(t+1)) ⩾LL(θ(t))。\\n综上，EM 算法的“E 步”和“M 步”可总结为以下两步。\\nE 步：令Qi(zi) = p(zi|xi; θ) 并写出Q(θ, θ(t))；\\nM 步：求使得Q(θ, θ(t)) 到达极大的θ(t+1)。\\n以上便是EM 算法的两种推导方法，下面证明两种推导方法中的Q 函数相等。\\nQ(θ|θ(t)) =\\nX\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t)) ln\\n\" m\\nY\\ni=1\\nP(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t))\\n\" m\\nX'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t))\\n\" m\\nX\\ni=1\\nln P(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t)) [ln P(x1, z1|θ) + ln P(x2, z2|θ) + ... + ln P(xm, zm|θ)]\\n)\\n=\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n+ ... +\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中\\nP\\nz1,z2,...,zm\\n\\x14 m\\nQ\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n\\x15\\n可作如下恒等变形：\\n=\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t)) · P(z1|x1, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nX\\nz2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t)) · P(z1|x1, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\nX\\nz2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t))\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\nX\\nz2,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(X\\nz2\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nP(z2|x2, θ(t))\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t))\\n#)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nP(z2|x2, θ(t)) ×\\nX\\nz3\\nP(z3|x3, θ(t)) × ... ×\\nX\\nzm\\nP(zm|xm, θ(t))\\n)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ) × {1 × 1 × ... × 1}\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n所以\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n同理可得\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='同理可得\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x2, z2|θ)\\n#\\n=\\nX\\nz2\\nP(z2|x2, θ(t)) ln P(x2, z2|θ)\\n...\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nzm\\nP(zm|xm, θ(t)) ln P(xm, zm|θ)\\n将上式代入Q(θ|θ(t)) 可得\\nQ(θ|θ(t)) =\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n+ ... +\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ) + ... +\\nX\\nzm\\nP(zm|xm, θ(t)) ln P(xm, zm|θ)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\nP(zi|xi, θ(t)) ln P(xi, zi|θ)\\n参考文献'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nm\\nX\\ni=1\\nX\\nzi\\nP(zi|xi, θ(t)) ln P(xi, zi|θ)\\n参考文献\\n[1] 陈希孺. 概率论与数理统计. 中国科学技术大学出版社, 2009.\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 87, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第8 章\\n集成学习\\n集成学习(ensemble learning) 描述的是组合多个基础的学习器（模型）的结果已达到更加鲁棒、效果\\n更好的学习器。在“西瓜书”作者周志华教授的谷歌学术主页的top 引用文章中，很大一部分都和集成学\\n习有关。\\n图8-3 周志华教授谷歌学术top10 引用文章(截止到2023-02-19)\\n如图8-3所示。在引用次数前10 的文章中，第1 名“Top 10 algorithms in data mining”是在ICDM’\\n06 中投票选出的数据挖掘十大算法，每个提名算法均由业内专家代表去阐述，然后进行投票，其中最终得\\n票排名第7 位的“Adaboost”即由周志华教授作为代表进行阐述；第2 名“Isolation forest”是通过集成学\\n习的技术用来做异常检测。第3 名的“Ensemble Methods: Foundations and Algorithms”则是周志华教'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 87, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='授所著的集成学习专著。第6 名“Ensembing neural networks: many could be better than all”催生了基\\n于优化的集成修剪(Ensemble pruning) 技术；第7 名的“Exploratory undersampling for class-imbalance\\nlearning”是以集成学习技术解决类别不平衡问题。\\n毫不夸张的说，周志华教授在集成学习领域深耕了很多年，是绝对的权威。而集成学习也是经受了时\\n间考验的非常有效的算法，常常被各位竞赛同学作为涨点提分的致胜法宝。下面，让我们一起认真享受\\n“西瓜书”作者最拿手的集成学习章节吧。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 88, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.1\\n个体与集成\\n基学习器(base learner) 的概念在论文中经常出现，可留意一下；另外，本节提到的投票法有两种，除\\n了本节的多数投票(majority voting)，还有概率投票(probability voting)，这两点在8.4 节中均会提及，即\\n硬投票和软投票。\\n8.1.1\\n式(8.1) 的解释\\nhi(x) 是编号为i 的基分类器给x 的预测标记，f(x) 是x 的真实标记，它们之间不一致的概率记为\\nϵ。\\n8.1.2\\n式(8.2) 的解释\\n注意到当前仅针对二分类问题y ∈{−1, +1}, 即预测标记hi(x) ∈{−1, +1} 。各个基分类器hi 的分\\n类结果求和之后结果的正、负或0，代表投票法产生的结果，即“少数服从多数”，符号函数sign，将正数\\n变成1，负数变成-1，0 仍然是0，所以H(x) 是由投票法产生的分类结果。\\n8.1.3\\n式(8.3) 的推导\\n由基分类器相互独立，假设随机变量X 为T 个基分类器分类正确的次数，因此随机变量X 服从二项'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 88, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='由基分类器相互独立，假设随机变量X 为T 个基分类器分类正确的次数，因此随机变量X 服从二项\\n分布：X ∼B(T, 1 −ϵ)，设xi 为每一个分类器分类正确的次数，则xi ∼B(1, 1 −ϵ)\\uffffi = 1\\uffff2\\uffff3\\uffff...\\uffffT\\uffff，那么有\\nX =\\nT\\nX\\ni=1\\nxi\\nE(X) =\\nT\\nX\\ni=1\\nE(xi) = (1 −ϵ)T\\n证明过程如下：\\nP(H(x) ̸= f(x)) =P(X ≤⌊T/2⌋)\\n⩽P(X ≤T/2)\\n= P\\n\\x14\\nX −(1 −ϵ)T ⩽T\\n2 −(1 −ϵ)T\\n\\x15\\n= P\\n\\x14\\nX −(1 −ϵ)T ⩽−T\\n2 (1 −2ϵ)]\\n\\x15\\n= P\\n\" T\\nX\\ni=1\\nxi −\\nT\\nX\\ni=1\\nE(xi) ⩽−T\\n2 (1 −2ϵ)]\\n#\\n= P\\n\"\\n1\\nT\\nT\\nX\\ni=1\\nxi −1\\nT\\nT\\nX\\ni=1\\nE(xi) ⩽−1\\n2 (1 −2ϵ)]\\n#\\n根据Hoeffding 不等式知\\nP\\n \\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩽−δ\\n!\\n⩽exp\\n\\x00−2mδ2\\x01\\n令δ = (1−2ϵ)\\n2\\n, m = T 得'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 88, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='E (xi) ⩽−δ\\n!\\n⩽exp\\n\\x00−2mδ2\\x01\\n令δ = (1−2ϵ)\\n2\\n, m = T 得\\nP(H(x) ̸= f(x)) =\\n⌊T /2⌋\\nX\\nk=0\\n \\nT\\nk\\n!\\n(1 −ϵ)kϵT −k\\n⩽exp\\n\\x12\\n−1\\n2T(1 −2ϵ)2\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2\\nBoosting\\n注意8.1 节最后一段提到：根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即\\n个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同\\n时生成的并行化方法。\\n本节Boosting 为前者的代表，Adaboost 又是Boosting 族算法的代表。\\n8.2.1\\n式(8.4) 的解释\\n这个式子是集成学习的加性模型，加性模型不采用梯度下降的思想，而是H(x) = PT −1\\nt=1 αtht(x) +\\nαT hT (x)，共迭代T 次，每次更新求解一个理论上最优的hT 和αT 。hT 和αT 的定义参见式(8.18) 和式(8.11)\\n8.2.2\\n式(8.5) 的解释\\n先考虑指数损失函数e−f(x)H(x) 的含义参见“西瓜书”图6.5 ：f 为真实函数，对于样本x 来说，\\nf(x) ∈{+1, −1} 只能取+1 和−1，而H(x) 是一个实数。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='f(x) ∈{+1, −1} 只能取+1 和−1，而H(x) 是一个实数。\\n当H(x) 的符号与f(x) 一致时，f(x)H(x) > 0，因此e−f(x)H(x) = e−|H(x)| < 1，且|H(x)| 越大指数\\n损失函数e−f(x)H(x) 越小。这很合理：此时|H(x)| 越大意味着分类器本身对预测结果的信心越大，损失\\n应该越小；若|H(x)| 在零附近，虽然预测正确，但表示分类器本身对预测结果信心很小，损失应该较大；\\n当H(x) 的符号与f(x) 不一致时，f(x)H(x) < 0，因此e−f(x)H(x) = e|H(x)| > 1，且|H(x)| 越大指\\n数损失函数越大。这很合理：此时|H(x)| 越大意味着分类器本身对预测结果的信心越大，但预测结果是\\n错的，因此损失应该越大；若|H(x)| 在零附近，虽然预测错误，但表示分类器本身对预测结果信心很小，\\n虽然错了，损失应该较小。\\n再解释符号Ex∼D[·] 的含义：D 为概率分布，可简单理解为在数据集D 中进行一次随机抽样，每个'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='再解释符号Ex∼D[·] 的含义：D 为概率分布，可简单理解为在数据集D 中进行一次随机抽样，每个\\n样本被取到的概率；E[·] 为经典的期望，则综合起来Ex∼D[·] 表示在概率分布D 上的期望，可简单理解为\\n对数据集D 以概率D 进行加权后的期望。\\n综上所述, 若数据集D 中样本x 的权值分布为D(x), 则式(8.5) 可写为:\\nℓexp(H | D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)H(x)\\n=\\nX\\nx∈D\\nD(x)\\n\\x00e−H(x)I(f(x) = 1) + eH(x)I(f(x) = −1)\\n\\x01\\n特别地, 若针对任意样本x, 若分布D(x) =\\n1\\n|D|, 其中|D| 为数据集D 样本个数, 则\\nℓexp(H | D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\n1\\n|D|\\nX\\nx∈D\\ne−f(x)H(x)\\n而这就是在求传统平均值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.3\\n式(8.6) 的推导\\n由公式(8.5) 中对于符号Ex∼D[·] 的解释可知\\nℓexp(H|D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)H(x)\\n=\\n|D|\\nX\\ni=1\\nD (xi)\\n\\x00e−H(xi)I (f (xi) = 1) + eH(xi)I (f (xi) = −1)\\n\\x01\\n=\\n|D|\\nX\\ni=1\\n\\x00e−H(xi)D (xi) I (f (xi) = 1) + eH(xi)D (xi) I (f (xi) = −1)\\n\\x01\\n=\\n|D|\\nX\\ni=1\\n\\x00e−H(xi)P (f (xi) = 1 | xi) + eH(xi)P (f (xi) = −1 | xi)\\n\\x01\\n其中D (xi) I (f (xi) = 1) = P (f (xi) = 1 | xi) 可以这样理解：D(xi) 表示在数据集D 中进行一次随机抽\\n样，样本xi 被取到的概率，D (xi) I (f (xi) = 1) 表示在数据集D 中进行一次随机抽样，使得f(xi) = 1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的样本xi 被抽到的概率，即为P (f (xi) = 1 | xi)。\\n当对H(xi) 求导时，求和号中只有含xi 项不为0，由求导公式\\n∂e−H(x)\\n∂H(x) = −e−H(x)\\n∂eH(x)\\n∂H(x) = eH(x)\\n有\\n∂ℓexp(H|D)\\n∂H(x)\\n= −e−H(x)P(f(x) = 1|x) + eH(x)P(f(x) = −1|x)\\n8.2.4\\n式(8.7) 的推导\\n令式(8.6) 等于零:\\n−e−H(x)P(f(x) = 1 | x) + eH(x)P(f(x) = −1 | x) = 0\\n移项:\\neH(x)P(f(x) = −1 | x) = e−H(x)P(f(x) = 1 | x)\\n两边同乘\\neH(x)\\nP (f(x)=−1|x):\\ne2H(x) = P(f(x) = 1 | x)\\nP(f(x) = −1 | x)\\n取ln(·):\\n2H(x) = ln P(f(x) = 1 | x)\\nP(f(x) = −1 | x)\\n两边同除1\\n2 即得式(8.7)。\\n8.2.5\\n式(8.8) 的推导\\nsign(H(x)) = sign\\n\\x121'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2 即得式(8.7)。\\n8.2.5\\n式(8.8) 的推导\\nsign(H(x)) = sign\\n\\x121\\n2 ln P(f(x) = 1|x)\\nP(f(x) = −1|x)\\n\\x13\\n=\\n(\\n1,\\nP(f(x) = 1|x) > P(f(x) = −1|x)\\n−1,\\nP(f(x) = 1|x) < P(f(x) = −1|x)\\n= arg max\\ny∈{−1,1}\\nP(f(x) = y|x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第一行到第二行显然成立，第二行到第三行是利用了arg max 函数的定义。arg max\\ny∈{−1,1}\\nP(f(x) = y|x) 表示使\\n得函数P(f(x) = y|x) 取得最大值的y 的值，展开刚好是第二行的式子。\\n这里解释一下贝叶斯错误率的概念。这来源于“西瓜书”P148 的式(7.6) 表示的贝叶斯最优分类器，\\n可以发现式(8.8) 的最终结果是式(7.6) 的二分类特殊形式。\\n到此为止，本节证明了指数损失函数是分类任务原本0/1 损失函数的一致的替代损失函数参见“西瓜书”P131 图\\n而指数损失函数有更好的数学性质，例如它是连续可微函数，因此接下来的式(8.9) 至式(8.19) 基于指数\\n损失函数推导AdaBoost 的理论细节。\\n8.2.6\\n式(8.9) 的推导\\nℓexp (αtht|Dt) = Ex∼Dt\\n\\x02\\ne−f(x)αtht(x)\\x03\\n= Ex∼Dt\\n\\x02\\ne−αtI (f(x) = ht(x)) + eαtI (f(x) ̸= ht(x))\\n\\x03'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x02\\ne−αtI (f(x) = ht(x)) + eαtI (f(x) ̸= ht(x))\\n\\x03\\n= e−αtPx∼Dt (f(x) = ht(x)) + eαtPx∼Dt (f(x) ̸= ht(x))\\n= e−αt (1 −ϵt) + eαtϵt\\n乍一看本式有些问题, 为什么要最小化ℓexp (αtht | Dt) ?\\n“西瓜书”图8.3 中的第3 行的表达式\\nht = L (D, Dt) 不是代表着应该最小化ℓexp (ht | Dt) 么?\\n或者从整体来看, 第t 轮迭代也应该最小化\\nℓexp (Ht | D) = ℓexp (Ht−1 + αtht | D), 这样最终T 轮迭代结束后得到的式(8.4) 就可以最小化ℓexp(H |\\nD) 了。实际上, 理解了AdaBoost 之后就会发现, ℓexp (αtht | Dt) 与ℓexp (Ht | D) 是等价的, 详见后面的\\n“AdaBoost 的个人推导”。另外, ht = L (D, Dt) 也是推导的结论之一, 即式(8.18), 而不是无缘无故靠直觉\\n用L (D, Dt) 得到ht 。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='用L (D, Dt) 得到ht 。\\n暂时不管以上疑问, 权且按作者思路推导一下：\\n第1 个等号与式(8.5) 的区别仅在于到底针对αtht(x) 还是H(x), 代入即可;\\n第2 个等号是考虑到ht(x) 和f(x) 均只能取−1 和+1 两个值, 其中I(·) 为指示函数;\\n第3 个等号对中括号的两项分别求Ex∼Dt[·], 而eαt 和e−αt 与x 无关, 可以作为常数项拿到Ex∼Dt[.]\\n外面, 而Ex∼Dt [I (f(x) = ht(x))] 表示在数据集D 上、样本权值分布为Dt 时f(x) 和ht(x) 相等次数的\\n期望, 即Px∼Dt (f(x) = ht(x)), 也就是正确率, 即(1 −ϵt); 同理, Ex∼Dt [I (f(x) ̸= ht(x))] 表示在数据集\\nD 上、样本权值分布为Dt 时f(x) 和ht(x) 不相等次数的期望, 即Px∼Dt (f(x) ̸= ht(x)), 也就是错误率\\nϵt;\\n第4 个等号即为将Px∼Dt (f(x) = ht(x)) 替换为(1 −ϵt) 、将Px∼Dt (f(x) ̸= ht(x)) 替换为ϵt 的结'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='果。\\n注意本节符号略有混乱, 如前所述式(8.4) 的H(x) 是连续实值函数, 但在“西瓜书”图8.3 最后一行\\n的输出H(x) 明显只能取−1 和+1 两个值(与式(8.2) 相同), 本节除了“西瓜书”图8.3 最后一行的输出\\n之外, H(x) 均以式(8.4) 的连续实值函数为准。\\n8.2.7\\n式(8.10) 的解释\\n指数损失函数对αt 求偏导，为了得到使得损失函数取最小值时αt 的值。\\n8.2.8\\n式(8.11) 的推导\\n令公式(8.10) 等于0 移项即得到的该式。此时αt 的取值使得该基分类器经αt 加权后的损失函数最\\n小。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图8-4 原始论文对式8.12 的相关推导\\n8.2.9\\n式(8.12) 的解释\\n本式的推导和原始论文[1] 的推导略有差异，虽然并不影响后面式(8.18) 以及式(8.19) 的推导结果。\\nAdaBoost 第t 轮迭代应该求解如下优化问题从而得到αt 和ht(x) :\\n(αt, ht(x)) = arg min\\nα,h\\nℓexp (Ht−1 + αh | D)\\n对于该问题, 先对于固定的任意α > 0, 求解ht(x); 得到ht(x) 后再求αt◦。\\n在原始论文的第346 页，对式(8.12) 的推导如图8-4所示。可以发现原文献中保留了参数c (即α )。\\n当然, 对于任意α > 0, 并不影响推导结果。\\n如果暂且不管以上的差异，我们按照作者的思路推导的话，将Ht(x) = Ht−1(x) + ht(x) 带入公式\\n(8.5) 即可，因为理想的ht 可以纠正Ht−1 的全部错误，所以这里指定ht 其权重系数αt 为1。如果权重\\n系数αt 是个常数的话，对后续结果也没有影响。\\n8.2.10'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='系数αt 是个常数的话，对后续结果也没有影响。\\n8.2.10\\n式(8.13) 的推导\\n由ex 的二阶泰勒展开为1 + x + x2\\n2 + o(x2) 得:\\nℓexp (Ht−1 + ht|D) = Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)e−f(x)ht(x)\\x03\\n≃Ex∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)ht(x) + f 2(x)h2\\nt(x)\\n2\\n\\x13\\x15\\n因为f(x) 与ht(x) 取值都为1 或-1，所以f 2(x) = h2\\nt(x) = 1，所以得:\\nℓexp (Ht−1 + ht|D) = Ex∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)ht(x) + 1\\n2\\n\\x13\\x15\\n实际上，此处保留一阶泰勒展开项即可，后面提到的Gradient Boosting 理论框架就是只使用了一阶\\n泰勒展开；当然二阶项为常数，也并不影响推导结果，原文献[1] 中也保留了二阶项。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 93, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.11\\n式(8.14) 的推导\\nht(x) = arg min\\nh\\nℓexp (Ht−1 + h|D)\\n= arg min\\nh\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)h(x) + 1\\n2\\n\\x13\\x15\\n= arg max\\nh\\nEx∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n= arg max\\nh\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\nEx∼D [e−f(x)Ht−1(x)]f(x)h(x)\\n\\x15\\n理想的ht(x) 是使得Ht(x) 的指数损失函数取得最小值时的ht(x)，该式将此转化成某个期望的最大值。\\n第2 个等号就是将式(8.13) 代入；第3 个等号是因为\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)h(x) + 1\\n2\\n\\x13\\x15\\n=Ex∼D\\n\\x143\\n2e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)h(x)\\n\\x15\\n=Ex∼D\\n\\x143\\n2e−f(x)Ht−1(x)\\n\\x15\\n−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 93, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2e−f(x)Ht−1(x)\\n\\x15\\n−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n本式自变量为h(x), 而Ex∼D\\n\\x02 3\\n2e−f(x)Ht−1(x)\\x03\\n与h(x) 无关, 也就是一个常数; 只需最大化第二项−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f\\n即可, 将负号去掉, 原最小化问题变为最大化问题;\\n第4 个等号仍是因为Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)\\x03\\n是与自变量h(x) 无关的正常数(因为指数函数与原问题等\\n价，例如arg maxx (1 −x2) 与arg maxx 2 (1 −x2) 的结果均为x = 0);\\n8.2.12\\n式(8.16) 的推导\\n首先解释下符号Ex∼D 的含义，注意在本章中有两个符号D 和D，其中D 表示数据集，而D 表示\\n数据集D 的样本分布，可以理解为在数据集D 上进行一次随机采样，样本x 被抽到的概率是D(x)，那\\n么符号Ex∼D 表示的是在概率分布D 上的期望，可以简单地理解为对数据及D 以概率D 加权之后的期\\n望，因此有：\\nE(g(x)) =\\n|D|\\nX\\ni=1\\nf(xi)g(xi)\\n故可得\\nEx∼D\\n\\x02'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 93, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='望，因此有：\\nE(g(x)) =\\n|D|\\nX\\ni=1\\nf(xi)g(xi)\\n故可得\\nEx∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)H(xi)\\n由式(8.15) 可知\\nDt (xi) = D (xi)\\ne−f(xi)Ht−1(xi)\\nEx∼D [e−f(x)Ht−1(x)]\\n所以式(8.16) 可以表示为\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\nEx∼D [e−f(x)Ht−1(x)]f(x)h(x)\\n\\x15\\n=\\n|D|\\nX\\ni=1\\nD (xi)\\ne−f(xi)Ht−1(xi)\\nEx∼D [e−f(x)Ht−1(x)] f(xi)h(xi)\\n=\\n|D|\\nX\\ni=1\\nDt (xi) f (xi) h (xi)\\n=Ex∼Dt[f(x)h(x)]\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.2.13\\n式(8.17) 的推导\\n当f(x) = h(x) 时，I(f(x) ̸= h(x)) = 0，f(x)h(x) = 1，1 −2I(f(x) ̸= h(x)) = 1；\\n当f(x) ̸= h(x) 时，I(f(x) ̸= h(x)) = 1，f(x)h(x) = −1，1 −2I(f(x) ̸= h(x)) = −1。\\n综上，左右两式相等。\\n8.2.14\\n式(8.18) 的推导\\n本式基于式(8.17) 的恒等关系，由式(8.16) 推导而来。\\nEx∼Dt[f(x)h(x)] = Ex∼Dt[1 −2I(f(x) ̸= h(x))]\\n= Ex∼Dt[1] −2Ex∼Dt[I(f(x) ̸= h(x))]\\n= 1 −2Ex∼Dt[I(f(x) ̸= h(x))]\\n类似于式(8.14) 的第3 个和第4 个等号，由式(8.16) 的结果开始推导：\\nht(x) = arg max\\nh\\nEx∼Dt[f(x)h(x)]\\n= arg max\\nh\\n(1 −2Ex∼Dt[I(f(x) ̸= h(x))])'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= arg max\\nh\\n(1 −2Ex∼Dt[I(f(x) ̸= h(x))])\\n= arg max\\nh\\n(−2Ex∼Dt[I(f(x) ̸= h(x))])\\n= arg min Ex∼Dt[I(f(x) ̸= h(x))]\\n此式表示理想的ht(x) 在分布Dt 下最小化分类误差, 因此有“西瓜书”图8.3 第3 行ht(x) = L (D, Dt),\\n即分类器ht(x) 可以基于分布Dt 从数据集D 中训练而得, 而我们在训练分类器时, 一般来说最小化的损\\n失函数就是分类误差。\\n8.2.15\\n式(8.19) 的推导\\nDt+1(x) =\\nD(x)e−f(x)Ht(x)\\nEx∼D [e−f(x)Ht(x)]\\n= D(x)e−f(x)Ht−1(x)e−f(x)αtht(x)\\nEx∼D [e−f(x)Ht(x)]\\n= Dt(x) · e−f(x)αtht(x) Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)\\x03\\nEx∼D [e−f(x)Ht(x)]\\n第1 个等号是将式(8.15) 中的t 换为t + 1 (同时t −1 换为t);'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='第1 个等号是将式(8.15) 中的t 换为t + 1 (同时t −1 换为t);\\n第2 个等号是将Ht(x) = Ht−1(x) + αtht(x) 代入分子即可;\\n第3 个等号是乘以\\nEx∼D[e−f(x)Ht−1(x)]\\nEx∼D[e−f(x)Ht−1(x)] 后, 凑出式(8.15) 的Dt(x) 表达式, 以符号Dt(x) 替换即得。到\\n此之后, 得到Dt+1(x) 与Dt(x) 的关系, 但为了确保Dt+1(x) 是一个分布, 需要对得到的Dt+1(x) 进行规\\n范化, 即“西瓜书”图8.3 第7 行的Zt 。式(8.19) 第3 行最后一个分式将在规范化过程被吸收。\\nboosting 算法是根据调整后的样本再去训练下一个基分类器，这就是“重赋权法”的样本分布的调整\\n公式。\\n8.2.16\\nAdaBoost 的个人推导\\n西瓜书中对AdaBoost 的推导和原论文[1] 上有些地方有差异，综合原论文和一些参考资料，这里给\\n出一版更易于理解的推导，亦可参见我们的视频教程。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='出一版更易于理解的推导，亦可参见我们的视频教程。\\nAdaBoost 的目标是学得T 个ht(x) 和相应的T 个αt, 得到式(8.4) 的H(x), 使式(8.5) 指数损失函\\n数ℓexp(H | D) 最小, 这就是求解所谓的“加性模型”。特别强调一下, 分类器ht(x) 如何得到及其相应的\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n权重αt 等于多少都是需要求解的(ht(x) = L (D, Dt), 即基于分布Dt 从数据集D 中经过最小化训练误差\\n训练出分类器ht, 也就是式(8.18), αt 参见式(8.18)。\\n“通常这是一个复杂的优化问题（同时学得T 个ht(x) 和相应的T 个αt 很困难)。前向分步算法求解这\\n一优化问题的想法是：因为学习的是加法模型, 如果能够从前向后, 每一步只学习一个基函数ht(x) 及其系\\n数αt, 逐步逼近最小化指数损失函数ℓexp(H | D), 那么就可以简化优化的复杂度。”摘自李航《统计学习方法》[2] 第14\\n因此, AdaBoost 每轮迭代只需要得到一个基分类器和其投票权重, 设第t 轮迭代需得到基分类器\\nht(x), 对应的投票权重为αt, 则集成分类器Ht(x) = Ht−1(x) + αtht(x), 其中H0(x) = 0 。为表达式简\\n洁, 常常将ht(x) 简写为ht, Ht(x) 简写为Ht 。则第t 轮实际为如下优化问题（本节式(8.4) 到式(8.8)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='已经证明了指数损失函数是分类任务原本0/1 损失函数的一致替代损失函数):\\n(αt, ht) = arg min\\nα,h\\nℓexp (Ht−1 + αh | D)\\n表示每轮得到的基分类器ht(x) 和对应的权重αt 是最小化集成分类器Ht = Ht−1 + αtht 在数据集D 上、\\n样本权值分布为D (即初始化样本权值分布, 也就是D1 ) 时的指数损失函数ℓexp (Ht−1 + αh | D) 的结果。\\n这就是前向分步算法求解加性模型的思路。根据式(8.5) 将指数损失函数表达式代入, 则\\nℓexp (Ht−1 + αh | D) = Ex∼D\\n\\x02\\ne−f(x)(Ht−1(x)+αh(x))\\x03\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)(Ht−1(xi)+αh(xi))\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−f(xi)αh(xi)\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00e−αI (f (xi) = h (xi)) + eαI (f (xi) ̸= h (xi))\\n\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01\\n上式推导中, 由于f (xi) 和h (xi) 均只能取−1, +1 两个值, 因此当f (xi) = h (xi) 时, f (xi) h (xi) = 1,\\n当f (xi) ̸= h (xi) 时, f (xi) h (xi) = −1 。另外, f (xi) 和h (xi) 要么相等, 要么不相等, 二者只能有一个\\n为真, 因此以下等式恒成立:\\nI (f (xi) = h (xi)) + I (f (xi) ̸= h (xi)) = 1\\n所以\\ne−αI (f (xi) = h (xi)) + eαI (f (xi) ̸= h (xi))\\n=e−αI (f (xi) = h (xi)) + e−αI (f (xi) ̸= h (xi)) −e−αI (f (xi) ̸= h (xi)) + eαI (f (xi) ̸= h (xi))\\n=e−α (I (f (xi) = h (xi)) + I (f (xi) ̸= h (xi))) +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n=e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n将此结果代入ℓexp (Ht−1 + αh | D), 得注: 以下表达式后面求解权重αt 时仍会使用\\nℓexp (Ht−1 + αh | D) =\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n\\x01\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−α +\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n= e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα −e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n外面; 第一项e−α P|D|\\ni=1 D′\\nt (xi) 与h(x) 无关, 因此对于任意α > 0, 使ℓexp (Ht−1 + αh | D) 最小的h(x)\\n只需要使第二项最小即可, 即\\nht = arg min\\nh\\n\\x00eα −e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n对于任意α > 0, 有eα −e−α > 0, 所以上式中与h(x) 无关的正系数可以省略:\\nht = arg min\\nh\\n|D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n此即式(8.18) 另一种表达形式。注意, 为了确保D′\\nt(x) 是一个分布, 需要对其进行规范化, 即Dt(x) = D′\\nt(x)\\nZt ,\\n然而规范化因子Zt = P|D|\\ni=1 D′\\nt (xi) 为常数, 并不影响最小化的求解。正是基于此结论, AdaBoost 通过\\nht = L (D, Dt ) 得到第t 轮的基分类器。“西瓜书”图8.3 的第3 行'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='ht = L (D, Dt ) 得到第t 轮的基分类器。“西瓜书”图8.3 的第3 行\\nDt+1 (xi) = D (xi) e−f(xi)Ht(xi)\\n= D (xi) e−f(xi)(Ht−1(xi)+αtht(xi))\\n= D (xi) e−f(xi)Ht−1(xi)e−f(xi)αtht(xi)\\n= Dt (xi) e−f(xi)αtht(xi)\\n此即类似式(8.19) 的分布权重更新公式。\\n现在只差权重αt 表达式待求。对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得\\n∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′\\nt (xi) + (eα −e−α) P|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\n\\x11\\n∂α\\n= −e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα + e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\nP|D|\\ni=1 D′\\nt (xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1\\nDt (xi) I (f (xi) ̸= h (xi)) = Ex∼Dt [I (f (xi) ̸= h (xi))]\\n= ϵt\\n对上述等式化简, 得\\ne−α\\neα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1\\n2 ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n即式(8.11)。从该式可以发现, 当ϵt = 1 时, αt →∞, 此时集成分类器将由基分类器ht 决定, 而这很可能\\n是由于过拟合产生的结果, 例如不前枝决策树, 如果一直分下去, 一般情况下总能得到在训练集上分类误差\\n很小甚至为0 的分类器, 但这并没有什么意义。所以一般在AdaBoost 中使用弱分类器, 如决策树桩(即单\\n层决策树)。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='层决策树)。\\n另外, 由以上指数损失函数ℓexp (Ht−1 + αh | D) 的推导可以发现\\nℓexp (Ht−1 + αh | D) =\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−f(xi)αh(xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi) e−f(xi)αh(xi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图8-5 Adaboost 原始文献推论2\\n这与指数损失函数ℓexp (αtht | Dt) 的表达式基本一致:\\nℓexp (αtht | Dt) = Ex∼Dt\\n\\x02\\ne−f(x)αtht(x)\\x03\\n=\\n|D|\\nX\\ni=1\\nDt (xi) e−f(xi)αtht(xt)\\n而D′\\nt(x) 的规范化过程并不影响对ℓexp (Ht−1 + αh | D) 求最小化操作, 因此最小化式(8.9) 等价于最小化\\nℓexp (Ht−1 + αh | D), 这就是式(8.9) 的来历，故并无问题。\\n到此为止, 就逐一完成了“西瓜书”图8.3 中第3 行的ht 的训练(并计算训练误差)、第6 行的权重\\nαt 计算公式以及第7 行的分布Dt 更新公式来历的理论推导。\\n8.2.17\\n进一步理解权重更新公式\\nAdaboost 原始文献[1] 第12 页(pdf 显示第348 页) 有如下推论，如图8-5所示:'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='即Px∼Dt (ht−1(x) ̸= f(x)) = 0.5 。用通俗的话来说就是, ht−1 在数据集D 上、分布为Dt 时的分类\\n误差为0.5, 即相当于随机猜测(最糟糕的二分类器是分类误差为0.5, 当二分类器分类误差为1 时相当于\\n分类误差为0 , 因为将预测结果反过来用就是了)。而ht 由式(8.18) 得到\\nht = arg min\\nh\\nEx∼Dt[I(f(x) ̸= h(x))] = arg min\\nh\\nPx∼Dt(h(x) ̸= f(x))\\n即ht 是在数据集D 上、分布为Dt 时分类误差最小的分类器, 因此在数据集D 上、分布为Dt 时, ht 是\\n最好的分类器, 而ht−1 是最差的分类器, 故二者差别最大。“西瓜书”第8.1 节的图8.2 形象的说明了“集\\n成个体应‘好而不同’”, 此时可以说ht−1 和ht 非常“不同”。证明如下:\\n对于ht−1 来说, 分类误差ϵt−1 为\\nϵt−1 = Px∼Dt−1 (ht−1(x) ̸= f(x)) = Ex∼Dt−1 [I (ht−1(x) ̸= f(x))]\\n=\\n|D|\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\n|D|\\nX\\ni=1\\nDt−1 (xi) I (ht−1(x) ̸= f(x))\\n=\\nP|D|\\ni=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\nP|D|\\ni=1 Dt−1 (xi) I (ht−1(x) = f(x)) + P|D|\\ni=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\n在第t 轮, 根据分布更新公式(8.19) 或“西瓜书”图8.3 第7 行(规范化因子Zt−1 为常量):\\nDt = Dt−1\\nZt−1\\ne−f(x)αt−1ht−1(x)\\n其中根据式(8.11), 第t −1 轮的权重\\nαt−1 = 1\\n2 ln 1 −ϵt−1\\nϵt−1\\n= ln\\ns\\n1 −ϵt−1\\nϵt−1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n代入Dt 的表达式, 则\\nDt =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nDt−1\\nZt−1 ·\\nq\\nϵt−1\\n1−ϵt−1\\n, if ht−1(x) = f(x)\\nDt−1\\nZt−1 ·\\nq\\n1−ϵt−1\\nϵt−1\\n, if ht−1(x) ̸= f(x)\\n那么ht−1 在数据集D 上、分布为Dt 时的分类误差Px∼Dt (ht−1(x) ̸= f(x) ) 为(注意, 下式第二行的分\\n母等于1, 因为I (ht−1(x) = f(x)) + I (ht−1(x) ̸= f(x)) = 1 )\\nPx∼Dt (ht−1(x) ̸= f(x)) = Ex∼Dt [I (ht−1(x) ̸= f(x))]\\n=\\nP|D|\\ni=1 Dt (xi) I (ht−1 (xi) ̸= f (xi))\\nP|D|\\ni=1 Dt (xi) I (ht−1 (xi) = f (xi)) + P|D|\\ni=1 Dt (xi) I (ht−1 (xi) ̸= f (xi))\\n=\\nP|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nP|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1\\nϵt−1 I (ht−1 (xi) ̸= f (xi))\\nP|D|\\nDt−1(xi)\\nZt−1\\ni=1\\n·\\nq\\nϵt−1\\n1−ϵt−1 I (ht−1 (xi) = f (xi)) + P|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1\\nϵt−1 I (ht−1 (xi) ̸= f (xi))\\n=\\nq\\n1−ϵt−1\\nϵt−1\\n· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\nq\\nϵt−1\\n1−ϵt−1 · P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) = f (xi)) +\\nq\\n1−ϵt−1\\nϵt−1\\n· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\n=\\nq\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\nq\\nϵt−1\\n1−ϵt−1 · (1 −ϵt−1) +\\nq\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\n= 1\\n2\\n8.2.18\\n能够接受带权样本的基学习算法'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='q\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\n= 1\\n2\\n8.2.18\\n能够接受带权样本的基学习算法\\n在Adaboost 算法的推导过程中，我们发现能够接受并利用带权样本的算法才能很好的嵌入到Ad-\\naboost 的框架中作为基学习器。因此这里举一些能够接受带权样本的基学习算法的例子，分别是SVM 和\\n基于随机梯度下降(SGD) 的对率回归：\\n其实原理很简单: 对于SVM 来说, 针对“西瓜书”P130 页的优化目标式(6.29) 来说, 第二项为损失\\n项, 此时每个样本的损失ℓ0/1\\n\\x00yi\\n\\x00w⊤xi + b\\n\\x01\\n−1\\n\\x01\\n直接相加, 即样本权值分布为D (xi) =\\n1\\nm, 其中m 为数\\n据集D 样本个数; 若样本权值更新为Dt (xi), 则此时损失求和项应该变为\\nm\\nX\\ni=1\\nmDt (xi) · ℓ0/1\\n\\x00yi\\n\\x00w⊤xi + b\\n\\x01\\n−1\\n\\x01\\n若将D (xi) =\\n1\\nm 替换Dt (xi), 则就是每个样本的损失ℓ0/1\\n\\x00yi\\n\\x00w⊤xi + b\\n\\x01\\n−1\\n\\x01\\n直接相加。如此更改后, 最\\n后推导结果影响的是式(6.39), 将由C = αi + µi 变为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='直接相加。如此更改后, 最\\n后推导结果影响的是式(6.39), 将由C = αi + µi 变为\\nC · mDt (xi) = αi + µi\\n进而由αi, µi ≥0 导出0 ≤αi ≤C · mDt (xi) 。\\n对于基于随机梯度下降(SGD) 的对率回归, 每次随机选择一个样本进行梯度下降, 总体上的期望损失\\n即为式(3.27), 此时每个样本被选到的概率相同, 相当于D (xi) =\\n1\\nm 。若样本权值更新为Dt (xi), 则类似\\n于SVM, 针对式(3.27) 只需要给第i 项乘以mDt (xi) 即可, 相当于每次随机梯度下降选择样本时以概率\\nDt (xi) 选择样本xi 即可。\\n注意, 这里总的损失中出现了样本个数m 。这是因为在定义损失时末求均值, 若对式(6.29) 的第二\\n项和式(3.27) 乘以\\n1\\nm 则可以将m 抵消掉。然而常数项在最小化式(3.27) 实际上并不影响什么, 对于式\\n(6.29) 来说只要选择平衡参数C 时选为原来的m 倍即可。\\n当然, 正如“西瓜书”P177 第三段中所说, “对无法接受带权样本的基学习算法, 则可通过“重采样'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='法’ 来处理, 即在每一轮学习中, 根据样本分布对训练集重新进行采样, 再用重采样而得的样本集对基学习\\n器进行训练”。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.3\\nBagging 与随机森林\\n8.3.1\\n式(8.20) 的解释\\nI (ht(x) = y) 表示对T 个基学习器，每一个都判断结果是否与y 一致，y 的取值一般是−1 和1，如\\n果基学习器结果与y 一致，则I (ht(x) = y) = 1，如果样本不在训练集内，则I (x /∈Dt) = 1，综合起来\\n看就是，对包外的数据，用“投票法”选择包外估计的结果，即1 或-1。\\n8.3.2\\n式(8.21) 的推导\\n由式(8.20) 知，Hoob(x) 是对包外的估计，该式表示估计错误的个数除以总的个数，得到泛化误差的\\n包外估计。注意在本式直接除以D | (训练集D 样本个数), 也就是说此处假设T 个基分类器的各自的包\\n外样本的并集一定为训练集D 。实际上, 这个事实成立的概率也是比较大的, 可以计算一下: 样本属于包\\n内的概率为0.632, 那么T 次独立的随机采样均属于包内的概率为0.632T , 当T = 5 时, 0.632T ≈0.1, 当'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='T = 10 时, 0.632T ≈0.01, 这么来看的话T 个基分类器的各自的包外样本的并集为训练集D 的概率的确\\n实比较大。\\n8.3.3\\n随机森林的解释\\n在8.3.2 节开篇第一句话就解释了随机森林的概念：随机森林是Bagging 的一个扩展变体，是以决策\\n树为基学习器构建Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。\\n完整版随机森林当然更复杂，这时只须知道两个重点：(1) 以决策树为基学习器；(2) 在基学习器训练\\n过程中，选择划分属性时只使用当前结点属性集合的一个子集。\\n8.4\\n结合策略\\n8.4.1\\n式(8.22) 的解释\\nH(x) = 1\\nT\\nT\\nX\\ni=1\\nhi(x)\\n对基分类器的结果进行简单的平均。\\n8.4.2\\n式(8.23) 的解释\\nH(x) =\\nT\\nX\\ni=1\\nwihi(x)\\n对基分类器的结果进行加权平均。\\n8.4.3\\n硬投票和软投票的解释\\n“西瓜书”中第183 页提到了硬投票(hard voting) 和软投票(soft voting)，本页左侧注释也提到多数投'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='票法的英文术语使用不太一致，有文献称为majority voting。本人看到有些文献中，硬投票使用majority\\nvoting（多数投票），软投票使用probability voting（概率投票），所以还是具体问题具体分析比较稳妥。\\n8.4.4\\n式(8.24) 的解释\\nH(x) =\\n(\\ncj,\\nif PT\\ni=1 hj\\ni(x) > 0.5 PN\\nk=1\\nPT\\ni=1 hk\\ni (x)\\nreject,\\notherwise.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n当某一个类别j 的基分类器的结果之和，大于所有结果之和的1\\n2，则选择该类别j 为最终结果。\\n8.4.5\\n式(8.25) 的解释\\nH(x) = carg max\\nj\\n∑T\\ni=1 hj\\ni (x)\\n相比于其他类别，该类别j 的基分类器的结果之和最大，则选择类别j 为最终结果。\\n8.4.6\\n式(8.26) 的解释\\nH(x) = carg max\\nj\\n∑T\\ni=1 wihj\\ni (x)\\n相比于其他类别，该类别j 的基分类器的结果之和最大，则选择类别j 为最终结果，与式(8.25) 不同的\\n是，该式在基分类器前面乘上一个权重系数，该系数大于等于0，且T 个权重之和为1。\\n8.4.7\\n元学习器(meta-learner) 的解释\\n书中第183 页最后一行提到了元学习器(meta-learner)，简单解释一下，因为理解meta 的含义有时\\n对于理解论文中的核心思想很有帮助。\\n元(meta)，非常抽象，例如此处的含义，即次级学习器，或者说基于学习器结果的学习器；另外还有'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='元(meta)，非常抽象，例如此处的含义，即次级学习器，或者说基于学习器结果的学习器；另外还有\\n元语言，就是描述计算机语言的语言，还有元数学，研究数学的数学等等；\\n另外，论文中经常出现的还有meta-strategy，即元策略或元方法，比如说你的研究问题是多分类问题，\\n那么你提出了一种方法，例如对输入特征进行变换（或对输出类别做某种变换），然后再基于普通的多分\\n类方法进行预测，这时你的方法可以看成是一种通用的框架，它虽然针对多分类问题开发，但它需要某个\\n具体多分类方法配合才能实现，那么这样的方法是一种更高层级的方法，可以称为是一种meta-strategy。\\n8.4.8\\nStacking 算法的解释\\n该算法其实非常简单，对于数据集，试想你现在有了个基分类器预测结果，也就是说数据集中的每个\\n样本均有个预测结果，那么怎么结合这个预测结果呢？\\n本节名为“结合策略”，告诉你各种结合方法，但其实最简单的方法就是基于这个预测结果再进行一\\n次学习，即针对每个样本，将这个预测结果作为输入特征，类别仍为原来的类别，既然无法抉择如何将这\\n些结果进行结合，那么就“学习”一下吧。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='些结果进行结合，那么就“学习”一下吧。\\n“西瓜书”图8.9 伪代码第9 行中将第个样本进行变换，特征为个基学习器的输出，类别标记仍为原\\n来的，将所有训练集中的样本进行转换得到新的数据集后，再基于进行一次学习即可，也就是Stacking 算\\n法。\\n至于说“西瓜书”图8.9 中伪代码第1 行到第3 行使用的数据集与第5 行到第10 行使用的数据集之\\n间的关系，在“西瓜书”图8.9 下方的一段话有详细的讨论，不再赘述。\\n8.5\\n多样性\\n8.5.1\\n式(8.27) 的解释\\nA (hi|x) = (hi(x) −H(x))2\\n该式表示个体学习器结果与预测结果的差值的平方，即为个体学习器的“分歧”。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 101, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.5.2\\n式(8.28) 的解释\\n¯A(h|x) =\\nT\\nX\\ni=1\\nwiA (hi|x)\\n=\\nT\\nX\\ni=1\\nwi (hi(x) −H(x))2\\n该式表示对各个个体学习器的“分歧”加权平均的结果，即集成的“分歧”。\\n8.5.3\\n式(8.29) 的解释\\nE (hi|x) = (f(x) −hi(x))2\\n该式表示个体学习器与真实值之间差值的平方，即个体学习器的平方误差。\\n8.5.4\\n式(8.30) 的解释\\nE(H|x) = (f(x) −H(x))2\\n该式表示集成与真实值之间差值的平方，即集成的平方误差。\\n8.5.5\\n式(8.31) 的推导\\n由(8.28) 知\\n¯A(h|x) =\\nT\\nX\\ni=1\\nwi (hi(x) −H(x))2\\n=\\nT\\nX\\ni=1\\nwi(hi(x)2 −2hi(x)H(x) + H(x)2)\\n=\\nT\\nX\\ni=1\\nwihi(x)2 −H(x)2\\n又因为\\nT\\nX\\ni=1\\nwiE (hi|x) −E(H|x)\\n=\\nT\\nX\\ni=1\\nwi (f(x) −hi(x))2 −(f(x) −H(x))2\\n='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 101, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nT\\nX\\ni=1\\nwi (f(x) −hi(x))2 −(f(x) −H(x))2\\n=\\nT\\nX\\ni=1\\nwihi(x)2 −H(x)2\\n所以\\n¯A(h|x) =\\nT\\nX\\ni=1\\nwiE (hi|x) −E(H|x)\\n8.5.6\\n式(8.32) 的解释\\nT\\nX\\ni=1\\nwi\\nZ\\nA (hi|x) p(x)dx =\\nT\\nX\\ni=1\\nwi\\nZ\\nE (hi|x) p(x)dx −\\nZ\\nE(H|x)p(x)dx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 102, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nR\\nA (hi|x) p(x)dx 表示个体学习器在全样本上的“分歧”，PT\\ni=1 wi\\nR\\nA (hi|x) p(x)dx 表示集成在全样本上\\n的“分歧”。式(8.31) 的意义在于, 对于示例x 有¯A(h | x) = ¯E(h | x) −E(H | x) 成立, 即个体学习器分\\n歧的加权均值等于个体学习器误差的加权均值减去集成H(x) 的误差。\\n将这个结论应用于全样本上, 即为式(8.32)。\\n例如Ai =\\nR\\nA (hi | x) p(x)dx, 这是将x 作为连续变量来处理的, 所以这里是概率密度p(x) 和积分\\n号; 若按离散变量来处理, 则变为Ai = P\\nx∈D A (hi | x) px; 其实高等数学中讲过, 积分就是连续求和。\\n8.5.7\\n式(8.33) 的解释\\nEi =\\nZ\\nE (hi|x) p(x)dx\\n表示个体学习器在全样本上的泛化误差。\\n8.5.8\\n式(8.34) 的解释\\nAi =\\nZ\\nA (hi|x) p(x)dx\\n表示个体学习器在全样本上的分歧。\\n8.5.9'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 102, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Ai =\\nZ\\nA (hi|x) p(x)dx\\n表示个体学习器在全样本上的分歧。\\n8.5.9\\n式(8.35) 的解释\\nE =\\nZ\\nE(H|x)p(x)dx\\n表示集成在全样本上的泛化误差。\\n8.5.10\\n式(8.36) 的解释\\nE = ¯E −¯A\\n¯E 表示个体学习器泛化误差的加权均值，¯A 表示个体学习器分歧项的加权均值，该式称为“误差-分歧分\\n解”。\\n8.5.11\\n式(8.40) 的解释\\n当p1 = p2 时, κ = 0; 当p1 = 1 时, κ = 1; 一般来说p1 ⩾p2, 即κ ⩾0, 但偶尔也有p1 < p2 的情况,\\n此时κ < 0 。有关p1, p2 的意义参见式(8.41) 和式(8.42) 的解释。\\n8.5.12\\n式(8.41) 的解释\\n当p1 = p2 时, κ = 0; 当p1 = 1 时, κ = 1; 一般来说p1 ⩾p2, 即κ ⩾0, 但偶尔也有p1 < p2 的情况,\\n此时κ < 0 。有关p1, p2 的意义参见式(8.41) 和式(8.42) 的解释。\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 102, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='此时κ < 0 。有关p1, p2 的意义参见式(8.41) 和式(8.42) 的解释。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n8.5.13\\n式(8.42) 的解释\\n将式(8.42) 拆分为如下形式，将会很容易理解其含义：\\np2 = a + b\\nm\\n· a + c\\nm\\n+ c + d\\nm\\n· b + d\\nm\\n其中a+b\\nm\\n为分类器hi 将样本预测为+1 的概率,\\na+c\\nm\\n为分类器hj 将样本预测为+1 的概率, 二者相乘\\na+b\\nm · a+c\\nm 可理解为分类器hi 与hj 将样本预测为+1 的概率; c+d\\nm 为分类器hi 将样本预测为−1 的概率,\\nb+d\\nm 为分类器hj 将样本预测为−1 的概率, 二者相乘c+d\\nm · b+d\\nm 可理解为分类器hi 与hj 将样本预测为−1\\n的概率。\\n注意a+b\\nm · a+c\\nm 与\\na\\nm 的不同, c+d\\nm · b+d\\nm 与\\nd\\nm 的不同:\\na + b\\nm\\n· a + c\\nm\\n= p (hi = +1) p (hj = +1) , a\\nm = p (hi = +1, hj = +1)\\nc + d\\nm\\n· b + d\\nm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m = p (hi = +1, hj = +1)\\nc + d\\nm\\n· b + d\\nm\\n= p (hi = −1) p (hj = −1) , d\\nm = p (hi = −1, hj = −1)\\n即a+b\\nm · a+c\\nm\\n和c+d\\nm · b+d\\nm 是分别考虑分类器hi 与hj 时的概率( hi 与hj 独立), 而\\na\\nm 和\\nd\\nm 是同时考虑\\nhi 与hj 时的概率(联合概率)。\\n8.5.14\\n多样性增强的解释\\n在8.5.3 节介绍了四种多样性增强的方法, 通俗易懂, 几乎不需要什么注解, 仅强调几个概念:\\n(1) 数据样本扰动中提到了“不稳定基学习器”(例如决策树、神经网络等) 和“稳定基学习器”(例\\n如线性学习器、支持向量机、朴素贝叶斯、k 近邻学习器等), 对稳定基学习器进行集成时数据样本扰动技\\n巧效果有限。这也就可以解释为什么随机森林和GBDT 等以决策树为基分学习器的集成方法很成功吧,\\nGradient Boosting 和Bagging 都是以数据样本扰动来增强多样性的; 而且, 掌握这个经验后在实际工程应'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='用中就可以排除一些候选基分类器, 但论文中的确经常见到以支持向量机为基分类器Bagging 实现, 这可\\n能是由于LIBSVM 简单易用的原因吧。\\n(2)“西瓜书”图8.11 随机子空间算法, 针对每个基分类器ht 在训练时使用了原数据集的部分输入属\\n性（末必是初始属性, 详见第189 页左上注释), 因此在最终集成时“西瓜书”图8.11 最后一行也要使用\\n相同的部分属性。\\n(3) 输出表示扰动中提到了“翻转法”(Flipping Output), 看起来是一个并没有道理的技巧, 为什么要\\n将训练样本的标记改变呢? 若认为原训练样本标记是完全可靠的, 这不是人为地加入噪声么? 但西瓜书作\\n者2017 年提出的深度森林[3] 模型中也用到了该技巧, 正如本小节名为“多样性增强”, 虽然从局部来看\\n引入了标记噪声, 但从模型集成的角度来说却是有益的。\\n8.6\\nGradient Boosting/GBDT/XGBoost 联系与区别\\n在集成学习中，梯度提升(Gradient Boosting, GB)、梯度提升树(GB Decision Tree, GBDT) 很常见，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='尤其是近几年非常流行的XGBoost 很是耀眼，此处单独介绍对比这些概念。\\n8.6.1\\n梯度下降法\\n本部分内容参考了孙文瑜教授的最优化方法[4] 设目标函数f(x) 在xk 附近连续可微, 且∇f (xk) =\\n∇f(x)\\n∇x\\n\\x0c\\x0c\\x0c\\nx=xk ̸= 0 。将f(x) 在xk 处进行一阶Taylor 展开\\nf(x) ≈f (xk) + ∇f (xk)⊤(x −xk)\\n记x −xk = ∆x, 则上式可写为\\nf (xk + ∆x) ≈f (xk) + ∇f (xk)⊤∆x\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n显然, 若∇f (xk)⊤∆x < 0 则有f (xk + ∆x) < f (xk), 即相比于f (xk), 自变量增量∆x 会使f(x)\\n函数值下降; 若要使f(x) = f (xk + ∆x) 下降最快, 只要选择∆x 使∇f (xk)⊤∆x 最小即可, 而此时\\n∇f (xk)⊤∆x < 0, 因此使绝对值| f (xk)⊤∆x 最大即可。将∆x 分成两部分: ∆x = αkdk, 其中dk 为待\\n求单位向量, αk > 0 为待解常量; dk 表示往哪个方向改变x 函数值下降最快, 而αk 表示沿这个方向的步\\n长。因此, 求解∆x 的问题变为\\n(αk, dk) = arg min\\nα,d\\n∇f (xk)⊤αd\\n将以上优化问题分为两步求解, 即\\ndk = arg min\\nd\\n∇f (xk)⊤d\\ns.t. ∥d∥2 = 1\\nαk = arg min\\nα\\n∇f (xk)⊤dkα\\n以上求解αk 的优化问题明显有问题, 因为对于∇f (xk)⊤dk < 0 来说, 显然αk = +∞时取的最小值, 求'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='解αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)\\n对于凸函数来说, 以上两步可以得到最优解; 但对于非凸函数来说, 联合求解得到dk 和αk, 与先求dk 然\\n后基于此再求αk 的结果应该有时是不同的。由Cauchy-Schwartz 不等式\\n\\x0c\\x0c\\x0c∇f (xk)⊤dk\\n\\x0c\\x0c\\x0c ≤∥∇f (xk)∥2 ∥dk∥2\\n可知, 当且仅当dk = −\\n∇f(xk)\\n∥∇f(xk)∥2 时, ∇f (xk)⊤dk 最小, −∇f (xk)⊤dk 最大。对于αk, 若f (xk + αdk) 对\\nα 的导数存在, 则可简单求解如下单变量方程即可:\\n∂f (xk + αdk)\\n∂α\\n= 0\\n例1: 试求f(x) = x2 在xk = 2 处的梯度方向dk 和步长αk 。解: 对f(x) 在xk = 2 处进行一阶Taylor\\n展开:\\nf(x) = f (xk) + f ′ (xk) (x −xk)\\n= x2\\nk + 2xk (x −xk)\\n= x2\\nk + 2xkαd'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= x2\\nk + 2xk (x −xk)\\n= x2\\nk + 2xkαd\\n由于此时自变量为一维, 因此只有两个方向可选, 要么正方向, 要么负方向。此时f ′ (xk) = 4, 因此dk =\\n−f ′(xk)\\n|f ′(xk)| = −1 。接下来求αk, 将xk 和dk 代入:\\nf (xk + αdk) = f(2 −α) = (2 −α)2\\n进而有\\n∂f (xk + αdk)\\n∂α\\n= −2(2 −α)\\n令导数等于0 , 得αk = 2 。此时\\n∆x = αkdk = −2\\n则xk + ∆x = 0, 函数值f (xk + ∆x) = 0 。例2: 试求f(x) = ∥x∥2\\n2 = x⊤x 在xk = [x1\\nk, x2\\nk]\\n⊤= [3, 4]⊤处\\n的梯度方向dk 和步长αk 。解: 对f(x) 在xk = [x1\\nk, x2\\nk]\\n⊤= [3, 4]⊤处进行一阶Taylor 展开:\\nf(x) = f (xk) + ∇f (xk)⊤(x −xk)\\n= ∥x∥2\\n2 + 2x⊤\\nk (x −xk)\\n= ∥x∥2\\n2 + 2x⊤\\nk αd\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= ∥x∥2\\n2 + 2x⊤\\nk (x −xk)\\n= ∥x∥2\\n2 + 2x⊤\\nk αd\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n此时∇f (xk) = [6, 8]⊤, 因此dk = −\\n∇f(xk)\\n∥∇f(xk)∥2 = [−0.6, −0.8]⊤。接下来求αk, 将xk 和dk 代入:\\nf (xk + αdk) = (3 −0.6α)2 + (4 −0.8α)2\\n= α2 −10α + 25\\n= (α −5)2\\n因此可得αk = 5 (或对α 求导, 再令导数等于0 )。此时\\n∆x = αkdk = [−3, −4]⊤\\n则xk + ∆x = [0, 0]⊤, 函数值f (xk + ∆x) = 0 。通过以上分析, 只想强调两点: (1) 梯度下降法求解下降\\n最快的方向dk 时应该求解如下优化问题:\\ndk = arg min\\nd\\n∇f (xk)⊤d s.t. ∥d∥2 = C\\n其中C 为常量, 即不必严格限定∥dk∥2 = 1, 只要固定向量长度, 与αk 搭配即可。(2) 梯度下降法求解步\\n长αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='长αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)\\n实际应用中, 很多时候不会去求最优的αk, 而是靠经验设置一个步长。\\n8.6.2\\n从梯度下降的角度解释AdaBoost\\nAdaBoost 第t 轮迭代时最小化式(8.5) 的指数损失函数\\nℓexp (Ht | D) = Ex∼D\\n\\x02\\ne−f(x)Ht(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)Ht(x)\\n对ℓexp (Ht | D) 每一项在Ht−1 处泰勒展开\\nℓexp (Ht | D) ≈\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x) −f(x)e−f(x)Ht−1(x) (Ht(x) −Ht−1(x))\\n\\x01\\n=\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n\\x01\\n= Ex∼D\\n\\x02\\ne−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n\\x03\\n其中Ht = Ht−1 +αtht 。注意: αt, ht 是第t 轮待解的变量。另外补充一下, 在上式展开中的变量为Ht(x),\\n在Ht−1 处一阶导数为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='在Ht−1 处一阶导数为\\n∂e−f(x)Ht(x)\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\n= −f(x)e−f(x)Ht−1(x)\\n如果看不习惯上述泰勒展开过程, 可令变量z = Ht(x) 和函数g(z) = e−f(x)z, 对g(z) 在z0 = Ht−1(x) 处\\n泰勒展开, 得\\ng(z) ≈g (z0) + g′ (z0) (z −z0)\\n= g (z0) −f(x)e−f(x)z0 (z −z0)\\n= e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x) (Ht(x) −Ht−1(x))\\n= e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n注意此处ht(x) ∈{−1, +1}, 类似于梯度下降法中的约束∥dk∥2 = 1 。类似于梯度下降法求解下降最快的\\n方向dk, 此处先求ht (先不管αt ):\\nht = arg min\\nh\\nX\\nx∈D\\nD(x)\\n\\x00−e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\ns.t. h(x) ∈{−1, +1}\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x00−e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\ns.t. h(x) ∈{−1, +1}\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n将负号去掉, 最小化变为最大化问题\\nht = arg max\\nh\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\n= arg max\\nh\\nEx∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\ns.t. h(x) ∈{−1, +1}\\n这就是式(8.14) 的第3 个等号的结果, 因此其余推导参见8.2.16节即可。由于这里的h(x) 约束较强, 因此\\n不能直接取负梯度方向, 书中经过推导得到了ht(x) 的表达式, 即式(8.18)。实际上, 可以将此结果理解为\\n满足约束条件的最快下降方向。求得ht(x) 之后再求αt (8.2.16节“AdaBoost 的个人推导”注解中已经写\\n过一遍, 此处仅粘贴至此, 具体参见8.2.16节注解，尤其是ℓexp (Ht−1 + αht | D) 表达式的由来):\\nαk = arg min\\nα\\nℓexp (Ht−1 + αht | D)\\n对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得\\n∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′\\nt (xi) + (eα −e−α) P|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\n\\x11\\n∂α\\n= −e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα + e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\nP|D|\\ni=1 D′\\nt (xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1\\nDt (xi) I (f (xi) ̸= h (xi)) = Ex∼Dt [I (f (xi) ̸= h (xi))]\\n= ϵt\\n对上述等式化简, 得\\ne−α\\neα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='eα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1\\n2 ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n即式(8.11)。通过以上推导可以发现: AdaBoost 每一轮的迭代就是基于梯度下降法求解损失函数为指数\\n损失函数的二分类问题约束条件ht(x) ∈{−1, +1} 。\\n8.6.3\\n梯度提升(Gradient Boosting)\\n将AdaBoost 的问题一般化, 即不限定损失函数为指数损失函数, 也不局限于二分类问题, 则可以将式\\n(8.5) 写为更一般化的形式\\nℓ(Ht | D) = Ex∼D [err (Ht(x), f(x))]\\n= Ex∼D [err (Ht−1(x) + αtht(x), f(x))]\\n问题时, f(x) ∈R, 损失函数可使用平方损失err (Ht(x), f(x)) = (Ht(x) −f(x))2 。针对该一般化的损失\\n函数和一般的学习问题, 要通过T 轮迭代得到学习器\\nH(x) =\\nT\\nX\\nt=1\\nαtht(x)\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='H(x) =\\nT\\nX\\nt=1\\nαtht(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n类似于AdaBoost, 第t 轮得到αt, ht(x), 可先对损失函数在Ht−1(x) 处进行泰勒展开:\\nℓ(Ht | D) ≈Ex∼D\\n\"\\nerr (Ht−1(x), f(x)) + ∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\n(Ht(x) −Ht−1(x))\\n#\\n= Ex∼D\\n\"\\nerr (Ht−1(x), f(x)) + ∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\nαtht(x)\\n#\\n= Ex∼D [err (Ht−1(x), f(x))] + Ex∼D\\n\"\\n∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\nαtht(x)\\n#\\n注意, 在上式展开中的变量为Ht(x), 且有Ht(x) = Ht−1(x)+αtht(x) (类似于梯度下降法中x = xk + αkdk)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='。上式中括号内第1 项为常量ℓ(Ht−1 | D), 最小化ℓ(Ht | D) 只须最小化第2 项即可。先不考虑权重αt,\\n求解如下优化问题可得ht(x) :\\nht(x) = arg min\\nh\\nEx∼D\\n\"\\n∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\x0c\\x0c\\x0c\\nHt(x)=Ht−1(x)\\nh(x)\\n#\\ns.t. constraints for h(x)\\n解得ht(x) 之后, 再求解如下优化问题可得权重αt :\\nαt = arg min\\nα\\nEx∼D [err (Ht−1(x) + αht(x), f(x))]\\n以上就是梯度提升(Gradient Boosting) 的理论框架, 即每轮通过梯度(Gradient) 下降的方式将T 个弱学\\n习器提升(Boosting) 为强学习器。可以看出AdaBoost 是其特殊形式。\\nGradient Boosting 算法的官方版本参见[5] 第5-6 页(第1193-1194 页)，其中算法部分见算法1\\nAlgorithm 1 Gradient_Boost(A, p, r)\\n1: F0(x) = arg minρ\\nPN'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1: F0(x) = arg minρ\\nPN\\ni=1 L (yi, ρ)\\n2: for m = 1 doM\\n3:\\n˜yi = −\\nh\\n∂L(yi,F (xj))\\n∂F (xi)\\ni\\nF (x)=Fm−1(x) , i = 1, N\\n4:\\nam = arg mina,β\\nPN\\ni=1 [˜yi −βh (xi; a)]2\\n5:\\nρm = arg minρ\\nPN\\ni=1 L (yi, Fm−1 (xi) + ρh (xi; am))\\n6:\\nFm(x) = Fm−1(x) + ρmh (x; am)\\n感觉该伪代码针对的还是在任意损失函数L (yi, F (xi)) 下的回归问题。Algorithm 1 中第3 步和第4\\n步意思是用βh (xi, a) 拟合F(x) = Fm−1(x) 处负梯度, 但第4 步表示只求参数am, 第5 步单独求解参数\\nρm, 这里的疑问是为什么第4 步要用最小二乘法（即3.2 节的线性回归）去拟合负梯度（又称伪残差）?\\n简单理解如下: 第4 步要解的h (xi, a) 相当于梯度下降法中的待解的下降方向d, 在梯度下降法中也'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='已提到不必严格限制∥d∥2 = 1, 长度可以由步长α 调节(例如前面梯度下降方解释中的例1 , 若直接取\\ndk = −f ′ (xk) = −4, 则可得αk = 0.5, 仍有∆x = αkdk = −2), 因此第4 步直接用h (xi, a) 拟合负梯度,\\n与梯度下降中约束∥d∥2 = 1 的区别在于末对负梯度除以其模值进行归一化而已。\\n那为什么不是直接令h (xi, a) 等于负梯度呢? 因为这里实际是求假设函数h, 将数据集中所有的xi\\n经假设函数h 映射到对应的伪残差(负梯度) ˜yi, 所以只能做线性回归了。\\n李航《统计学习方法》[2] 第8.4.3 节中的算法8.4 并末显式体现参数ρm, 这应该是第2 步的(c) 步完\\n成的, 因为(b) 步只是拟合一棵回归树(相当于Algorithm 1 第4 步解得h (xi, a) ), 而(c) 步才确定每个\\n叶结点的取值(相当于Algorithm 1 第5 步解得ρm, 只是每个叶结点均对应一个ρm); 而且回归问题中基\\n函数为实值函数，可以将参数ρm 吸收到基函数中。\\n8.6.4\\n梯度提升树(GBDT)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='函数为实值函数，可以将参数ρm 吸收到基函数中。\\n8.6.4\\n梯度提升树(GBDT)\\n本部分无实质GBDT 内容，仅为梳理GBDT 的概念，具体可参考给出的资源链接。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于GBDT，一般资料是按Gradient Boosting+CART 处理回归问题讲解的，如林轩田《机器学习\\n技法》课程第11 讲。但是，分类问题也可以用回归来处理，例如3.3 节的对数几率回归，只需将平方损\\n失换为对率损失（参见式(3.27) 和式(6.33)，二者关系可参见第3 章注解中有关式(3.27) 的推导）即可。\\n细节可以搜索林轩田老师的《机器学习基石》和《机器学习技法》两门课程以及配套的视频。\\n8.6.5\\nXGBoost\\n本部分无实质XGBoost 内容，仅为梳理XGBoost 的概念，具体可参考给出的资源链接。\\n首先，XGBoost 是eXtreme Gradient Boosting 的简称。其次，XGBoost 与GBDT 的关系，可大致类\\n比为LIBSVM 与SVM（或SMO 算法）的关系。LIBSVM 是SVM 算法的一种高效实现软件包，XGBoost\\n是GBDT 的一种高效实现；在实现层面，LIBSVM 对SMO 算法进行了许多改进，XGBoost 也对GBDT'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='进行了许多改进；另外，LIBSVM 扩展了许多SVM 变体，XGBoost 也不再仅仅是标准的GBDT，也扩\\n展了一些其它功能。最后，XGBoost 是由陈天奇开发的；XGBoost 论文可以参考[6]，XGBoost 工具包、\\n文档和源码等均可以在Github 上搜索到。\\n参考文献\\n[1] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical view\\nof boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):337–407,\\n2000.\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n[3] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. In IJCAI,'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='pages 3553–3559, 2017.\\n[4] 朱德通孙文瑜, 徐成贤. 最优化方法. 最优化方法, 2010.\\n[5] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics,\\npages 1189–1232, 2001.\\n[6] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the\\n22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794,\\n2016.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第9 章\\n聚类\\n到目前为止，前面章节介绍的方法都是针对监督学习(supervised learning) 的，本章介绍的聚类(clus-\\ntering) 和下一章介绍的降维属于无监督学习(unsupervised learning)。\\n9.1\\n聚类任务\\n单词“cluster”既是动词也是名词，作为名词时翻译为“簇”，即聚类得到的子集；一般谈到“聚类”\\n这个概念时对应其动名词形式“clustering”。\\n9.2\\n性能度量\\n本节给出了聚类性能度量的三种外部指标和两种内部指标，其中式(9.5) ~ 式(9.7) 是基于式(9.1) ~\\n式(9.4) 导出的三种外部指标，而式(9.12) 和式(9.13) 是基于式(9.8) ~ 式(9.11) 导出的两种内部指标。\\n读本节内容需要心里清楚的一点：本节给出的指标仅是该领域的前辈们定义的指标，在个人研究过程中可\\n以根据需要自己定义，说不定就会被业内同行广泛使用。\\n9.2.1\\n式(9.5) 的解释\\n给定两个集合A 和B，则Jaccard 系数定义为如下公式'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='9.2.1\\n式(9.5) 的解释\\n给定两个集合A 和B，则Jaccard 系数定义为如下公式\\nJC = |A T B|\\n|A S B| =\\n|A T B|\\n|A| + |B| −|A T B|\\nJaccard 系数可以用来描述两个集合的相似程度。推论：假设全集U 共有n 个元素，且A ⊆U，B ⊆U，\\n则每一个元素的位置共有四种情况：\\n1. 元素同时在集合A 和B 中，这样的元素个数记为M11\\n2. 元素出现在集合A 中，但没有出现在集合B 中，这样的元素个数记为M10\\n3. 元素没有出现在集合A 中，但出现在集合B 中，这样的元素个数记为M01\\n4. 元素既没有出现在集合A 中，也没有出现在集合B 中，这样的元素个数记为M00\\n根据Jaccard 系数的定义，此时的Jaccard 系数为如下公式\\nJC =\\nM11\\nM11 + M10 + M01\\n由于聚类属于无监督学习，事先并不知道聚类后样本所属类别的类别标记所代表的意义，即便参考模型的\\n类别标记意义是已知的，我们也无法知道聚类后的类别标记与参考模型的类别标记是如何对应的，况且聚'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='类别标记意义是已知的，我们也无法知道聚类后的类别标记与参考模型的类别标记是如何对应的，况且聚\\n类后的类别总数与参考模型的类别总数还可能不一样，因此只用单个样本无法衡量聚类性能的好坏。\\n由于外部指标的基本思想就是以参考模型的类别划分为参照，因此如果某一个样本对中的两个样本在\\n聚类结果中同属于一个类，在参考模型中也同属于一个类，或者这两个样本在聚类结果中不同属于一个类，\\n在参考模型中也不同属于一个类，那么对于这两个样本来说这是一个好的聚类结果。\\n总的来说所有样本对中的两个样本共存在四种情况：\\n1. 样本对中的两个样本在聚类结果中属于同一个类，在参考模型中也属于同一个类\\n2. 样本对中的两个样本在聚类结果中属于同一个类，在参考模型中不属于同一个类\\n3. 样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中属于同一个类\\n4. 样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中也不属于同一个类\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n综上所述，即所有样本对存在着书中式(9.1) ~ 式(9.4) 的四种情况，现在假设集合A 中存放着两个样本\\n都同属于聚类结果的同一个类的样本对，即A = SS S SD，集合B 中存放着两个样本都同属于参考模型\\n的同一个类的样本对，即B = SS S DS，那么根据Jaccard 系数的定义有：\\nJC = |A T B|\\n|A S B| =\\n|SS|\\n|SS S SD S DS| =\\na\\na + b + c\\n也可直接将书中式(9.1) ~ 式(9.4) 的四种情况类比推论，即M11 = a，M10 = b，M01 = c，所以\\nJC =\\nM11\\nM11 + M10 + M01\\n=\\na\\na + b + c\\n9.2.2\\n式(9.6) 的解释\\n其中\\na\\na+b 和\\na\\na+c 为Wallace 提出的两个非对称指标，a 代表两个样本在聚类结果和参考模型中均属于\\n同一类的样本对的个数，a + b 代表两个样本在聚类结果中属于同一类的样本对的个数，a + c 代表两个样'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='本在参考模型中属于同一类的样本对的个数，这两个非对称指标均可理解为样本对中的两个样本在聚类结\\n果和参考模型中均属于同一类的概率。由于指标的非对称性，这两个概率值往往不一样，因此Fowlkes 和\\nMallows 提出利用几何平均数将这两个非对称指标转化为一个对称指标，即Fowlkes and Mallows Index,\\nFMI。\\n9.2.3\\n式(9.7) 的解释\\nRand Index 定义如下：\\nRI =\\na + d\\na + b + c + d =\\na + d\\nm(m −1)/2 = 2(a + d)\\nm(m −1)\\n由第一个等号可知RI 肯定不大于1。之所以a + b + c + d = m(m −1)/2, 这是因为式(9.1) ~ 式(9.4) 遍\\n历了所有(xi, xj) 组合对(i ̸= j) : 其中i = 1 时j 可以取2 到m 共m −1 个值, i = 2 时j 可以取3\\n到m 共m −2 个值, · · · · · · , i = m −1 时j 仅可以取m 共1 个值, 因此(xi, xj) 组合对的个数为从1 到'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m −1 求和, 根据等差数列求和公式即得m(m −1)/2 。\\n这个指标可以理解为两个样本都属于聚类结果和参考模型中的同一类的样本对的个数与两个样本都\\n分别不属于聚类结果和参考模型中的同一类的样本对的个数的总和在所有样本对中出现的频率，可以简单\\n理解为聚类结果与参考模型的一致性。\\n9.2.4\\n式(9.8) 的解释\\n簇内距离的定义式：求和号左边是(xi, xj) 组合个数的倒数，求和号右边是这些组合的距离和，所以\\n两者相乘定义为平均距离。\\n9.2.5\\n式(9.12) 的解释\\n式中, k 表示聚类结果中簇的个数。该式的DBI 值越小越好, 因为我们希望“物以类聚”, 即同一簇的\\n样本尽可能彼此相似, avg (Ci) 和avg (Cj) 越小越好; 我们希望不同簇的样本尽可能不同, 即dcen (Ci, Cj)\\n越大越好。勘误: 第25 次印刷将分母dcen\\n\\x00µi, µj\\n\\x01\\n改为dcen (Ci, Cj)\\n9.3\\n距离计算\\n距离计算在各种算法中都很常见，本节介绍的距离计算方式和“西瓜书”10.6 节介绍的马氏距离基本'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='距离计算在各种算法中都很常见，本节介绍的距离计算方式和“西瓜书”10.6 节介绍的马氏距离基本\\n囊括了一般的距离计算方法。另外可能还会碰到“西瓜书”10.5 节的测地线距离。\\n本节有很多概念和名词很常见，比如本节开篇介绍的距离度量的四个基本性质、闵可夫斯基距离、欧\\n氏距离、曼哈顿距离、切比雪夫距离、数值属性、离散属性、有序属性、无序属性、非度量距离等，注意\\n对应的中文和英文。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n9.3.1\\n式(9.21) 的解释\\n该式符号较为抽象, 下面计算“西瓜书”第76 页表4.1 西瓜数据集2.0 属性根蒂上“蜷缩”和“稍蜷”\\n两个离散值之间的距离。\\n此时, u 为“根蒂”, a 为属性根蒂上取值为“蜷缩”, b 为属性根蒂上取值为“稍蜷”, 根据边注, 此\\n时样本类别已知(好瓜/坏瓜), 因此k = 2 。\\n从“西瓜书”表4.1 中可知, 根蒂为蜷缩的样本共有8 个(编号1 ∼5、编号12、编号16 ∼17), 即\\nmu,a = 8, 根蒂为稍蜷的样本共有7 个(编号6 ∼9 和编号13 ∼15), 即mu,b = 7; 设i = 1 对应好瓜,\\ni = 2 对应坏瓜, 好瓜中根蒂为蜷缩的样本共有5 个（编号1 ∼5 ), 即mu,a,1 = 5, 好瓜中根蒂为稍蜷的样\\n本共有3 个(编号6 8), 即mu,b,1 = 3, 坏瓜中根蒂为蜷缩的样本共有3 个(编号12 和编号16 ∼17 ), 即'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='mu,a,2 = 3, 坏瓜中根蒂为稍蜷的样本共有4 个（编号9 和编号13 ∼15), 即mu,b,2 = 4, 因此VDM 距离为\\nVDMp(a, b) =\\n\\x0c\\x0c\\x0c\\x0c\\nmu,a,1\\nmu,a\\n−mu,b,1\\nmu,b\\n\\x0c\\x0c\\x0c\\x0c\\np\\n+\\n\\x0c\\x0c\\x0c\\x0c\\nmu,a,2\\nmu,a\\n−mu,b,2\\nmu,b\\n\\x0c\\x0c\\x0c\\x0c\\np\\n=\\n\\x0c\\x0c\\x0c\\x0c\\n5\\n8 −3\\n7\\n\\x0c\\x0c\\x0c\\x0c\\np\\n+\\n\\x0c\\x0c\\x0c\\x0c\\n3\\n8 −4\\n7\\n\\x0c\\x0c\\x0c\\x0c\\np\\n9.4\\n原型聚类\\n本节介绍了三个原型聚类算法, 其中k 均值算法最为经典, 几乎成为聚类的代名词, 在Matlab、scikit-\\nlearn 等主流的科学计算包中均有kmeans 函数供调用。学习向量量化也是无监督聚类的一种方式, 在向量\\n检索的引擎，比如facebook faiss 中发挥重要的应用。\\n前两个聚类算法比较易懂, 下面主要推导第三个聚类算法：高斯混合聚类。\\n9.4.1\\n式(9.28) 的解释\\n该式就是多元高斯分布概率密度函数的定义式:\\np(x) =\\n1\\n(2π)\\nn\\n2 |Σ|\\n1\\n2 e−1\\n2 (x−µ)⊤Σ−1(x−µ)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='p(x) =\\n1\\n(2π)\\nn\\n2 |Σ|\\n1\\n2 e−1\\n2 (x−µ)⊤Σ−1(x−µ)\\n对应到我们常见的一元高斯分布概率密度函数的定义式:\\np(x) =\\n1\\n√\\n2πσ e−(x−µ)2\\n2σ2\\n其中\\n√\\n2π = (2π)\\n1\\n2 对应(2π)\\nn\\n2 , σ 对应|Σ|\\n1\\n2 , 指数项中分母中的方差σ2 对应协方差矩阵Σ, (x−µ)2\\nσ2\\n对应(x−\\nµ)⊤Σ−1(x −µ)。\\n概率密度函数p(x) 是x 的函数。其中对于某个特定的x 来说, 函数值p(x) 就是一个数, 若x 的\\n维度为2 , 则可以将函数p(x) 的图像可视化, 是三维空间的一个曲面。类似于一元高斯分布p(x) 与横\\n轴p(x) = 0 之间的面积等于1 (即\\nR\\np(x)dx = 1 )，p(x) 曲面与平面p(x) = 0 之间的体积等于1 （即\\nR\\np(x)dx = 1 。\\n注意, “西瓜书”中后面将p(x) 记为p(x | µ, Σ) 。\\n9.4.2\\n式(9.29) 的解释\\n对于该式表达的高斯混合分布概率密度函数pM(x), 与式(9.28) 中的p(x) 不同的是, 它由k 个不同'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的多元高斯分布加权而来。具体来说, p(x) 仅由参数µ, Σ 确定, 而pM(x) 由k 个“混合系数”αi 以及k\\n组参数µi, Σi 确定。\\n在“西瓜书”中该式下方(P207 最后一段) 中介绍了样本的生成过程, 实际也反应了“混合系数”αi 的\\n含义, 即αi 为选择第i 个混合成分的概率, 或者反过来说, αi 为样本属于第i 个混合成分的概率。重新描述\\n一下样本生成过程, 根据先验分布α1, α2, . . . , αk 选择其中一个高斯混合成分(即第i 个高斯混合成分被选\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n到的概率为αi ), 假设选到了第i 个高斯混合成分, 其参数为µi, Σi; 然后根据概率密度函数p (x | µi, Σi)\\n(即将式(9.28) 中的µ, Σ 替换为µi, Σi) 进行采样生成样本x 。两个步骤的区别在于第1 步选择高斯混\\n合成分时是从k 个之中选其一(相当于概率密度函数是离散的), 而第2 步生成样本时是从x 定义域中根\\n据p (x | µi, Σi ) 选择其中一个样本, 样本x 被选中的概率即为p (x | µi, Σi) 。即第1 步对应于离散型随\\n机变量, 第2 步对应于连续型随机变量。\\n9.4.3\\n式(9.30) 的解释\\n若由上述样本生成方式得到训练集D = {x1, x2, . . . , xm}, 现在的问题是对于给定样本xj, 它是由哪\\n个高斯混合成分生成的呢? 该问题即求后验概率pM (zj | xj), 其中zj ∈{1, 2, . . . , k} 。下面对式(9.30) 进\\n行推导。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='行推导。\\n对于任意样本, 在不考虑样本本身之前(即先验), 若瞎猜一下它由第i 个高斯混合成分生成的概率\\nP (zj = i), 那么肯定按先验概率α1, α2, . . . , αk 进行猜测, 即P (zj = i) = αi 。若考虑样本本身带来的信\\n息(即后验), 此时再猜一下它由第i 个高斯混合成分生成的概率pM (zj = i | xj), 根据贝叶斯公式, 后验概\\n率pM (zj = i | xj) 可写为\\npM (zj = i | xj) = P (zj = i) · pM (xj | zj = i)\\npM (xj)\\n分子第1 项P (zj = i) = αi; 第2 项即第i 个高斯混合成分生成样本xj 的概率p (xj | µi, Σi), 根据式\\n(9.28) 将x, µ, Σ 替换为xj, µi, Σi 即得; 分母pM (xj) 即为将xj 代入式(9.29) 即得。\\n注意, 西瓜书中后面将pM (zj = i | xj) 记为γji, 其中1 ≤j ≤m, 1 ≤i ≤k。\\n9.4.4\\n式(9.31) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='9.4.4\\n式(9.31) 的解释\\n若将所有γji 组成一个矩阵Γ, 其中γji 为第j 行第例的元素, 矩阵Γ 大小为m × k, 即\\nΓ =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nγ11\\nγ12\\n· · ·\\nγ1k\\nγ21\\nγ22\\n· · ·\\nγ2k\\n...\\n...\\n...\\n...\\nγm1\\nγm2\\n· · ·\\nγmk\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nm×k\\n其中m 为训练集样本个数, k 为高斯混合模型包含的混合模型个数。可以看出, 式(9.31) 就是找出矩阵Γ\\n第j 行的所有k 个元素中最大的那个元素的位置。\\n9.4.5\\n式(9.32) 的解释\\n对于训练集D = {x1, x2, . . . , xm}, 现在要把m 个样本划分为k 个簇, 即认为训练集D 的样本是根\\n据k 个不同的多元高斯分布加权而得的高斯混合模型生成的。\\n现在的问题是, k 个不同的多元高斯分布的参数{(µi, Σi) | 1 ⩽i ⩽k} 及它们各自的权重α1, α2, . . . , αk\\n不知道, m 个样本归到底属于哪个簇也不知道, 该怎么办呢?'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='不知道, m 个样本归到底属于哪个簇也不知道, 该怎么办呢?\\n其实这跟k 均值算法类似, 开始时既不知道k 个簇的均值向量, 也不知道m 个样本归到底属于哪个\\n簇, 最后我们采用了贪心策略, 通过迭代优化来近似求解式(9.24)。\\n本节的高斯混合聚类求解方法与k 均值算法, 只是具体问题具体解法不同, 从整体上来说, 它们都应用\\n了7.6 节的期望最大化算法(EM 算法)。\\n具体来说, 现假设已知式(9.30) 的后验概率, 此时即可通过式(9.31) 知道m 个样本归到底属于哪个簇,\\n再来求解参数{(αi, µi, Σi) | 1 ⩽i ⩽k}, 怎么求解呢? 对于每个样本xj 来说, 它出现的概率是pM (xj), 既\\n然现在训练集D 中确实出现了xj, 我们当然希望待求解的参数{(αi, µi, Σi) | 1 ⩽i ⩽k} 能够使这种可能\\n性pM (xj) 最大; 又因为我们假设m 个样本是独立的, 因此它们恰好一起出现的概率就是Qm\\nj=1 pM (xj),\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n即所谓的似然函数; 一般来说, 连乘容易造成下溢( m 个大于0 小于1 的数相乘, 当m 较大时, 乘积会非\\n常非常小, 以致于计算机无法表达这么小的数, 产生下溢), 所以常用对数似然替代, 即式(9.32)。\\n9.4.6\\n式(9.33) 的推导\\n根据公式(9.28) 可知：\\np (xj|µi, Σi) =\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2 (xj −µi)T Σ−1\\ni\\n(xj −µi)\\n\\x13\\n又根据公式(9.32)，由\\n∂LL(D)\\n∂µi\\n=\\n∂LL(D)\\n∂p (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂µi\\n= 0\\n其中：\\n∂LL(D)\\n∂p (xj|µi, \\uffffi) =\\n∂Pm\\nj=1 ln\\n\\x10Pk\\nl=1 αl · p (xj|µl, Σl)\\n\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\n∂ln\\n\\x10Pk\\nl=1 αl · p (xj|µl, Σl)\\n\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\nαi\\nPk'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\nαi\\nPk\\nl=1 αl · p (xj|µl, Σl)\\n∂p (xj|µi, Σi)\\n∂µi\\n=\\n∂\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x10\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x11\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 ·\\n∂exp\\n\\x10\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x11\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 · exp\\n\\x12\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x13\\n· −1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 · exp\\n\\x12\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x13\\n· Σ−1\\ni\\n(xj −µi)\\n= p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n其中，由矩阵求导的法则∂aT Xa\\n∂a\\n= 2Xa 可得：\\n−1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n= −1\\n2 · 2Σ−1\\ni'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='−1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n= −1\\n2 · 2Σ−1\\ni\\n(µi −xj)\\n= Σ−1\\ni\\n(xj −µi)\\n因此有：\\n∂LL(D)\\n∂µi\\n=\\nm\\nX\\nj=1\\nαi\\nPk\\nl=1 αl · p (xj|µl, \\uffffl)\\n· p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi) = 0\\n9.4.7\\n式(9.34) 的推导\\n由式(9.30)\\nγji = pM (zj = i|Xj) =\\nαi · p (Xj|µi, Σi)\\nPk\\nl=1 αl · p (Xj|µl, Σl)\\n带入式(9.33)\\nm\\nX\\nj=1\\nγji (Xj −µi) = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n移项, 得\\nm\\nX\\nj=1\\nγjixj =\\nm\\nX\\nj=1\\nγjiµi = µi ·\\nm\\nX\\nj=1\\nγji\\n第二个等号是因为µi 对于求和变量j 来说是常量, 因此可以提到求和号外面; 因此\\nµi =\\nPm\\nj=1 γjixj\\nPm\\nj=1 γji\\n9.4.8\\n式(9.35) 的推导\\n根据公式(9.28) 可知：\\np(xj|µi, Σi) =\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\n又根据公式(9.32)，由\\n∂LL(D)\\n∂Σi\\n= 0\\n可得\\n∂LL(D)\\n∂Σi\\n=\\n∂\\n∂Σi\\n\" m\\nX\\nj=1\\nln\\n \\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\n!#\\n=\\nm\\nX\\nj=1\\n∂\\n∂Σi\\n\"\\nln\\n \\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\n!#\\n=\\nm\\nX\\nj=1\\nαi ·\\n∂\\n∂Σi\\n(p(xj|µi, Σi))\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n其中\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) ='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='l=1 αl · p(xj|µl, Σl)\\n其中\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) =\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f9\\n\\uf8fb\\n=\\n∂\\n∂Σi\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3exp\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8ed\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\n= p(xj|µi, Σi) ·\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8ed\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\n= p(xj|µi, Σi) ·\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0ln\\n1\\n(2π)\\nn\\n2 −\\n1\\n2 ln |Σi| −1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\uf8f9\\n\\uf8fb\\n= p(xj|µi, Σi) ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2\\n∂(ln |Σi|)\\n∂Σi\\n−\\n1\\n2\\n∂\\n\\x02\\n(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x03\\n∂Σi\\n\\uf8f9\\n\\uf8fb\\n由矩阵微分公式\\n∂|X|'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x02\\n(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x03\\n∂Σi\\n\\uf8f9\\n\\uf8fb\\n由矩阵微分公式\\n∂|X|\\n∂X = |X| · (X−1)T ,\\n∂aT X−1B\\n∂X\\n= −X−T abT X−T 可得\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) = p(xj|µi, Σi) ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n将此式代回\\n∂LL(D)\\n∂Σi\\n中可得\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n又由公式(9.30) 可知\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= γji，所以上式可进一步化简为\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nγji ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n令上式等于0 可得\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nγji ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb= 0\\n移项推导有：\\nm\\nX\\nj=1\\nγji ·\\n\\x02\\n−I + (xj −µi)(xj −µi)T Σ−1\\ni\\n\\x03\\n= 0\\nm\\nX\\nj=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x02\\n−I + (xj −µi)(xj −µi)T Σ−1\\ni\\n\\x03\\n= 0\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T Σ−1\\ni\\n=\\nm\\nX\\nj=1\\nγjiI\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T =\\nm\\nX\\nj=1\\nγjiΣi\\nΣ−1\\ni\\n·\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T =\\nm\\nX\\nj=1\\nγji\\nΣi =\\nPm\\nj=1 γji(xj −µi)(xj −µi)T\\nPm\\nj=1 γji\\n此即为公式(9.35)。\\n9.4.9\\n式(9.36) 的解释\\n该式即LL(D) 添加了等式约束Pk\\ni=1 αi = 1 的拉格朗日形式。\\n9.4.10\\n式(9.37) 的推导\\n重写式(9.32) 如下:\\nLL(D) =\\nm\\nX\\nj=1\\nln\\n k\\nX\\nl=1\\nαl · p (xj | µl, Σl)\\n!\\n这里将第2 个求和号的求和变量由式(9.32) 的i 改为了l, 这是为了避免对αi 求导时与变量i 相混淆。将\\n式(9.36) 中的两项分别对αi 求导, 得\\n∂LL(D)\\n∂αi\\n=\\n∂Pm\\nj=1 ln\\n\\x10Pk'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='∂LL(D)\\n∂αi\\n=\\n∂Pm\\nj=1 ln\\n\\x10Pk\\nl=1 αl · p (xj | µl, Σl)\\n\\x11\\n∂αi\\n=\\nm\\nX\\nj=1\\n1\\nPk\\nl=1 αl · p (xj | µl, Σl)\\n· ∂Pk\\nl=1 αl · p (xj | µl, Σl)\\n∂αi\\n=\\nm\\nX\\nj=1\\n1\\nPk\\nl=1 αl · p (xj | µl, Σl)\\n· p (xj | µi, Σi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n∂\\n\\x10Pk\\nl=1 αl −1\\n\\x11\\n∂αi\\n= ∂(α1 + α2 + . . . + αi + . . . + αk −1)\\n∂αi\\n= 1\\n综合两项求导结果, 并令导数等于零即得式(9.37)。\\n9.4.11\\n式(9.38) 的推导\\n注意, 在西瓜书第14 次印刷中式(9.38) 上方的一行话进行了勘误: “两边同乘以αi, 对所有混合成分\\n求和可知λ = −m ”, 将原来的“样本”修改为“混合成分”。\\n对公式(9.37) 两边同时乘以αi 可得\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n+ λαi = 0\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λαi\\n两边对所有混合成分求和可得\\nk\\nX\\ni=1\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λ\\nk\\nX\\ni=1\\nαi\\nm\\nX\\nj=1\\nk\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= −λ\\nk\\nX\\ni=1\\nαi\\nm\\nX\\nj=1\\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λ\\nk\\nX\\ni=1\\nαi\\n因为\\nk\\nX\\ni=1\\nαi · p(xj|µi, \\uffffi)\\nPk\\nl=1 αl · p(xj|µl, \\uffffl)\\n=\\nPk\\ni=1 αi · p(xj|µi, \\uffffi)\\nPk\\nl=1 αl · p(xj|µl, \\uffffl)\\n= 1\\n且Pk\\ni=1 αi = 1，所以有m = −λ，因此\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λαi = mαi\\n因此\\nαi =\\n1\\nm\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n又由公式(9.30) 可知\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= γji，所以上式可进一步化简为\\nαi =\\n1\\nm\\nm\\nX\\nj=1\\nγji\\n此即为公式(9.38)。\\n9.4.12\\n图9.6 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='αi =\\n1\\nm\\nm\\nX\\nj=1\\nγji\\n此即为公式(9.38)。\\n9.4.12\\n图9.6 的解释\\n第1 行初始化参数, 本页接下来的例子是按如下策略初始化的: 混合系数αi = 1\\nk; 任选训练集中的k\\n个样本分别初始化k 个均值向量µi(1 ⩽i ⩽k); 使用对角元素为0.1 的对角阵初始化k 个协方差矩阵\\nΣi(1 ⩽i ⩽k) 。\\n第3~5 行根据式(9.30) 计算共m × k 个γji 。\\n第6~10 行分别根据式(9.34)、式(9.35)、式(9.38) 使用刚刚计算得到的γji 更新均值向量、协方差\\n矩阵、混合系数; 注意第8 行计算协方差矩阵时使用的是第7 行计算得到的均值向量, 这并没错, 因为协方\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n差矩阵Σ′\\ni 与均值向量µ′\\ni 是对应的, 而非µi; 第7 行的µ′\\ni 在第8 行使用之后会在下一轮迭代中第4 行计\\n算γji 再次使用。\\n整体来说, 第2 ~12 行就是一个EM 算法的具体使用例子, 学习完7.6 节EM 算法可能根本无法理解\\n其思想。此例中有两组变量, 分别是γji 和(αi, µi, Σi), 它们之间相互影响, 但都是末知的, 因此EM 算法\\n就有了用武之地: 初始化其中一组变量(αi, µi, Σi), 然后计算γji; 再根据γji 根据最大似然推导出的公式\\n更新(αi, µi, Σi), 反复迭代, 直到满足停止条件。\\n9.5\\n密度聚类\\n本节介绍的DBSCAN 算法并不难懂，只是本节在最后举例时并没有说清楚密度聚类算法与前面原型\\n聚类算法的区别，当然这也可能是作者有意为之，因为在“西瓜书”本章习题9.7 题就提到了“凸聚类”\\n的概念。具体来说，前面介绍的聚类算法只能产生“凸聚类”，而本节介绍的DBSCAN 则能产生“非凸聚'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的概念。具体来说，前面介绍的聚类算法只能产生“凸聚类”，而本节介绍的DBSCAN 则能产生“非凸聚\\n类”，其本质原因，个人感觉在于聚类时使用的距离度量，均值算法使用欧氏距离，而DBSCAN 使用类似\\n于测地线距离（只是类似，并不相同，测地线距离参见“西瓜书”10.5 节），因此可以产生如下聚类结果\\n（中间为典型的非凸聚类）。\\n注意，虽然左图为“凸聚类”（四个簇都有一个凸包），但均值算法却无法产生此结果，因为最大的簇\\n太大了，其外围样本与另三个小簇的中心之间的距离更近，因此中间最大的簇肯定会被均值算法划分到不\\n同的簇之中，这显然不是我们希望的结果。\\n密度聚类算法可以产生任意形状的簇，不需要事先指定聚类个数k，并且对噪声鲁棒。\\n9.5.1\\n密度直达、密度可达与密度相连\\nxj 由xi 密度直达, 该概念最易理解, 但要特别注意: 密度直达除了要求xj 位于xi 的ϵ−领域的条件\\n之外, 还额外要求xi 是核心对象; ϵ-领域满足对称性, 但xj 不一定为核心对象, 因此密度直达关系通常不\\n满足对称性。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='满足对称性。\\nxj 由xi 密度可达，该概念基于密度直达，因此样本序列p1, p2, . . . , pn 中除了pn = xj 之外, 其余样\\n本均为核心对象(当然包括p1 = xi ), 所以同理, 一般不满足对称性。\\n以上两个概念中, 若xj 为核心对象, 已知xj 由xi 密度直达/可达, 则xi 由xj 密度直达/ 可达, 即满\\n足对称性(也就是说, 核心对象之间的密度直达/可达满足对称性)。\\nxi 与xj 密度相连, 不要求xi 与xj 为核心对象, 所以满足对称性。\\n9.5.2\\n图9.9 的解释\\n在第1 ∼7 行中, 算法先根据给定的邻域参数(ϵ, MinPts) 找出所有核心对象, 并存于集合Ω之中; 第\\n4 行的if 判断语句即在判别xj 是否为核心对象。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在第10 ∼24 行中, 以任一核心对象为出发点(由第12 行实现), 找出其密度可达的样本生成聚类簇\\n(由第14 ∼21 行实现), 直到所有核心对象被访问过为止（由第10 行和第23 行配合实现)。具体来说:\\n其中第14 ∼21 行while 循环中的if 判断语句（第16 行）在第一次循环时一定为真（因为Q 在第\\n12 行初始化为某核心对象), 此时会往队列Q 中加入q 密度直达的样本(已知q 为核心对象, q 的ϵ-领域\\n中的样本即为q 密度直达的), 队列遵循先进先出规则, 接下来的循环将依次判别q 的ϵ-领域中的样本是\\n否为核心对象(第16 行), 若为核心对象, 则将密度直达的样本( ϵ-领域中的样本) 加入Q。根据密度可达\\n的概念, while 循环中的if 判断语句（第16 行）找出的核心对象之间一定是相互密度可达的, 非核心对象\\n一定是密度相连的。\\n第14 ∼21 行while 循环每跳出一次, 即生成一个聚类簇。每次生成聚类\\x00之前, 会记录当前末访问过'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='样本集合(第11 行Γold = Γ ), 然后当前要生成的聚类簇每决定录取一个样本后会将该样本从厂去除(第\\n13 行和第19 行), 因此第14~21 行while 循环每跳出一次后, Γold 与Γ 差别即为聚类簇的样本成员(第\\n22 行), 并将该聚类簇中的核心对象从第1 ∼7 行生成的核心对象集合Ω中去除。\\n符号“\\\\”为集合求差, 例如集合A = {a, b, c, d, e, f}, B = {a, d, f, g, h}, 则A\\\\B 为A\\\\B = {b, c, e},\\n即将A, B 所有相同元素从A 中去除。\\n9.6\\n层次聚类\\n本节主要介绍了层次聚类的代表算法AGNES。\\n式(9.41) (9.43) 介绍了三种距离计算方式, 这与“西瓜书”9.3 节中介绍的距离不同之处在于, 此三种\\n距离计算面向集合之间, 而9.3 节的距离则面向两点之间。正如“西瓜书”第215 页左上边注所示, 集合间\\n的距离计算常采用豪斯多夫距离(Hausdorff distance)。\\n算法AGNES 很简单, 就是不断重复执行合并距离最近的两个聚类簇。“西瓜书”图9.11 为具体实现'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='方法, 核心就是在合并两个聚类簇后更新距离矩阵（第11 ∼23 行), 之所以看起来复杂, 是因为该实现只\\n更新原先距离矩阵中发生变化的行和列, 因此需要为此做一些调整。\\n在第1 ∼9 行, 算法先对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化。注意, 距离矩阵\\n中, 第i 行为聚类簇Ci 到各聚类簇的距离, 第i 列为各聚类簇到聚类簇Ci 的距离, 由第7 行可知, 距离矩\\n阵为对称矩阵, 即使用的集合间的距离计算方法满足对称性。\\n第18 ∼21 行更新距离矩阵M 的第i∗行与第i∗列, 因为此时的聚类簇Ci∗已经合并了Cj∗, 因此与\\n其余聚类簇之间的距离都发生了变化, 需要更新。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第10 章\\n降维与度量学习\\n10.1\\n预备知识\\n本章内容需要较多的线性代数和矩阵分析的基础，因此将相关的预备知识整体整理如下：\\n10.1.1\\n符号约定\\n向量元素之间分号“;”表示列元素分隔符, 如α = (a1; a2; . . . ; ai; . . . ; am) 表示m × 1 的列向量; 而逗\\n号“,”表示行元素分隔符, 如α = (a1, a2, . . . , ai, . . . , am) 表示1 × m 的行向量。\\n10.1.2\\n矩阵与单位阵、向量的乘法\\n(1) 矩阵左乘对角阵相当于矩阵每行乘以对应对角阵的对角线元素, 如:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1x11\\nλ1x12\\nλ1x13\\nλ2x21\\nλ2x22\\nλ2x23\\nλ3x31\\nλ3x32\\nλ3x33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n(2) 矩阵右乘对角阵相当于矩阵每列乘以对应对角阵的对角线元素, 如:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1x11\\nλ2x12\\nλ3x13\\nλ1x21\\nλ2x22\\nλ3x23\\nλ1x31\\nλ2x32\\nλ3x33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n(3) 矩阵左乘行向量相当于矩阵每行乘以对应行向量的元素之和, 如:\\nh\\nλ1\\nλ2\\nλ3\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= λ1\\nh\\nx11\\nx12\\nx13\\ni\\n+ λ2\\nh\\nx21\\nx22\\nx23\\ni\\n+ λ3\\nh\\nx31\\nx32\\nx33\\ni\\n=\\n\\x10\\nλ1x11 + λ2x21 + λ3x31, λ1x12 + λ2x22 + λ3x32, λ1x13 + λ2x23 + λ3x33\\n\\x11\\n(4) 矩阵右乘列向量相当于矩阵每列乘以对应列向量的元素之和, 如:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\nx31\\n\\uf8f9'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n= λ1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\nx31\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+ λ2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx12\\nx22\\nx32\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+ λ3\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx13\\nx23\\nx33\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n3\\nX\\ni=1\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8edλi\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx1i\\nx2i\\nx3i\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n= (λ1x11 + λ2x12 + λ3x13; λ1x21 + λ2x22 + λ3x23; λ1x31 + λ2x32 + λ3x33)\\n综上, 左乘是对矩阵的行操作, 而右乘则是对矩阵的列操作, 第(2) 个和第(4) 个结论后面推导过程中\\n灵活应用较多。\\n10.2\\n矩阵的F 范数与迹\\n(1) 对于矩阵A ∈Rm×n, 其Frobenius 范数(简称F 范数) ∥A∥F 定义为\\n∥A∥F =\\n m\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2\\n! 1\\n2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n其中aij 为矩阵A 第i 行第j 列的元素, 即\\nA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n...\\n...\\n...\\n...\\n...\\n...\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n...\\n...\\n...\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n(2) 若A = (α1, α2, . . . , αj, . . . , αn), 其中αj = (a1j; a2j; . . . ; aij; . . . ; amj) 为其列向量, A ∈Rm×n, αj ∈\\nRm×1, 则∥A∥2\\nF = Pn\\nj=1 ∥αj∥2\\n2;'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Rm×1, 则∥A∥2\\nF = Pn\\nj=1 ∥αj∥2\\n2;\\n同理, 若A = (β1; β2; . . . ; βi; . . . ; βm), 其中βi = (ai1, ai2, . . . , aij, . . . , ain) 为其行向量, A ∈Rm×n, βi ∈\\nR1×n, 则∥A∥2\\nF = Pm\\ni=1 ∥βi∥2\\n2 。\\n证明: 该结论是显而易见的, 因为∥αj∥2\\n2 = Pm\\ni=1 |aij|2, 而∥A∥F = Pm\\ni=1\\nPn\\nj=1 |aij|2 。\\n(3) 若λj\\n\\x00A⊤A\\n\\x01\\n表示n 阶方阵A⊤A 的第j 个特征值, tr\\n\\x00A⊤A\\n\\x01\\n是A⊤A 的迹（对角线元素之和);\\nλi\\n\\x10\\nAA⊤\\x11\\n表示m 阶方阵AA⊤的第i 个特征值, tr\\n\\x10\\nAA⊤\\x11\\n是AA⊤的迹, 则\\n∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n=\\nn\\nX\\nj=1\\nλj\\n\\x00A⊤A\\n\\x01\\n= tr\\n\\x00AA⊤\\x01\\n=\\nm\\nX\\ni=1\\nλi\\n\\x00AA⊤\\x01\\n证明: (a) 先证∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n:'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nm\\nX\\ni=1\\nλi\\n\\x00AA⊤\\x01\\n证明: (a) 先证∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n:\\n令B = A⊤A ∈Rn×n, bij 表示B 第i 行第j 列元素, tr(B) = Pn\\nj=1 bjȷ\\nB = A⊤A =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2\\n· · ·\\nam2\\n...\\n...\\n...\\n...\\n...\\n...\\na1j\\na2j\\n· · ·\\naij\\n· · ·\\namj\\n...\\n...\\n...\\n...\\n...\\n...\\na1n\\na2n\\n· · ·\\nain\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n...\\n...\\n...\\n...\\n...\\n...\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n...\\n...\\n...\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n由矩阵运算规则, bjj 等于A⊤的第j 行与A 的第j 列的内积, 因此\\ntr(B) =\\nn\\nX\\nj=1\\nbjj =\\nn\\nX\\nj=1\\n m\\nX\\ni=1\\n|aij|2\\n!\\n=\\nm\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2 = ∥A∥2\\nF\\n以上第三个等号交换了求和号次序（类似于交换积分号次序), 显然这不影响求和结果。\\n(b) 同理, 可证∥A∥2\\nF = tr\\n\\x00AA⊤\\x01\\n:\\nC = AA⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n...\\n...\\n...\\n...\\n...\\n...\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n...\\n...\\n...\\n...\\n...\\n...\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\na11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='a11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2\\n· · ·\\nam2\\n...\\n...\\n...\\n...\\n...\\n...\\na1j\\na2j\\n· · ·\\naij\\n· · ·\\namj\\n...\\n...\\n...\\n...\\n...\\n...\\na1n\\na2n\\n· · ·\\nain\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由矩阵运算规则, cii 等于A 的第i 行与A⊤的第i 列的内积（红色元素), 因此\\ntr(C) =\\nm\\nX\\ni=1\\ncii =\\nm\\nX\\ni=1\\n n\\nX\\nj=1\\n|aij|2\\n!\\n=\\nm\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2 = ∥A∥2\\nF\\n有关方阵的特征值之和等于对角线元素之和, 可以参见线性代数教材, 如同济大学主编的《线性代数\\n(第五版)》第五章第2 节“方阵的特征值与特征向量”(第117 页):\\n设n 阶矩阵A = (aij) 的特征值为λ1, λ2, · · · , λn, 不难证明\\n(i) λ1 + λ2 + · · · + λn = a11 + a22 + · · · + ann;\\n(ii) λ1λ2 · · · λn = |A|.\\n10.3\\nk 近邻学习\\n10.3.1\\n式(10.1) 的解释\\nP(err) = 1 −\\nX\\nc∈Y\\nP(c|x)P(c|z)\\n首先, P(c | x) 表示样本x 为类别c 的后验概率, P(c | z) 表示样本z 为类别c 的后验概率; 其次,'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='P(c | x)P(c | z) 表示样本x 和样本z 同时为类别c 的概率;\\n再次, P\\nc∈Y P(c | x)P(c | z) 表示样本x 和样本z 类别相同的概率; 这一点可以进一步解释，设\\nY = {c1, c2, · · · , cN}, 则该求和式子变为:\\nP (c1 | x) P (c1 | z) + P (c2 | x) P (c2 | z) + · · · + P (cN | x) P (cN | z)\\n即样本x 和样本z 同时为c1 的概率, 加上同时为c2 的概率, · · · · · · , 加上同时为cN 的概率, 即样本\\nx 和样本z 类别相同的概率;\\n最后, P(err) 表示样本x 和样本z 类别不相同的概率, 即1 减去二者类别相同的概率。\\n10.3.2\\n式(10.2) 的推导\\n式(10.2) 推导关键在于理解第二行的“约等(≃) ”关系和第三行的“小于等于(⩽) ”关系。\\n第二行的“约等(≃) ”关系的依据在于该式前面一段话: “假设样本独立同分布, 且对任意x 和任'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='第二行的“约等(≃) ”关系的依据在于该式前面一段话: “假设样本独立同分布, 且对任意x 和任\\n意小正数δ, 在x 附近δ 距离范围内总能找到一个训练样本”, 这意味着对于任意测试样本在训练集中\\n都可以找出一个与其非常像(任意小正数δ ) 的近邻, 这里还有一个假设书中末提及: P(c | x) 必须是\\n连续函数(对于连续函数f(x) 和任意小正数δ, f(x) ≃f(x + δ)), 即对于两个非常像的样本z 与x 有\\nP(c | x) ≃P(c | z), 即\\nX\\nc∈Y\\nP(c | x)P(c | z) ≃\\nX\\nc∈Y\\nP 2(c | x)\\n第三行的“小于等于(⩽) ”关系更简单: 由于c∗∈Y, 所以P 2 (c∗| x) ⩽P\\nc∈Y P 2(c | x), 也就是“小\\n于等于(⩽) ”左边只是右边的一部分, 所以肯定是小于等于的关系;\\n第四行就是数学公式a2 −b2 = (a + b)(a −b)；\\n第五行是由于1 + P (c∗| x) ⩽2, 这是由于概率值P (c∗| x) ⩽1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='第五行是由于1 + P (c∗| x) ⩽2, 这是由于概率值P (c∗| x) ⩽1\\n经过以上推导, 本节最后给出一个惊人的结论: 最近邻分类器虽简单, 但它的泛化错误率不超过贝叶斯\\n最优分类器的错误率的两倍!\\n然而这是一个没啥实际用途的结论, 因为这个结论必须满足两个假设条件, 且不说P(c | x) 是连续函\\n数（第一个假设）是否满足, 单就“对任意x 和任意小正数δ, 在x 附近δ 距离范围内总能找到一个训练\\n样本”(第二个假设) 是不可能满足的, 这也就有了10.2 节开头一段的讨论, 抛开“任意小正数δ ”不谈, 具\\n体到δ = 0.001 都是不现实的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.4\\n低维嵌入\\n10.4.1\\n图10.2 的解释\\n只要注意一点就行：在图(a) 三维空间中，红色线是弯曲的，但去掉高度这一维（竖着的坐标轴）后，\\n红色线变成直线，而直线更容易学习。\\n10.4.2\\n式(10.3) 的推导\\n已知Z = {z1, z2, . . . , zi, . . . , zm} ∈Rd′×m, 其中zi = (zi1; zi2; . . . ; zid′) ∈Rd′×1; 降维后的内积矩阵\\nB = Z⊤Z ∈Rm×m, 其中第i 行第j 列元素bij, 特别的\\nbii = z⊤\\ni zi = ∥zi∥2 , bjj = z⊤\\nj zj = ∥zj∥2 , bij = z⊤\\ni zj\\nMDS 算法的目标是∥zi −zj∥= distij = ∥xi −xj∥, 即保持样本的欧氏距离在d′ 维空间和原始d 维空间\\n相同(d′ ⩽d) 。\\ndist2\\nij = ∥zi −zj∥2 = (zi1 −zj1)2 + (zi2 −zj2)2 + . . . + (zid′ −zjd′)2\\n=\\n\\x00z2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\n\\x00z2\\ni1 −2zi1zj1 + z2\\nj1\\n\\x01\\n+\\n\\x00z2\\ni2 −2zi2zj2 + z2\\nj2\\n\\x01\\n+ . . . +\\n\\x00z2\\nid′ −2zid′zjd′ + z2\\njd′\\n\\x01\\n=\\n\\x00z2\\ni1 + z2\\ni2 + . . . + z2\\nid′\\n\\x01\\n+\\n\\x00z2\\nj1 + z2\\nj2 + . . . + z2\\njd′\\n\\x01\\n−2 (zi1zj1 + zi2zj2 + . . . + zid′zjd′)\\n= ∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n= bii + bjj −2bij\\n本章矩阵运算非常多, 刚刚是从矩阵元素层面的推导; 实际可发现上式运算结果基本与标量运算规则\\n相同, 因此后面会尽可能不再从元素层面推导。具体来说:\\ndist2\\nij = ∥zi −zj∥2 = (zi −zj)⊤(zi −zj)\\n= z⊤\\ni zi −z⊤\\ni zj −z⊤\\nj zi + z⊤\\nj zj\\n= z⊤\\ni zi + z⊤\\nj zj −2z⊤\\ni zj\\n= ∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n= bii + bjj −2bij\\n上式第三个等号化简是由于内积z⊤'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i zj\\n= bii + bjj −2bij\\n上式第三个等号化简是由于内积z⊤\\ni zj 和z⊤\\nj zi 均为标量, 因此转置等于本身。\\n10.4.3\\n式(10.4) 的推导\\n首先解释两个条件:\\n(1) 令降维后的样本Z 被中心化, 即Pm\\ni=1 zi = 0 注意Z ∈Rd′×m, d′ 是样本维度(属性个数), m 是样\\n本个数, 易知Z 的每一行有m 个元素(每行表示样本集的一维属性), Z 的每一列有d′ 个元素(每列表示\\n一个样本)。\\n式Pm\\ni=1 zi = 0 中的zi 明显表示的是第i 列, m 列相加得到一个零向量0d′×1, 意思是样本集合中所\\n有样本的每一维属性之和均等于0 , 因此被中心化的意思是将样本集合Z 的每一行（属性）减去该行的均\\n值。\\n(2) 显然, 矩阵B 的行与列之各均为零, 即Pm\\ni=1 bij = Pm\\nj=1 bij = 0 。\\n注意bij = z⊤\\ni zj (也可以写为bij = z⊤\\nj zi, 其实就是对应元素相乘, 再求和)\\nm\\nX\\ni=1\\nbij =\\nm\\nX\\ni=1\\nz⊤\\nj zi = z⊤\\nj\\nm\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nX\\ni=1\\nbij =\\nm\\nX\\ni=1\\nz⊤\\nj zi = z⊤\\nj\\nm\\nX\\ni=1\\nzi = z⊤\\nj · 0d′×1 = 0\\nm\\nX\\nj=1\\nbij =\\nm\\nX\\nj=1\\nz⊤\\ni zj = z⊤\\ni\\nm\\nX\\nj=1\\nzj = z⊤\\ni · 0d′×1 = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n接下来我们推导式(10.4), 将式(10.3) 的dist2\\nij 表达式代入:\\nm\\nX\\ni=1\\ndist2\\nij =\\nm\\nX\\ni=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\ni=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nz⊤\\ni zj\\n根据定义:\\nm\\nX\\ni=1\\n∥zi∥2 =\\nm\\nX\\ni=1\\nz⊤\\ni zi =\\nm\\nX\\ni=1\\nbii = tr(B)\\nm\\nX\\ni=1\\n∥zj∥2 = ∥zj∥2\\nm\\nX\\ni=1\\n1 = m ∥zj∥2 = mz⊤\\nj zj = mbjj\\n根据前面结果:\\nm\\nX\\ni=1\\nz⊤\\ni zj =\\n m\\nX\\ni=1\\nz⊤\\ni\\n!\\nzj = 01×d′ · zj = 0\\n代入上式即得:\\nm\\nX\\ni=1\\ndist2\\nij =\\nm\\nX\\ni=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nz⊤\\ni zj\\n= tr(B) + mbjj\\n10.4.4\\n式(10.5) 的推导\\n与式(10.4) 类似:\\nm\\nX\\nj=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='10.4.4\\n式(10.5) 的推导\\n与式(10.4) 类似:\\nm\\nX\\nj=1\\ndist2\\nij =\\nm\\nX\\nj=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\nj=1\\n∥zi∥2 +\\nm\\nX\\nj=1\\n∥zj∥2 −2\\nm\\nX\\nj=1\\nz⊤\\ni zj\\n= mbii + tr(B)\\n10.4.5\\n式(10.6) 的推导\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\ndist2\\nij =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nz⊤\\ni zj\\n其中各子项的推导如下：\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zi∥2 = m\\nm\\nX\\ni=1\\n∥zi∥2 = m tr(B)\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zj∥2 = m\\nm\\nX\\nj=1\\n∥zj∥2 = m tr(B)\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nz⊤\\ni zj = 0\\n最后一个式子是来自于书中的假设，假设降维后的样本Z 被中心化。\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='z⊤\\ni zj = 0\\n最后一个式子是来自于书中的假设，假设降维后的样本Z 被中心化。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.4.6\\n式(10.10) 的推导\\n由式(10.3) 可得\\nbij = −1\\n2(dist2\\nij −bii −bjj)\\n由式(10.6) 和(10.9) 可得\\ntr(B) =\\n1\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\ndist2\\nij\\n= m\\n2 dist2\\n·\\n由式(10.4) 和(10.8) 可得\\nbjj = 1\\nm\\nm\\nX\\ni=1\\ndist2\\nij −1\\nmtr(B)\\n= dist2\\n·j −1\\n2dist2\\n·\\n由式(10.5) 和式(10.7) 可得\\nbii = 1\\nm\\nm\\nX\\nj=1\\ndist2\\nij −1\\nmtr(B)\\n= dist2\\ni· −1\\n2dist2\\n·\\n综合可得\\nbij = −1\\n2(dist2\\nij −bii −bjj)\\n= −1\\n2(dist2\\nij −dist2\\ni· + 1\\n2dist2\\n·· −dist2\\n·j + 1\\n2dist2\\n··)\\n= −1\\n2(dist2\\nij −dist2\\ni· −dist2\\n·j + dist2\\n··)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= −1\\n2(dist2\\nij −dist2\\ni· −dist2\\n·j + dist2\\n··)\\n在式(10.10) 后紧跟着一句话: “由此即可通过降维前后保持不变的距离矩阵D 求取内积矩阵B”,\\n我们来解释一下这句话。\\n首先解释式(10.10) 等号右侧的变量含义:\\ndistij = ∥zi −zj∥表示降维后zi 与zj 的欧氏距离, 注\\n意这同时也应该是原始空间xi 与xj 的距离, 因为降维的目标（也是约束条件）是“任意两个样本在d′ 维\\n空间中的欧氏距离等于原始空间中的距离”, 也就是说dist2\\nij 是降维前后的距离矩阵D 的元素distij 的平\\n方; 其次, 式(10.10) 等号左侧bij 是降维后内积矩阵B 的元素, 即B 的元素bij 可以由距离矩阵D 来表达\\n求取。\\n10.4.7\\n式(10.11) 的解释\\n由题设知，d∗为V 的非零特征值，因此B = VΛV⊤可以写成B = V∗Λ∗V⊤\\n∗，其中Λ∗∈Rd×d 为d\\n个非零特征值构成的特征值对角矩阵，而V∗∈Rm×d 为Λ∗∈Rd×d 对应的特征值向量矩阵，因此有\\nB =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='B =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n故而Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m\\n10.4.8\\n图10.3 关于MDS 算法的解释\\n首先要清楚此处降维算法要完成的任务: 获得d 维空间的样本集合X ∈Rd×m 在d′ 维空间的表示\\nZ ∈Rd′×m, 并且保证距离矩阵D ∈Rm×m 相同, 其中d′ < d, m 为样本个数, 距离矩阵即样本之间的欧氏\\n距离。那么怎么由X ∈Rd×m 得到Z ∈Rd′×m 呢?\\n经过推导发现(式(10.3) 式(10.10)), 在保证距离矩阵D ∈Rm×m 相同的前提下, d′ 维空间的样本集\\n合Z ∈Rd′×m 的内积矩阵B = Z⊤Z ∈Rm×m 可以由距离矩阵D ∈Rm×m 得到(参见式(10.10)), 此时只\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n要对B 进行矩阵分解即可得到Z; 具体来说, 对B 进行特征值分解可得B = VΛV⊤, 其中V ∈Rm×m 为\\n特征值向量矩阵, \\uffff∈Rm×m 为特征值构成的对角矩阵, 接下来分类讨论:\\n(1) 当d > m 时, 即样本属性比样本个数还要多此时, 样本集合X ∈Rd×m 的d 维属性一定是线性相\\n关的(即有品几余), 因为矩阵X 的秩不会大于m (此处假设矩阵X 的秩恰好等于m ), 因此Λ ∈Rm×m\\n主对角线有m 个非零值, 进而B =\\n\\x10\\nVΛ1/2\\x11 \\x10\\nΛ1/2V⊤\\x11\\n, 得到的Z = Λ1/2V⊤∈Rd′×m 实际将d 维属性降\\n成了d′ = m 维属性。\\n(2) 当d < m 时, 即样本个数比样本属性多这是现实中最常见的一种情况。此时Λ ∈Rm×m 至多\\n有d 个非零值（此处假设恰有d 个非零值), 因此B = VΛV⊤可以写成B = V∗Λ∗V⊤\\n∗, 其中Λ∗∈Rd×d\\n为d 个非零值特征值构成的特征值对角矩阵, V∗∈Rm×d 为Λ∗∈Rd×d 相应的特征值向量矩阵, 进而\\nB =\\n\\x10'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='B =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n, 求得Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m, 此时属性没有冗杂, 因此按降维的规则（降维后距\\n离矩阵不变）并不能实现有效降维。\\n由以上分析可以看出, 降维后的维度d′ 实际为B 特征值分解后非零特征值的个数。\\n10.5\\n主成分分析\\n注意，作者在数次印刷中对本节符号进行修订，详见勘误修订，直接搜索页码即可，此处仅按个人推\\n导需求定义符号，可能与不同印次书中符号不一致。\\n10.5.1\\n式(10.14) 的推导\\n预备知识:\\n在一个坐标系中, 任意向量等于其在各个坐标轴的坐标值乘以相应坐标轴单位向量之和。例如, 在\\n二维直角坐标系中, x 轴和y 轴的单位向量分别为v1 = (1; 0) 和v2 = (0; 1), 向量r = (2; 3) 可以\\n表示为r = 2v1 + 3v2; 其实v1 = (1; 0) 和v2 = (0; 1) 只是二维平面的一组标准正交基, 但二维平面\\n实际有无数标准正交基, 如v′\\n1 =\\n\\x10\\n1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n和v′\\n2 =\\n\\x10\\n−1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n, 此时向量r =\\n5'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2;\\n1\\n√\\n2\\n\\x11\\n和v′\\n2 =\\n\\x10\\n−1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n, 此时向量r =\\n5\\n√\\n2v′\\n1 +\\n1\\n√\\n2v′\\n2, 其中\\n5\\n√\\n2 = (v′\\n1)⊤r,\\n1\\n√\\n2 = (v′\\n2)⊤r, 即新坐标系里的坐标。\\n下面开始推导:\\n对于d 维空间Rd×1 来说, 传统的坐标系为{v1, v2, . . . , vk, . . . , vd}, 其中vk 为除第k 个元素为1\\n其余元素均0 的d 维列向量; 此时对于样本点xi = (xi1; xi2; . . . ; xid) ∈Rd×1 来说亦可表示为xi =\\nxi1v1 + xi2v2 + . . . + xidvd 。\\n现假定投影变换后得到的新坐标系为{w1, w2, . . . , wk, . . . , wd} （即一组新的标准正交基), 则xi 在\\n新坐标系中的坐标为\\n\\x00w⊤\\n1 xi; w⊤\\n2 xi; . . . ; w⊤\\nd xi\\n\\x01\\n。若丢弃新坐标系中的部分坐标, 即将维度降低到d′ < d\\n(不失一般性, 假设丢掉的是后d −d′ 维坐标), 并令'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(不失一般性, 假设丢掉的是后d −d′ 维坐标), 并令\\nW = (w1, w2, . . . , wd′) ∈Rd×d′\\n则xi 在低维坐标系中的投影为\\nzi = (zi1; zi2; . . . ; zid′) =\\n\\x00w⊤\\n1 xi; w⊤\\n2 xi; . . . ; w⊤\\nd′xi\\n\\x01\\n= W⊤xi\\n若基于zi 来重构xi, 则会得到ˆxi = Pd′\\nj=1 zijwj = Wzi (“西瓜书”P230 第11 行)。\\n有了以上符号基础, 接下来将式(10.14) 化简成式(10.15) 目标函数形式(可逐一核对各项维数以验证\\n推导是否有误):\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\r\\nd′\\nX\\nj=1\\nzijwj −xi\\n\\r\\r\\r\\r\\r\\n2\\n2\\n(1)\\n=\\nm\\nX\\ni=1\\n∥Wzi −xi∥2\\n2\\n(2)\\n=\\nm\\nX\\ni=1\\n\\r\\rWW⊤xi −xi\\n\\r\\r2\\n2\\n(3)\\n=\\nm\\nX\\ni=1\\n\\x00WW⊤xi −xi\\n\\x01⊤\\x00WW⊤xi −xi\\n\\x01\\n(4)\\n=\\nm\\nX\\ni=1\\n\\x00x⊤\\ni WW⊤WW⊤xi −2x⊤\\ni WW⊤xi + x⊤\\ni xi\\n\\x01\\n(5)\\n=\\nm\\nX\\ni=1\\n\\x00x⊤\\ni WW⊤xi −2x⊤\\ni WW⊤xi + x⊤\\ni xi\\n\\x01\\n(6)\\n=\\nm\\nX\\ni=1\\n\\x00−x⊤\\ni W⊤xi + x⊤\\ni xi\\n\\x01\\n(1)\\n=\\nm\\nX\\ni=1\\n\\x10\\n−\\n\\x00W⊤xi\\n\\x01⊤\\x00W⊤xi\\n\\x01\\n+ x⊤\\ni xi\\n\\x11\\n(8)\\n=\\nm\\nX\\ni=1\\n\\x10\\n−\\n\\r\\rW⊤xi\\n\\r\\r2\\n2 + x⊤\\ni xi\\n\\x11\\n(9)\\n∝−\\nm\\nX\\ni=1\\n\\r\\rW⊤xi\\n\\r\\r2\\n2\\n上式从第三个等号到第四个等号: 由于\\n\\x00WW⊤\\x01⊤=\\n\\x00W⊤\\x01⊤(W)⊤= WW⊤, 因此\\n\\x00WW⊤xi\\n\\x01⊤= x⊤\\ni'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x00WW⊤\\x01⊤=\\n\\x00W⊤\\x01⊤(W)⊤= WW⊤, 因此\\n\\x00WW⊤xi\\n\\x01⊤= x⊤\\ni\\n\\x00WW⊤\\x01⊤= x⊤\\ni WW⊤\\n代入即得第四个等号; 从第四个等号到第五个等号: 由于w⊤\\ni wj = 0, (i ̸= j), ∥wi∥= 1, 因此W⊤W =\\nI ∈Rd′×d′, 代入即得第五个等号。由于最终目标是寻找W 使目标函数(10.14) 最小, 而x⊤\\ni xi 与W 无关,\\n因此在优化时可以去掉。令X = (x1, x2, . . . , xm) ∈Rd×m, 即每列为一个样本, 则式(10.14) 可继续化简为\\n(参见10.2节中“矩阵的F 范数与迹”)\\n−\\nm\\nX\\ni=1\\n\\r\\rW⊤xi\\n\\r\\r2\\n2 = −\\n\\r\\rW⊤X\\n\\r\\r2\\nF\\n= −tr\\n\\x10\\x00W⊤X\\n\\x01 \\x00W⊤X\\n\\x01⊤\\x11\\n= −tr\\n\\x00W⊤XX⊤W\\n\\x01\\n这里W⊤xi = zi, 这里仅为得到式(10.15) 的形式才最终保留W 和xi 的; 若令Z = (z1, z2, . . . , zm) ∈\\nRd′×m 为低维坐标系中的样本集合, 则Z = W⊤X, 即zi 为矩阵Z 的第i 列; 而Pm\\ni=1\\n\\r\\rW⊤xi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i=1\\n\\r\\rW⊤xi\\n\\r\\r2\\n2 =\\nPm\\ni=1 ∥zi∥2\\n2 表示Z 所有列向量2 范数的平方, 也就是Z 所有元素的平方和, 即为∥Z∥2\\nF, 此即第一个等号的\\n由来; 而根据10.0 中“矩阵的F 范数与迹”中第(3) 个结论, 即对于矩阵Z 有∥Z∥2\\nF = tr\\n\\x00Z⊤Z\\n\\x01\\n= tr\\n\\x00ZZ⊤\\x01\\n,\\n其中tr(·) 表示求矩阵的迹, 即对角线元素之和, 此即第二个等号的由来; 第三个等号将转置化简即得。\\n到此即得式(10.15) 的目标函数, 约束条件W⊤W = I 已在推导中说明。\\n式(10.15) 的目标函数式(10.14) 结果略有差异, 接下来推导Pm\\ni=1 xix⊤\\ni = XX⊤以弥补这个差异（这\\n个结论可以记下来)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n先化简Pm\\ni=1 xix⊤\\ni , 首先:\\nxix⊤\\ni =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nxi1\\nxi2\\n...\\nxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nh\\nxi1\\nxi2\\n· · ·\\nxid\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nxi1xi1\\nxi1xi2\\n· · ·\\nxi1xid\\nxi2xi1\\nxi2xi2\\n· · ·\\nxi2xid\\n...\\n...\\n...\\n...\\nxidxi1\\nxidxi2\\n· · ·\\nxidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n整体代入求和号Pm\\ni=1 xix⊤\\ni , 得\\nm\\nX\\ni=1\\nxix⊤\\ni =\\nm\\nX\\ni=1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nxi1xi1\\nxi1xi2\\n· · ·\\nxi1xid\\nxi2xi1\\nxi2xi2\\n· · ·\\nxi2xid\\n...\\n...\\n...\\n...\\nxidxi1\\nxidxi2\\n· · ·\\nxidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n...\\n...\\n...\\n...\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n再化简XX⊤∈Rd×d:\\nXX⊤=\\nh\\nx1\\nx2\\n· · ·\\nxd\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx⊤\\n1\\nx⊤\\n2\\n...\\nx⊤\\nd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n将列向量xi = (xi1; xi2; . . . ; xid) ∈Rd×1 代入:\\nXX⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\n· · ·\\nxm1\\nx12\\nx22\\n· · ·\\nxm2\\n...\\n...\\n...\\n...\\nx1d\\nx2d\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n•\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx12\\n· · ·\\nx1d\\nx21\\nx22\\n· · ·\\nx2d\\n...\\n...\\n...\\n...\\nxm1\\nxm2\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nm×d\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m×d\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n...\\n...\\n...\\n...\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n综合Pm\\ni=1 xix⊤\\ni 和XX⊤的化简结果, 即Pm\\ni=1 xix⊤\\ni = XX⊤(协方差矩阵)。根据刚刚推导得到的结\\n论, 式(10.14) 最后的结果即可化为式(10.15) 的目标函数:\\ntr\\n \\nW⊤\\n m\\nX\\ni=1\\nxix⊤\\ni\\n!\\nW\\n!\\n= tr\\n\\x00W⊤XX⊤W\\n\\x01\\n式(10.15) 描述的优化问题的求解详见式(10.17) 最后的解释。\\n10.5.2\\n式(10.16) 的解释\\n先说什么是方差：\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于包含n 个样本的一组数据X = {x1, x2, . . . , xn} 来说, 均值M 为\\nM = x1 + x2 + . . . + xn\\nn\\n=\\nn\\nX\\ni=1\\nxi\\n则方差σ2\\nX 公式为\\nσ2 = (x1 −M)2 + (x2 −M)2 + . . . + (xn −M)2\\nn\\n= 1\\nn\\nn\\nX\\ni=1\\n(xi −M)2\\n方差衡量了该组数据偏离均值的程度; 样本越分散, 其方差越大。\\n再说什么是协方差:\\n若还有包含n 个样本的另一组数据X′ = {x′\\n1, x′\\n2, . . . , x′\\nn}, 均值为M ′, 则下式\\nσ2\\nXX′ = (x1 −M) (x′\\n1 −M ′) + (x2 −M) (x′\\n2 −M ′) + . . . + (xn −M) (x′\\nn −M ′)\\nn\\n= 1\\nn\\nn\\nX\\ni=1\\n(xi −M) (x′\\ni −M ′)\\n称为两组数据的协方差。σ2\\nXX′ 能说明第一组数据x1, x2, . . . , xn 和第二组数据x′\\n1, x′\\n2, . . . , x′'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1, x′\\n2, . . . , x′\\nn 的变\\n化情况。具体来说, 如果两组数据总是同时大于或小于自己的均值, 则(xi −M) (x′\\ni −M ′) > 0, 此时\\nσ2\\nXX′ > 0; 如果两组数据总是一个大于(或小于) 自己的均值而别一个小于(或大于) 自己的均值, 则\\n(xi −M) (x′\\ni −M ′) < 0, 此时σ2\\nXX′ < 0; 如果两组数据与自己的均值的大小关系无规律, 则(xi −M) (x′\\ni −M ′)\\n的正负号随机变化, 其平均数σ2\\nXX, 则会趋近于0 。引用百度百科协方差词条原话: “从直观上来看, 协方\\n差表示的是两个变量总体误差的期望。如果两个变量的变化趋势一致, 也就是说如果其中一个大于自身的\\n期望值时另外一个也大于自身的期望值, 那么两个变量之间的协方差就是正值; 如果两个变量的变化趋势\\n相反, 即其中一个变量大于自身的期望值时另外一个却小于自身的期望值, 那么两个变量之间的协方差就\\n是负值。如果两个变量是统计独立的, 那么二者之间的协方差就是0 , 但是, 反过来并不成立。协方差为0\\n的两个随机变量称为是不相关的。”'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的两个随机变量称为是不相关的。”\\n最后说什么是协方差矩阵：\\n结合本书中的符号:\\nX = (x1, x2, . . . , xm) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx11\\nx21\\n· · ·\\nxm1\\nx12\\nx22\\n· · ·\\nxm2\\n...\\n...\\n...\\n...\\nx1d\\nx2d\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n矩阵X 每一行表示一维特征, 每一列表示该数据集的一个样本; 而本节开始已假定数据样本进行了中心化,\\n即Pm\\ni=1 xi = 0 ∈Rd×1 (中心化过程可通过X\\n\\x00I −1\\nm11⊤) 实现, 其中I ∈Rm×m 为单位阵, 1 ∈Rm×1 为\\n全1 列向量, 参见习题10.3), 即上式矩阵的每一行平均值等于零(其实就是分别对所有xi 的每一维坐标\\n进行中心化, 而不是分别对单个样本xi 中心化）对于包含d 个特征的特征空间（或称d 维特征空间）来\\n说, 每一维特征可以看成是一个随机变量, 而X 中包含m 个样本, 也就是说每个随机变量有m 个数据, 根\\n据前面XX⊤的矩阵表达形式:\\n1\\nmXX⊤= 1\\nm\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1\\nmXX⊤= 1\\nm\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n...\\n...\\n...\\n...\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×d\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n根据前面的结果知道\\n1\\nmXX⊤的第i 行第j 列的元素表示X 中第i 行和X⊤第j 列（即X 中第j 行）\\n的方差(i = j) 或协方差(i ̸= j) 。注意: 协方差矩阵对角线元素为各行的方差。\\n接下来正式解释式(10.16): 对于X = (x1, x2, . . . , xm) ∈Rd×m, 将其投影为Z = (z1, z2, . . . , zm) ∈\\nRd′×m, 最大可分性出发, 我们希望在新空间的每一维坐标轴上样本都尽可能分散(即每维特征尽可能分\\n散, 也就是Z 各行方差最大; 参见图10.4 所示, 原空间只有两维坐标, 现考虑降至一维, 希望在新坐标系\\n下样本尽可能分散, 图中画出了一种映射后的坐标系, 显然橘红色坐标方向样本更分散, 方差更大), 即寻\\n找W ∈Rd×d′ 使协方差矩阵\\n1\\nmZZ⊤对角线元素之和(矩阵的迹）最大（即使Z 各行方差之和最大), 由\\n于Z = W⊤X, 而常系数\\n1\\nm 在最大化时并不发生影响, 求矩阵对角线元素之和即为矩阵的迹, 综上即得式\\n(10.16)。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1\\nm 在最大化时并不发生影响, 求矩阵对角线元素之和即为矩阵的迹, 综上即得式\\n(10.16)。\\n另外, 中心化后X 的各行均值为零, 变换后Z = W⊤X 的各行均值仍为零, 这是因为Z 的第i 行\\n(1 ⩽i ⩽d′) 为\\n\\x08\\nw⊤\\ni x1, w⊤\\ni x2, . . . , w⊤\\ni xm\\n\\t\\n, 该行之和w⊤\\ni\\nPm\\nj=1 xj = w⊤\\ni 0 = 0 。\\n最后, 有关方差的公式, 有人认为应该除以样本数量m, 有人认为应该除以样本数量减1 即m −1 。\\n简单来说, 根据总体样本集求方差就除以总体样本数量, 而根据抽样样本集求方差就除以抽样样本集数量\\n减1; 总体样本集是真正想调查的对象集合, 而抽样样本集是从总体样本集中被选出来的部分样本组成的\\n集合, 用来估计总体样本集的方差; 一般来说, 总体样本集是不可得的, 我们拿到的都是抽样样本集。严格\\n上来说，样本方差应该除以n −1 才会得到总体样本的无偏估计, 若除以n 则得到的是有偏估计。\\n式(10.16) 描述的优化问题的求解详见式(10.17) 最后的解释。\\n10.5.3\\n式(10.17) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='10.5.3\\n式(10.17) 的推导\\n由式（10.15）可知，主成分分析的优化目标为\\nmin\\nW\\n−tr (WTXXTW)\\ns.t.\\nWTW = I\\n其中，X = (x1, x2, . . . , xm) ∈Rd×m, W = (w1, w2, . . . , wd′) ∈Rd×d′，I ∈Rd′×d′ 为单位矩阵。对于带矩\\n阵约束的优化问题，根据[1] 中讲述的方法可得此优化目标的拉格朗日函数为\\nL(W, Θ) = −tr (WTXXTW) + ⟨Θ, WTW −I⟩\\n= −tr (WTXXTW) + tr\\n\\x00ΘT(WTW −I)\\n\\x01\\n其中，Θ ∈Rd′×d′ 为拉格朗日乘子矩阵，其维度恒等于约束条件的维度，且其中的每个元素均为未知的\\n拉格朗日乘子，⟨Θ, WTW−I⟩= tr\\n\\x00ΘT(WTW −I)\\n\\x01\\n为矩阵的内积[2]。若此时仅考虑约束wT\\ni wi = 1(i =\\n1, 2, ..., d′)，则拉格朗日乘子矩阵Θ 此时为对角矩阵，令新的拉格朗日乘子矩阵为Λ = diag(λ1, λ2, ..., λd′) ∈\\nRd′×d′，则新的拉格朗日函数为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Rd′×d′，则新的拉格朗日函数为\\nL(W, Λ) = −tr (WTXXTW) + tr\\n\\x00ΛT(WTW −I)\\n\\x01\\n对拉格朗日函数关于W 求导可得\\n∂L(W, Λ)\\n∂W\\n=\\n∂\\n∂W\\n\\x02\\n−tr (WTXXTW) + tr\\n\\x00ΛT(WTW −I)\\n\\x01\\x03\\n= −\\n∂\\n∂W tr (WTXXTW) +\\n∂\\n∂W tr\\n\\x00ΛT(WTW −I)\\n\\x01\\n由矩阵微分公式\\n∂\\n∂X tr (XTBX) = BX + BTX,\\n∂\\n∂X tr\\n\\x00BXTX\\n\\x01\\n= XBT + XB 可得\\n∂L(W, Λ)\\n∂W\\n= −2XXTW + WΛ + WΛT\\n= −2XXTW + W(Λ + ΛT)\\n= −2XXTW + 2WΛ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n令\\n∂L(W, Λ)\\n∂W\\n= 0 可得\\n−2XXTW + 2WΛ = 0\\nXXTW = WΛ\\n将W 和Λ 展开可得\\nXXTwi = λiwi,\\ni = 1, 2, ..., d′\\n显然，此式为矩阵特征值和特征向量的定义式，其中λi, wi 分别表示矩阵XXT 的特征值和单位特征\\n向量。由于以上是仅考虑约束wT\\ni wi = 1 所求得的结果，而wi 还需满足约束wT\\ni wj = 0(i ̸= j)。观察\\nXXT 的定义可知，XXT 是一个实对称矩阵，实对称矩阵的不同特征值所对应的特征向量之间相互正交，\\n同一特征值的不同特征向量可以通过施密特正交化使其变得正交，所以通过上式求得的wi 可以同时满足\\n约束wT\\ni wi = 1, wT\\ni wj = 0(i ̸= j)。根据拉格朗日乘子法的原理可知，此时求得的结果仅是最优解的必要\\n条件，而且XXT 有d 个相互正交的单位特征向量，所以还需要从这d 个特征向量里找出d′ 个能使得目\\n标函数达到最优值的特征向量作为最优解。将XXTwi = λiwi 代入目标函数可得'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='标函数达到最优值的特征向量作为最优解。将XXTwi = λiwi 代入目标函数可得\\nmin\\nW −tr (WTXXTW) = max\\nW\\ntr (WTXXTW)\\n= max\\nW\\nd′\\nX\\ni=1\\nwT\\ni XXTwi\\n= max\\nW\\nd′\\nX\\ni=1\\nwT\\ni · λiwi\\n= max\\nW\\nd′\\nX\\ni=1\\nλiwT\\ni wi\\n= max\\nW\\nd′\\nX\\ni=1\\nλi\\n显然，此时只需要令λ1, λ2, ..., λd′ 和w1, w2, . . . , wd′ 分别为矩阵XXT 的前d′ 个最大的特征值和单\\n位特征向量就能使得目标函数达到最优值。\\n10.5.4\\n根据式(10.17) 求解式(10.16)\\n注意式(10.16) 中W ∈Rd×d′, 只有d′ 列, 而式(10.17) 可以得到d 列, 如何根据式(10.17) 求解式\\n(10.16) 呢? 对XX⊤W = WΛ 两边同乘W⊤, 得\\nW⊤XX⊤W = W⊤WΛ = Λ\\n注意使用了约束条件W⊤W = I; 上式左边与式(10.16) 的优化目标对应矩阵相同, 而右边Λ ∈Rd′×d′'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='是由XXX⊤的d′ 个特征值组成的对角阵, 两边同时取矩阵的迹, 得\\ntr\\n\\x00W⊤XX⊤W\\n\\x01\\n= tr(Λ) =\\nd′\\nX\\ni=1\\nλi\\nd 个特征值, 因此当然是取出最大的前d′ 个特征值, 而W 即特征值对应的标准化特征向量组成的矩阵。\\n特别注意, 图10.5 只是得到了投影矩阵W, 而降维后的样本为Z = W⊤X。\\n10.6\\n核化线性降维\\n注意, 本节符号在第14 次印刷中进行了修订, 另外有一点需要注意的是，在上一节中用zi 表示xi 降\\n维后的像, 而本节用zi 表示xi 在高维特征空间中的像。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n本节推导实际上有一个前提, 以式(10.19) 为例（式(10.21) 仅将zi 换为ϕ (xi) 而已), 那就是zi 已\\n经中心化(计算方差要用样本减去均值, 式(10.19) 是均值为零时特殊形式, 详见式(10.16) 的解释), 但\\nzi = ϕ (xi) 是xi 高维特征空间中的像, 即使xi 已进行中心化, 但zi 却不一定是中心化的, 此时本节推导\\n均不再成立。推广工作详见KPCA[3] 的附录A。\\n10.6.1\\n式(10.19) 的解释\\n首先, 类似于式(10.14) 的推导后半部分内容可知Pm\\ni=1 ziz⊤\\ni = ZZ⊤, 其中Z 的每一列为一个样本, 设\\n高维空间的维度为h, 则Z ∈Rh×m, 其中m 为数据集样本数量。\\n其次, 式(10.19) 中的W 为从高维空间降至低维(维度为d ) 后的正交基, 在第14 次印刷中加入表述\\nW = (w1, w2, . . . , wd), 其中W ∈Rh×d, 降维过程为X = W⊤Z 。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='最后, 式(10.19) 类似于式(10.17), 是为了求解降维投影矩阵W = (w1, w2, . . . , wd) 。但问题在于\\nZZ⊤∈Rh×h, 当维度h 很大时(注意本节为核化线性降维, 第六章核方法中高斯核会把样本映射至无穷\\n维), 此时根本无法求解Z⊤的特征值和特征向量。因此才有了后面的式(10.20)。\\n第14 次印刷及之后印次, 式(10.19) 为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01\\nwj = λjwj, 而在之前的印次中表达有误, 实际应\\n该为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01\\nW = WΛ, 类似于式(10.17)。而这两种表达本质相同, λjwj 为WΛ 的第j 列, 仅此而\\n已。\\n10.6.2\\n式(10.20) 的解释\\n本节为核化线性降维, 而式(10.19) 是在维度为h 的高维空间运算, 式(10.20) 变形（咋一看似乎有点\\n无厘头) 的目的是为了避免直接在高维空间运算, 即想办法能够使用式(6.22) 的核技巧, 也就是后面的式\\n(10.24)。\\n第14 次印刷及之后印次该式没问题, 之前的式(10.20) 应该是:\\nW =\\n m\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='第14 次印刷及之后印次该式没问题, 之前的式(10.20) 应该是:\\nW =\\n m\\nX\\ni=1\\nziz⊤\\ni\\n!\\nWΛ−1 =\\nm\\nX\\ni=1\\n\\x00zi\\n\\x00z⊤\\ni WΛ−1\\x01\\x01\\n=\\nm\\nX\\ni=1\\n(ziαi)\\n其中αi = z⊤\\ni WΛ−1 ∈R1×d, z⊤\\ni ∈R1×h, W ∈Rh×d, Λ ∈Rd×d 为对角阵。这个结果看似等号右侧也包含\\nW, 但将此式代入式(10.19) 后经化简可避免在高维空间的运算, 而将目标转化为求低维空间的αi ∈R1×d,\\n详见式(10.24) 的推导。\\n10.6.3\\n式(10.21) 的解释\\n该式即为将式(10.19) 中的zi 换为ϕ (xi) 的结果。\\n10.6.4\\n式(10.22) 的解释\\n该式即为将式(10.20) 中的zi 换为ϕ (xi) 的结果。\\n10.6.5\\n式(10.24) 的推导\\n已知zi = ϕ(xi)，类比X = {x1, x2, ..., xm} 可以构造Z = {z1, z2, ..., zm}，所以公式(10.21) 可变换\\n为\\n m\\nX\\ni=1\\nϕ(xi)ϕ(xi)T\\n!\\nwj =\\n m\\nX\\ni=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='为\\n m\\nX\\ni=1\\nϕ(xi)ϕ(xi)T\\n!\\nwj =\\n m\\nX\\ni=1\\nzizT\\ni\\n!\\nwj = ZZTwj = λjwj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n又由公式(10.22) 可知\\nwj =\\nm\\nX\\ni=1\\nϕ (xi) αj\\ni =\\nm\\nX\\ni=1\\nziαj\\ni = Zαj\\n其中，αj = (αj\\n1; αj\\n2; ...; αj\\nm) ∈Rm×1。所以公式(10.21) 可以进一步变换为\\nZZTZαj = λjZαj\\nZZTZαj = Zλjαj\\n由于此时的目标是要求出wj，也就等价于要求出满足上式的αj，显然，此时满足ZTZαj = λjαj 的αj\\n一定满足上式，所以问题转化为了求解满足下式的αj：\\nZTZαj = λjαj\\n令ZTZ = K，那么上式可化为\\nKαj = λjαj\\n此式即为公式(10.24)，其中矩阵K 的第i 行第j 列的元素(K)ij = zT\\ni zj = ϕ(xi)Tϕ(xj) = κ (xi, xj)\\n10.6.6\\n式(10.25) 的解释\\n式(10.25) 仅需将第14 次印刷中式(10.22) 的wj 表达式转置后代入即可。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='式(10.25) 仅需将第14 次印刷中式(10.22) 的wj 表达式转置后代入即可。\\n该式的意义在于, 求解新样本x ∈Rd×1 映射至高维空间ϕ(x) ∈Rh×1 后再降至低维空高维空间Rh×1\\n的运算。但是由于此处没有类似第6 章支持向量的概念, 可以发现式(10.25) 计算时需要对所有样本求和,\\n因此它的计算开销比较大。\\n注意, 此处书中符号使用略有混乱, 因为在式(10.19) 中zi 表示xi 在高维特征空间中的像, 而此处又\\n用zj 表示新样本x 映射为ϕ(x) 后再降维至Rd′×1 空间时的第j 维坐标。\\n10.7\\n流形学习\\n不要被“流形学习”的名字所欺骗, 本节开篇就明确说了, 它是一类借鉴了拓扑流形概念的降维方法而\\n已, 因此称为“流形学习”。10.2 节MDS 算法的降维准则是要求原始空间中样本之间的距离在低维空间中\\n得以保持, 10.3 节PCA 算法的降维准则是要求低维子空间对样本具有最大可分性, 因为它们都是基于线\\n性变换来进行降维的方法（参见式(10.13)，故称为线性降维方法。\\n10.7.1\\n等度量映射(Isomap) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='10.7.1\\n等度量映射(Isomap) 的解释\\n如图“西瓜书”10.8 所示, Isomap 算法与MDS 算法的区别仅在于距离矩阵D ∈Rm×m 的计算方法\\n不同。在MDS 算法中, 距离矩阵D ∈Rm×m 即为普通的样本之间欧氏距离; 而本节的Isomap 算法中, 距\\n离矩阵D ∈Rm×m 由“西瓜书”图10.8 的Step1 ~Step5 生成, 即遵循流形假设。当然, 对新样本降维时\\n也有不同, 这在“西瓜书”图10.8 下的一段话中已阐明。\\n另外解释一下测地线距离, 欧氏距离即两点之间的直线距离, 而测地线距离是实际中可以到达的路径,\\n如“西瓜书”图10.7(a) 中黑线(欧氏距离) 和红线(测地线距离)。\\n10.7.2\\n式(10.28) 的推导\\nwij =\\nP\\nk∈Qi\\nC−1\\njk\\nP\\nl,s∈Qi\\nC−1\\nls\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由书中上下文可知，式(10.28) 是如下优化问题的解。\\nmin\\nw1,w2,...,wm\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\rxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\r\\r\\r\\r\\n2\\n2\\ns.t.\\nX\\nj∈Qi\\nwij = 1\\n若令xi ∈Rd×1, Qi = {q1\\ni , q2\\ni , ..., qn\\ni }，则上述优化问题的目标函数可以进行如下恒等变形\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\rxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\r\\r\\r\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\r\\nX\\nj∈Qi\\nwijxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\r\\r\\r\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\r\\r\\r\\r\\r\\nX\\nj∈Qi\\nwij(xi −xj)\\n\\r\\r\\r\\r\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n∥Xiwi∥2\\n2\\n=\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi\\n其中wi = (wiq1\\ni , wiq2\\ni , ..., wiqn\\ni ) ∈Rn×1，Xi =\\n\\x00xi −xq1\\ni , xi −xq2\\ni , ..., xi −xqn\\ni\\n\\x01\\n∈Rd×n。同理，约束条件'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i , xi −xq2\\ni , ..., xi −xqn\\ni\\n\\x01\\n∈Rd×n。同理，约束条件\\n也可以进行如下恒等变形\\nX\\nj∈Qi\\nwij = wi\\nTI = 1\\n其中I = (1, 1, ..., 1) ∈Rn×1 为n 行1 列的元素值全为1 的向量。因此，上述优化问题可以重写为\\nmin\\nw1,w2,...,wm\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi\\ns.t. wi\\nTI = 1\\n显然，此问题为带约束的优化问题，因此可以考虑使用拉格朗日乘子法来进行求解。由拉格朗日乘子法可\\n得此优化问题的拉格朗日函数为\\nL(w1, w2, . . . , wm, λ) =\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi + λ\\n\\x00wi\\nTI −1\\n\\x01\\n对拉格朗日函数关于wi 求偏导并令其等于0 可得\\n∂L(w1, w2, . . . , wm, λ)\\n∂wi\\n=\\n∂\\n\\x02Pm\\ni=1 wiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n=\\n∂\\n\\x02\\nwiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n又由矩阵微分公式\\n∂xT Bx\\n∂x\\n='),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n又由矩阵微分公式\\n∂xT Bx\\n∂x\\n=\\n\\x00B + BT\\x01\\nx,\\n∂xT a\\n∂x\\n= a 可得\\n∂\\n\\x02\\nwiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 2XT\\ni Xiwi + λI = 0\\nXT\\ni Xiwi = −1\\n2λI\\n若XT\\ni Xi 可逆，则\\nwi = −1\\n2λ(XT\\ni Xi)−1I\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n又因为wiTI = ITwi = 1，则上式两边同时左乘IT 可得\\nITwi = −1\\n2λIT(XT\\ni Xi)−1I = 1\\n−1\\n2λ =\\n1\\nIT(XT\\ni Xi)−1I\\n将其代回wi = −1\\n2λ(XT\\ni Xi)−1I 即可解得\\nwi =\\n(XT\\ni Xi)−1I\\nIT(XT\\ni Xi)−1I\\n若令矩阵(XT\\ni Xi)−1 第j 行第k 列的元素为C−1\\njk ，则\\nwij = wiqj\\ni =\\nP\\nk∈Qi\\nC−1\\njk\\nP\\nl,s∈Qi\\nC−1\\nls\\n此即为公式(10.28)。显然，若XT\\ni Xi 可逆，此优化问题即为凸优化问题，且此时用拉格朗日乘子法求得的\\nwi 为全局最优解。\\n10.7.3\\n式(10.31) 的推导\\n以下推导需要使用预备知识中的10.2节: 矩阵的F 范数与迹。\\n观察式(10.29), 求和号内实际是一个列向量的2 范数平方, 令vi = zi −P\\nj∈Qi wijzj, vi 的维度与zi\\n相同, vi ∈Rd′×1, 则式(10.29) 可重写为\\nmin'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='相同, vi ∈Rd′×1, 则式(10.29) 可重写为\\nmin\\nz1,z2,...,zm\\nm\\nX\\ni=1\\n∥vi∥2\\n2\\ns.t. vi = zi −\\nX\\nj∈Qi\\nwijzj, i = 1, 2, . . . , m\\n令Z = (z1, z2, . . . , zi, . . . , zm) ∈Rd′×m, Ii = (0; 0; . . . ; 1; . . . ; 0) ∈Rm×1, 即Ii 为m × 1 的列向量, 除\\n第i 个元素等于1 之外其余元素均为零, 则\\nzi = ZIi\\n令(W)ij = wij （P237 页第1 行), 即W = (w1, w2, . . . , wi, . . . , wm)⊤∈Rm×m, 也就是说W 的第i 行的\\n转置（没错, 就是第i 行) 对应第i 个样数wi （这里符号之所以别扭是因为wij 已用来表示列向量wi 的\\n第j 个元素, 但为了与习惯保持一致即wij 表示W 的第i 行第j 列元素, 只能忍忍, 此处暂时别扭着），即\\nW = (w1, w2, . . . , wi, . . . , wm)⊤=\\n\\uf8ee'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='W = (w1, w2, . . . , wi, . . . , wm)⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nw11\\nw21\\n· · ·\\nwi1\\n· · ·\\nwm1\\nw12\\nw22\\n· · ·\\nwi2\\n· · ·\\nwm2\\n...\\n...\\n...\\n...\\n...\\n...\\nw1j\\nw2j\\n· · ·\\nwij\\n· · ·\\nwmj\\n...\\n...\\n...\\n...\\n...\\n...\\nw1m\\nw2m\\n· · ·\\nwim\\n· · ·\\nwmm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n对于wi ∈Rm×1 来说, 只有xi 的K 个近邻样本对应的下标对应的wij ̸= 0, j ∈Qi, 且它们的和等于\\n1 , 则\\nX\\nj∈Qi\\nwijzj = Zwi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n因此\\nvi = zi −\\nX\\nj∈Qi\\nwijzj = ZIi −Zwi = Z (Ii −wi)\\n令V = (v1, v2, . . . , vi, . . . , vm) ∈Rd′×m, I = (I1, I2, . . . , Ii, . . . , Im) ∈Rm×m, 则\\nV = Z\\n\\x00I −W⊤\\x01\\n= Z\\n\\x00I⊤−W⊤\\x01\\n= Z(I −W)⊤\\n根据前面的预备知识, 并将上式V 和式(10.30) 代入, 得式(10.31) 目标函数:\\nm\\nX\\ni=1\\n∥vi∥2\\n2 = ∥V∥2\\nF\\n= tr\\n\\x00VV⊤\\x01\\n= tr\\n\\x10\\x00Z(I −W)⊤\\x01 \\x00Z(I −W)⊤\\x01⊤\\x11\\n= tr\\n\\x00Z(I −W)⊤(I −W)Z⊤\\x01\\n= tr\\n\\x00ZMZZ⊤\\x01\\n接下来求解式(10.31)。\\n参考式(10.17) 的推导, 应用拉格朗日乘子法, 先写出拉格朗日函数\\nL(Z, Λ) = tr\\n\\x00ZMZ⊤\\x01\\n+\\n\\x00ZZ⊤−I\\n\\x01\\nΛ\\n令P = Z⊤(否则有点别扭), 则拉格朗日函数变为\\nL(P, Λ) = tr\\n\\x00P⊤MP\\n\\x01\\n+'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='令P = Z⊤(否则有点别扭), 则拉格朗日函数变为\\nL(P, Λ) = tr\\n\\x00P⊤MP\\n\\x01\\n+\\n\\x00P⊤P −I\\n\\x01\\n\\uffff\\n求导并令导数等于0 :\\n∂L(P, Λ)\\n∂P\\n= ∂tr\\n\\x00P⊤MP\\n\\x01\\n∂P\\n+ ∂\\n\\x00P⊤P −I\\n\\x01\\n∂P\\nΛ\\n= 2MP −2PΛ = 0\\n特征值对角阵; 然后两边再同时左乘P⊤并取矩阵的迹, 注意P⊤P = I ∈Rd′×d′, 得tr\\n\\x00P⊤MP\\n\\x01\\n=\\ntr\\n\\x00P⊤PΛ\\n\\x01\\n= tr(Λ) 因此, P = Z⊤是由M ∈Rm×m 最小的d′ 个特征值对应的特征向量组成的矩阵。\\n10.8\\n度量学习\\n回忆10.5.1 节的Isomap 算法相比与10.2 节的MDS 算法的区别在于距离矩阵的计算方法不同，\\nIsomap 算法在计算样本间距离时使用的（近似）测地线距离，而MDS 算法使用的是欧氏距离，也就是说\\n二者的距离度量不同。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.8.1\\n式(10.34) 的解释\\n为了推导方便, 令u = (u1; u2; . . . ; ud) = xi −xj ∈Rd×1, 其中uk = xik −xjk, 则式(10.34) 重写为\\nu⊤Mu = ∥u∥2\\nM, 其中M ∈Rd×d, 具体到元素级别的表达:\\nu⊤Mu =\\nh\\nu1\\nu2\\n. . .\\nud\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nm11\\nm12\\n. . .\\nm1d\\nm21\\nm22\\n. . .\\nm2d\\n...\\n...\\n...\\n...\\nmd1\\nmd2\\n. . .\\nmdd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nu1\\nu2\\n...\\nud\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\nh\\nu1\\nu2\\n. . .\\nud\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nu1m11 + u2m12 + . . . + udm1d\\nu1m21 + u2m22 + . . . + udm2d\\n...\\nu1md1 + u2md2 + . . . + udmdd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= u1u1m11 + u1u2m12 + . . . + u1udm1d'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= u1u1m11 + u1u2m12 + . . . + u1udm1d\\n+ u2u1m21 + u2u2m22 + . . . + u2udm2d\\n. . .\\n+ udu1md1 + udu2md2 + . . . + ududmdd\\n注意, 对应到本式符号, 式(10.33) 的结果即为上面最后一个等式的对角线部分, 即\\nu1u1m11 + u2u2m22 + . . . + ududmdd\\n而式(10.32) 的结果则要更进一步, 去除对角线部分中的权重mii(1 ⩽i ⩽d) 部分, 即\\nu1u1 + u2u2 + . . . + udud\\n对比以上三个结果, 即式(10.32) 的平方欧氏距离, 式(10.33) 的加权平方欧氏距离, 式(10.34) 的马氏距\\n离, 可以细细体会度量矩阵究竟带来了什么。\\n因此, 所谓“度量学习”, 即将系统中的平方欧氏距离换为式(10.34) 的马氏距离, 通过优化某个目标函\\n数, 得到最恰当的度量矩阵M （新的距离度量计算方法）的过程。书中在式(10.34) (10.38) 介绍的NCA'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='即为一个具体的例子, 可以从中品味“度量学习”的本质。\\n对于度量矩阵M 要求半正定, 文中提到必有正交基P 使得M 能写为M = PP⊤, 此时马氏距离\\nu⊤Mu = u⊤PP⊤u =\\n\\r\\rP⊤u\\n\\r\\r2\\n2 。\\n10.8.2\\n式(10.35) 的解释\\n这就是一种定义而已，没什么别的意思。传统近邻分类器使用多数投票法，有投票权的样本为xi 最近\\n的K 个近邻, 即KNN; 但也可以将投票范围扩大到整个样本集, 但每个样本的投票权重不一样，距离xi 越\\n近的样本投票权重越大，例如可取为第5 章式(5.19) 当βi = 1 时的高斯径向基函数exp\\n\\x10\\n−∥xi −xj∥2\\x11\\n。从式中可以看出, 若xj 与xi 重合, 则投票权重为1 , 距离越大该值越小。式(10.35) 的分母是对所有投\\n票值规一化至[0, 1] 范围, 使之为概率。\\n可能会有疑问：式(10.35) 分母求和变量l 是否应该包含xi 的下标即l = i ? 其实无所谓, 进一步说\\n其实是否进行规一化也无所谓, 熟悉KNN 的话就知道, 在预测时是比较各类投票数的相对大小, 各类样本'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='对xi 的投票权重的分母在式(10.35) 中相同, 因此不影响相对大小。\\n注意啊, 这里有计算投票权重时用到了距离度量, 所以可以进一步将其换为马氏距离, 通过优化某个目\\n标(如式(10.38)）得到最优的度量矩阵M。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n10.8.3\\n式(10.36) 的解释\\n先简单解释留一法(LOO), KNN 是选出样本xi 的在样本集中最近的K 个近邻, 而现在将范围扩大,\\n使用样本集中的所有样本进行投票, 每个样本的投票权重为式(10.35), 将各类样本的投票权重分别求和,\\n注意xi 自己的类别肯定与自己相同（现在是训练阶段, 还没到对末见样本的预测阶段, 训练集样本的类别\\n信息均已知), 但自己不能为自己投票吧, 所以要将自己除外, 即留一法。\\n假设训练集共有N 个类别, Ωn 表示第n 类样本的下标集合(1 ⩽n ⩽N), 对于样本xi 来说, 可以分\\n别计算N 个概率:\\npxi\\nn =\\nX\\nj∈Ωn\\npij, 1 ⩽n ⩽N\\n注意, 若样本xi 的类别为n∗, 则在根据上式计算pxi\\nn∗时要将xi 的下标去除, 即刚刚解释的留一法(自己\\n不能为自己投票)。pxi\\nn∗即为训练集将样本xi 预测为第n∗类的概率, 若pxi\\nn∗在所有的pxi\\nn (1 ⩽n ⩽N) 中\\n最大, 则预测正确, 反之预测错误。\\n其中pxi'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='n∗在所有的pxi\\nn (1 ⩽n ⩽N) 中\\n最大, 则预测正确, 反之预测错误。\\n其中pxi\\nn∗即为式(10.36)。\\n10.8.4\\n式(10.37) 的解释\\n换为刚才式(10.36) 的符号, 式(10.37) 即为Pm\\ni=1 pxi\\nn∗, 也就是所有训练样本被训练集预测正确的概率\\n之和。我们当然希望这个概率和最大, 但若采用平方欧氏距离时, 对于某个训练集来说这个概率和是固定\\n的; 但若采用了马氏距离, 这个概率和与度量矩阵M 有关。\\n10.8.5\\n式(10.38) 的解释\\n刚才式(10.37) 中提到希望寻找一个度量矩阵M 使训练样本被训练集预测正确的概率之和最大, 即\\nmaxM\\nPm\\ni=1 pxi\\nn∗, 但优化问题习惯是最小化, 所以改为minM −Pm\\ni=1 pxi\\nn∗即可, 而式(10.38) 目标函数中的\\n常数1 并不影响优化结果, 有没有无所谓的。\\n式(10.38) 中有关将M = PP⊤代入的形式参见前面式(10.34) 的解释最后一段。\\n10.8.6\\n式(10.39) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='10.8.6\\n式(10.39) 的解释\\n式(10.39) 是本节第二个“度量学习”的具体例子。优化目标函数是要求必连约束集合M 中的样本\\n对之间的距离之和尽可能的小，而约束条件则是要求勿连约束集合C 中的样本对之间的距离之和大于1 。\\n这里的“1”应该类似于第6 章SVM 中间隔大于“1”, 纯属约定, 没有推导。\\n参考文献\\n[1] Michael Grant. Lagrangian optimization with matrix constrains, 2015.\\n[2] Wikipedia contributors. Frobenius inner product, 2020.\\n[3] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Kernel principal component analy-\\nsis. In Artificial Neural Networks—ICANN’97: 7th International Conference Lausanne, Switzerland,'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='October 8–10, 1997 Proceeedings, pages 583–588. Springer, 2005.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第11 章\\n特征选择与稀疏学习\\n11.1\\n子集搜索与评价\\n开篇给出了“特征选择”的概念, 并谈到特征选择与第10 章的降维有相似的动机。特征选择与降维的\\n区别在于特征选择是从所有特征中简单地选出相关特征, 选择出来的特征就是原来的特征; 降维则对原来\\n的特征进行了映射变换, 降维后的特征均不再是原来的特征。\\n本节涉及“子集评价”的式(14.1) 和式(14.2) 与第4 章的式(4.2) 和式(4.1) 相同, 这是因为“决策\\n树算法在构建树的同时也可看作进行了特征选择”(参见“11.7 阅读材料”)。接下来在11.2 节、11.3 节、\\n11.4 节分别介绍的三类特征选择方法: 过滤式(filter)、包裹式(wrapper) 和嵌入式(embedding)。\\n11.1.1\\n式(11.1) 的解释\\n此为信息熵的定义式，其中pk, k = 1, 2, . . . |Y| 表示D 中第i 类样本所占的比例。可以看出，样本越'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='纯，即pk →0 或pk →1 时，Ent(D) 越小，其最小值为0（约定0 log2 0 = 0）。\\n11.1.2\\n式(11.2) 的解释\\nEnt(D) = −\\n|Y|\\nX\\ni=1\\npk log2 pk\\n此为信息熵的定义式，其中pk, k = 1, 2, . . . |Y| 表示D 中第i 类样本所占的比例。可以看出，样本越纯，\\n即pk →0 或pk →1 时，Ent(D) 越小，其最小值为0。此时必有pi = 1, p\\\\i = 0, i = 1, 2, . . . , |Y|。\\n11.2\\n过滤式选择\\n“过滤式方法先对数据集进行特征选择, 然后再训练学习器, 特征选择过程与后续学习器无关。这相当\\n于先用特征选择过程对初始特征进行’ 过滤’, 再用过滤后的特征来训练模型。”, 这是本节开篇第一段原话,\\n之所以重写于此, 是因为这段话里包含了“过滤”的概念, 该概念并非仅针对特征选择, 那些所有先对数据\\n集进行某些预处理, 然后基于预处理结果再训练学习器的方法(预处理过程独立于训练学习器过程) 均可'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='集进行某些预处理, 然后基于预处理结果再训练学习器的方法(预处理过程独立于训练学习器过程) 均可\\n以称之为“过滤式算法”。特别地, 本节介绍的Relief 方法只是过滤式特征选择方法的其中一种而已。\\n从式(11.3) 可以看出, Relief 方法本质上基于“空间上相近的样本具有相近的类别标记”假设。Relief\\n基于样本与同类和异类的最近邻之间的距离来计算相关统计量δj, 越是满足前提假设, 原则上样本与同类\\n最近邻之间的距离diff\\n\\x00xj\\ni, xj\\ni,nh\\n\\x012 会越小, 样本与异类最近邻之间的距离diff\\n\\x00xj\\ni, xj\\ni, nm\\n\\x012 会越大，因此相\\n关统计量δj 越大，对应属性的分类能力就越强。\\n对于能处理多分类问题的扩展变量Relief-F, 由于有多个异类, 因此对所有异类最近邻进行加权平均,\\n各异类的权重为其在数据集中所占的比例。\\n11.2.1\\n包裹式选择\\n“与过滤式特征选择不考虑后续学习器不同, 包裹式特征选择直接把最终将要使用的学习器的性能作\\n为特征子集的评价准则。换言之, 包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='为特征子集的评价准则。换言之, 包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定\\n做’ 的特征子集。”, 这是本节开篇第一段原话, 之所以重写于此, 是因为这段话里包含了“包裹”的概念,\\n该概念并非仅针对特征选择, 那些所有基于学习器的性能作为评价准则对数据集进行预处理的方法(预处\\n理过程依赖训练所得学习器的测试性能）均可以称之为“包裹式算法”。特别地, 本节介绍的LVW 方法只\\n是包裹式特征选择方法的其中一种而已。\\n图11.1 中, 第1 行E = ∞表示初始化学习器误差为无穷大, 以至于第1 轮迭代第9 行的条件就一定\\n为真; 第2 行d = |A| 中的|A| 表示特征集A 的包含的特征个数; 第9 行E′ < E 表示学习器L 在特征子\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n集A′ 上的误差比当前特征子集A 上的误差更小, (E′ = E) ∨(d′ < d) 表示学习器L 在特征子集A′ 上的\\n误差与当前特征子集A 上的误差相当但A′ 中包含的特征数更小; 表示“逻辑与”，V 表示“逻辑或”。注\\n意到, 第5 行至第17 行的while 循环中t 并非一直增加, 当第9 行条件满足时t 会被清零。\\n最后, 本节LVW 算法基于拉斯维加斯方法框架, 可以仔细琢磨体会拉斯维加斯方法和蒙特卡罗方法\\n的区别。一个通俗的解释如下：\\n蒙特卡罗算法: 采样越多, 越近似最优解;\\n拉斯维加斯算法: 采样越多, 越有机会找到最优解;\\n举个例子, 假如筐里有100 个苹果, 让我每次闭眼拿1 个, 挑出最大的。于是我随机拿1 个，再随机拿\\n1 个跟它比，留下大的，再随机拿1 个...... 我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多,\\n挑出的苹果就越大, 但我除非拿100 次, 否则无法肯定挑出了最大的。这个挑苹果的算法, 就属于蒙特卡罗'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='算法一一尽量找好的, 但不保证是最好的。而拉斯维加斯算法, 则是另一种情况。假如有一把锁, 给我100\\n把钥匙, 只有1 把是对的。于是我每次随机拿1 把钥匙去试, 打不开就再换1 把。我试的次数越多, 打开\\n(最优解) 的机会就越大, 但在打开之前, 那些错的钥匙都是没有用的。这个试钥匙的算法, 就是拉斯维加斯\\n的一一尽量找最好的, 但不保证能找到。\\n11.3\\n嵌入式选择与L1 正则化\\n“嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即\\n在学习器训练过程中自动地进行了特征选择。”，具体可以对比本节式(11.7) 的例子与前两节方法的本质\\n区别，细细体会本节第一段的这句有关“嵌入式”的概念描述。\\n11.3.1\\n式(11.5) 的解释\\n该式为线性回归的优化目标式，yi 表示样本i 的真实值，而w⊤xi 表示其预测值，这里使用预测值和\\n真实值差的平方衡量预测值偏离真实值的大小。\\n11.3.2\\n式(11.6) 的解释\\n该式为加入了L2 正规化项的优化目标，也叫“岭回归”，λ 用来调节误差项和正规化项的相对重要性，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='该式为加入了L2 正规化项的优化目标，也叫“岭回归”，λ 用来调节误差项和正规化项的相对重要性，\\n引入正规化项的目的是为了防止w 的分量过大而导致过拟合的风险。\\n11.3.3\\n式(11.7) 的解释\\n该式将11.6 中的L2 正规化项替换成了L1 正规化项，也叫LASSO 回归。关于L2 和L1 两个正规化\\n项的区别，“西瓜书”图11.2 给出了很形象的解释。具体来说，结合L1 范数优化的模型参数分量取值尽\\n量稀疏，即非零分量个数尽量小，因此更容易取得稀疏解。\\n11.3.4\\n式(11.8) 的解释\\n从本式开始至本节结束, 都在介绍近端梯度下降求解L1 正则化问题。若将本式对应到式(11.7), 则本\\n式中f(w) = Pm\\ni=1\\n\\x00yi −w⊤xi\\n\\x012, 注意变量为w （若感觉不习惯就将其用x 替换好了)。最终推导结果仅\\n含f(w) 的一阶导数∇f(w) = −Pm\\ni=1 2\\n\\x00yi −w⊤xi\\n\\x01\\nxi 。\\n11.3.5\\n式(11.9) 的解释\\n该式即为L-Lipschitz(利普希茨) 条件的定义。简单来说, 该条件约束函数的变化不能太快。将式(11.9)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='变形则更为直观(注: 式中应该是2 范数, 而非2 范数平方):\\n∥∇f (x′) −∇f(x)∥2\\n∥x′ −x∥2\\n⩽L,\\n(∀x, x′)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n进一步地, 若x′ →x, 即\\nlim\\nx′→x\\n∥∇f (x′) −∇f(x)∥2\\n∥x′ −x∥2\\n这明显是在求解函数∇f(x) 的导数绝对值（模值。因此, 式(11.9) 即要求f(x) 的二阶导数不大于L, 其\\n中L 称为Lipschitz 常数。\\n“Lipschitz 连续”可以形象得理解为: 以陆地为例, Lipschitz 连续就是说这块地上没有特别陡的坡; 其\\n中最陡的地方有多陡呢? 这就是所谓的Lipschitz 常数。\\n11.3.6\\n式(11.10) 的推导\\n首先注意优化目标式和11.7 LASSO 回归的联系和区别，该式中的x 对应到式11.7 的w，即我们优\\n化的目标。再解释下什么是L−Lipschitz 条件，根据维基百科的定义：它是一个比通常连续更强的光滑性\\n条件。直觉上，利普希茨连续函数限制了函数改变的速度，符合利普希茨条件的函数的斜率，必小于一个\\n称为利普希茨常数的实数（该常数依函数而定）。注意这里存在一个笔误，在wiki 百科的定义中，式11.9\\n应该写成'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='应该写成\\n|∇f (x′) −∇f(x)| ⩽L |x′ −x|\\n(∀x, x′)\\n移项得\\n|∇f (x′) −∇f(x)|\\n|x′ −x|\\n⩽L\\n(∀x, x′)\\n由于上式对所有的x, x′ 都成立，由导数的定义，上式可以看成是f(x) 的二阶导数恒不大于L。即\\n∇2f(x) ⩽L\\n得到这个结论之后，我们来推导式11.10。由泰勒公式，xk 附近的f(x) 通过二阶泰勒展开式可近似为\\nˆf(x) ≃f (xk) + ⟨∇f (xk) , x −xk⟩+ ∇2f(xk)\\n2\\n∥x −xk∥2\\n⩽f (xk) + ⟨∇f (xk) , x −xk⟩+ L\\n2 ∥x −xk∥2\\n= f (xk) + ∇f (xk)⊤(x −xk) + L\\n2 (x −xk)⊤(x −xk)\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk)⊤(x −xk) + 2\\nL∇f (xk)⊤(x −xk)\\n\\x13\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk)⊤(x −xk) + 2\\nL∇f (xk)⊤(x −xk) + 1\\nL2 ∇f(xk)⊤∇f(xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='L2 ∇f(xk)⊤∇f(xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk) + 1\\nL∇f (xk)\\n\\x13⊤\\x12\\n(x −xk) + 1\\nL∇f (xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)\\n= L\\n2\\n\\r\\r\\r\\rx −\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\r\\r\\r\\r\\n2\\n2\\n+ const\\n其中const = f(xk) −\\n1\\n2L∇f (xk)⊤∇f (xk)\\n11.3.7\\n式(11.11) 的解释\\n这个很容易理解，因为2 范数的最小值为0，当xk+1 = xk −1\\nL∇f (xk) 时，ˆf(xk+1) ⩽ˆf(xk) 恒成立，\\n同理ˆf(xk+2) ⩽ˆf(xk+1), · · · ，因此反复迭代能够使ˆf(x) 的值不断下降。\\n11.3.8\\n式(11.12) 的解释\\n注意ˆf(x) 在式(11.11) 处取得最小值, 因此, 以下不等式肯定成立:\\nˆf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf (xk)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在式(11.10) 推导中有f(x) ⩽ˆf(x) 恒成立, 因此, 以下不等式肯定成立:\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n在式(11.10) 推导中还知道f (xk) = ˆf (xk), 因此\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆf (xk) = f (xk)\\n也就是说通过迭代xk+1 = xk −1\\nL∇f (xk) 可以使f(x) 的函数值逐步下降。\\n同理, 对于函数g(x) = f(x) + λ∥x∥1, 可以通过最小化ˆg(x) = ˆf(x) + λ∥x∥1 逐步求解。式(11.12) 就\\n是在最小化ˆg(x) = ˆf(x) + λ∥x∥1◦。\\n以上优化方法被称为Majorization-Minimization。可以搜索相关资料做详细了解。\\n11.3.9\\n式(11.13) 的解释\\n这里将式11.12 的优化步骤拆分成了两步，首先令z = xk −1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='式(11.13) 的解释\\n这里将式11.12 的优化步骤拆分成了两步，首先令z = xk −1\\nL∇f (xk) 以计算z，然后再求解式11.13，\\n得到的结果是一致的。\\n11.3.10\\n式(11.14) 的推导\\n令优化函数\\ng(x) = L\\n2 ∥x −z∥2\\n2 + λ∥x∥1\\n= L\\n2\\nd\\nX\\ni=1\\n\\r\\rxi −zi\\r\\r2\\n2 + λ\\nd\\nX\\ni=1\\n\\r\\rxi\\r\\r\\n1\\n=\\nd\\nX\\ni=1\\n\\x12L\\n2\\n\\x00xi −zi\\x012 + λ\\n\\x0c\\x0cxi\\x0c\\x0c\\n\\x13\\n这个式子表明优化g(x) 可以被拆解成优化x 的各个分量的形式，对分量xi，其优化函数\\ng\\n\\x00xi\\x01\\n= L\\n2\\n\\x00xi −zi\\x012 + λ\\n\\x0c\\x0cxi\\x0c\\x0c\\n求导得\\ndg (xi)\\ndxi\\n= L\\n\\x00xi −zi\\x01\\n+ λsgn\\n\\x00xi\\x01\\n其中\\nsign\\n\\x00xi\\x01\\n=\\n(\\n1,\\nxi > 0\\n−1,\\nxi < 0\\n称为符号函数[1]，对于xi = 0 的特殊情况，由于|xi| 在xi = 0 点出不光滑，所以其不可导，需单独\\n讨论。令\\ndg(xi)\\ndxi\\n= 0 有\\nxi = zi −λ\\nL sign\\n\\x00xi\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='讨论。令\\ndg(xi)\\ndxi\\n= 0 有\\nxi = zi −λ\\nL sign\\n\\x00xi\\x01\\n此式的解即为优化目标g(xi) 的极值点，因为等式两端均含有未知变量xi，故分情况讨论。\\n1. 当zi > λ\\nL 时：a. 假设xi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL > 0 与假设矛盾；b. 假设\\nxi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL > 0 和假设相符和，下面来检验xi = zi −λ\\nL 是否是使\\n函数g(xi) 的取得最小值。当xi > 0 时，\\ndg (xi)\\ndxi\\n= L\\n\\x00xi −zi\\x01\\n+ λ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n在定义域内连续可导，则g(xi) 的二阶导数\\nd2g (xi)\\ndxi2\\n= L\\n由于L 是Lipschitz 常数恒大于0，因为xi = zi −λ\\nL 是函数g(xi) 的最小值。\\n2. 当zi < −λ\\nL 时：a. 假设xi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL < 0 与假设矛盾；b. 假设\\nxi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL < 0 与假设相符，由上述二阶导数恒大于0 可知，\\nxi = zi + λ\\nL 是g(xi) 的最小值。\\n3. 当−λ\\nL ⩽zi ⩽λ\\nL 时：a. 假设xi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL ⩽0 与假设矛盾；b. 假设\\nxi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL ⩾0 与假设矛盾。\\n4. 最后讨论xi = 0 的情况，此时g(xi) = L\\n2 (zi)\\n2\\n• 当|zi| > λ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2 (zi)\\n2\\n• 当|zi| > λ\\nL 时，由上述推导可知g(xi) 的最小值在xi = zi −λ\\nL 处取得，因为\\ng(xi)|xi=0 −g(xi)|xi=zi−λ\\nL = L\\n2\\n\\x00zi\\x012 −\\n\\x12\\nλzi −λ2\\n2L\\n\\x13\\n= L\\n2\\n\\x12\\nzi −λ\\nL\\n\\x132\\n> 0\\n因此当|zi| > λ\\nL 时，xi = 0 不会是函数g(xi) 的最小值。\\n• 当−λ\\nL ⩽zi ⩽λ\\nL 时，对于任何∆x ̸= 0 有\\ng(∆x) = L\\n2\\n\\x00∆x −zi\\x012 + λ|∆x|\\n= L\\n2\\n\\x12\\n(∆x)2 −2∆x · zi + 2λ\\nL |∆x|\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2\\n\\x12\\n(∆x)2 −2∆x · zi + 2λ\\nL ∆x\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2 (∆x)2 + L\\n2\\n\\x00zi\\x012\\n> g(xi)|xi=0\\n因此xi = 0 是g(xi) 的最小值点。\\n综上所述，11.14 成立。\\n该式称为软阈值(Soft Thresholding) 函数，很常见，建议掌握。另外，常见的变形是式(11.13) 中的'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='或时的形式，其解直接代入式(11.14) 即可。与软阈值函数相对的是硬阈值函数，是将式(11.13) 中的1\\n范数替换为0 范数的优化问题的闭式解。\\n11.4\\n稀疏表示与字典学习\\n稀疏表示与字典学习实际上是信号处理领域的概念。本节内容核心就是K-SVD 算法。\\n11.4.1\\n式(11.15) 的解释\\n这个式子表达的意思很容易理解，即希望样本xi 的稀疏表示αi 通过字典B 重构后和样本xi 的原始\\n表示尽量相似，如果满足这个条件，那么稀疏表示αi 是比较好的。后面的1 范数项是为了使表示更加稀\\n疏。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n11.4.2\\n式(11.16) 的解释\\n为了优化11.15，我们采用变量交替优化的方式(有点类似EM 算法)，首先固定变量B，则11.15 求\\n解的是m 个样本相加的最小值，因为公式里没有样本之间的交互(即文中所述αu\\ni αv\\ni (u ̸= v) 这样的形式)，\\n因此可以对每个变量做分别的优化求出αi，求解方法见式(11.13)，式(11.14)。\\n11.4.3\\n式(11.17) 的推导\\n这是优化11.15 的第二步，固定住αi, i = 1, 2, . . . , m，此时式11.15 的第二项为一个常数，优化11.15\\n即优化minB\\nPm\\ni=1 ∥xi −Bαi∥2\\n2。其写成矩阵相乘的形式为minB ∥X −BA∥2\\n2，将2 范数扩展到F 范数即\\n得优化目标为minB ∥X −BA∥2\\nF。\\n11.4.4\\n式(11.18) 的推导\\n这个公式难点在于推导BA = Pk\\nj=1 bjαj。大致的思路是bjαj 会生成和矩阵BA 同样维度的矩阵，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='j=1 bjαj。大致的思路是bjαj 会生成和矩阵BA 同样维度的矩阵，\\n这个矩阵对应位置的元素是BA 中对应位置元素的一个分量，这样的分量矩阵一共有k 个，把所有分量\\n矩阵加起来就得到了最终结果。推导过程如下：\\nBA =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\n1\\nb1\\n2\\n·\\n·\\n·\\nb1\\nk\\nb2\\n1\\nb2\\n2\\n·\\n·\\n·\\nb2\\nk\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nbd\\n1\\nbd\\n2\\n·\\n·\\n·\\nbd\\nk\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×k\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nα1\\n1\\nα1\\n2\\n·\\n·\\n·\\nα1\\nm\\nα2\\n1\\nα2\\n2\\n·\\n·\\n·\\nα2\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nαk\\n1\\nαk\\n2\\n·\\n·\\n·\\nαk\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nk×m\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPk\\nj=1 b1\\njαj\\n1\\nPk\\nj=1 b1\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b1\\njαj\\nm\\nPk\\nj=1 b2\\njαj\\n1\\nPk\\nj=1 b2\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\n1\\nPk\\nj=1 bd\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\nbjαj =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\nj\\nb2\\nj\\n·\\n·\\n·\\nbd\\nj\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n·\\nh\\nαj\\n1\\nαj\\n2\\n·\\n·\\n·\\nαj\\nm\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\njαj\\n1\\nb1\\njαj\\n2\\n·\\n·\\n·\\nb1\\njαj\\nm\\nb2\\njαj\\n1\\nb2\\njαj\\n2\\n·\\n·\\n·\\nb2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nbd\\njαj\\n1\\nbd\\njαj\\n2\\n·\\n·\\n·\\nbd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n求和可得：\\nk\\nX\\nj=1\\nbjαj =\\nk\\nX\\nj=1\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb1\\nj\\nb2\\nj\\n·\\n·\\n·\\nbd\\nj\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n·\\nh\\nαj\\n1\\nαj\\n2\\n·\\n·\\n·\\nαj\\nm\\ni\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPk\\nj=1 b1\\njαj\\n1\\nPk\\nj=1 b1\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b1\\njαj\\nm\\nPk\\nj=1 b2\\njαj\\n1\\nPk\\nj=1 b2\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\n1\\nPk\\nj=1 bd\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nd×m\\n得证。\\n将矩阵B 分解成矩阵列bj, j = 1, 2, . . . , k 带来一个好处，即和11.16 的原理相同，矩阵列与列之间\\n无关，因此可以分别优化各个列，即将min'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='无关，因此可以分别优化各个列，即将min\\nB ∥. . . B . . . ∥2\\nF 转化成了min\\nbi ∥. . . bi . . . ∥2\\nF，得到第三行的等式\\n之后，再利用文中介绍的K-SVD 算法求解即可。\\n11.5\\nK-SVD 算法\\n本节前半部分铺垫概念，后半部分核心就是K-SVD。作为字典学习的最经典的算法，K-SVD[2] 自\\n2006 年发表以来已逾万次引用。理解K-SVD 的基础是SVD，即奇异值分解，参见“西瓜书”附录A.3。\\n对于任意实矩阵A ∈Rm×n, 都可分解为A = UΣV⊤, 其中U ∈Rm×m, V ∈Rn×n, 分别为m 阶和\\nn 阶正交矩阵(复数域时称为酉矩阵), 即U⊤U = I, V⊤V = I (逆矩阵等于自身的转置), Σ ∈Rm×n, 且除\\n(Σ)ii = σi 之外其它位置的元素均为零, σi 称为奇异值, 可以证明, 矩阵A 的秩等于非零奇异值的个数。\\n正如西瓜书附录A.3 所述, K-SVD 分解中主要使用SVD 解决低秩矩阵近似问题。之所以称为K-SVD,\\n原文献中专门有说明:'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='原文献中专门有说明:\\nWe shall call this algorithm ”K-SVD” to parallel the name K-means.\\nWhile K-means applies K\\ncomputations of means to update the codebook, K-SVD obtains the updated dictionary by K SVD com-\\nputations, each determining one column.\\n具体来说, 就是原文献中的字典共有K 个原子(列), 因此需要迭代K 次; 这类似于K 均值算法欲将\\n数据聚成K 个簇, 需要计算K 次均值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n图11-6 K-SVD 算法在论文中的描述\\nK-SVD 算法伪代码详见图11-6, 符号与本节符号有差异。具体来说, 原文献中字典矩阵用D 表示(书\\n中用B ), 稀疏系数用xi 表示(书中用αi ), 数据集用Y 表示(书中用X)。\\n在初始化字典矩阵D 以后, K-SVD 算法迭代过程分两步: 第1 步Sparse Coding Stage 就是普通的已\\n知字典矩阵D 的稀疏表示问题, 可以使用很多现成的算法完成此步, 不再详述; K-SVD 的核心创新点在第\\n2 步Codebook Update Stage, 在该步骤中分K 次分别更新字典矩阵D 中每一列, 更新第k 列dk 时其它\\n各列都是固定的, 如原文献式(21) 所示:\\n∥Y −DX∥2\\nF =\\n\\r\\r\\r\\r\\rY −\\nK\\nX\\nj=1\\ndjxj\\nT\\n\\r\\r\\r\\r\\r\\n2\\nF\\n=\\n\\r\\r\\r\\r\\r\\n \\nY −\\nX\\nj̸=k\\ndjxj\\nT\\n!\\n−dkxk\\nT\\n\\r\\r\\r\\r\\r\\n2\\nF\\n=\\n\\r\\rEk −dkxk\\nT\\n\\r\\r2\\nF .\\n注意, 矩阵dkxk\\nT 的秩为1 (其中, xk'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\n\\r\\rEk −dkxk\\nT\\n\\r\\r2\\nF .\\n注意, 矩阵dkxk\\nT 的秩为1 (其中, xk\\nT 表示稀疏系数矩阵X 的第k 行, 以区别于其第k 列xk ), 对比\\n西瓜书附录A.3 中的式(A.34), 这就是一个低秩矩阵近似问题, 即对于给定矩阵Ek, 求其最优1 秩近似矩\\n阵dkxk\\nT ; 此时可对Ek 进行SVD 分解, 类似于西瓜书附录式(A.35), 仅保留最大的1 个奇异值; 具体来说\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nEk = U∆V⊤, 仅保留∆中最大的奇异值∆(1, 1), 则dkxk\\nT = U1∆(1, 1)V⊤\\n1 , 其中U1, V1 分别为U, V 的第\\n1 列, 此时dk = U1, xk\\nT = ∆(1, 1)V⊤\\n1 。但这样更新会破坏第1 步中得到的稀疏系数的稀疏性!\\n为了保证第1 步中得到的稀疏系数的稀疏性, K-SVD 并不直接对Ek 进行SVD 分解, 而是根据xk\\nT 仅\\n取出与xk\\nT 非零元素对应的部分列, 例如行向量xk\\nT 只有第1、3、5、8、9 个元素非零, 则仅取出Ek 的第\\n1、3、5、8、9 列组成矩阵进行SVD 分解ER\\nk = U∆V⊤, 则\\n˜dk = U1,\\n˜xk\\nT = ∆(1, 1)V⊤\\n1\\n即得到更新后的˜dk 和˜xk\\nT (注意, 此时的行向量˜xk\\nT 长度仅为原xk\\nT 非零元素个数, 需要按原xk\\nT 对其\\n余位置填0)。如此迭代K 次即得更新后的字典矩阵˜D, 以供下一轮Sparse Coding 使用。K −SVD 原文'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='献中特意提到, 在K 次迭代中要使用最新的稀疏系数˜xk\\nT , 但并没有说是否要用最新的˜dk (推测应该也要\\n用最新的˜dk ):\\nIn the K-SVD algorithm, we sweep through the columns and use always the most updated coeﬀicients\\nas they emerge from preceding SVD steps. Parallel versions of this algorithm can also be considered, where\\nall updates of the previous dictionary are done based on the same X. Experiments show that while this\\nversion also converges, it yields an inferior solution and typically requires more than four times the number\\nof iterations.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='of iterations.\\n11.6\\n压缩感知\\n虽然压缩感知与稀疏表示关系密切, 但它是彻彻底底的信号处理领域的概念。“西瓜书”在本章有几个\\n专业术语翻译与信号处理领域人士的习惯翻译略不一样：比如第258 页的Restricted Isometry Property\\n(RIP)“西瓜书”翻译为“限定等距性”, 信号处理领域一般翻译为“有限等距性质”; 第259 页的Basis\\nPursuit De-Noising、第261 页的Basis Pursuit 和Matching Pursuit 中的“Pursuit”“西瓜书”翻译为\\n“寻踪”, 信号处理领域一般翻译为“追踪”, 即基追踪降噪、基追踪、匹配追踪。\\n11.6.1\\n式(11.21) 的解释\\n将式(11.21) 进行变形\\n(1 −δk) ⩽∥Aks∥2\\n2\\n∥s∥2\\n2\\n⩽(1 + δk)\\n注意不等式中间, 若s 为输入信号, 则分母∥s∥2\\n2 为输入信号的能量, 分子∥Aks∥2\\n2 为对应的观测信号的能\\n量, 即RIP 要求观测信号与输入信号的能量之比在一定的范围之内; 例如当δk 等于0 时, 观测信号与输入'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='信号的能量相等, 即实现了等距变换，相关文献可以参考[3]; RIP 放松了对矩阵A 的约束(而且A 并非\\n方阵), 因此称为“有限”等距性质。\\n11.6.2\\n式(11.25) 的解释\\n该式即为核范数定义: 矩阵的核范数（迹范数）为矩阵的奇异值之和。\\n有关“凸包”的概念, 引用百度百科里的两句原话: 在二维欧几里得空间中, 凸包可想象为一条刚好包\\n著所有点的橡皮圈; 用不严谨的话来讲, 给定二维平面上的点集, 凸包就是将最外层的点连接起来构成的凸\\n多边形, 它能包含点集中所有的点。\\n个人理解, 将rank(X) 的“凸包”是X 的核范数∥X∥∗这件事简单理解为∥X∥∗是rank(X) 的上限即\\n可, 即∥X∥∗恒大于rank(X), 类似于式(11.10) 中的式子恒大于f(x) 。\\n参考文献\\n[1] Wikipedia contributors. Sign function, 2020.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 147, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n[2] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete\\ndictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311–4322, 2006.\\n[3] 杨孝春. 欧氏空间中的等距变换与等距映射. 四川工业学院学报, 1999.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第12 章\\n计算学习理论\\n正如本章开篇所述，计算学习理论研究目的是分析学习任务的困难本质，为学习算法提供理论保证，\\n并根据分析结果指导算法设计。例如，“西瓜书”定理12.1、定理12.3、定理12.6 所表达意思的共同点是，\\n泛化误差与经验误差之差的绝对值以很大概率(1 −δ) 很小，且这个差的绝对值随着训练样本个数(m) 的\\n增加而减小，随着模型复杂度（定理12.1 为假设空间包含的假设个数|H|，定理12.3 中为假设空间的VC\\n维，定理12.6 中为(经验)Rademacher 复杂度）的减小而减小。因此，若想要得到一个泛化误差很小的模\\n型，足够的训练样本是前提，最小化经验误差是实现途径，另外还要选择性能相同的模型中模型复杂度最\\n低的那一个；“最小化经验误差”即常说的经验风险最小化，“选择模型复杂度最低的那一个”即结构风险\\n最小化，可以参见“西瓜书”6.4 节最后一段的描述，尤其是式(6.42) 所表达的含义。\\n12.1\\n基础知识'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='12.1\\n基础知识\\n统计学中有总体集合和样本集合之分, 比如要统计国内本科生对机器学习的掌握情况, 此时全国所有\\n的本科生就是总体集合, 但总体集合往往太大而不具有实际可操作性, 一般都是取总体集合的一部分, 比如\\n从双一流A 类、双一流B 类、一流学科建设高校、普通高校中各找一部分学生(即样本集合) 进行调研,\\n以此来了解国内本科生对机器学习的掌握情况。在机器学习中, 样本空间(参见1.2 节) 对应总体集合, 而\\n我们手头上的样例集D 对应样本集合, 样例集D 是从样本空间中采样而得, 分布D 可理解为当从样本空\\n间采样获得样例集D 时每个样本被采到的概率, 我们用D(t) 表示样本空间第t 个样本被采到的概率。\\n12.1.1\\n式(12.1) 的解释\\n该式为泛化误差的定义式，所谓泛化误差，是指当样本x 从真实的样本分布D 中采样后其预测值\\nh(x) 不等于真实值y 的概率。在现实世界中，我们很难获得样本分布D，我们拿到的数据集可以看做是\\n从样本分布D 中独立同分布采样得到的。在西瓜书中，我们拿到的数据集，称为样例集D[也叫观测集、\\n样本集，注意与花体D 的区别]。\\n12.1.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='样本集，注意与花体D 的区别]。\\n12.1.2\\n式(12.2) 的解释\\n该式为经验误差的定义式，所谓经验误差，是指观测集D 中的样本xi, i = 1, 2, · · · , m 的预测值h(xi)\\n和真实值yi 的期望误差。\\n12.1.3\\n式(12.3) 的解释\\n假设我们有两个模型h1 和h2，将它们同时作用于样本x 上，那么他们的”不合“度定义为这两个模\\n型预测值不相同的概率。\\n12.1.4\\n式(12.4) 的解释\\nJensen 不等式：这个式子可以做很直观的理解，比如说在二维空间上，凸函数可以想象成开口向上的\\n抛物线，假如我们有两个点x1, x2，那么f(E(x)) 表示的是两个点的均值的纵坐标，而E(f(x)) 表示的是\\n两个点纵坐标的均值，因为两个点的均值落在抛物线的凹处，所以均值的纵坐标会小一些。\\n12.1.5\\n式(12.5) 的解释\\n随机变量的观测值是随机的, 进一步地, 随机过程的每个时刻都是一个随机变量。\\n式中,\\n1\\nm\\nPm\\ni=1 xi 表示m 个独立随机变量各自的某次观测值的平均,\\n1\\nm\\nPm\\ni=1 E (xi) 表示m 个独立随\\n机变量各自的期望的平均。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1\\nm\\nPm\\ni=1 E (xi) 表示m 个独立随\\n机变量各自的期望的平均。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n式(12.5) 表示事件\\n1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi) ⩾ϵ 出现的概率不大于(i.e., ⩽) e−2mϵ2; 式(12.6) 的事\\n件\\n\\x0c\\x0c 1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi)\\n\\x0c\\x0c ⩾ϵ 等价于以下事件:\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩾ϵ\\n∨\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩽−ϵ\\n其中, ∨表示逻辑或(以上其实就是将绝对值表达式拆成两部分而已)。这两个子事件并无交集, 因此\\n总概率等于两个子事件概率之和; 而\\n1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi) ⩽−ϵ 与式(12.5) 表达的事情对称, 因此\\n概率相同。\\nHoeffding 不等式表达的意思是\\n1\\nm\\nPm\\ni=1 xi 和\\n1\\nm\\nPm\\ni=1 E (xi) 两个值应该比较接近, 二者之差大于ϵ 的\\n概率很小(不大于2e−2mϵ2 )。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='概率很小(不大于2e−2mϵ2 )。\\n如果对Hoeffding 不等式的证明感兴趣，可以参考Hoeffding 在1963 年发表的论文[1]，这篇文章也\\n被引用了逾万次。\\n12.1.6\\n式(12.7) 的解释\\nMcDiarmid 不等式：首先解释下前提条件：\\nsup\\nx1,...,xm,x′\\ni\\n|f (x1, . . . , xm) −f (x1, . . . , xi−1, x′\\ni, xi+1, . . . , xm)| ⩽ci\\n表示当函数f 某个输入xi 变到x′\\ni 的时候，其变化的上确sup 仍满足不大于ci。所谓上确界sup 可以理\\n解成变化的极限最大值，可能取到也可能无穷逼近。当满足这个条件时，McDiarmid 不等式指出：函数值\\nf(x1, . . . , xm) 和其期望值E (f(x1, . . . , xm)) 也相近，从概率的角度描述是：它们之间差值不小于ϵ 这样\\n的事件出现的概率不大于exp\\n\\x10\\n−2ϵ2\\n∑\\ni c2\\ni\\n\\x11\\n，可以看出当每次变量改动带来函数值改动的上限越小，函数值和\\n其期望越相近。\\n12.2\\nPAC 学习'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='，可以看出当每次变量改动带来函数值改动的上限越小，函数值和\\n其期望越相近。\\n12.2\\nPAC 学习\\n本节内容几乎都是概念, 建议多读几遍，仔细琢磨一下。\\n概率近似正确(Probably Approximately Correct, PAC) 学习, 可以读为[pæk] 学习。\\n本节第2 段讨论的目标概念, 可简单理解为真实的映射函数;\\n本节第3 段讨论的假设空间, 可简单理解为学习算法不同参数时的存在, 例如线性分类超平面f(x) =\\nw⊤x + b, 每一组(w, b) 取值就是一个假设;\\n本节第4 段讨论的可分的(separable) 和不可分的(non-separable), 例如西瓜书第100 页的图5.4, 若\\n假设空间是线性分类器, 则(a)(b)(c) 是可分的, 而(d) 是不可分的; 当然, 若假设空间为椭圆分类器(分类\\n边界为椭圆), 则(d) 也是可分的;\\n本节第5 段提到的“等效的假设”指的是第7 页图1.3 中的A 和B 两条曲线都可以完美拟合有限的\\n样本点, 故称之为“等效”的假设; 另外本段最后还给出了概率近似正确的含义, 即“以较大概率学得误差'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='满足预设上限的模型”。\\n定义12.1 PAC 辨识的式(12.9) 表示输出假设h 的泛化误差E(h) ⩽ϵ 的概率不小于1 −δ; 即“学习\\n算法L 能以较大概率(至少1 −δ ) 学得目标概念c 的近似(误差最多为ϵ )”。\\n定义12.2 PAC 可学习的核心在于, 需要的样本数目m 是1/ϵ, 1/δ, size(x), size(c) 的多项式函数。\\n定义12.3 PAC 学习算法的核心在于, 完成PAC 学习所需要的时间是1/ϵ, 1/δ, size(x), size(c) 的多项\\n式函数。\\n定义12.4 样本复杂度指完成PAC 学习过程需要的最少的样本数量, 而在实际中当然也希望用最少的\\n样本数量完成学习过程。\\n在定义12.4 之后, 抛出来三个问题:\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n• 研究某任务在什么样的条件下可学得较好的模型？（定义12.2）\\n• 某算法在什么样的条件下可进行有效的学习?（定义12.3）\\n• 需多少训练样例才能获得较好的模型？（定义12.4）\\n有限假设空间指H 中包含的假设个数是有限的, 反之则为无限假设空间; 无限假设空间更为常见, 例\\n如能够将图5.4(a)(b)(c) 中的正例和反例样本分开的线性超平面个数是无限多的。\\n12.2.1\\n式(12.9) 的解释\\nPAC 辨识的定义：E(h) 表示算法L 在用观测集D 训练后输出的假设函数h，它的泛化误差(见公\\n式12.1)。这个概率定义指出，如果h 的泛化误差不大于ϵ 的概率不小于1 −δ，那么我们称学习算法L\\n能从假设空间H 中PAC 辨识概念类C。\\n12.3\\n有限假设空间\\n本节内容分两部分, 第1 部分“可分情形”时, 可以达到经验误差bE(h) = 0, 做的事情是以1 −δ 概率\\n学得目标概念的ϵ 近似, 即式(12.12); 第2 部分“不可分情形”时, 无法达到经验误差bE(h) = 0, 做的事'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='情是以1 −δ 概率学得minh∈H E(h) 的ϵ 近似, 即式(12.20)。无论哪种情形, 对于h ∈H, 可以得到该假\\n设的泛化误差E(h) 与经验误差bE(h) 的关系, 即“当样例数目m 较大时, h 的经验误差是泛化误差很好\\n的近似”, 即式(12.18); 实际研究中经常需要推导类似的泛化误差上下界。\\n从式12.10 到式12.14 的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c 的有效近\\n似。只要训练集D 的规模能使学习算法L 以概率1 −δ 找到目标假设的ϵ 近似即可。下面就是用数学公\\n式进行抽象。\\n12.3.1\\n式(12.10) 的解释\\nP(h(x) = y) = 1 −P(h(x) ̸= y) 因为它们是对立事件，P(h(x) ̸= y) = E(h) 是泛化误差的定义(见\\n12.1)，由于我们假定了泛化误差E(h) > ϵ，因此有1 −E(h) < 1 −ϵ。\\n12.3.2\\n式(12.11) 的解释\\n先解释什么是h 与D“表现一致”，12.2 节开头阐述了这样的概念，如果h 能将D 中所有样本按与真实'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h (x1) = y1)∧. . .∧(h (xm) = ym) 为True。\\n因为每个事件是独立的，所以上式可以写成P ((h (x1) = y1) ∧. . . ∧(h (xm) = ym)) = Qm\\ni=1 P (h (xi) = yi)。\\n根据对立事件的定义有：Qm\\ni=1 P (h (xi) = yi) = Qm\\ni=1 (1 −P (h (xi) ̸= yi))，又根据公式(12.10)，有\\nm\\nY\\ni=1\\n(1 −P (h (xi) ̸= yi)) <\\nm\\nY\\ni=1\\n(1 −ϵ) = (1 −ϵ)m\\n12.3.3\\n式(12.12) 的推导\\n首先解释为什么”我们事先并不知道学习算法L 会输出H 中的哪个假设“，因为一些学习算法对\\n用一个观察集D 的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知\\n机学习到的假设h 参数的值。泛化误差大于ϵ 且经验误差为0 的假设(即在训练集上表现完美的假设)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='机学习到的假设h 参数的值。泛化误差大于ϵ 且经验误差为0 的假设(即在训练集上表现完美的假设)\\n出现的概率可以表示为P(h ∈H : E(h) > ϵ ∧bE(h) = 0)，根据式12.11，每一个这样的假设h 都满足\\nP(E(h) > ϵ ∧bE(h) = 0) < (1 −ϵ)m，假设一共有|H| 这么多个这样的假设h，因为每个假设h 满足\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nE(h) > ϵ 且bE(h) = 0 是互斥的，因此总的概率P(h ∈H : E(h) > ϵ ∧bE(h) = 0) 就是这些互斥事件之和，\\n即\\nP\\n\\x10\\nh ∈H : E(h) > ϵ ∧bE(h) = 0\\n\\x11\\n=\\n|H|\\nX\\ni\\nP\\n\\x10\\nE(hi) > ϵ ∧bE(hi) = 0\\n\\x11\\n< |H|(1 −ϵ)m\\n小于号依据公式(12.11)。\\n第二个小于号实际上是要证明|H|(1 −ϵ)m < |H|e−mϵ，即证明(1 −ϵ)m < e−mϵ，其中ϵ ∈(0, 1]，m\\n是正整数，推导如下：\\n当ϵ = 1 时，显然成立，当ϵ ∈(0, 1) 时，因为左式和右式的值域均大于0，所以可以左右两边同时取\\n对数，又因为对数函数是单调递增函数，所以即证明m ln(1 −ϵ) < −mϵ，即证明ln(1 −ϵ) < −ϵ，这个式\\n子很容易证明：令f(ϵ) = ln(1 −ϵ) + ϵ，其中ϵ ∈(0, 1)，f ′(ϵ) = 1 −\\n1\\n1−ϵ = 0 ⇒ϵ = 0 取极大值0，因此'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='1\\n1−ϵ = 0 ⇒ϵ = 0 取极大值0，因此\\nln(1 −ϵ) < −ϵ 也即|H|(1 −ϵ)m < |H|e−mϵ 成立。\\n12.3.4\\n式(12.13) 的解释\\n回到我们要回答的问题：到底需要多少样例才能学得目标概念c 的有效近似。只要训练集D 的规模\\n能使学习算法L 以概率1 −δ 找到目标假设的ϵ 近似即可。根据式12.12，学习算法L 生成的假设大于目\\n标假设的ϵ 近似的概率为P\\n\\x10\\nh ∈H : E(h) > ϵ ∧bE(h) = 0\\n\\x11\\n< |H|e−mϵ，因此学习算法L 生成的假设落在\\n目标假设的ϵ 近似的概率为1 −P\\n\\x10\\nh ∈H : E(h) > ϵ ∧bE(h) = 0\\n\\x11\\n≥1 −|H|e−mϵ，这个概率我们希望至少\\n是1 −δ，因此1 −δ ⩽1 −|H|e−mϵ ⇒|H|e−mϵ ⩽δ\\n12.3.5\\n式(12.14) 的推导\\n|H|e−mϵ ⩽δ\\ne−mϵ ⩽\\nδ\\n|H|\\n−mϵ ⩽ln δ −ln |H|\\nm ⩾1\\nϵ\\n\\x12\\nln |H| + ln 1\\nδ\\n\\x13'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='|H|\\n−mϵ ⩽ln δ −ln |H|\\nm ⩾1\\nϵ\\n\\x12\\nln |H| + ln 1\\nδ\\n\\x13\\n这个式子告诉我们，在假设空间H 是PAC 可学习的情况下，输出假设h 的泛化误差ϵ 随样本数目\\nm 增大而收敛到0，收敛速率为O( 1\\nm)。这也是我们在机器学习中的一个共识，即可供模型训练的观测集\\n样本数量越多，机器学习模型的泛化性能越好。\\n12.3.6\\n引理12.1 的解释\\n根据式(12.2), bE(h) =\\n1\\nm\\nPm\\ni=1 I (h (xi) ̸= yi), 而指示函数I(·) 取值非0 即1 , 也就是说0 ≤\\nI (h (xi) ̸= yi) ≤1; 对于式(12.1) 的E(h) 实际上表示I (h (xi) ̸= yi) 为1 的期望E (I (h (xi) ̸= yi)) (泛化\\n误差表示样本空间中任取一个样本, 其预测类别不等于真实类别的概率), 当假设h 确定时, 泛化误差固定\\n不变, 因此可记为E(h) =\\n1\\nm\\nPm\\ni=1 E (I (h (xi) ̸= yi)) 。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='不变, 因此可记为E(h) =\\n1\\nm\\nPm\\ni=1 E (I (h (xi) ̸= yi)) 。\\n此时, 将bE(h) 和E(h) 代入式(12.15) 到式(12.17), 对比式(12.5) 和式(12.6) 的Hoeffding 不等式可\\n知, 式(12.15) 对应式(12.5), 式(12.16) 与式(12.15) 对称, 式(12.17) 对应式(12.6)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.3.7\\n式(12.18) 的推导\\n令δ = 2e−2mϵ2，则ϵ =\\nq\\nln(2/δ)\\n2m ，由式(12.17)\\nP(|E(h) −bE(h)| ⩾ϵ) ⩽2 exp\\n\\x00−2mϵ2\\x01\\nP(|E(h) −bE(h)| ⩾ϵ) ⩽δ\\nP(|E(h) −bE(h)| ⩽ϵ) ⩾1 −δ\\nP(−ϵ ⩽E(h) −bE(h) ⩽ϵ) ⩾1 −δ\\nP( bE(h) −ϵ ⩽E(h) ⩽bE(h) + ϵ) ⩾1 −δ\\n带入ϵ =\\nq\\nln(2/δ)\\n2m\\n得证。\\n这个式子进一步阐明了当观测集样本数量足够大的时候，h 的经验误差是其泛化误差很好的近似。\\n12.3.8\\n式(12.19) 的推导\\n令h1, h2, . . . , h|H| 表示假设空间H 中的假设，有\\nP(∃h ∈H : |E(h) −bE(h)| > ϵ)\\n=P\\n\\x10\\x10\\x0c\\x0c\\x0cEh1 −bEh1\\n\\x0c\\x0c\\x0c > ϵ\\n\\x11\\n∨. . . ∨\\n\\x10\\n|Eh|H| −bEh|H||>ϵ\\n\\x11\\x11\\n⩽\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='|Eh|H| −bEh|H||>ϵ\\n\\x11\\x11\\n⩽\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ)\\n这一步是很好理解的，存在一个假设h 使得|E(h) −bE(h)| > ϵ 的概率可以表示为对假设空间内\\n所有的假设hi, i ∈1, . . . , |H|，使得\\n\\x0c\\x0c\\x0cEhi −bEhi\\n\\x0c\\x0c\\x0c > ϵ 这个事件成立的” 或” 事件。因为P(A ∨B) =\\nP(A) + P(B) −P(A ∧B)，而P(A ∧B) ⩾0，所以最后一行的不等式成立。\\n由式12.17：\\nP(|E(h) −bE(h)| ⩾ϵ) ⩽2 exp\\n\\x00−2mϵ2\\x01\\n⇒\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ) ⩽2|H| exp\\n\\x00−2mϵ2\\x01\\n因此：\\nP(∃h ∈H : |E(h) −bE(h)| > ϵ) ⩽\\nX\\nh∈H\\nP(|E(h) −bE(h)| > ϵ)\\n⩽2|H| exp\\n\\x00−2mϵ2\\x01\\n其对立事件：\\nP(∀h ∈H : |E(h) −bE(h)| ⩽ϵ) = 1 −P(∃h ∈H : |E(h) −bE(h)| > ϵ)\\n⩾1 −2|H| exp\\n\\x00−2mϵ2\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='⩾1 −2|H| exp\\n\\x00−2mϵ2\\x01\\n令δ = 2|H|e−2mϵ2，则ϵ =\\nq\\nln |H|+ln(2/δ)\\n2m\\n，带入上式中即可得到\\nP\\n \\n∀h ∈H : |E(h) −bE(h)| ⩽\\nr\\nln |H| + ln(2/δ)\\n2m\\n!\\n⩾1 −δ\\n其中∀h ∈H 这个前置条件可以省略。\\n12.3.9\\n式(12.20) 的解释\\n这个式子是”不可知PAC 可学习“的定义式，不可知是指当目标概念c 不在算法L 所能生成的假设\\n空间H 里。可学习是指如果H 中泛化误差最小的假设是arg minh∈H E(h)，且这个假设的泛化误差满足\\n其与目标概念的泛化误差的差值不大于ϵ 的概率不小于1 −δ。我们称这样的假设空间H 是不可知PAC\\n可学习的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.4\\nVC 维\\n不同于12.3 节的有限假设空间，从本节开始，本章剩余内容均针对无限假设空间。\\n12.4.1\\n式(12.21) 的解释\\n这个是增长函数的定义式。增长函数ΠH(m) 表示假设空间H 对m 个样本所能赋予标签的最大可能\\n的结果数。比如对于两个样本的二分类问题，一共有4 中可能的标签组合[[0, 0], [0, 1], [1, 0], [1, 1]]，如果假\\n设空间H1 能赋予这两个样本两种标签组合[[0, 0], [1, 1]]，则ΠH1(2) = 2。显然，H 对样本所能赋予标签\\n的可能结果数越多，H 的表示能力就越强。增长函数可以用来反映假设空间H 的复杂度。\\n12.4.2\\n式(12.22) 的解释\\n值得指出的是，这个式子的前提假设有误，应当写成对假设空间H，m ∈N，0 < ϵ < 1，存在h ∈H\\n详细证明参见原论文On the uniform convergence of relative frequencies of events to their probabilities'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='[2]，在该论文中，定理的形式如下：\\nTheorem 2 The probability that the relative frequency of at least one event in class S differs from its\\nprobability in an experiment of size l by more then ε, for l ≧2/ε2, satisfies the inequality\\nP\\n\\x00π(l) > ε\\n\\x01\\n≦4mS(2l)e−ε2l/8.\\n注意定理描述中使用的是“at least one event in class S”, 因此应该是class S 中“存在”one event\\n而不是class S 中的“任意”event。\\n另外, 该定理为基于增长函数对无限假设空间的泛化误差分析, 与上一节有限假设空间的定理12.1。在\\n证明定理12.1 的式(12.19) 过程中, 实际证明的结论是\\nP(∃h ∈H : |E(h) −bE(h)| > ϵ) ⩽2|H|e−2mϵ2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='P(∃h ∈H : |E(h) −bE(h)| > ϵ) ⩽2|H|e−2mϵ2\\n根据该结论可得式(12.19) 的原型（式(12.19) 就是将ϵ 用δ 表示):\\nP(∀h ∈H : |E(h) −bE(h)| ⩽ϵ) ⩽1 −2|H|e−2mϵ2\\n这是因为事件∃h ∈H : |E(h) −bE(h)| > ϵ 与事件∀h ∈H : |E(h) −bE(h)| ⩽ϵ 为对立事件。\\n注意到当使用|E(h) −bE(h)| > ϵ 表达时对应于“存在”, 当使用|E(h) −bE(h)| ⩽ϵ 表达时则对应于\\n“任意”。\\n综上所述, 式(12.22) 使用|E(h) −bE(h)| > ϵ, 所以这里应该对应于“存在”。\\n12.4.3\\n式(12.23) 的解释\\n这是VC 维的定义式：VC 维的定义是能被H 打散的最大示例集的大小。“西瓜书”中例12.1 和例\\n12.2 给出了形象的例子。\\n式(12.23) 中的{m : ΠH(m) = 2m} 表示一个集合, 集合的元素是能使ΠH(m) = 2m 成立的所有m;'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='最外层的max 表示取集合的最大值。注意, 这里仅讨论二分类问题。注意，VC 维的定义式上的底数2 表\\n示这个问题是2 分类的问题。如果是n 分类的问题，那么定义式中底数需要变为n。\\nVC 维的概念还是很容易理解的, 有个常见的思维误区西瓜书也指出来了, 即“这并不意味着所有大小\\n为d 的示例集都能被假设空间H 打散”, 也就是说只要“存在大小为d 的示例集能被假设空间H 打散”\\n即可, 这里的区别与前面“定理12.2 的解释”中提到的“任意”与“存在”的关系一样。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.4.4\\n引理12.2 的解释\\n首先解释下数学归纳法的起始条件” 当m = 1, d = 0 或d = 1 时，定理成立”，当m = 1, d = 0 时，\\n由VC 维的定义(式12.23) VC(H) = max {m : ΠH(m) = 2m} = 0 可知ΠH(1) < 2，否则d 可以取到1，\\n又因为ΠH(m) 为整数，所以ΠH(1) ∈[0, 1]，式12.24 右边为P0\\ni=0\\n \\n1\\ni\\n!\\n= 1，因此不等式成立。当\\nm = 1, d = 1 时，因为一个样本最多只能有两个类别，所以ΠH(1) = 2，不等式右边为P1\\ni=0\\n \\n1\\ni\\n!\\n= 2，\\n因此不等式成立。\\n再介绍归纳过程，这里采样的归纳方法是假设式(12.24) 对(m −1, d −1) 和(m −1, d) 成立，推导\\n出其对(m, d) 也成立。证明过程中引入观测集D = {x1, x2, . . . , xm} 和观测集D′ = {x1, x2, . . . , xm−1}，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='其中D 比D′ 多一个样本xm，它们对应的假设空间可以表示为：\\nH|D = {(h (x1) , h (x2) , . . . , h (xm)) |h ∈H}\\nH|D′ = {(h (x1) , h (x2) , . . . , h (xm−1)) |h ∈H}\\n如果假设h ∈H 对xm 的分类结果为+1，或为−1，那么任何出现在H|D′ 中的串都会在H|D 中出\\n现一次或者两次。这里举个例子就很容易理解了，假设m = 3：\\nH|D = {(+, −, −), (+, +, −), (+, +, +), (−, +, −), (−, −, +)}\\nH|D′ = {(+, +), (+, −), (−, +), (−, −)}\\n其中串(+, +) 在H|D 中出现了两次(+, +, +), (+, +, −)，H|D′ 中得其他串(+, −), (−, +), (−, −) 均\\n只在H|D 中出现了一次。这里的原因是每个样本是二分类的，所以多出的样本xm 要么取+，要么取−，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='只在H|D 中出现了一次。这里的原因是每个样本是二分类的，所以多出的样本xm 要么取+，要么取−，\\n要么都取到(至少两个假设h 对xm 做出了不一致的判断)。记号HD′|D 表示在H|D 中出现了两次的H|D′\\n组成的集合，比如在上例中HD′|D = {(+, +)}，有\\n\\x0c\\x0cH|D\\n\\x0c\\x0c =\\n\\x0c\\x0cH|D′\\n\\x0c\\x0c +\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c\\n由于H|D′ 表示限制在样本集D′ 上的假设空间H 的表达能力(即所有假设对样本集D′ 所能赋予的\\n标记种类数)，样本集D′ 的数目为m −1，根据增长函数的定义，假设空间H 对包含m −1 个样本的集合\\n所能赋予的最大标记种类数为ΠH(m −1)，因此|H|D′| ⩽ΠH(m −1)。又根据数学归纳法的前提假设，有：\\n\\x0c\\x0cH|D′\\n\\x0c\\x0c ⩽ΠH(m −1) ⩽\\nd\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n由记号H|D′ 的定义可知，|H|D′| ⩾\\nj\\n|H|D|\\n2\\nk\\n，又由于|H|D′| 和|HD′|D| 均为整数，因此|HD′|D| ⩽\\nj\\n|H|D|\\n2\\nk\\n，由于样本集D 的大小为m，根据增长函数的概念，有\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c ⩽\\nj'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2\\nk\\n，由于样本集D 的大小为m，根据增长函数的概念，有\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c ⩽\\nj\\n|H|D|\\n2\\nk\\n⩽ΠH(m −1)。假设\\nQ 表示能被HD′|D 打散的集合，因为根据HD′|D 的定义，HD 必对元素xm 给定了不一致的判定，因此\\nQ ∪{xm} 必能被H|D 打散，由前提假设H 的VC 维为d，因此HD′|D 的VC 维最大为d −1，综上有\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c ⩽ΠH(m −1) ⩽\\nd−1\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n因此：\\n\\x0c\\x0cH|D\\n\\x0c\\x0c =\\n\\x0c\\x0cH|D′\\n\\x0c\\x0c +\\n\\x0c\\x0cHD′|D\\n\\x0c\\x0c\\n⩽\\nd\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n+\\nd+1\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n=\\nd\\nX\\ni=0\\n  \\nm −1\\ni\\n!\\n+\\n \\nm −1\\ni −1\\n!!\\n=\\nd\\nX\\ni=0\\n \\nm\\ni\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n注：最后一步依据组合公式，推导如下：\\n \\nm −1\\ni\\n!\\n+\\n \\nm −1\\ni −1\\n!\\n=\\n(m −1)!\\n(m −1 −i)!i! +\\n(m −1)!\\n(m −1 −i + 1)!(i −1)!\\n=\\n(m −1)!(m −i)\\n(m −i)(m −1 −i)!i! +\\n(m −1)!i\\n(m −i)!(i −1)!i\\n= (m −1)!(m −i) + (m −1)!i\\n(m −i)!i!\\n= (m −1)!(m −i + i)\\n(m −i)!i!\\n= (m −1)!m\\n(m −i)!i!\\n=\\nm!\\n(m −i)!i! =\\n \\nm\\ni\\n!\\n12.4.5\\n式(12.28) 的解释\\nΠH(m) ⩽\\nd\\nX\\ni=0\\n \\nm\\ni\\n!\\n⩽\\nd\\nX\\ni=0\\n \\nm\\ni\\n! \\x10m\\nd\\n\\x11d−i\\n=\\n\\x10m\\nd\\n\\x11d\\nd\\nX\\ni=0\\n \\nm\\ni\\n! \\x12 d\\nm\\n\\x13i\\n⩽\\n\\x10m\\nd\\n\\x11d\\nm\\nX\\ni=0\\n \\nm\\ni\\n! \\x12 d\\nm\\n\\x13i\\n=\\n\\x10m\\nd\\n\\x11d\\x12\\n1 + d\\nm\\n\\x13m\\n<\\n\\x10e · m\\nd\\n\\x11d'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i\\n! \\x12 d\\nm\\n\\x13i\\n=\\n\\x10m\\nd\\n\\x11d\\x12\\n1 + d\\nm\\n\\x13m\\n<\\n\\x10e · m\\nd\\n\\x11d\\n第一步到第二步和第三步到第四步均因为m ⩾d，第四步到第五步是由于二项式定理[3]：(x + y)n =\\nPn\\nk=0\\n \\nn\\nk\\n!\\nxn−kyk，其中令k = i, n = m, x = 1, y =\\nd\\nm 得\\n\\x00 m\\nd\\n\\x01d Pm\\ni=0\\n \\nm\\ni\\n!\\n\\x00 d\\nm\\n\\x01i =\\n\\x00 m\\nd\\n\\x01d (1 + d\\nm)m，\\n最后一步的不等式即需证明\\n\\x001 + d\\nm\\n\\x01m ⩽ed，因为\\n\\x001 + d\\nm\\n\\x01m =\\n\\x001 + d\\nm\\n\\x01 m\\nd d，根据自然对数底数e 的定义\\n[4]，\\n\\x001 + d\\nm\\n\\x01 m\\nd d < ed，注意原文中用的是⩽，但是由于e = lim d\\nm →0\\n\\x001 + d\\nm\\n\\x01 m\\nd 的定义是一个极限，所以\\n应该是用<。\\n12.4.6\\n式(12.29) 的解释\\n这里应该是作者的笔误，根据式12.22，E(h)−bE(h) 应当被绝对值符号包裹。将式12.28 带入式12.22\\n得\\nP\\n\\x10\\n|E(h) −bE(h)| > ϵ\\n\\x11\\n⩽4\\n\\x122em\\nd\\n\\x13d'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='得\\nP\\n\\x10\\n|E(h) −bE(h)| > ϵ\\n\\x11\\n⩽4\\n\\x122em\\nd\\n\\x13d\\nexp\\n\\x12\\n−mϵ2\\n8\\n\\x13\\n令4\\n\\x00 2em\\nd\\n\\x01d exp\\n\\x10\\n−mϵ2\\n8\\n\\x11\\n= δ 可解得\\nδ =\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ\\nm\\n带入式12.22，则定理得证。这个式子是用VC 维表示泛化界，可以看出，泛化误差界只与样本数量m 有\\n关，收敛速率为\\nq\\nln m\\nm\\n(书上简化为\\n1\\n√m)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.4.7\\n式(12.30) 的解释\\n这个是经验风险最小化的定义式。即从假设空间中找出能使经验风险最小的假设。\\n12.4.8\\n定理12.4 的解释\\n首先回忆PAC 可学习的概念，见定义12.2，而可知/不可知PAC 可学习之间的区别仅仅在于概念类\\nc 是否包含于假设空间H 中。令\\nδ′ = δ\\n2\\nr\\n(ln 2/δ′)\\n2m\\n= ϵ\\n2\\n结合这两个标记的转换，由推论12.1 可知：\\nbE(g) −ϵ\\n2 ⩽E(g) ⩽bE(g) + ϵ\\n2\\n至少以1 −δ/2 的概率成立。写成概率的形式即：\\nP\\n\\x10\\n|E(g) −bE(g)| ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ/2\\n即P\\n\\x10\\x10\\nE(g) −bE(g) ⩽ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ/2，因此P\\n\\x10\\nE(g) −bE(g) ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ/2 且\\nP\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n⩾1 −δ/2 成立。再令\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ′\\nm\\n= ϵ\\n2\\n由式12.29 可知\\nP'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='s\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ′\\nm\\n= ϵ\\n2\\n由式12.29 可知\\nP\\n\\x10\\x0c\\x0c\\x0cE(h) −bE(h)\\n\\x0c\\x0c\\x0c ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ\\n2\\n同理，P\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\n⩾1−δ/2 且P\\n\\x10\\nE(h) −bE(h) ⩾−ϵ\\n2\\n\\x11\\n⩾1−δ/2 成立。由P\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n⩾\\n1−δ/2 和P\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\n⩾1−δ/2 均成立可知则事件E(g)−bE(g) ⩾−ϵ\\n2 和事件E(h)−bE(h) ⩽ϵ\\n2\\n同时成立的概率为：\\nP\\n\\x10\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n=P\\n\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n+ P\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\n−P\\n\\x10\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∨\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ/2 + 1 −δ/2 −1\\n=1 −δ\\n即\\nP\\n\\x10\\x10\\nE(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='E(g) −bE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −bE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ\\n因此\\nP\\n\\x10\\nbE(g) −E(g) + E(h) −bE(h) ⩽ϵ\\n2 + ϵ\\n2\\n\\x11\\n= P\\n\\x10\\nE(h) −E(g) ⩽bE(h) −bE(g) + ϵ\\n\\x11\\n⩾1 −δ\\n再由h 和g 的定义，h 表示假设空间中经验误差最小的假设，g 表示泛化误差最小的假设，将这两个假设\\n共用作用于样本集D，则一定有bE(h) ⩽bE(g)，因此上式可以简化为：\\nP (E(h) −E(g) ⩽ϵ) ⩾1 −δ\\n根据式12.32 和式12.34，可以求出m 为关于(1/ϵ, 1/δ, size(x), size(c)) 的多项式，因此根据定理12.2，定\\n理12.5，得到结论任何VC 维有限的假设空间H 都是(不可知)PAC 可学习的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.5\\nRademacher 复杂度\\n上一节中介绍的基于VC 维的泛化误差界是分布无关、数据独立的，本节将要介绍的Rademacher 复\\n杂度则在一定程度上考虑了数据分布。\\n12.5.1\\n式(12.36) 的解释\\n这里解释从第一步到第二步的推导，因为前提假设是2 分类问题，yk ∈{−1, +1}，因此I (h(xi) ̸= yi) ≡\\n1−yih(xi)\\n2\\n。这是因为假如yi = +1, h(xi) = +1 或yi = −1, h(xi) = −1，有I (h(xi) ̸= yi) = 0 = 1−yih(xi)\\n2\\n；\\n反之，假如yi = −1, h(xi) = +1 或yi = +1, h(xi) = −1，有I (h(xi) ̸= yi) = 1 = 1−yih(xi)\\n2\\n。\\n12.5.2\\n式(12.37) 的解释\\n由公式12.36 可知，经验误差bE(h) 和\\n1\\nm\\nPm\\ni=1 yih (xi) 呈反比的关系，因此假设空间中能使经验误差\\n最小的假设h 即是使\\n1\\nm\\nPm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='最小的假设h 即是使\\n1\\nm\\nPm\\ni=1 yih (xi) 最大的h。\\n12.5.3\\n式(12.38) 的解释\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi)\\n[解析]：上确界sup 这个概念前面已经解释过，见式(12.7) 的解析。相比于式(12.37), 样例真实标记\\nyi 换为了Rademacher 随机变量σi, arg maxh∈H 换为了上确界suph∈H◦该式表示, 对于样例集D =\\n{x1, x2, . . . , xm}, 假设空间H 中的假设对其预测结果{h (x1) , h (x2) , . . . , h (xm)} 与随机变量集合σ =\\n{σ1, σ2, . . . , σm} 的契合程度。接下来解释一下该式的含义。1\\nm\\nPm\\ni=1 σih (xi) 中的σ = {σ1, σ2, . . . , σm} 表\\n示单次随机生成的结果（生成后就固定不动), 而{h (x1) , h (x2) , . . . , h (xm)} 表示某个假设h ∈H 的预测\\n结果, 至于\\n1\\nm\\nPm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='结果, 至于\\n1\\nm\\nPm\\ni=1 σih (xi) 的取值则取决于本次随机生成的σ 和假设h 的预测结果的契合程度。\\n进一步地, suph∈H\\n1\\nm\\nPm\\ni=1 σih (xi) 中的σ = {σ1, σ2, . . . , σm} 仍表示单次随机生成的结果(生成后就\\n固定不动), 但此时需求解的是假设空间H 中所有假设与σ 最契合的那个h 。\\n例如, σ = {−1, +1, −1, +1} （即m = 4, 这里σ 仅为本次随机生成结果而已, 下次生成结果可能是另\\n一组结果), 假设空间H = {h1, h2, h3}, 其中\\n{h1 (x1) , h1 (x2) , h1 (x3) , h1 (x4)} = {−1, −1, −1, −1}\\n{h2 (x1) , h2 (x2) , h2 (x3) , h2 (x4)} = {−1, +1, −1, −1}\\n{h3 (x1) , h3 (x2) , h3 (x3) , h3 (x4)} = {+1, +1, +1, +1}\\n易知\\n1\\nm\\nPm\\ni=1 σih1 (xi) = 0, 1\\nm\\nPm'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='易知\\n1\\nm\\nPm\\ni=1 σih1 (xi) = 0, 1\\nm\\nPm\\ni=1 σih2 (xi) = 2\\n4, 1\\nm\\nPm\\ni=1 σih3 (xi) = 0, 因此\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi) = 2\\n4\\n12.5.4\\n式(12.39) 的解释\\nEσ\\n\"\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi)\\n#\\n[解析]：这个式子可以用来衡量假设空间H 的表达能力，对变量σ 求期望可以理解为当变量σ 包含所有\\n可能的结果时，假设空间H 中最契合的假设h 和变量的平均契合程度。因为前提假设是2 分类的问题，\\n因此σi 一共有2m 种，这些不同的σi 构成了数据集D = {(x1, y1), (x2, y2), . . . , (xm, ym)} 的”对分“(12.4\\n节)，如果一个假设空间的表达能力越强，那么就越有可能对于每一种σi，假设空间中都存在一个h 使得\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nh(xi) 和σi 非常接近甚至相同，对所有可能的σi 取期望即可衡量假设空间的整体表达能力，这就是这个\\n式子的含义。\\n12.5.5\\n式(12.40) 的解释\\n对比式12.39，这里使用函数空间F 代替了假设空间H，函数f 代替了假设h，很容易理解，因为假设\\nh 即可以看做是作用在数据xi 上的一个映射，通过这个映射可以得到标签yi。注意前提假设实值函数空间\\nF : Z →R，即映射f 将样本zi 映射到了实数空间，这个时候所有的σi 将是一个标量即σi ∈{+1, −1}。\\n12.5.6\\n式(12.41) 的解释\\n这里所要求的是F 关于分布D 的Rademacher 复杂度，因此从D 中采出不同的样本Z，计算这些\\n样本对应的Rademacher 复杂度的期望。\\n12.5.7\\n定理12.5 的解释\\n首先令记号\\nbEZ(f) = 1\\nm\\nm\\nX\\ni=1\\nf (zi)\\nΦ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nm\\nX\\ni=1\\nf (zi)\\nΦ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11\\n即bEZ(f) 表示函数f 作为假设下的经验误差，Φ(Z) 表示泛化误差和经验误差的差的上确界。再令\\nZ′ 为只与Z 有一个示例(样本) 不同的训练集，不妨设zm ∈Z 和z′\\nm ∈Z′ 为不同的示例，那么有\\nΦ (Z′) −Φ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −bEZ′(f)\\n\\x11\\n−sup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11\\n⩽sup\\nf∈F\\n\\x10\\nbEZ(f) −bEZ′(f)\\n\\x11\\n= sup\\nf∈F\\nPm\\ni=1 f(zi) −Pm\\ni=1 f(z′\\ni)\\nm\\n= sup\\nf∈F\\nf (zm) −f (z′\\nm)\\nm\\n⩽1\\nm\\n第一个不等式是因为上确界的差不大于差的上确界[5]，第四行的等号由于Z′ 与Z 只有zm 不相同，\\n最后一行的不等式是因为前提假设F : Z →[0, 1]，即f(zm), f(z′\\nm) ∈[0, 1]。同理\\nΦ(Z) −Φ (Z′) = sup\\nf∈F\\nf (z′\\nm) −f (zm)\\nm\\n⩽1\\nm\\n综上二式有：'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='f∈F\\nf (z′\\nm) −f (zm)\\nm\\n⩽1\\nm\\n综上二式有：\\n|Φ(Z) −Φ (Z′)| ⩽1\\nm\\n将Φ 看做函数f(注意这里的f 不是Φ 定义里的f)，那么可以套用McDiarmid 不等式的结论式12.7\\nP (Φ(Z) −EZ[Φ(Z)] ⩾ϵ) ⩽exp\\n\\x12 −2ϵ2\\nP\\ni c2\\ni\\n\\x13\\n令exp\\n\\x10\\n−2ϵ2\\n∑\\ni c2\\ni\\n\\x11\\n= δ 可以求得ϵ =\\nq\\nln(1/δ)\\n2m ，所以\\nP\\n \\nΦ(Z) −EZ[Φ(Z)] ⩾\\nr\\nln(1/δ)\\n2m\\n!\\n⩽δ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n由逆事件的概率定义得\\nP\\n \\nΦ(Z) −EZ[Φ(Z)] ⩽\\nr\\nln(1/δ)\\n2m\\n!\\n⩾1 −δ\\n即书中式12.44 的结论。下面来估计EZ[Φ(Z)] 的上界：\\nEZ[Φ(Z)] = EZ\\n\\x14\\nsup\\nf∈F\\n\\x10\\nE[f] −bEZ(f)\\n\\x11\\x15\\n= EZ\\n\\x14\\nsup\\nf∈F\\nEZ′\\nh\\nbEZ′(f) −bEZ(f)\\ni\\x15\\n⩽EZ,Z′\\n\\x14\\nsup\\nf∈F\\n\\x10\\nbEZ′(f) −bEZ(f)\\n\\x11\\x15\\n= EZ,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\n(f (z′\\ni) −f (zi))\\n#\\n= Eσ,Z,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσi (f (z′\\ni) −f (zi))\\n#\\n⩽Eσ,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (z′\\ni)\\n#\\n+ Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\n−σif (zi)\\n#\\n= 2Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (zi)\\n#\\n= 2Rm(F)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= 2Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (zi)\\n#\\n= 2Rm(F)\\n第二行等式是外面套了一个对服从分布D 的示例集Z′ 求期望，因为EZ′∼D[ bEZ′(f)] = E(f)，而采样\\n出来的Z′ 和Z 相互独立，因此有EZ′∼D[ bEZ(f)] = bEZ(f)。\\n第三行不等式基于上确界函数sup 是个凸函数，将supf∈F 看做是凸函数f，将bEZ′(f)−bEZ(f) 看做变\\n量x 根据Jesen 不等式(式12.4)，有EZ\\nh\\nsupf∈F EZ′\\nh\\nbEZ′(f) −bEZ(f)\\nii\\n⩽EZ,Z′\\nh\\nsupf∈F\\n\\x10\\nbEZ′(f) −bEZ(f)\\n\\x11i\\n，\\n其中EZ,Z′[·] 是EZ[EZ′[·]] 的简写形式。\\n第五行引入对Rademacher 随机变量的期望，由于函数值空间是标量，因为σi 也是标量，即σi ∈\\n{−1, +1}，且σi 总以相同概率可以取到这两个值，因此可以引入Eσ 而不影响最终结果。\\n第六行利用了上确界的和不小于和的上确界[5]，因为第一项中只含有变量z′，所以可以将EZ 去掉，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='第六行利用了上确界的和不小于和的上确界[5]，因为第一项中只含有变量z′，所以可以将EZ 去掉，\\n因为第二项中只含有变量z，所以可以将EZ′ 去掉。\\n第七行利用σ 是对称的，所以−σ 的分布和σ 完全一致，所以可以将第二项中的负号去除，又因为\\nZ 和Z′ 均是从D 中i.i.d. 采样得到的数据，因此可以将第一项中的z′\\ni 替换成z，将Z′ 替换成Z。\\n最后根据定义式12.41 可得EZ[Φ(Z)] = 2Rm(F)，式(12.42) 得证。\\n12.6\\n定理12.6 的解释\\n针对二分类问题, 定理12.5 给出了“泛化误差”和“经验误差”的关系, 即:\\n• 式(12.47) 基于Rademacher 复杂度Rm(H) 给出了泛化误差E(h) 的上界;\\n• 式(12.48) 基于经验Rademacher 复杂度bRD(H) 给出了泛化误差E(h) 的上界。\\n可能大家都会有疑问：定理12.6 的设定其实也适用于定理12.5, 即值域为二值的{−1, +1} 也属于值\\n域为连续值的[0, 1] 的一种特殊情况, 这一点从接下来的式(12.49) 的转换可以看出。那么, 为什么还要针'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='对二分类问题专门给出定理12.6 呢?\\n根据(经验)Rademacher 复杂度的定义可以知道, Rm(H) 和bRD(H) 均大于零(参见前面有关式(12.39)\\n的解释, 书中式(12.39) 下面的一行也提到该式取值范围是[0, 1]); 因此, 相比于定理12.5 来说, 定理12.6 的\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n上界更紧, 因为二者的界只有中间一项关于(经验)Rademacher 复杂度的部分不同, 在定理12.5 中是两倍\\n的(经验)Rademacher 复杂度, 而在定理12.6 中是一倍的(经验)Rademacher 复杂度, 而(经验)Rademacher\\n复杂度大于零。\\n因此, 为二分类问题量身定制的定理12.6 相比于通用的定理12.5 来说, 二者的区别在于定理12.6 考\\n虑了二分类的特殊情况, 得到了比定理12.5 更紧的泛化误差界, 仅此而已。\\n下面做一些证明：\\n(1) 首先通过式(12.49) 将值域为{−1, +1} 的假设空间H 转化为值域为[0, 1] 的函数空间FH ;\\n(2) 接下来是该证明最核心部分, 即证明式(12.50) 的结论bRZ (FH) = 1\\n2 bRD(H) : 第1 行等号就是定\\n义12.8; 第2 行等号就是根据式(12.49) 将fh (xi, yi) 换为I (h (xi) ̸= yi); 第3 行等号类似于式(12.36) 的'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='第2 个等号; 第4 行等号说明如下:\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσi\\n1 −yih (xi)\\n2\\n= sup\\nh∈H\\n1\\n2m\\nm\\nX\\ni=1\\nσi + sup\\nh∈H\\n1\\n2m\\nm\\nX\\ni=1\\n−yiσih (xi)\\n2\\n其中suph∈H\\n1\\n2m\\nPm\\ni=1 σi 与h 无关, 所以suph∈H\\n1\\n2m\\nPm\\ni=1 σi =\\n1\\n2m\\nPm\\ni=1 σi, 即第4 行等号; 第5 行等号是\\n由于Eσ\\n\\x02 1\\nm\\nPm\\ni=1 σi\\n\\x03\\n= 0, 例如当m = 2 时, 所有可能得σ 包括(−1, −1), (−1, +1), (+1, −1) 和(+1, +1),\\n求期望后显然结果等于0 ; 第6 行等号正如边注所说, “−yiσi 与σi 分布相同”(原因跟定理12.5 中证明\\nEZ[Φ(Z)] ⩽2Rm(F) 相同, 即求期望时要针对所有可能的σ 参见“西瓜书”第282 页第8 行); 第7 行等\\n号再次使用了定义12.8。\\n(3) 关于式(12.51), 根据式(12.50) 的结论, 可证明如下:\\nRm (FH) = EZ\\nh\\nbRZ (FH)\\ni'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Rm (FH) = EZ\\nh\\nbRZ (FH)\\ni\\n= ED\\n\\x141\\n2\\nbRD(H)\\n\\x15\\n= 1\\n2ED\\nh\\nbRD(H)\\ni\\n= 1\\n2Rm(H)\\n其中第2 个等号由Z 变为D 只是符号根据具体情况的适时变化而已。\\n(4) 最后, 将式(12.49) 定义的fh 替换定理12.5 中的函数f, 则\\nE[f(z)] = E[I(h(x) ̸= y)] = E(h)\\n1\\nm\\nm\\nX\\ni=1\\nf (zi) = 1\\nm\\nm\\nX\\ni=1\\nI (h (xi) ̸= yi) = bE(h)\\n将式(12.51) 代入式(12.42), 即用1\\n2Rm(H) 替换式(12.42) 的Rm(F), 式(12.47) 得证;\\n将式(12.50) 代入式(12.43), 即用1\\n2 bRD(H) 替换式(12.43) 的bRZ(F), 式(12.48) 得证。\\n这里有个疑问在于，定理12.5 的前提是“实值函数空间F : Z →[0, 1] ”, 而式(12.49) 得到的函数\\nfh(z) 的值域实际为{0, 1}, 仍是离散的而非实值的; 当然, 定理12.5 的证明也只需要其函数值在[0, 1] 范'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='围内即可, 并不需要其连续。\\n12.6.1\\n式(12.52) 的证明\\n比较繁琐，同书上所示，参见Foundations of Machine Learning[6]\\n12.6.2\\n式(12.53) 的推导\\n根据式12.28 有ΠH(m) ⩽\\n\\x00 e·m\\nd\\n\\x01d，根据式12.52 有Rm(H) ⩽\\nq\\n2 ln ΠH(m)\\nm\\n，因此ΠH(m) ⩽\\nq\\n2d ln em\\nd\\nm\\n，\\n再根据式12.47 E(h) ⩽bE(h) + Rm(H) +\\nq\\nln(1/δ)\\n2m\\n即证。\\n12.7\\n稳定性\\n上上节中介绍的基于VC 维的泛化误差界是分布无关、数据独立的，上一节介绍的Rademacher 复杂\\n度则在一定程度上考虑了数据分布，但二者得到的结果均与具体学习算法无关；本节将要介绍的稳定性分\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n析可以获得与算法有关的分析结果。算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之\\n发生较大的变化。\\n12.7.1\\n泛化/经验/留一损失的解释\\n根据式(12.54) 上方关于损失函数的描述：“刻画了假设的预测标记与真实标记之间的差别”，这里针\\n对的是二分类，预测标记和真实标记均只能取和两个值，它们之间的“差别”又能是什么呢？\\n因此，当“差别”取为时，式(12.54) 的泛化损失就是式(12.1) 的泛化误差，式(12.55) 的经验损失\\n就是式(12.2) 的经验误差，如果类似于式(12.1) 和式(12.2) 继续定义留一误差，那么式(12.56) 就对应\\n于留一误差。\\n12.7.2\\n式(12.57) 的解释\\n根据三角不等式[7]，有|a + b| ≤|a| + |b|，将a = ℓ(LD, z) −ℓ(LDi)，b = ℓ(LDi,z) −ℓ\\n\\x00LD\\\\i,z\\n\\x01\\n带入\\n即可得出第一个不等式，根据D\\\\i 表示移除D 中第i 个样本，Di 表示替换D 中第i 个样本，那么a, b'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的变动均为一个样本，根据式12.57，a ⩽β, b ⩽β，因此a + b ⩽2β。\\n12.7.3\\n定理12.8 的解释\\n西瓜书在该定理下方已明确给出该定理的意义, 即“定理12.8 给出了基于稳定性分析推导出的学习算\\n法L 学得假设的泛化误差界”, 式(12.58) 和式(12.59) 分别基于经验损失和留一损失给出了泛化损失的\\n上界。接下来讨论两个相关问题:\\n(1) 定理12.8 的条件包括损失函数有界, 即0 ⩽ℓ(LD, z) ⩽M; 如本节第1 条注解“泛化/经验/留一\\n损失的解释”中所述, 若“差别”取为I (LD(x), y), 则泛化损失对应于泛化误差, 此时上限M = 1 。\\n(2) 在前面泛化误差上界的推导中（例如定理12.1、定理12.3、定理12.6、定理12.7), 上界中与样本数\\nm 有关的项收玫率均为O(1/√m), 但在该定理中却是O(β√m); 一般来讲, 随着样本数m 的增加, 经验误\\n差/损失应该收玫于泛化误差/损失, 因此这里假设β = 1/m (书中式(12.59) 下方第3 行写为β = O(1/m)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='), 而在第2 条注解“定义12.10 的解释”中已经提到β 的取值的确会随着样本数m 的增多会变小, 虽然\\n书中并没有严格去讨论β 随m 增多的变化规律, 但至少直觉上是对的。\\n12.7.4\\n式(12.60) 的推导\\n将β =\\n1\\nm 带入至式(12.58) 即得证。\\n12.7.5\\n经验损失最小化\\n顾名思义, “经验损失最小化”指通过最小化经验损失来求得假设函数。\\n这里, “对于损失函数ℓ, 若学习算法L 所输出的假设满足经验损失最小化, 则称算法L 满足经验风险\\n最小化原则, 简称算法是ERM 的”。在”西瓜书”第278 页, 若学习算法L 输出的假设h 满足式(12.30),\\n则也称L 为满足经验风险最小化原则的算法。而很明显, 式(12.30) 是在最小化经验误差。\\n那么最小化经验误差和最小化经验损失有什么区别么?\\n在” 西瓜书“第286 页左下角边注中提到, “最小化经验误差和最小化经验损失有时并不相同, 这是由\\n于存在某些病态的损失函数ℓ使得最小化经验损失并不是最小化经验误差”。\\n对于“误差”、“损失”、“风险”等概念的辨析，参见“西瓜书”第2 章2.1 节的注解。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='对于“误差”、“损失”、“风险”等概念的辨析，参见“西瓜书”第2 章2.1 节的注解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n12.7.6\\n定理(12.9) 的证明的解释\\n首先明确几个概念，ERM 表示算法L 满足经验风险最小化(Empirical Risk Minimization)。由于L\\n满足经验误差最小化，则可令g 表示假设空间中具有最小泛化损失的假设，即\\nℓ(g, D) = min\\nh∈H ℓ(h, D)\\n再令\\nϵ′ = ϵ\\n2\\nδ\\n2 = 2 exp\\n\\x10\\n−2m (ϵ′)2\\x11\\n将ϵ′ = ϵ\\n2 带入到δ\\n2 = 2 exp\\n\\x10\\n−2m (ϵ′)2\\x11\\n可以解得m =\\n2\\nϵ2 ln 4\\nδ，由Hoeffding 不等式12.6，\\nP\\n \\x0c\\x0c\\x0c\\x0c\\x0c\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi)\\n\\x0c\\x0c\\x0c\\x0c\\x0c ⩾ϵ\\n!\\n⩽2 exp\\n\\x00−2mϵ2\\x01\\n其中\\n1\\nm\\nPm\\ni=1 E (xi) = ℓ(g, D)，1\\nm\\nPm\\ni=1 xi = bℓ(g, D)，带入可得\\nP(|ℓ(g, D) −bℓ(g, D)| ⩾ϵ\\n2) ⩽δ\\n2\\n根据逆事件的概率可得\\nP(|ℓ(g, D) −bℓ(g, D)| ⩽ϵ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='2) ⩽δ\\n2\\n根据逆事件的概率可得\\nP(|ℓ(g, D) −bℓ(g, D)| ⩽ϵ\\n2) ⩾1 −δ\\n2\\n即文中|ℓ(g, D) −bℓ(g, D)| ⩽ϵ\\n2 至少以1 −δ/2 的概率成立。\\n由\\n2\\nm + (4 + M)\\nq\\nln(2/δ)\\n2m\\n= ϵ\\n2 可以求解出\\n√m =\\n(4 + M)\\nq\\nln(2/δ)\\n2\\n+\\nq\\n(4 + M)2 ln(2/δ)\\n2\\n−4 × ϵ\\n2 × (−2)\\n2 × ϵ\\n2\\n即m = O\\n\\x00 1\\nϵ2 ln 1\\nδ\\n\\x01\\n。\\n由P(|ℓ(g, D) −bℓ(g, D)| ⩽ϵ\\n2) ⩾1 −δ\\n2 可以按照同公式12.31 中介绍的相同的方法推导出\\nP(ℓ(L, D) −ℓ(g, D) ⩽ϵ) ⩾1 −δ\\n又因为m 为与(1/ϵ, 1/δ, size(x), size(c)) 相关的多项式的值，因此根据定理12.2，定理12.5，得到结\\n论H 是(不可知)PAC 可学习的。\\n参考文献'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='论H 是(不可知)PAC 可学习的。\\n参考文献\\n[1] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the\\nAmerican statistical association, 58(301):13–30, 1963.\\n[2] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of\\nevents to their probabilities. In Measures of complexity, pages 11–30. Springer, 2015.\\n[3] Wikipedia contributors. Binomial theorem, 2020.\\n[4] Wikipedia contributors. E, 2020.'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='[4] Wikipedia contributors. E, 2020.\\n[5] robjohn. Supremum of the difference of two functions, 2013.\\n[6] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. 2018.\\n[7] Wikipedia contributors. Triangle inequality, 2020.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第13 章\\n半监督学习\\n13.1\\n未标记样本\\n“西瓜书”两张插图可谓本节亮点: 图13.1 直观地说明了使用末标记样本后带来的好处; 图13.2 对比\\n了主动学习、(纯) 半监督学习和直推学习, 尤其是巧妙地将主动学习的概念融入进来。\\n直推学习是综合运用手头上已有的少量有标记样本和大量末标记样本, 对这些大量末标记样本预测其\\n标记; 而(纯) 半监督学习是综合运用手头上已有的少量有标记样本和大量末标记样本, 对新的末标记样本\\n预测其标记。\\n对于直推学习, 当然可以仅利用有标记样本训练一个学习器, 再对末标记样本进行预测, 此即传统的监\\n督学习; 对于(纯) 半监督学习, 当然也可以舍弃大量末标记样本, 仅利用有标记样本训练一个学习器, 再对\\n新的末标记样本进行预测。但图13.1 直观地说明了使用末标记样本后带来的好处, 然而利用了末标记样本\\n后是否真的会如图13.1 所示带来预期的好处呢? 此即13.7 节阅读材料中提到的安全半监督学习。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='后是否真的会如图13.1 所示带来预期的好处呢? 此即13.7 节阅读材料中提到的安全半监督学习。\\n接下来在13.2 节、13.3 节、13.4 节、13.5 节介绍的四种半监督学习方法, 都可以应用于直推学习, 但\\n若要应用于(纯) 半监督学习, 则要有额外的考虑, 尤其是13.4 节介绍的图半监督学习, 因为该节最后一段\\n也明确提到“构图过程仅能考虑训练样本集, 难以判知新样本在图中的位置, 因此, 在接收到新样本时, 或\\n是将其加入原数据集对图进行重构并重新进行标记传播, 或是需引入额外的预测机制”。\\n13.2\\n生成式方法\\n本节与9.4.3 节的高斯混合聚类密切相关, 有关9.4.3 节的公式推导参见附录, 建议将高斯混合聚类的\\n内容理解之后再学习本节算法。\\n13.2.1\\n式(13.1) 的解释\\n高斯混合分布的定义式。该式即为9.4.3 节的式(9.29)，式(9.29) 中的k 个混合成分对应于此处的N\\n个可能的类别。\\n13.2.2\\n式(13.2) 的推导'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='个可能的类别。\\n13.2.2\\n式(13.2) 的推导\\n首先, 该式的变量Θ ∈{1, 2, . . . , N} 即为式(9.30) 中的zj ∈{1, 2, . . . , k} 。\\n从公式第1 行到第2 行是对概率进行边缘化(marginalization)；通过引入Θ 并对其求和PN\\ni=1 以抵\\n消引入的影响。从公式第2 行到第3 行推导如下\\np(y = j, Θ = i|x) = p(y = j, Θ = i, x)\\np(x)\\n= p(y = j, Θ = i, x)\\np(Θ = i, x)\\n· p(Θ = i, x)\\np(x)\\n= p(y = j|Θ = i, x) · p(Θ = i|x)\\np(y = j | x) 表示x 的类别y 为第j 个类别标记的后验概率（注意条件是已知x);\\np(y = j, Θ = i | x) 表示x 的类别y 为第j 个类别标记且由第i 个高斯混合成分生成的后验概率（注\\n意条件是已知x );\\np(y = j | Θ = i, x) 表示第i 个高斯混合成分生成的x 其类别y 为第j 个类别标记的概率（注意条件'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='是已知Θ 和x, 这里修改了西瓜书式(13.3) 下方对p(y = j | Θ = i, x) 的表述);\\np(Θ = i | x) 表示x 由第i 个高斯混合成分生成的后验概率（注意条件是已知x) 。\\n“西瓜书”第296 页第2 行提到“假设样本由高斯混合模型生成, 且每个类别对应一个高斯混合成分”\\n, 也就是说, 如果已知x 是由哪个高斯混合成分生成的, 也就知道了其类别。而p(y = j | Θ = i, x) 表示已\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n知Θ 和x 的条件概率（已知Θ 就足够, 不需x 的信息), 因此\\np(y = j | Θ = i, x) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1,\\ni = j\\n0,\\ni ̸= j\\n13.2.3\\n式(13.3) 的推导\\n根据式(13.1)\\np(x) =\\nN\\nX\\ni=1\\nαi · p (x|µi, Σi)\\n因此\\np(Θ = i|x) = p(Θ = i, x)\\nP(x)\\n=\\nαi · p (x|µi, Σi)\\nPN\\ni=1 αi · p (x|µi, Σi)\\n13.2.4\\n式(13.4) 的推导\\n第二项很好解释，当不知道类别信息的时候，样本xj 的概率可以用式13.1 表示，所有无类别信息的\\n样本Du 的似然是所有样本的乘积，因为ln 函数是单调的，所以也可以将ln 函数作用于这个乘积消除因\\n为连乘产生的数值计算问题。第一项引入了样本的标签信息，由\\np(y = j|Θ = i, x) =\\n(\\n1,\\ni = j\\n0,\\ni ̸= j\\n可知，这项限定了样本xj 只可能来自于yj 所对应的高斯分布。\\n13.2.5\\n式(13.5) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='13.2.5\\n式(13.5) 的解释\\n参见式(13.3)，这项可以理解成样本xj 属于类别标签i(或者说由第i 个高斯分布生成) 的后验概率。\\n其中αi, µiΣi 可以通过有标记样本预先计算出来。即：\\nαi =\\nli\\n|Dl|, where |Dl| = PN\\ni=1 li\\nµi = 1\\nli\\nP\\n(xj,yj)∈Dl∧yj=i xj\\nΣi = 1\\nli\\nP\\n(xj,yj)∈Dl∧yj=i (xj −µi) (xj −µi)⊤\\n其中li 表示第i 类样本的有标记样本数目, |Dl| 为有标记样本集样本总数, ∧为“逻辑与”。\\n13.2.6\\n式(13.6) 的解释\\n这项可以由\\n∂LL(Dl ∪Du)\\n∂µi\\n= 0\\n而得，将式13.4 的两项分别记为：\\nLL(Dl) =\\nX\\n(xj,yj∈Dl)\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs) · p(yi|Θ = s, xj)\\n!\\n=\\nX\\n(xj,yj∈Dl)\\nln\\n\\x10\\nαyj · p(xj|µyj, Σyj)\\n\\x11\\nLL(Du) =\\nX\\nxj∈Du\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs)\\n!\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='X\\nxj∈Du\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs)\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n首先，LL(Dl) 对µi 求偏导，LL(Dl) 求和号中只有yj = i 的项能留下来，即\\n∂LL (Dl)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi)\\nLL(Du) 对µi 求导，参考9.33 的推导：\\n∂LL (Du)\\n∂µi\\n=\\nX\\nxj∈Du\\nαi\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n· p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n=\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi)\\n综上，\\n∂LL (Dl ∪Du)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='综上，\\n∂LL (Dl ∪Du)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi) +\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi)\\n= Σ−1\\ni\\n\\uf8eb\\n\\uf8ed\\nX\\n(xj,yj)∈Dl∧yj=i\\n(xj −µi) +\\nX\\nxj∈Du\\nγji · (xj −µi)\\n\\uf8f6\\n\\uf8f8\\n= Σ−1\\ni\\n\\uf8eb\\n\\uf8ed\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj +\\nX\\nxj∈Du\\nγji · xj −\\nX\\n(xj,yj)∈Dl∧yj=i\\nµi −\\nX\\nxj∈Du\\nγji · µi\\n\\uf8f6\\n\\uf8f8\\n令∂LL(Dl∪Du)\\n∂µi\\n= 0，两边同时左乘Σi 并移项：\\nX\\nxj∈Du\\nγji · µi +\\nX\\n(xj,yj)∈Dl∧yj=i\\nµi =\\nX\\nxj∈Du\\nγji · xj +\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj\\n上式中，µi 可以作为常量提到求和号外面，而P\\n(xj,yj)∈Dl∧yj=i 1 = li，即第i 类样本的有标记样本数\\n目，因此\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji +\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\n\\uf8f6\\n\\uf8f8µi =\\nX\\nxj∈Du\\nγji · xj +'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='X\\n(xj,yj)∈Dl∧yj=i\\n1\\n\\uf8f6\\n\\uf8f8µi =\\nX\\nxj∈Du\\nγji · xj +\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj\\n即得式(13.6)。\\n13.2.7\\n式(13.7) 的解释\\n首先LL(Dl) 对Σi 求偏导，类似于式(13.6)\\n∂LL (Dl)\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · p (xj|µi, Σi) ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n然后LL(Du) 对Σi 求偏导，类似于式(9.35)\\n∂LL (Du)\\n∂Σi\\n=\\nX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n综合可得：\\n∂LL (Dl ∪Du)\\n∂Σi\\n=\\nX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n+\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n+\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n\\uf8f6\\n\\uf8f8· 1\\n2Σ−1\\ni\\n令∂LL(Dl∪Du)\\n∂Σi\\n= 0，两边同时右乘2Σi 并移项：\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj∈Dl∧yj=i\\nΣ−1\\ni'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i\\n(xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤\\n=\\nX\\nxj∈Du\\nγji · I +\\nX\\n(xj,yj)∈Dl∧yj=i\\nI\\n=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji + li\\n\\uf8f6\\n\\uf8f8I\\n两边同时左乘以Σi：\\nX\\nxj∈Du\\nγji · (xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj)∈Dl∧yj=i\\n(xj −µi) (xj −µi)⊤=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji + li\\n\\uf8f6\\n\\uf8f8Σi\\n即得式(13.7)。\\n13.2.8\\n式(13.8) 的解释\\n类似于式(9.36)，写出LL(Dl ∪Du) 的拉格朗日形式\\nL (Dl ∪Du, λ) = LL (Dl ∪Du) + λ\\n N\\nX\\ns=1\\nαs −1\\n!\\n= LL (Dl) + LL (Du) + λ\\n N\\nX\\ns=1\\nαs −1\\n!\\n类似于式(9.37)，对αi 求偏导。对于LL(Du)，求导结果与式(9.37) 的推导过程一样\\n∂LL (Du)\\n∂αi\\n=\\nX\\nxj∈Du\\n1\\nPN\\ns=1 αs · p (xj|µs, Σs)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='∂αi\\n=\\nX\\nxj∈Du\\n1\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n· p (xj|µi, Σi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n对于LL(Dl)，类似于式(13.6) 和式(13.7) 的推导过程\\n∂LL (Dl)\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi · p (xj|µi, Σi) · ∂(αi · p (xj|µi, Σi))\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi · p (xj|µi, Σi) · p (xj|µi, Σi)\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi\\n= 1\\nαi\\n·\\nX\\n(xj,yj)∈Dl∧yj=i\\n1 = li\\nαi\\n上式推导过程中，重点注意变量是αi ，p(xj|µi, Σi) 是常量；最后一行αi 相对于求和变量为常量，因\\n此作为公因子提到求和号外面；li 为第i 类样本的有标记样本数目。\\n综合两项结果：\\n∂L (Dl ∪Du, λ)\\n∂αi\\n= li\\nαi\\n+\\nX\\nxj∈Du\\np (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='xj∈Du\\np (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ\\n令\\n∂LL(Dl ∪Du)\\n∂αi\\n= 0 并且两边同乘以αi，得\\nαi · li\\nαi\\n+\\nX\\nxj∈Du\\nαi · p (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ · αi = 0\\n结合式(9.30) 发现，求和号内即为后验概率γji, 即\\nli +\\nX\\nxi∈Du\\nγji + λαi = 0\\n对所有混合成分求和，得\\nN\\nX\\ni=1\\nli +\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji +\\nN\\nX\\ni=1\\nλαi = 0\\n这里PN\\ni=1 αi = 1 ，因此PN\\ni=1 λαi = λ PN\\ni=1 αi = λ，根据9.30 中γji 表达式可知\\nN\\nX\\ni=1\\nγji =\\nN\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\nΣN\\ns=1αs · p(xj|µs, Σs) =\\nPN\\ni=1 αi · p(xj|µi, Σi)\\nPN\\ns=1 αs · p(xj|µs, Σs)\\n= 1\\n再结合加法满足交换律，所以\\nN\\nX\\ni=1\\nX\\nxi∈Du'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= 1\\n再结合加法满足交换律，所以\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji =\\nX\\nxi∈Du\\nN\\nX\\ni=1\\nγji =\\nX\\nxi∈Du\\n1 = u\\n以上分析过程中，P\\nxj∈Du 形式与Pu\\nj=1 等价，其中u 为未标记样本集的样本个数；PN\\ni=1 li = l 其中\\nl 为有标记样本集的样本个数；将这些结果代入\\nN\\nX\\ni=1\\nli +\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji +\\nN\\nX\\ni=1\\nλαi = 0\\n解出l + u + λ = 0 且l + u = m 其中m 为样本总个数，移项即得λ = −m，最后带入整理解得\\nli +\\nX\\nxj∈Du\\nγji −λαi = 0\\n即li + P\\nxj∈Du γji −mαi = 0, 整理即得式13.8。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 168, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n13.3\\n半监督SVM\\n从本节名称“半监督SVM”即可知道与第6 章的SVM 内容联系紧密。建议理解了SVM 之后再学\\n习本节算法，会发现实际很简单；否则会感觉无从下手，难以理解。\\n由本节开篇的两段介绍可知，S3VM 是SVM 在半监督学习上的推广，是此类算法的总称而非某个具\\n体的算法，其最著名的代表是TSVM。\\n13.3.1\\n图13.3 的解释\\n注意对比S3VM 划分超平面穿过的区域与SVM 划分超平面穿过的区域的差别，明显S3VM 划分超\\n平面周围样本较少，也就是“数据低密度区域”，即“低密度分隔”。\\n13.3.2\\n式(13.9) 的解释\\n这个公式和式(6.35) 基本一致，除了引入了无标记样本的松弛变量ξi, i = l + 1, · · · m 和对应的权重\\n系数Cu 和无标记样本的标记指派ˆyi。因此，欲理解本节内容应该先理解SVM，否则会感觉无从下手，难\\n以理解。\\n13.3.3\\n图13.4 的解释\\n解释一下第6 行:'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 168, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='以理解。\\n13.3.3\\n图13.4 的解释\\n解释一下第6 行:\\n(1) ˆyiˆyj < 0 意味着末标记样本xi, xj 在此次迭代中被指派的标记ˆyi, ˆyj 相反(正例+1 和反例−1 各\\n1 个);\\n(2) ξi > 0 意味着末标记样本xi 在此次迭代中为支持向量: (a) 在间隔带内但仍与自己标记同侧\\n(0 < ξi < 1), (b) 在间隔带内但与自己标记异侧(1 < ξi < 2), (c) 不在间隔带且与自己标记异侧(ξi > 2);\\n三种情况分别如下图(a)(b)(c) 所示:\\n(3) ξi + ξj > 2 分两种情况: (I) (ξi > 1) ∧(ξj > 1), 表示都位于自己指派标记异侧, 交换它们的标记后,\\n二者就都位于自己新指派标记同侧了, 如下图所示(1 < ξi, ξj < 2) :\\n可以发现, 当1 < ξi, ξj < 2 时, 交换之后虽然松弛变量仍然大于0 , 但至少ξi + ξj 比交换之前变小了;\\n若进一步的, 当ξi, ξj > 2 时, 则交换之后ξi + ξj 将变为0 , 如下图所示:\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 168, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 169, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n可以发现, 交换之后两个样本均被分类正确, 因此松弛变量均等于0 ; 至于ξi, ξj 其中之一位于1 ∼2\\n之间, 另一个大于2 , 情况类似, 不单列出分析。\\n(II) (0 < ξi < 1) ∧(ξj > 2 −ξi), 表示有一个与自己标记同侧, 有一个与自己标记异侧, 此时可分两种\\n情况: (II.1) 1 < ξj < 2, 表示样本与自己标记异侧, 但仍在间隔带内:\\n可以发现, 此时两个样本位置超平面同一侧, 交换标记之后似乎没发生什么变化, 但是仔细观察会发现\\n交换之后ξi + ξj 比交换之前变小了; (II.2) ξj > 2, 表示样本在间隔带外:\\n可以发现, 交换之后其中之一被正确分类, ξi + ξj 比交换之前也变小了。综上所述, 当ξi + ξj > 2 时,\\n交换指派标记ˆyi, ˆyj 可以使ξi + ξj 下降, 也就是说分类结果会得到改善。再解释一下第11 行: 逐步增长\\nCu, 但不超过Cl, 末标记样本的权重小于有标记样本。\\n13.3.4\\n式(13.10) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 169, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='Cu, 但不超过Cl, 末标记样本的权重小于有标记样本。\\n13.3.4\\n式(13.10) 的解释\\n将该式变形为C+\\nu\\nC−\\nu = u−\\nu+ , 即样本个数多的权重小, 样本个数少的权重大, 总体上保持二者的作用相同。\\n13.4\\n图半监督学习\\n本节共讲了两种方法，其中式(13.11) ~ 式(13.17) 讲述了一个针对二分类问题的标记传播方法，式\\n(13.18) ~ 式(13.21) 讲述了一个针对多分类问题的标记传播方法，两种方法的原理均为两种方法的原理均\\n为“相似的样本应具有相似的标记”，只是面向的问题不同，而且具体实现的方法也不同。\\n13.4.1\\n式(13.12) 的推导\\n注意, 该方法针对二分类问题的标记传播方法。我们希望能量函数E(f) 越小越好, 注意到式(13.11)\\n的0 < (W)ij ⩽1, 且样本xi 和样本xj 越相似(即∥xi −xj∥2 越小) 则(W)ij 越大, 因此要求式(13.12)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n中的(f (xi) −f (xj))2 相应地越小越好(即“相似的样本应具有相似的标记”), 如此才能达到能量函数\\nE(f) 越小的目的。首先对式(13.12) 的第1 行式子进行展开整理:\\nE(f) = 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ij (f (xi) −f (xj))2\\n= 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ij\\n\\x00f 2 (xi) −2f (xi) f (xj) + f 2 (xj)\\n\\x01\\n= 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xi) + 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n然后证明Pm\\ni=1\\nPm\\nj=1(W)ijf 2 (xi) = Pm\\ni=1\\nPm\\nj=1(W)ijf 2 (xj), 并变形:\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) =\\nm\\nX\\nj=1\\nm\\nX\\ni=1\\n(W)jif 2 (xi) =\\nm\\nX\\ni=1\\nm\\nX'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nX\\nj=1\\nm\\nX\\ni=1\\n(W)jif 2 (xi) =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xi)\\n=\\nm\\nX\\ni=1\\nf 2 (xi)\\nm\\nX\\nj=1\\n(W)ij\\n其中, 第1 个等号是把变量i, j 分别用j, i 替代(统一替换公式中的符号并不影响公式本身); 第2 个等\\n号是由于W 是对称矩阵(即(W)ij = W)ji ), 并交换了求和号次序(类似于多重积分中交换积分号次序),\\n到此完成了该步骤的证明; 第3 个等号是由于f 2 (xi) 与求和变量j 无关, 因此拿到了该求和号外面(与\\n求和变量无关的项相对于该求和变量相当于常数), 该步骤的变形主要是为了得到di 。令di = Pm\\nj=1(W)ij\\n( 既是W 第i 行元素之和, 实际亦是第i 列元素之和, 因为由于W 是对称矩阵, 即(W)ij = W)ji, 因此\\ndi = Pm\\nj=1(W)ji, 即第例元素之和), 则\\nE(f) =\\nm\\nX\\ni=1\\ndif 2 (xi) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='dif 2 (xi) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n即式(13.12) 的第3 行, 其中第一项Pm\\ni=1 dif 2 (xi) 可以写为如下矩阵形式:\\n= f TDf\\n第二项Pm\\ni=1\\nPm\\nj=1(W)ijf (xi) f (xj) 也可以写为如下矩阵形式:\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n=\\nh\\nf (x1)\\nf (x2)\\n· · ·\\nf (xm)\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(W)11\\n(W)12\\n· · ·\\n(W)1m\\n(W)21\\n(W)22\\n· · ·\\n(W)2m\\n...\\n...\\n...\\n...\\n(W)m1\\n(W)m2\\n· · ·\\n(W)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nf (x1)\\nf (x2)\\n...\\nf (xm)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= f TW f\\n所以E(f) = f TD −f TW f = f T(D −W )f, 即式(13.12)。\\n13.4.2\\n式(13.13) 的推导\\n本式就是将式(13.12) 用分块矩阵形式表达而已, 拆分为标记样本和末标记样本两部分。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='本式就是将式(13.12) 用分块矩阵形式表达而已, 拆分为标记样本和末标记样本两部分。\\n另外解释一下该式之前一段话中第一句的含义:“具有最小能量的函数f 在有标记样本上满足f (xi) =\\nyi(i = 1, 2, . . . , l), 在末标记样本上满足∆f = 0 ”, 前半句是很容易理解的, 有标记样本上满足f (xi) =\\nyi(i = 1, 2, . . . , l), 这时末标记样本的f (xi) 是待求变量且应该使E(f) 最小, 因此应将式(13.12) 对末标\\n记样本的f (xi) 求导并令导数等于0 即可, 此即表达式∆f = 0, 此处可以查看该算法的原始文献。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n13.4.3\\n式(13.14) 的推导\\n将式(13.13) 根据矩阵运算规则进行变形, 这里第一项西瓜书中的符号有歧义，应该表示成\\nh\\nf T\\nl\\nf T\\nu\\ni\\n即一个R1×(l+u) 的行向量。根据矩阵乘法的定义，有：\\nE(f) =\\nh\\nf T\\nl\\nf T\\nu\\ni \"\\nDll −W ll\\n−W lu\\n−W ul\\nDuu −W uu\\n# \"\\nf l\\nf u\\n#\\n=\\nh\\nf T\\nl (Dll −W ll) −f T\\nuW ul\\n−f T\\nl W lu + f T\\nu (Duu −W uu)\\ni \"\\nf l\\nf u\\n#\\n=\\n\\x00f T\\nl (Dll −W ll) −f T\\nuW ul\\n\\x01\\nf l +\\n\\x00−f T\\nl W lu + f T\\nu (Duu −W uu)\\n\\x01\\nf u\\n= f T\\nl (Dll −W ll) f l −f T\\nuW ulf l −f T\\nl W luf u + f T\\nu (Duu −W uu) f u\\n= f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T\\nu (Duu −W uu) f u\\n其中最后一步，f T\\nl W luf u =\\n\\x00f T\\nl W luf u\\n\\x01T = f T\\nu W ulf l，因为这个式子的结果是一个标量。\\n13.4.4\\n式(13.15) 的推导\\n首先，基于式(13.14) 对f u 求导：\\n∂E(f)\\n∂f u\\n= ∂f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T\\nu (Duu −W uu) f u\\n∂f u\\n= −2W ulf l + 2 (Duu −W uu) f u\\n令结果等于0 即得13.15。\\n注意式中各项的含义:\\nf u 即函数f 在末标记样本上的预测结果;\\nDuu, W uu, W ul 均可以由式(13.11) 得到;\\nf l 即函数f 在有标记样本上的预测结果(即已知标记, 详见“西瓜书”P301 倒数第3 行);\\n也就是说可以根据式(13.15) 根据Dl 上的标记信息(即f l ) 求得末标记样本的标记(即f u ), 式'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(13.17) 仅是式(13.15) 的进一步变形化简, 不再细述。\\n仔细回顾该方法, 实际就是根据“相似的样本应具有相似的标记”的原则, 构建了目标函数式(13.12),\\n求解式(13.12) 得到了使用标记样本信息表示的末标记样本的预测标记。\\n13.4.5\\n式(13.16) 的解释\\n根据矩阵乘法的定义计算可得该式，其中需要注意的是，对角矩阵D 的拟等于其各个对角元素的倒\\n数。\\n13.4.6\\n式(13.17) 的推导\\n第一项到第二项是根据矩阵乘法逆的定义：(AB)−1 = B−1A−1，在这个式子中\\nPuu = D−1\\nuuWuu\\nPul = D−1\\nuuWul\\n均可以根据Wij 计算得到，因此可以通过标记fl 计算未标记数据的标签fu。\\n13.4.7\\n式(13.18) 的解释\\n其中Y 的第i 行表示第i 个样本的类别; 具体来说, 对于前l 个有标记样本来说, 若第i 个样本的类\\n别为j(1 ≤j ≤|Y|), 则Y 的第行第j 列即为1 , 第行其余元素为0 ; 对于后u 个末标记样本来说, Y 统\\n一为零。注意|Y| 表示集合Y 的势, 即包含元素(类别) 的个数。\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='一为零。注意|Y| 表示集合Y 的势, 即包含元素(类别) 的个数。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n13.4.8\\n式(13.20) 的解释\\nF∗= lim\\nt→∞F(t) = (1 −α)(I −αS)−1Y\\n[解析]：由式(13.19)\\nF(t + 1) = αSF(t) + (1 −α)Y\\n当t 取不同的值时，有：\\nt = 0 : F(1) = αSF(0) + (1 −α)Y\\n= αSY + (1 −α)Y\\nt = 1 : F(2) = αSF(1) + (1 −α)Y = αS(αSY + (1 −α)Y) + (1 −α)Y\\n= (αS)2Y + (1 −α)\\n \\n1\\nX\\ni=0\\n(αS)i\\n!\\nY\\nt = 2 : F(3) = αSF(2) + (1 −α)Y\\n= αS\\n \\n(αS)2Y + (1 −α)\\n \\n1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n!\\n+ (1 −α)Y\\n= (αS)3Y + (1 −α)\\n \\n2\\nX\\ni=0\\n(αS)i\\n!\\nY\\n可以观察到规律\\nF(t) = (αS)tY + (1 −α)\\n t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n则\\nF∗= lim\\nt→∞F(t) = lim'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n则\\nF∗= lim\\nt→∞F(t) = lim\\nt→∞(αS)tY + lim\\nt→∞(1 −α)\\n t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n其中第一项由于S = D−1\\n2 WD−1\\n2 的特征值介于[-1, 1] 之间[1]，而α ∈(0, 1)，所以limt→∞(αS)t = 0，第\\n二项由等比数列公式\\nlim\\nt→∞\\nt−1\\nX\\ni=0\\n(αS)i = I −limt→∞(αS)t\\nI −αS\\n=\\nI\\nI −αS = (I −αS)−1\\n综合可得式(13.20)。\\n13.4.9\\n公式(13.21)\\n这里主要是推导式(13.21) 的最优解即为式(13.20)。将式(13.21) 的目标函数进行变形:\\n第1 部分:\\n先将范数平方拆开为四项:\\n\\r\\r\\r\\r\\r\\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n\\r\\r\\r\\r\\r\\n2\\n=\\n \\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n!  \\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n!⊤\\n= 1\\ndi\\nFiF⊤\\ni + 1\\ndj\\nFjF⊤\\nj −\\n1\\np\\ndidj\\nFiF⊤\\nj −\\n1\\np\\ndjdi\\nFjF⊤'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i + 1\\ndj\\nFjF⊤\\nj −\\n1\\np\\ndidj\\nFiF⊤\\nj −\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n其中Fi ∈R1×|Y| 表示矩阵F 的第i 行, 即第i 个示例xi 的标记向量。将第1 项中的Pm\\ni,j=1 写为两\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n个和求号Pm\\ni=1\\nPm\\ni=1 的形式, 并将上面拆分的四项中的前两项代入, 得\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\ndi\\nFiF⊤\\ni =\\nm\\nX\\ni=1\\n1\\ndi\\nFiF⊤\\ni\\nm\\nX\\nj=1\\n(W)ij =\\nm\\nX\\ni=1\\n1\\ndi\\nFiF⊤\\ni · di =\\nm\\nX\\ni=1\\nFiF⊤\\ni\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\ndj\\nFjF⊤\\nj =\\nm\\nX\\nj=1\\n1\\ndj\\nFjF⊤\\nj\\nm\\nX\\ni=1\\n(W)ij =\\nm\\nX\\nj=1\\n1\\ndj\\nFjF⊤\\nj · dj =\\nm\\nX\\nj=1\\nFjF⊤\\nj\\n以上化简过程中, 两个求和号可以交换求和次序; 又因为W 为对称阵, 因此对行求和与对列求和效果\\n一样, 即di = Pm\\nj=1(W)ij = Pm\\nj=1(W)ji （已在式(13.12) 推导时说明）。显然,\\nm\\nX\\ni=1\\nFiF⊤\\ni =\\nm\\nX\\nj=1\\nFjF⊤\\nj =\\nm\\nX\\ni=1\\n∥Fi∥2 = ∥F∥2\\nF = tr\\n\\x00FF⊤\\x01'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='X\\nj=1\\nFjF⊤\\nj =\\nm\\nX\\ni=1\\n∥Fi∥2 = ∥F∥2\\nF = tr\\n\\x00FF⊤\\x01\\n以上推导过程中, 第1 个等号显然成立, 因为二者仅是求和变量名称不同; 第2 个等号即将FiF⊤\\ni 写\\n为∥Fi∥2 形式; 从第2 个等号的结果可以看出这明显是在求矩阵F 各元素平方之和, 也就是矩阵F 的\\nFrobenius 范数（简称F 范数）的平方, 即第3 个等号; 根据矩阵F 范数与矩阵的迹的关系有第4 个等号\\n(详见本章预备知识: 矩阵的F 范数与迹)。接下来, 将上面拆分的四项中的第三项代入，得\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj = tr\\n\\x00S⊤FF⊤\\x01\\n= tr\\n\\x00SFF⊤\\x01\\n具体来说, 以上化简过程为:\\nS =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)11\\n(S)12\\n· · ·\\n(S)1m\\n(S)21\\n(S)22\\n· · ·\\n(S)2m\\n...\\n...\\n...\\n...\\n(S)m1\\n(S)m2\\n· · ·\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= D−1\\n2 WD−1\\n2\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n√d1\\n1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n= D−1\\n2 WD−1\\n2\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n√d1\\n1\\n√d2\\n...\\n1\\n√dm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(W)11\\n(W)12\\n· · ·\\n(W)1m\\n(W)21\\n(W)22\\n· · ·\\n(W)2m\\n...\\n...\\n...\\n...\\n(W)m1\\n(W)m2\\n· · ·\\n(W)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n√d1\\n1\\n√d2\\n...\\n1\\n√dm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n由以上推导可以看出(S)ij =\\n1\\n√\\ndidj (W)ij, 即第1 个等号; 而\\nFF⊤=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1\\nF2\\n...\\nFm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nh\\nF⊤\\n1\\nF⊤\\n2\\n· · ·\\nF⊤\\nm\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n1\\nF1F⊤\\n2\\n. . .\\nF1F⊤\\nm\\nF2F⊤\\n1\\nF2F⊤\\n2\\n· · ·\\nF2F⊤\\nm\\n...\\n...\\n...\\n...\\nFmF⊤\\n1\\nFmF⊤\\n2\\n· · ·\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n若令A = S ◦FF⊤, 其中\\uffff表示Hadmard 积, 即矩阵S 与矩阵FF⊤元素对应相乘（参见百度百科哈'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='达玛积), 因此\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(A)ij\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n可以验证, 上式的矩阵A = S ◦FF⊤元素之和Pm\\ni,j=1(A)ij 等于tr\\n\\x00S⊤FF⊤\\x01\\n, 这是因为\\ntr\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)11\\n(S)12\\n· · ·\\n(S)1m\\n(S)21\\n(S)22\\n· · ·\\n(S)2m\\n...\\n...\\n...\\n...\\n(S)m1\\n(S)m2\\n· · ·\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n1\\nF1F⊤\\n2\\n· · ·\\nF1F⊤\\nm\\nF2F⊤\\n1\\nF2F⊤\\n2\\n· · ·\\nF2F⊤\\nm\\n...\\n...\\n...\\n...\\nFmF⊤\\n1\\nFmF⊤\\n2\\n· · ·\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)11\\n(S)21\\n...\\n(S)m1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n1\\nF2F⊤\\n1\\n...\\nFmF⊤\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)12\\n(S)22\\n...\\n(S)m2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(S)12\\n(S)22\\n...\\n(S)m2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\n2\\nF2F⊤\\n2\\n...\\nFmF⊤\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ . . . +\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(S)1m\\n(S)2m\\n...\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⊤\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nF1F⊤\\nm\\nF2F⊤\\nm\\n...\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\nm\\nX\\ni=1\\n(S)i1FiF⊤\\n1 +\\nm\\nX\\ni=1\\n(S)i2FiF⊤\\n2 + . . . +\\nX\\ni=1\\n(S)imFiF⊤\\nm\\n=\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj\\n即第2 个等号; 易知矩阵S 是对称阵\\n\\x00S⊤= S\\n\\x01\\n, 即得第3 个等号。又由于内积FiF⊤\\nj 是一个数(即大\\n小为1 × 1 的矩阵), 因此其转置等于本身,\\nFiF⊤\\nj =\\n\\x00FiF⊤\\nj\\n\\x01⊤=\\n\\x00F⊤\\nj\\n\\x01⊤(Fi)⊤= FjF⊤\\ni\\n因此\\n1\\np\\ndidj\\nFiF⊤\\nj =\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n进而上面拆分的四项中的第三项和第四项相等:\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n综上所述(以上拆分的四项中前两项相等、后两项相等, 正好抵消系数1\\n2 ):\\n1\\n2\\n\\uf8eb\\n\\uf8ed\\nm\\nX\\ni,j=1\\n(W)ij\\n\\r\\r\\r\\r\\r\\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n\\r\\r\\r\\r\\r\\n2\\uf8f6\\n\\uf8f8= tr\\n\\x00FF⊤\\x01\\n−tr\\n\\x10\\nSFF⊤\\x11\\n第2 部分:\\n西瓜书中式(13.21) 的第2 部分与原文献[2] 中式(4) 的第2 部分不同:\\nQ(F) = 1\\n2\\nn\\nX\\ni,j=1\\nWij\\n\\r\\r\\r\\r\\r\\nFi\\n√Dii\\n−\\nFj\\np\\nDjj\\n\\r\\r\\r\\r\\r\\n2\\n+ µ\\nn\\nX\\ni=1\\n∥Fi −Yi∥2 ,\\n原文献中第2 部分包含了所有样本(求和变量上限为n ), 而西瓜书只包含有标记样本, 并且第304 页\\n第二段提到“式(13.21) 右边第二项是迫使学得结果在有标记样本上的预测与真实标记尽可能相同”; 若\\n按原文献式(4) 在第二项中将末标记样本也包含进来, 由于对于末标记样本Yi = 0, 因此直观上理解是迫'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='使末标记样本学习结果尽可能接近0 , 这显然是不对的; 有关这一点作者在第24 次印刷勘误中进行了补\\n充: “考虑到有标记样本通常很少而末标记样本很多, 为缓解过拟合, 可在式(13.21) 中引入针对末标记样\\n本的L2 范数项µ Pl+u\\ni=l+1 ∥Fi∥2, 式(13.21) 加上此项之后就与原文献的式(4) 完全相同了。将第二项写为\\nF 范数形式:\\nm\\nX\\ni=1\\n∥Fi −Yi∥2 = ∥F −Y∥2\\nF\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n综上, 式(13.21) 目标函数Q(F) = tr\\n\\x00FF⊤\\x01\\n−tr\\n\\x00SFF⊤\\x01\\n+ µ∥F −Y∥2\\nF, 求导:\\n∂Q(F)\\n∂F\\n= ∂tr\\n\\x00FF⊤\\x01\\n∂F\\n−∂tr\\n\\x00SFF⊤\\x01\\n∂F\\n+ µ∂∥F −Y∥2\\nF\\n∂F\\n= 2F −2SF + 2µ(F −Y)\\n令µ = 1−α\\nα , 并令∂Q(F)\\n∂F\\n= 2F −2SF + 2 1−α\\nα (F −Y) = 0, 移项化简即可得式(13.20), 即式(13.20) 是正则\\n化框架式(13.21) 的解。\\n13.5\\n基于分歧的方法\\n“西瓜书”的伟大之处在于巧妙地融入了很多机器学习的研究分支, 而非仅简单介绍经典的机器学习\\n算法。比如本节处于半监督学习章节范围内, 巧妙地将机器学习的研究热点之一多视图学习[3](multi-view\\nlearning) 融入进来, 类似地还有本章第一节将主动学习融入进来, 在第10 章第一节将k 近邻算法融入进\\n来, 在最后一节巧妙地将度量学习(metric learning) 融入进来等等。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='来, 在最后一节巧妙地将度量学习(metric learning) 融入进来等等。\\n协同训练是多视图学习代表性算法之一, 本章叙述简单易懂。\\n13.5.1\\n图13.6 的解释\\n第2 行表示从样本集Du 中去除缓冲池样本Ds;\\n第4 行, 当j = 1 时'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='xj\\ni, x3−j\\ni\\n即为⟨x1\\ni , x2\\ni ⟩, 当j = 2 时'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='xj\\ni, x3−j\\ni\\n即为⟨x2\\ni , x1\\ni ⟩, 往后的3 −j 与此\\n相同; 注意本页左上角的注释: ⟨x1\\ni , x2\\ni ⟩与⟨x2\\ni , x1\\ni ⟩表示的是同一个样本, 因此第1 个视图的有标记标训练\\n集为D1\\nl = {(x1\\n1, y1) , . . . , (x1\\nl , yl)}, 第2 个视图的有标记标训练集为D2\\nl = {(x2\\n1, y1) , . . . , (x2\\nl , yl)};\\n第9 行到第11 行是根据第j 个视图的对缓冲池末标记样本预测结果置信度赋予伪标记, 准备交给第\\n3 −j 个视图使用。\\n13.6\\n半监督聚类\\n13.6.1\\n图13.7 的解释\\n注意算法第4 行到第21 行是依次对每个样本进行处理, 其中第8 行到第21 行是尝试将样本xi 到底\\n应该划入哪个族, 具体来说是按样本xi 到各均值向量的距离从小到大依次尝试, 若最小的不违背M 和C\\n中的约束, 则将样本xi 划入该簇并置is_merge=true, 此时第8 行的while 循环条件为假不再继续循环,'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='若从小到大依次尝试各簇后均违背M 和C 中的约束则第16 行的if 条件为真, 算法报错结束; 依次对每\\n个样本进行处理后第22 行到第24 行更新均值向量, 重新开始新一轮迭代, 直到均值向量均末更新。\\n13.6.2\\n图13.9 的解释\\n算法第6 行到第10 行即在聚类簇迭代更新过程中不改变种子样本的簇隶属关系；第11 行到第15 行\\n即对非种子样本进行普通的k-means 聚类过程；第16 行到第18 行更新均值向量，反复迭代，直到均值\\n向量均未更新。\\n参考文献\\n[1] Wikipedia contributors. Laplacian matrix, 2020.\\n[2] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf. Learning\\nwith local and global consistency. Advances in neural information processing systems, 16, 2003.\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n[3] Chang Xu, Dacheng Tao, and Chao Xu.\\nA survey on multi-view learning.\\narXiv preprint\\narXiv:1304.5634, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 177, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第14 章\\n概率图模型\\n本章介绍概率图模型，前三节分别介绍了有向图模型之隐马尔可夫模型以及无向图模型之马尔可夫随\\n机场和条件随机场；接下来两节分别介绍精确推断和近似推断；最后一节简单介绍了话题模型的典型代表\\n隐狄利克雷分配模型(LDA)。\\n14.1\\n隐马尔可夫模型\\n本节前三段内容实际上是本章的概述，从第四段才开始介绍“隐马尔可夫模型”。马尔可夫的大名相信\\n很多人听说过，比如马尔可夫链；虽然隐马尔可夫模型与马尔可夫链并非同一人提出，但其中关键字“马\\n尔可夫”蕴含的概念是相同的，即系统下一时刻的状态仅由当前状态决定。\\n14.1.1\\n生成式模型和判别式模型\\n一般来说, 机器学习的任务是根据输入特征x 预测输出变量y; 生成式模型最终求得联合概率P(x, y),\\n而判别式模型最终求得条件概率P(y | x) 。\\n统计机器学习算法都是基于样本独立同分布(independent and identically distributed, 简称i.i.d. .)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 177, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='的假设, 也就是说, 假设样本空间中全体样本服从一个末知的“分布”D, 我们获得的每个样本都是独立地\\n从这个分布上采样获得的。\\n对于一个样本(x, y), 联合概率P(x, y) 表示从样本空间中采样得到该样本的概率; 因为P(x, y) 表示\\n“生成”样本本身的概率, 故称之为“生成式模型”。而条件概率P(y | x) 则表示已知x 的条件下输出为y\\n的概率, 即根据x “判别”y, 因此称为“判别式模型”。\\n常见的对率回归、支持向量机等都属于判别式模型, 而朴素贝叶斯则属于生成式模型。\\n14.1.2\\n式(14.1) 的推导\\n由概率公式P(AB) = P(A | B) · P(B) 可得:\\nP (x1, y1, . . . , xn, yn) = P (x1, . . . , xn | y1, . . . , yn) · P (y1, . . . , yn)\\n其中, 进一步可将P (y1, . . . , yn) 做如下变换:\\nP (y1, . . . , yn) = P (yn | y1, . . . , yn−1) · P (y1, . . . , yn−1)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 177, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= P (yn | y1, . . . , yn−1) · P (yn−1 | y1, . . . , yn−2) · P (y1, . . . , yn−2)\\n= . . . . . .\\n= P (yn | y1, . . . , yn−1) · P (yn−1 | y1, . . . , yn−2) · . . . · P (y2 | y1) · P (y1)\\n由于状态y1, . . . , yn 构成马尔可夫链, 即yt 仅由yt−1 决定; 基于这种依赖关系, 有\\nP (yn | y1, . . . , yn−1) = P (yn | yn−1)\\nP (yn−1 | y1, . . . , yn−2) = P (yn−1 | yn−2)\\nP (yn−2 | y1, . . . , yn−3) = P (yn−2 | yn−3)\\n因此P (y1, . . . , yn) 可化简为\\nP (y1, . . . , yn) = P (yn | yn−1) · P (yn−1 | yn−2) · . . . · P (y2 | y1) · P (y1)\\n= P (y1)\\nn\\nY\\ni=2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 177, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='= P (y1)\\nn\\nY\\ni=2\\nP (yi | yi−1)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n而根据“西瓜书”图14.1 表示的变量间的依赖关系: 在任一时刻, 观测变量的取值仅依赖于状态变量,\\n即xt 由yt 确定, 与其它状态变量及观测变量的取值无关。因此\\nP (x1, . . . , xn | y1, . . . , yn) = P (x1 | y1, . . . , yn) · . . . · P (xn | y1, . . . , yn)\\n= P (x1 | y1) · . . . · P (xn | yn)\\n=\\nn\\nY\\ni=1\\nP (xi | yi)\\n综上所述, 可得\\nP (x1, y1, . . . , xn, yn) = P (x1, . . . , xn | y1, . . . , yn) · P (y1, . . . , yn)\\n=\\n n\\nY\\ni=1\\nP (xi | yi)\\n!\\n·\\n \\nP (y1)\\nn\\nY\\ni=2\\nP (yi | yi−1)\\n!\\n= P (y1) P (x1 | y1)\\nn\\nY\\ni=2\\nP (yi | yi−1) P (xi | yi)\\n14.1.3'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='n\\nY\\ni=2\\nP (yi | yi−1) P (xi | yi)\\n14.1.3\\n隐马尔可夫模型的三组参数\\n状态转移概率和输出观测概率都容易理解, 简单解释一下初始状态概率。特别注意, 初始状态概率中\\nπi = P (y1 | si) , 1 ⩽i ⩽N, 这里只有y1, 因为y2 及以后的其它状态是由状态转移概率和y1 确定的, 具体\\n参见课本第321 页“给定隐马尔可夫模型λ, 它按如下过程产生观测序列{x1, x2, . . . , xn} ” 的四个步骤。\\n14.2\\n马尔可夫随机场\\n本节介绍无向图模型的著名代表之一：马尔可夫随机场。本节的部分概念（例如势函数、极大团等）\\n比较抽象，我亦无好办法，只能建议多读几遍，从心里接受这些概念就好。另外，从因果关系角度来讲，\\n首先是因为满足全局、局部或成对马尔可夫性的无向图模型称为马尔可夫随机场，所以马尔可夫随机场才\\n具有全局、局部或成对马尔可夫性。\\n14.2.1\\n式(14.2) 和式(14.3) 的解释\\n注意式(14.2) 之前的介绍是“则联合概率P(x) 定义为”, 而在式(14.3) 之前也有类似的描述。因'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='此, 可以将式(14.2) 和式(14.3) 理解为一种定义, 记住并接受这个定义就好了。实际上, 该定义是根据\\nHammersley-Clifford 定理而得, 可以具体了解一下该定理, 这里不再赘述。\\n值得一提的是, 在接下来讨论“条件独立性”时, 即式(14.4) 式(14.7) 的推导过程直接使用了该定义。\\n注意：在有了式(14.3) 的定义后, 式(14.2) 已作废, 不再使用。\\n14.2.2\\n式(14.4) 到式(14.7) 的推导\\n首先, 式(14.4) 直接使用了式(14.3) 有关联合概率的定义。\\n对于式(14.5), 第一行两个等号变形就是概率论中的知识; 第二行的变形直接使用了式(14.3) 有关联\\n合概率的定义; 第三行中, 由于ψAC (x′\\nA, xC) 与变量x′\\nB 无关, 因此可以拿到求和号P\\nx′\\nB 外面, 即\\nX\\nx′\\nA\\nX\\nx′\\nB\\nψAC (x′\\nA, xC) ψBC (x′\\nB, xC) =\\nX\\nx′\\nA\\nψAC (x′\\nA, xC)\\nX\\nx′\\nB\\nψBC (x′\\nB, xC)\\n→_→'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='X\\nx′\\nA\\nψAC (x′\\nA, xC)\\nX\\nx′\\nB\\nψBC (x′\\nB, xC)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n举个例子, 假设x = {x1, x2, x3} , y = {y1, y2, y3}, 则\\n3\\nX\\ni=1\\n3\\nX\\nj=1\\nxiyj = x1y1 + x1y2 + x1y3 + x2y1 + x2y2 + x2y3 + x3y1 + x3y2 + x3y3\\n= x1 × (y1 + y2 + y3) + x2 × (y1 + y2 + y3) + x3 × (y1 + y2 + y3)\\n= (x1 + x2 + x3) × (y1 + y2 + y3) =\\n \\n3\\nX\\ni=1\\nxi\\n!  \\n3\\nX\\nj=1\\nyj\\n!\\n同理可得式(14.6)。类似于式(14.6), 还可以得到P (xB | xC) =\\nψBC(xB,xC)\\n∑\\nx′\\nB ψBC(x′\\nB,xC)\\n最后, 综合可得式(14.7) 成立, 即马尔可夫随机场“条件独立性”得证。\\n14.2.3\\n马尔可夫毯(Markov blanket)\\n本节共提到三个性质, 分别是全局马尔可夫性、局部马尔可夫性和成对马尔可夫性, 三者本质上是一'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='本节共提到三个性质, 分别是全局马尔可夫性、局部马尔可夫性和成对马尔可夫性, 三者本质上是一\\n样的, 只是适用场景略有差异。\\n在“西瓜书”第325 页左上角边注提到“马尔可夫\\x00”的概念, 专门提一下这个概念主要是其名字与\\n马尔可夫链、隐马尔可夫模型、马尔可夫随机场等很像; 但实际上, 马尔可夫\\x00是一个局部的概念, 而马尔\\n可夫链、隐马尔可夫模型、马尔可夫随机场则是整体模型级别的概念。\\n对于某变量, 当它的马尔可夫\\x00 (即其所有邻接变量, 包含父变量、子变量、子变量的其他父变量等组\\n成的集合）确定时, 则该变量条件独立于其它变量, 即局部马尔可夫性。\\n14.2.4\\n势函数(potential function)\\n势函数贯穿本节，但却一直以抽象函数符号形式出现，直到本节最后才简单介绍势函数的具体形式，\\n个人感觉这为理解本节内容增加不少难度。具体来说，若已知势函数，例如以“西瓜书”图14.4 为例的和\\n取值，则可以根据式(14.3) 基于最大团势函数定义的联合概率公式解得各种可能变量值指派的联合概率，'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='进而完成一些预测工作；若势函数未知，在假定势函数的形式之后，应该就需要根据数据去学习势函数的\\n参数。\\n14.2.5\\n式(14.8) 的解释\\n此为势函数的定义式，即将势函数写作指数函数的形式。指数函数满足非负性，且便于求导，因此在\\n机器学习中具有广泛应用，例如西瓜书式(8.5) 和式(13.11)。\\n14.2.6\\n式(14.9) 的解释\\n此为定义在变量xQ 上的函数HQ (·) 的定义式，第二项考虑单节点，第一项考虑每一对节点之间的关\\n系。\\n14.3\\n条件随机场\\n条件随机场是给定一组输入随机变量x 条件下, 另一组输出随机变量y 构成的马尔可夫随机场, 即本\\n页边注中所说“条件随机场可看作给定观测值的马尔可夫随机场”, 条件随机场的“条件”应该就来源于\\n此吧, 因为需要求解的概率为条件联合概率P(y | x), 因此它是一种判别式模型, 参见“西瓜书”图14.6。\\n14.3.1\\n式(14.10) 的解释\\nP\\n\\x00yv|x, yV \\\\{v}\\n\\x01\\n= P\\n\\x00yv|x, yn(v)\\n\\x01\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n[解析]：根据局部马尔科夫性，给定某变量的邻接变量，则该变量独立与其他变量，即该变量只与其邻接\\n变量有关，所以式(14.10) 中给定变量v 以外的所有变量与仅给定变量v 的邻接变量是等价的。\\n特别注意, 本式下方写到“则(y, x) 构成一个条件随机场”; 也就是说, 因为(y, x) 满足式(14.10), 所\\n以(y, x) 构成一个条件随机场, 类似马尔可夫随机场与马尔可夫性的因果关系。\\n14.3.2\\n式(14.11) 的解释\\n注意本式前面的话：“条件概率被定义为”。至于式中使用的转移特征函数和状态特征函数，一般这两\\n个函数取值为1 或0，当满足特征条件时取值为1，否则为0。\\n14.3.3\\n学习与推断\\n本节前4 段内容（标题“14.4.1 变量消去”之前）至关重要，可以看作是14.4 节和14.5 节的引言，为\\n后面这两节内容做铺垫，因此一定要反复研读几遍，因为这几段内容告诉你接下来两节要解决什么问题，\\n心中装着问题再去看书会事半功倍，否则即使推明白了公式也不知道为什么要去推这些公式。本节介绍两'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='心中装着问题再去看书会事半功倍，否则即使推明白了公式也不知道为什么要去推这些公式。本节介绍两\\n种精确推断方法，下一节则介绍两种近似推断方法。\\n14.3.4\\n式(14.14) 的推导\\n该式本身的含义很容易理解, 即为了求P (x5) 对联合分布中其他无关变量（即x1, x2, x3, x4 ）进行积\\n分(或求和) 的过程, 也就是“边际化”(marginalization)。\\n关键在于为什么从第1 个等号可以得到第2 个等号, 边注中提到“基于有向图模型所描述的条件独立\\n性”, 此即第7 章式(7.26)。这里的变换类似于式(7.27) 的推导过程, 不再赘述。\\n总之，在消去变量的过程中，在消去每一个变量时需要保证其依赖的变量已经消去，因此消去顺序应\\n该是有向概率图中的一条以目标节点为终点的拓扑序列。\\n14.3.5\\n式(14.15) 和式(14.16) 的推导\\n这里定义新符号mij (xj), 请一定理解并记住其含义。依次推导如下:\\nm12 (x2) =\\nX\\nx1\\nP (x1) P (x2 | x1) =\\nX\\nx1\\nP (x2, x1) = P (x2)\\nm23 (x3) =\\nX\\nx2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='X\\nx1\\nP (x2, x1) = P (x2)\\nm23 (x3) =\\nX\\nx2\\nP (x3 | x2) m12 (x2) =\\nX\\nx2\\nP (x3, x2) = P (x3)\\nm43 (x3) =\\nX\\nx4\\nP (x4 | x3) m23 (x3) =\\nX\\nx4\\nP (x4, x3) = P (x3) (这里与书中不一样\\n!\\nm35 (x5) =\\nX\\nx3\\nP (x5 | x3) m43 (x3) =\\nX\\nx3\\nP (x5, x3) = P (x5)\\n注意: 这里的过程与“西瓜书”中不太一样, 但本质一样, 因为m43 (x3) = P\\nx4 P (x4 | x3) = 1 。\\n14.3.6\\n式(14.17) 的解释\\n忽略图14.7(a) 中的箭头，然后把无向图中的每条边的两个端点作为一个团将其分解为四个团因子的\\n乘积。Z 为规范化因子确保所有可能性的概率之和为1。本式就是基于极大团定义的联合概率分布，参见\\n式(14.3)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.3.7\\n式(14.18) 的推导\\n原理同式14.15, 区别在于把条件概率替换为势函数。由于势函数的定义是抽象的, 无法类似于P\\nx4 P (x4 | x3) =\\n1 去处理P\\nx4 ψ (x3, x4) 。\\n但根据边际化运算规则，可以知道：\\nm12 (x2) = P\\nx1 ψ12 (x1, x2) 只含x2 不含x1;\\nm23 (x3) = P\\nx2 ψ23 (x2, x3) m12 (x2) 只含x3 不含x2;\\nm43 (x3) = P\\nx4 ψ34 (x3, x4) m23 (x3) 只含x3 不含x4;\\nm35 (x5) = P\\nx3 ψ35 (x3, x5) m43 (x3) 只含x5 不含x3, 即最后得到P (x5) 。\\n14.3.8\\n式(14.19) 的解释\\n首先解释符号含义, k ∈n(i)\\\\j 表示k 属于除去j 之外的xi 的邻接结点, 例如n(1)\\\\2 为空集(因为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='x1 只有邻接结点2 ), n(2)\\\\3 = {1} (因为x2 有邻接结点1 和3 )，n(4) n3 为空集(因为x4 只有邻接结\\n点3 ), n(3)\\\\5 = {2, 4} （因为x3 有邻接结点2,4 和5 )。\\n接下来, 仍然以图14.7 计算P (x5) 为例:\\nm12 (x2) =\\nX\\nx1\\nψ12 (x1, x2)\\nY\\nk∈n(1)\\\\2\\nmk1 (x1) =\\nX\\nx1\\nψ12 (x1, x2)\\nm23 (x3) =\\nX\\nx2\\nψ23 (x2, x3)\\nY\\nk∈n(2)\\\\3\\nmk2 (x2) =\\nX\\nx1\\nψ12 (x1, x2) m12 (x2)\\nm43 (x3) =\\nX\\nx4\\nψ34 (x3, x4)\\nY\\nk∈n(4)\\\\3\\nmk4 (x4) =\\nX\\nx4\\nψ34 (x3, x4)\\nm35 (x5) =\\nX\\nx3\\nψ35 (x3, x5)\\nY\\nk∈n(3)\\\\5\\nmk3 (x3) =\\nX\\nx3\\nψ35 (x3, x5) m23 (x3) m43 (x3)\\n该式表示从节点i 传递到节点j 的过程，求和号表示要考虑节点i 的所有可能取值。连乘号解释见式'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='该式表示从节点i 传递到节点j 的过程，求和号表示要考虑节点i 的所有可能取值。连乘号解释见式\\n14.20。应当注意这里连乘号的下标不包括节点j，节点i 只需要把自己知道的关于j 以外的消息告诉节点\\nj 即可。\\n14.3.9\\n式(14.20) 的解释\\n应当注意这里是正比于而不是等于，因为涉及到概率的规范化。可以这么解释，每个变量可以看作一\\n个有一些邻居的房子，每个邻居根据其自己的见闻告诉你一些事情(消息)，任何一条消息的可信度应当与\\n所有邻居都有相关性，此处这种相关性用乘积来表达。\\n14.3.10\\n式(14.22) 的推导\\n假设x 有M 种不同的取值，xi 的采样数量为mi(连续取值可以采用微积分的方法分割为离散的取\\n值)，则\\nˆf = 1\\nN\\nM\\nX\\nj=1\\nf (xj) · mj\\n=\\nM\\nX\\nj=1\\nf (xj) · mj\\nN\\n≈\\nM\\nX\\nj=1\\nf (xj) · p(xj)\\n≈\\nZ\\nf(x)p(x)dx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.3.11\\n图14.8 的解释\\n图(a) 表示信念传播算法的第1 步, 即指定一个根结点, 从所有叶结点开始向根结点传递消息, 直到根\\n结点收到所有邻接结点的消息; 图(b) 表示信念传播算法的第2 步, 即从根结点开始向叶结点传递消息, 直\\n到所有叶结点均收到消息。\\n本图并不难理解, 接下来思考如下两个问题:\\n【思考1】如何编程实现本图信念传播的过程? 这其中涉及到很多问题, 例如从叶结点x4 向根结点传\\n递消息时, 当传递到x3 时如何判断应该向x2 传递还是向x5 传递? 当然, 你可能感觉x5 是叶结点, 所以\\n肯定是向x2 传递, 那是因为这个无向图模型很简单, 如果x5 和x3 之间还有很多个结点呢? 复计算问题”\\n, 但如果图模型很复杂而我本身只需要计算少量边际分布, 是否还应该使用信念传播呢? 其实计算边际分\\n布类似于第10.1 节提到的“懒惰学习”, 只有在计算边际分布时才需要计算某些“消息”。这可能要根据\\n实际情况在变量消去和信念传播两种方法之间取舍。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='实际情况在变量消去和信念传播两种方法之间取舍。\\n【思考2】14.4.2 节开头就说到“信念传播...... 较好地解决了求解多个边际分布时的重复计算问题”,\\n但如果图模型很复杂而我本身只需要计算少量边际分布, 是否还应该使用信念传播呢? 其实计算边际分布\\n类似于第10.1 节提到的“懒惰学习”, 只有在计算边际分布时才需要计算某些“消息”。这可能要根据实\\n际情况在变量消去和信念传播两种方法之间取舍。\\n14.4\\n近似推断\\n本节介绍两种近似推断方法：MCMC 采样和变分推断。提到推断，一般是为了求解某个概率分布（参\\n见上一节的例子），但需要特别说明的是，本节将要介绍的MCMC 采样并不是为了求解某个概率分布，而\\n是在已知某个概率分布的前提下去构造服从该分布的独立同分布的样本集合，理解这一点对于读懂14.5.1\\n节的内容非常关键，即14.5.1 节中的p(x) 是已知的；变分推断是概率图模型常用的推断方法，要尽可能\\n理解并掌握其中的细节。\\n14.4.1\\n式(14.21) 到式(14.25) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='理解并掌握其中的细节。\\n14.4.1\\n式(14.21) 到式(14.25) 的解释\\n这五个公式都是概率论课程中的基本公式, 很容易理解; 从14.5.1 节开始到式(14.25), 实际都在为\\nMCMC 采样做铺垫, 即为什么要做MCMC 采样? 以下分三点说明:\\n(1) 若已知概率密度函数p(x), 则可通过式(14.21) 计算函数f(x) 在该概率密度函数p(x) 下的期望;\\n这个过程也可以先根据p(x) 抽取一组样本再通过式(14.22) 近似完成。\\n(2) 为什么要通过式(14.22) 近似完成呢? 这是因为“若x 不是单变量而是一个高维多元变量x, 且服\\n从一个非常复杂的分布, 则对式(14.24) 求积分通常很困难”。\\n(3) “然而, 若概率密度函数p(x) 很复杂, 则构造服从p 分布的独立同分布样本也很困难”, 这时可以\\n使用MCMC 采样技术完成采样过程。\\n式(14.23) 就是在区间A 中的概率计算公式, 而式(14.24) 与式(14.21) 的区别也就在于式(14.24) 限\\n定了积分变量x 的区间(可能写成定积分形式可能更容易理解)。\\n14.4.2'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='定了积分变量x 的区间(可能写成定积分形式可能更容易理解)。\\n14.4.2\\n式(14.26) 的解释\\n假设变量x 所在的空间有n 个状态(s1, s2, .., sn), 定义在该空间上的一个转移矩阵T ∈Rn×n 满足一\\n定的条件则该马尔可夫过程存在一个稳态分布π, 使得\\nπT = π\\n其中, π 是一个是一个n 维向量，代表s1, s2, .., sn 对应的概率. 反过来, 如果我们希望采样得到符合某个\\n分布π 的一系列变量x1, x2, .., xt, 应当采用哪一个转移矩阵T ∈Rn×n 呢？\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n事实上，转移矩阵只需要满足马尔可夫细致平稳条件\\nπiTij = πjTji\\n即式(14.26)，这里采用的符号与西瓜书略有区别以便于理解. 证明如下\\nπTj· =\\nX\\ni\\nπiTij =\\nX\\ni\\nπjTji = πj\\n假设采样得到的序列为x1, x2, .., xt−1, xt，则可以使用MH 算法来使得xt−1(假设为状态si) 转移到xt(假\\n设为状态sj) 的概率满足式。\\n本式为某个时刻马尔可夫链平稳的条件, 注意式中的p (xt) 和p (xt−1) 已知, 但状态转移概率T (xt−1 | xt)\\n和T (xt | xt−1) 末知。如何构建马尔可夫链转移概率至关重要, 不同的构造方法将产生不同的MCMC 算\\n法（可以认为MCMC 算法是一个大的框架或一种思想, 即“MCMC 方法先设法构造一条马尔可夫链, 使\\n其收玫至平稳分布恰为待估计参数的后验分布, 然后通过这条马尔可夫链来产生符合后验分布的样本, 并\\n基于这些样本来进行估计”, 具体如何构建马尔可夫链有多种实现途径, 接下来介绍的MH 算法就是其中'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='基于这些样本来进行估计”, 具体如何构建马尔可夫链有多种实现途径, 接下来介绍的MH 算法就是其中\\n一种)。\\n14.4.3\\n式(14.27) 的解释\\n若将本式xt−1 和x∗分别对应式(14.27) 的xt 和xt−1, 则本式与式(14.27) 区别仅在于状态转移概率\\nT (x∗| xt−1) 由先验概率Q (x∗| xt−1) 和被接受的概率A (x∗| xt−1) 的乘积表示。\\n14.4.4\\n式(14.28) 的推导\\n注意, 本式中的概率分布p(x) 和先验转移概率Q 均为已知, 因此可计算出接受概率。将本式代入式\\n(14.27) 可以验证本式是正确的。具体来说, 式(14.27) 等号左边将变为:\\np\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\nA\\n\\x00x∗| xt−1\\x01\\n=p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\nmin\\n\\x12\\n1, p (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n= min\\n\\x12\\np\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\n, p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01 p (x∗) Q (xt−1 | x∗)'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content=', p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01 p (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n= min\\n\\x00p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\n, p (x∗) Q\\n\\x00xt−1 | x∗\\x01\\x01\\n将A (xt−1 | x∗) 代入右边(符号式xt−1 和x∗调换位置), 同理可得如上结果, 即本式的接受概率形式\\n可保证式(14.27) 成立。\\n验证完毕之后可以再做一个简单的推导。其实若想要式(14.27) 成立, 简单令:\\nA\\n\\x00x∗| xt−1\\x01\\n= C · p (x∗) Q\\n\\x00xt−1 | x∗\\x01\\n(则等号右则的A (xt−1 | x∗) = C · p (xt−1) Q (x∗| xt−1) )\\n即可, 其中C 为大于零的常数, 且不能使A (x∗| xt−1) 和A (xt−1 | x∗) 大于1 （因为它们是概率)。注\\n意待解A (x∗| xt−1) 为接受概率, 在保证式(14.27) 成立的基础上, 其值应该尽可能大一些(但概率值不会'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='超过1 ), 否则在图14.9 描述的MH 算法中采样出的候选样本将会有大部分会被拒绝。所以, 常数C 尽可\\n能大一些, 那么C 最大可以为多少呢?\\n对于A (x∗| xt−1) = C·p (x∗) Q (xt−1 | x∗), 易知C 最大可以取值\\n1\\np(x∗)Q(xt−1|x∗), 再大则会使A (x∗| xt−1)\\n大于1 ; 对于A (xt−1 | x∗) = C · p (xt−1) Q (x∗| xt−1), 易知C 最大可以取值\\n1\\np(xt−1)Q(x∗|xt−1); 常数C 的取\\n值需要同时满足两个约束, 因此\\nC = min\\n\\x12\\n1\\n·p (x∗) Q (xt−1 | x∗),\\n1\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n将这个常数C 的表达式代入A (x∗| xt−1) = C · p (x∗) Q (xt−1 | x∗) 即得式(14.28)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.5\\n吉布斯采样与MH 算法\\n这里解释一下为什么说吉布斯采样是MH 算法的特例。\\n吉布斯采样算法如下(“西瓜书”第334 页):\\n(1) 随机或以某个次序选取某变量xi;\\n(2) 根据x 中除xi 外的变量的现有取值, 计算条件概率p (xi | x¯i), 其中x¯i = {x1, x2, . . . , xi−1, xi+1, . . . , xN};\\n(3) 根据p (xi | x¯i) 对变量xi 采样, 用采样值代替原值.\\n对应到式(14.27) 和式(14.28) 表示的MH 采样, 候选样本x∗与t −1 时刻样本xt−1 的区别仅在于第\\ni 个变量的取值不同, 即x∗\\n¯i 与xt−1\\n¯i\\n相同。先给几个概率等式:\\n(1) Q (x∗| xt−1) = p\\n\\x00x∗\\ni | xt−1\\n¯i\\n\\x01\\n(2) Q (xt−1 | x∗) = p\\n\\x00xt−1\\ni\\n| x∗\\n¯i\\n\\x01\\n(3) p (x∗) = p\\n\\x00x∗\\ni , x∗\\n¯i\\n\\x01\\n= p\\n\\x00x∗\\ni | x∗\\n¯i\\n\\x01\\np\\n\\x00x∗\\n¯i'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x00x∗\\ni , x∗\\n¯i\\n\\x01\\n= p\\n\\x00x∗\\ni | x∗\\n¯i\\n\\x01\\np\\n\\x00x∗\\n¯i\\n\\x01\\n(4) p (xt−1) = p\\n\\x00xt−1\\ni\\n, xt−1\\n¯i\\n\\x01\\n= p\\n\\x00xt−1\\ni\\n| xt−1\\n¯i\\n\\x01\\np\\n\\x00xt−1\\n¯i\\n\\x01\\n其中等式(1) 是由于吉布斯采样中“根据p (xi | xi) 对变量xi 采样”(参见以上第(3) 步), 即用户给\\n定的先验概率为p (xi | x¯i), 同理得等式(2); 等式(3) 就是将联合概率p (x∗) 换了种形式, 然写成了条件概\\n率和先验概率乘积, 同理得等式(4)。\\n对于式(14.28) 来说(注意: x∗\\n¯i = xt−1\\n¯i\\n)\\np (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1) =\\np (x∗\\ni | x∗\\ni ) p (x∗\\ni ) p\\n\\x00xt−1\\ni\\n| x∗\\n¯i\\n\\x01\\np\\n\\x00xt−1\\ni\\n| xt−1\\n¯i\\n\\x01\\np\\n\\x00xt−1\\n¯i\\n\\x01\\np\\n\\x00x∗\\ni | xt−1\\n¯i\\n\\x01 = 1\\n即在吉布斯采样中接受概率恒等于1 , 也就是说吉布斯采样是接受概率为1 的MH 采样。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='\\x01 = 1\\n即在吉布斯采样中接受概率恒等于1 , 也就是说吉布斯采样是接受概率为1 的MH 采样。\\n该推导参考了Bishop 的PRML 第544 页:\\nWe can obtain the Gibbs sampling procedure as a particular instance of the Metropolis-Hastings\\nalgorithm as follows. Consider a Metropolis-Hastings sampling step involving the variable zk in which\\nthe remaining variables z\\\\k remain fixed, and for which the transition probability from z to z⋆is given by\\nqk (z⋆| z) = p\\n\\x00z⋆\\nk | z\\\\k\\n\\x01\\n.\\nWe note that z⋆'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='qk (z⋆| z) = p\\n\\x00z⋆\\nk | z\\\\k\\n\\x01\\n.\\nWe note that z⋆\\n\\\\k = z\\\\k because these components are unchanged by the sampling step. Also, p(z) =\\np\\n\\x00zk | z\\\\k\\n\\x01\\np\\n\\x00z\\\\k\\n\\x01\\n. Thus the factor that determines the acceptance probability in the Metropolis-Hastings\\n(11.44) is given by\\nA (z⋆, z) = p (z⋆) qk (z | z⋆)\\np(z)qk (z⋆| z) =\\np\\n\\x00z⋆\\nk | z⋆\\n↓k\\n\\x01\\np\\n\\x10\\nz⋆\\n\\\\k\\n\\x11\\np\\n\\x10\\nzk | z⋆\\n\\\\k\\n\\x11\\np\\n\\x00zk | z\\\\k\\n\\x01\\np\\n\\x00z\\\\k\\n\\x01\\np\\n\\x00z⋆\\nk | z\\\\k\\n\\x01 = 1\\nwhere we have used z⋆\\n\\\\k = z\\\\k. Thus the Metropolis-Hastings steps are always accepted.\\n14.4.6'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='14.4.6\\n式(14.29) 的解释\\n连乘号是因为N 个变量的生成过程相互独立。求和号是因为每个变量的生成过程需要考虑中间隐变\\n量的所有可能性，类似于边际分布的计算方式。\\n14.4.7\\n式(14.30) 的解释\\n对式(14.29) 取对数。本式就是求对数后, 原来的连乘变为了连加, 即性质ln(ab) = ln a + ln b 。\\n接下来提到“图14.10 所对应的推断和学习任务主要是由观察到的变量x 来估计隐变量Z 和分布参\\n数变量Θ, 即求解p(z | x, Θ) 和Θ ”, 这里可以对应式(3.26) 来这样不严谨理解: Θ 对应式(3.26) 的w, b,\\n而z 对应式(3.26) 的y 。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.8\\n式(14.31) 的解释\\n对应7.6 节EM 算法中的M 步，参见第163 页的式(7.36) 和式(7.37)。\\n14.4.9\\n式(14.32) 到式(14.34) 的推导\\n从式(14.31) 到式(14.32) 之间的跳跃比较大, 接下来为了方便忽略分布参数变量Θ 。这里的主要问\\n题是后验概率p(z | x) 难于获得, 进而使用一个已知简单分布q(z) 去近似需要推导的复杂分布p(z | x), 这\\n就是变分推断的核心思想。\\n根据概率论公式p(x, z) = p(z | x)p(x), 得:\\np(x) = p(x, z)\\np(z | x)\\n分子分母同时除以q(z), 得:\\np(x) = p(x, z)/q(z)\\np(z | x)/q(z)\\n等号两边同时取自然对数, 得:\\nln p(x) = ln p(x, z)/q(z)\\np(z | x)/q(z) = ln p(x, z)\\nq(z)\\n−ln p(z | x)\\nq(z)\\n等号两边同时乘以q(z) 并积分, 得:\\nZ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='q(z)\\n−ln p(z | x)\\nq(z)\\n等号两边同时乘以q(z) 并积分, 得:\\nZ\\nq(z) ln p(x)dz =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz\\n对于等号左边的积分, 由于p(x) 与变量z 无关, 因此可以当作常数拿到积分号外面:\\nZ\\nq(z) ln p(x)dz = ln p(x)\\nZ\\nq(z)dz = ln p(x)\\n其中q(z) 为一个概率分布, 所以积分等于1 。至此, 前面式子变为:\\nln p(x) =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz\\n此即式(14.32), 等号右边第1 项即式(14.33) 称为Evidence Lower Bound (ELBO), 等号右边第2 项\\n即式(14.34) 为KL 散度（参见附录C.3）。我们的目标是用分布q(z) 去近似后验概率p(z | x), 而KL 散度\\n用于度量两个概率分布之间的差异, 其中KL 散度越小表示两个分布差异越小, 因此可以最小化式(14.34):'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='min\\nq(z) KL(q(z)∥p(z | x))\\n但这并没有什么意义, 因为p(z | x) 末知。注意, 式(14.32) 恒等于常数ln p(x), 因此最小化式(14.34)\\n等价于最大化式(14.33) 的ELBO。在本节接下来的推导中, 就是通过最大化式(14.33) 来求解p(z | x) 的\\n近似q(z) 。\\n14.4.10\\n式(14.35) 的解释\\n在“西瓜书”14.5.2 节开篇提到, “变分推断通过使用已知简单分布来逼近需推断的复杂分布”, 这里\\n我们使用q(z) 去近似后验分布p(z | x) 。而本式进一步假设复杂的多变量z 可拆解为一系列相互独立的\\n多变量zi, 进而有q(z) = QM\\ni=1 qi (zi), 以便于后面简化求解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.11\\n式(14.36) 的推导\\n将式(14.35) 代入式(14.33), 得:\\nL(q) =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz =\\nZ\\nq(z){ln p(x, z) −ln q(z)}dz\\n=\\nZ\\nM\\nY\\ni=1\\nqi (zi)\\n(\\nln p(x, z) −ln\\nM\\nY\\ni=1\\nqi (zi)\\n)\\ndz\\n=\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz −\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln\\nM\\nY\\ni=1\\nqi (zi) dz ≜L1(q) −L2(q)\\n接下来推导中大量使用交换积分号次序, 记积分项为Q(x, z), 则上式可变形为:\\nL(q) =\\nZ\\nQ(x, z)dz =\\nZ\\n· · ·\\nZ\\nQ(x, z)dz1 dz2 · · · dzM\\n根据积分相关知识, 在满足某种条件下, 积分号的次序可以任意交换。\\n对于第1 项L1(q), 交换积分号次序, 得:\\nL1(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz =\\nZ\\nqj\\n(Z'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='L1(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz =\\nZ\\nqj\\n(Z\\nln p(x, z)\\nM\\nY\\ni̸=j\\n(qi (zi) dzi)\\n)\\ndzj\\n令ln ˜p (x, zj) =\\nR\\nln p(x, z) QM\\ni̸=j (qi (zi) dzi) （这里与式(14.37) 略有不同, 具体参见接下来一条的解\\n释), 代入, 得:\\nL1(q) =\\nZ\\nqj ln ˜p (x, zj) dzj\\n对于第2 项L2(q) :\\nL2(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln\\nM\\nY\\ni=1\\nqi (zi) dz =\\nZ\\nM\\nY\\ni=1\\nqi (zi)\\nM\\nX\\ni=1\\nln qi (zi) dz\\n=\\nM\\nX\\ni=1\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln qi (zi) dz =\\nM\\nX\\ni1=1\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qi1 (zi1) dz\\n解释一下第2 行的第2 个等号后的结果, 这是因为课本在这里符号表示并不严谨, 求和变量和连乘'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='解释一下第2 行的第2 个等号后的结果, 这是因为课本在这里符号表示并不严谨, 求和变量和连乘\\n变量不能同时使用i, 这里求和变量和连乘变量分布使用i1 和i2 表示。对于求和号内的积分项，考虑当\\ni1 = j 时:\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qj (zj) dz =\\nZ\\nqj (zj)\\nY\\ni2̸=j\\nqi2 (zi2) ln qj (zj) dz\\n=\\nZ\\nqj (zj) ln qj (zj)\\n(Z Y\\ni2̸=j\\nqi2 (zi2)\\nY\\ni2̸=j\\ndzi2\\n)\\ndzj\\n注意到\\nR Q\\ni2̸=j qi2 (zi2) Q\\ni2̸=j dzi2 = 1, 为了直观说明这个结论, 假设这里只有q1 (z1), q2 (z2) 和\\nq3 (z3), 即:\\nZZZ\\nq1 (z1) q2 (z2) q3 (z3) dz1 dz2 dz3 =\\nZ\\nq1 (z1)\\nZ\\nq2 (z2)\\nZ\\nq3 (z3) dz3 dz2 dz1\\n对于概率分布, 我们有\\nR\\nq1 (z1) dz1 =\\nR\\nq2 (z2) dz2 =\\nR\\nq3 (z3) dz3 = 1, 代入即得。因此:\\nZ'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='R\\nq2 (z2) dz2 =\\nR\\nq3 (z3) dz3 = 1, 代入即得。因此:\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qj (zj) dz =\\nZ\\nqj (zj) ln qj (zj) dzj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n进而第2 项可化简为:\\nL2(q) =\\nM\\nX\\ni1=1\\nZ\\nqi1 (zi1) ln qi1 (zi1) dzi1\\n=\\nZ\\nqj (zj) ln qj (zj) dzj +\\nM\\nX\\ni1̸=j\\nZ\\nqi1 (zi1) ln qi1 (zi1) dzi1\\n由于这里只关注qj （即固定qi̸=j ）, 因此第2 项进一步表示为第j 项加上一个常数:\\nL2(q) =\\nZ\\nqj (zj) ln qj (zj) dzj + const\\n综上所述, 可得式(14.36) 的形式。\\n14.4.12\\n式(14.37) 到式(14.38) 的解释\\n首先解释式(14.38), 该式等号右侧就是式(14.36) 第2 个等号后面花括号中的内容, 之所以这里写成\\n了期望的形式, 这是将Q\\ni̸=j qi 看作为一个概率分布, 则该式表示函数ln p(x, z) 在概率分布Q\\ni̸=j qi 下的\\n期望, 类似于式(14.21) 和式(14.24)。'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='i̸=j qi 下的\\n期望, 类似于式(14.21) 和式(14.24)。\\n然后解释式(14.37), 该式就是一个定义, 即令等号右侧的项为ln ˜p (x, zj), 但该式却包含一个常数项\\nconst, 当然这并没有什么问题, 并不影响式(14.36) 本身。具体来说, 将本项反代回式(14.36) 第二个等号\\n右侧第1 项, 即:\\nZ\\nqj\\n(Z\\nln p(x, z)\\nM\\nY\\ni̸=j\\n(qi (zi) dzi)\\n)\\ndzj =\\nZ\\nqjEi̸=j[ln p(x, z)]dzj\\n=\\nZ\\nqj (ln ˜p (x, zj) −const ) dzj\\n=\\nZ\\nqj ln ˜p (x, zj) dzj −\\nZ\\nqj constd zj\\n=\\nZ\\nqj ln ˜p (x, zj) dzj −const\\n注意, 加或减一个常数const 实际等价, 只需const 定义时添个符号即可。将这个const 与式(14.36)\\n第2 个等号后面的const 合并（注意二者表示不同的值), 即式(14.36) 第3 个等号后面的const。\\n14.4.13\\n式(14.39) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='14.4.13\\n式(14.39) 的解释\\n对于式(14.36), 可继续变形为:\\nL(q) =\\nZ\\nqj ln ˜p (x, zj) dzj −\\nZ\\nqj ln qj dzj + const\\n=\\nZ\\nqj ln ˜p (x, zj)\\nqj\\ndzj + const\\n= −KL (qj∥˜p (x, zj)) + const\\n注意, 在前面关于“式(14.32) 式(14.34) 的推导”中提到, 我们的目标是用分布q(z) 去近似后验概率\\np(z | x), 而KL 散度则用于度量两个概率分布之间的差异, 其中KL 散度越小表示两个分布差异越小, 因\\n此可以最小化式(14.34), 但这并没有什么意义, 因为p(z | x) 末知。又因为式(14.32) 恒等于常数ln p(x),\\n因此最小化式(14.34) 等价于最大化式(14.33)。刚刚又得到式(14.33) 等于−KL (qj∥˜p (x, zj)) + const, 因\\n此最大化式(14.33) 等价于最小化这里的KL 散度, 因此可知当qj = ˜p (x, zj) 时这个KL 散度最小, 即式'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='(14.33) 最大, 也就是分布q(z) 与后验概率p(z | x) 最相似。\\n而根据式(14.37) 有ln ˜p (x, zj) = Ei̸=j[ln p(x, z)]+const, 再结合qj = ˜p (x, zj), 可知ln qj = Ei̸=j[ln p(x, z)]+\\nconst, 即本式。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.4.14\\n式(14.40) 的解释\\n对式(14.39) 两边同时取exp(·) 操作, 得\\nq∗\\nj (zj) = exp (Ei̸=j[ln p(x, z)] + const )\\n= exp (Ei̸=j[ln p(x, z)]) · exp( const )\\n两边同时取积分\\nR\\n(·)dzj 操作, 由于q∗\\nj (zj) 为概率分布, 所以\\nR\\nq∗\\nj (zj) dzj = 1, 因此有\\n1 =\\nZ\\nexp (Ei̸=j[ln p(x, z)]) · exp( const )dzj\\n= exp( const )\\nZ\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n这里就是将常数拿到了积分号外面, 因此:\\nexp( const ) =\\n1\\nR\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n代入刚开始的表达式, 可得本式:\\nq∗\\nj (zj) = exp (Ei̸=j[ln p(x, z)]) · exp( const )\\n=\\nexp (Ei̸=j[ln p(x, z)])\\nR'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='=\\nexp (Ei̸=j[ln p(x, z)])\\nR\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n实际上, 本式的分母为归一化因子, 以保证q∗\\nj (zj) 为概率分布。\\n14.5\\n话题模型\\n本节介绍话题模型的概念及其典型代表：隐狄利克雷分配模型（LDA）。\\n概括来说，给定一组文档，话题模型可以告诉我们这组文档谈论了哪些话题，以及每篇文档与哪些话\\n题有关。举个例子，社会中出现了一个热点事件，为了大致了解网民的思想动态，于是抓取了一组比较典\\n型的网页（博客、评论等）；每个网页就是一篇文档，我们通过分析这组网页，可以大致了解到网民都从什\\n么角度关注这件事情（每个角度可视为一个主题，其中LDA 模型中主题个数需要人工指定），并大致知道\\n每个网页都涉及哪些角度；这里学得的主题类似于聚类（参见第9 章）中所得的簇（没有标记），每个主\\n题最终由一个词频向量表示（即本节），通过分析该主题下的高频词，就可对其有大致的了解。\\n14.5.1\\n式(14.41) 的解释\\np(W , z, β, θ|α, η) =\\nT\\nY\\nt=1\\np(θt|α)\\nK\\nY\\nk=1\\np(βk|η)(\\nN\\nY'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='T\\nY\\nt=1\\np(θt|α)\\nK\\nY\\nk=1\\np(βk|η)(\\nN\\nY\\nn=1\\nP(wt,n|zt,n, βk)P(zt,n|θt))\\n此式表示LDA 模型下根据参数α, η 生成文档W 的概率。其中z, β, θ 是生成过程的中间变量。具体的生\\n成步骤可见概率图14.12，图中的箭头和式14.41 中的条件概率中的因果项目一一对应。这里共有三个连\\n乘符号，表示三个相互独立的概率关系。第一个连乘表示T 个文档每个文档的话题分布都是相互独立的。\\n第二个连乘表示K 个话题每个话题下单词的分布是相互独立的。最后一个连乘号表示每篇文档中的所有\\n单词的生成是相互独立的。\\n14.5.2\\n式(14.42) 的解释\\n本式就是狄利克雷分布的定义式, 参见“西瓜书”附录C1.6。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 189, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n14.5.3\\n式(14.43) 的解释\\n本式为对数似然, 其中p (wt | α, η) =\\nRRR\\np (wt, z, β, Θ | α, η) dzdβdΘ, 即通过边际化p (wt, z, β, Θ | α, η)\\n而得。\\n由于T 篇文档相互独立, 所以p(W, z, β, Θ | α, η) = QT\\nt=1 p (wt, z, β, Θ | α, η), 求对数似然后连乘变\\n为了连加, 即得本式。参见7.2 极大似然估计。\\n14.5.4\\n式(14.44) 的解释\\n本式就是联合概率、先验概率、条件概率之间的关系, 换种表示方法可能更易理解:\\npα,η(z, β, Θ | W) = pα,η(W, z, β, Θ)\\npα,η(W)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第15 章\\n规则学习\\n规则学习是“符号主义学习”的代表性方法，用来从训练数据中学到一组能对未见示例进行判别的规\\n则，形如“如果A 或B，并且C 的条件下，D 满足”这样的形式。因为这种学习方法更加贴合人类从数\\n据中学到经验的描述，具有非常良好的可解释性，是最早开始研究机器学习的技术之一。\\n15.1\\n剪枝优化\\n15.1.1\\n式(15.2) 和式(15.3) 的解释\\n似然率统计量LRS 定义为：\\nLRS = 2 ·\\n\\uf8eb\\n\\uf8edˆm+ log2\\n\\x10\\nˆm+\\nˆm++ ˆm−\\n\\x11\\n\\x10\\nm+\\nm++m−\\n\\x11 + ˆm−log2\\n\\x10\\nˆm−\\nˆm++ ˆm−\\n\\x11\\n\\x10\\nm−\\nm++m−\\n\\x11\\n\\uf8f6\\n\\uf8f8\\n同时，根据对数函数的定义，我们可以对式(15.3) 进行化简：\\nF−Gain = ˆm+ ×\\n\\x12\\nlog2\\nˆm+\\nˆm+ + ˆm−\\n−log2\\nm+\\nm+ + m−\\n\\x13\\n= ˆm+\\n \\nlog2\\nˆm+\\nˆm++ ˆm−\\nm+\\nm++m−\\n!'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='m+\\nm+ + m−\\n\\x13\\n= ˆm+\\n \\nlog2\\nˆm+\\nˆm++ ˆm−\\nm+\\nm++m−\\n!\\n可以观察到F_Gain 即为式(15.2) 中LRS 求和项中的第一项。这里“西瓜书”中做了详细的解释，FOIL\\n仅考虑正例的信息量，由于关系数据中正例数旺旺远少于反例数，因此通常对正例应该赋予更多的关注。\\n15.2\\n归纳逻辑程序设计\\n15.2.1\\n式(15.6) 的解释\\n定义析合范式的删除操作符为“−”，表示在A 和B 的析合式中删除成分B，得到成分A。\\n15.2.2\\n式(15.7) 的推导\\nC = A ∨B，把A = C1 −{L} 和L = C2 −{¬L} 带入即得。\\n15.2.3\\n式(15.9) 的推导\\n根据式(15.7) C = (C1 −{L}) ∨(C2 −{¬L}) 和析合范式的删除操作，等式两边同时删除析合项\\nC2 −{¬L} 有：\\nC −(C1 −{L}) = C2 −{¬L}\\n再次运用析合范式删除操作符的逆定义，等式两边同时加上析合项{¬L} 有：\\nC2 = (C −(C1 −{L})) ∨{¬L}\\n15.2.4\\n式(15.10) 的解释'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='C2 = (C −(C1 −{L})) ∨{¬L}\\n15.2.4\\n式(15.10) 的解释\\n该式是吸收(absorption) 操作的定义。注意作者在文章中所用的符号定义，用X\\nY 表示X 蕴含Y ，X\\n的子句或是Y 的归结项，或是Y 中某个子句的等价项。所谓吸收，是指替换部分逻辑子句（大写字母），\\n生成一个新的逻辑文字（小写字母）用于定义这些被替换的逻辑子句。在式(15.10) 中，逻辑子句A 被逻\\n辑文字q 替换。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 191, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n15.2.5\\n式(15.11) 的解释\\n该式是辨识(identification) 操作的定义。辨识操作依据已知的逻辑文字，构造新的逻辑子句和文字的\\n关系。在式(15.11) 中，已知p ←A ∧B 和p ←A ∧q，构造的新逻辑文字为q ←B。\\n15.2.6\\n式(15.12) 的解释\\n该式是内构(intra-construction) 操作的定义。内构操作找到关于同一逻辑文字中的共同逻辑子句部\\n分，并且提取其中不同的部分作为新的逻辑文字。在式(15.12) 中，逻辑文字p ←A ∧B 和p ←A ∧C 的\\n共同部分为p ←A ∧q，其中新逻辑文字q ←B\\nq ←C。\\n15.2.7\\n式(15.13) 的解释\\n该式是互构(inter-construction) 操作的定义。互构操作找到不同逻辑文字中的共同逻辑子句部分，并\\n定义新的逻辑文字已描述这个共同的逻辑子句。在式(15.13) 中，逻辑文字p ←A ∧B 和q ←A ∧C 的'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 191, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='共同逻辑子句A 提取出来，并用逻辑文字定义为r ←A。逻辑文字p 和q 的定义也用r 做相应的替换得\\n到p ←r ∧B 与q ←r ∧C。\\n15.2.8\\n式(15.16) 的推导\\nθ1 为作者笔误，由15.9\\nC2 = (C −(C1 −{L1})) ∨{L2}\\n因为L2 = (¬L1θ1)θ−1\\n2 ，替换得证。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 192, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\n第16 章\\n强化学习\\n强化学习作为机器学习的子领域，其本身拥有一套完整的理论体系，以及诸多经典和最新前沿算\\n法，“西瓜书”该章内容仅可作为综述查阅，若想深究建议查阅其他相关书籍（例如《Easy RL：强化\\n学习教程》\\n[1]）进行系统性学习。\\n16.1\\n任务与奖赏\\n本节理解强化学习的定义和相关术语的含义即可。\\n16.2\\nK-摇臂赌博机\\n16.2.1\\n式(16.2) 和式(16.3) 的推导\\nQn(k) = 1\\nn\\nn\\nX\\ni=1\\nvi\\n= 1\\nn\\n n−1\\nX\\ni=1\\nvi + vn\\n!\\n= 1\\nn ((n −1) × Qn−1(k) + vn)\\n= Qn−1(k) + 1\\nn (vn −Qn−1(k))\\n16.2.2\\n式(16.4) 的解释\\nP(k) =\\ne\\nQ(k)\\nτ\\nPK\\ni=1 e\\nQ(i)\\nτ\\n∝e\\nQ(k)\\nτ\\n∝Q(k)\\nτ\\n∝1\\nτ\\n如果τ 很大，所有动作几乎以等概率选择（探索）；如果τ 很小，Q 值大的动作更容易被选中（利用）。\\n16.3\\n有模型学习\\n16.3.1\\n式(16.7) 的解释\\n因为'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 192, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='16.3\\n有模型学习\\n16.3.1\\n式(16.7) 的解释\\n因为\\nπ(x, a) = P(action = a|state = x)\\n表示在状态x 下选择动作a 的概率，又因为动作事件之间两两互斥且和为动作空间，由全概率展开公式\\nP(A) =\\n∞\\nX\\ni=1\\nP(Bi)P(A | Bi)\\n可得\\nEπ[ 1\\nT r1 + T −1\\nT\\n1\\nT −1\\nT\\nX\\nt=2\\nrt | x0 = x]\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′( 1\\nT Ra\\nx→x′ + T −1\\nT\\nEπ[\\n1\\nT −1\\nT −1\\nX\\nt=1\\nrt | x0 = x′])\\n其中\\nr1 = π(x, a)P a\\nx→x′Ra\\nx→x′\\n最后一个等式用到了递归形式。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 193, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←\\nBellman 等式定义了当前状态与未来状态之间的关系，表示当前状态的价值函数可以通过下个状态的\\n价值函数来计算。\\n16.3.2\\n式(16.8) 的推导\\nV π\\nγ (x) = Eπ[\\n∞\\nX\\nt=0\\nγtrt+1 | x0 = x]\\n= Eπ[r1 +\\n∞\\nX\\nt=1\\nγtrt+1 | x0 = x]\\n= Eπ[r1 + γ\\n∞\\nX\\nt=1\\nγt−1rt+1 | x0 = x]\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′(Ra\\nx→x′ + γEπ[\\n∞\\nX\\nt=0\\nγtrt+1 | x0 = x′])\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′(Ra\\nx→x′ + γV π\\nγ (x′))\\n16.3.3\\n式(16.10) 的推导\\n参见式(16.7) 和式(16.8) 的推导\\n16.3.4\\n式(16.14) 的解释\\n为了获得最优的状态值函数V ，这里取了两层最优，分别是采用最优策略π∗和选取使得状态动作值\\n函数Q 最大的状态maxa∈A。\\n16.3.5'),\n",
       " Document(metadata={'source': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': './data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 193, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}, page_content='函数Q 最大的状态maxa∈A。\\n16.3.5\\n式(16.15) 的解释\\n最优Bellman 等式表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的\\n累积奖赏值的期望。\\n16.3.6\\n式(16.16) 的推导\\nV π(x) ⩽Qπ (x, π′(x))\\n=\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n\\x10\\nRπ′(x)\\nx→x′ + γV π (x′)\\n\\x11\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n\\x10\\nRπ′(x)\\nx→x′ + γQπ (x′, π′ (x′))\\n\\x11\\n=\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n\\x10\\nγRπ′(x′)\\nx′→x′′ + γ2V π (x′′)\\n\\x11!\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n\\x10\\nγRπ′(x′)\\nx′→x′′ + γ2Qπ (x′′, π′ (x′′))\\n\\x11!\\n⩽· · ·\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX'),\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(texts)\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建chroma向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tmp/ipykernel_36996/1449869134.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from zhipuai_embedding import ZhipuAIEmbeddings\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "embedding = ZhipuAIEmbeddings()\n",
    "#定义持久化路径\n",
    "persist_directory = \"./data_base/vector_db/chroma\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tmp/ipykernel_36996/3696418086.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs[:20], # 为了速度，只选择前 20 个切分的 doc 进行生成；使用千帆时因QPS限制，建议选择前 5 个doc\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n"
     ]
    }
   ],
   "source": [
    "question = \"什么是语言模型\"\n",
    "sim_docs = vectordb.similarity_search(question, k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      "第五章 推断\n",
      "\n",
      "在这一章中，我们将通过一个故事，引领你了解如何从产品评价和新闻文章中推导出情感和主题。\n",
      "\n",
      "让我们先想象一下，你是一名初创公司的数据分析师，你的任务是从各种产品评论和新闻文章中提取出关键的情感和主题。这些任务包括了标签提取、实体提取、以及理解文本的情感等等。在传统的机器学习流程中，你需要收集标签化的数据集、训练模型、确定如何在云端部署模型并进行推断。尽管这种方式可能会产生不错的效果，\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "二、信息提取\n",
      "\n",
      "2.1 商品信息提取\n",
      "\n",
      "信息提取是自然语言处理（NLP）的重要组成部分，它帮助我们从文本中抽取特定的、我们关心的信息。我们将深入挖掘客户评论中的丰富信息。在接下来的示例中，我们将要求模型识别两个关键元素：购买的商品和商品的制造商。\n",
      "\n",
      "想象一下，如果你正在尝试分析一个在线电商网站上的众多评论，了解评论中提到的商品是什么、由谁制造，以及相关的积极或消极情绪，将极大地帮助你追踪特定商品或\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "评论文本: {lamp_review} \"\"\" response = get_completion(prompt) print(response) ```\n",
      "\n",
      "{\n",
      "  \"情感倾向\": \"正面\",\n",
      "  \"是否生气\": false,\n",
      "  \"物品类型\": \"卧室灯\",\n",
      "  \"品牌\": \"Lumina\"\n",
      "}\n",
      "\n",
      "这个例子中，我们指导 LLM 将“是否生气”的情况格式化为布尔值，并输出 JSON 格式。你可以尝\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 检索到的第0个内容: \n",
      "第五章 推断\n",
      "\n",
      "在这一章中，我们将通过一个故事，引领你了解如何从产品评价和新闻文章中推导出情感和主题。\n",
      "\n",
      "让我们先想象一下，你是一名初创公司的数据分析师，你的任务是从各种产品评论和新闻文章中提取出关键的情感和主题。这些任务包括了标签提取、实体提取、以及理解文本的情感等等。在传统的机器学习流程中，你需要收集标签化的数据集、训练模型、确定如何在云端部署模型并进行推断。尽管这种方式可能会产生不错的效果，\n",
      "--------------\n",
      "MMR 检索到的第1个内容: \n",
      "```python prompt = f\"\"\" What is the sentiment of the following product review, which is delimited with triple backticks?\n",
      "\n",
      "Review text: {lamp_review} \"\"\" response = get_completion(prompt) print(respons\n",
      "--------------\n",
      "MMR 检索到的第2个内容: \n",
      "company that cares \\ about their customers and products!! \"\"\"\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,3)\n",
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
